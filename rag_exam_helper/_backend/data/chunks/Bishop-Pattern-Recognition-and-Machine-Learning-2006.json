[
  {
    "page_content": "Information Science and Statistics\nSeries Editors:\nM. Jordan\nJ. Kleinberg\nB. Scho¨lkopf",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 1,
      "page_label": "ii"
    }
  },
  {
    "page_content": "Information Science and Statistics \nAkaike and Kitagawa: The Practice of Time Series Analysis. \nBishop:  Pattern Recognition and Machine Learning. \nCowell, Dawid, Lauritzen, and Spiegelhalter: Probabilistic Networks and\nExpert Systems. \nDoucet, de Freitas, and Gordon: Sequential Monte Carlo Methods in Practice. \nFine: Feedforward Neural Network Methodology. \nHawkins and Olwell: Cumulative Sum Charts and Charting for Quality Improvement. \nJensen: Bayesian Networks and Decision Graphs. \nMarchette: Computer Intrusion Detection and Network Monitoring:\nA Statistical Viewpoint. \nRubinstein and Kroese: The Cross-Entropy Method:  A Unified Approach to \nCombinatorial Optimization, Monte Carlo Simulation, and Machine Learning. \nStudený: Probabilistic Conditional Independence Structures.\nVapnik: The Nature of Statistical Learning Theory, Second Edition.  \nWallace: Statistical and Inductive Inference by Minimum Massage Length.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 2,
      "page_label": "iii"
    }
  },
  {
    "page_content": "Christopher M. Bishop\nPattern Recognition and\nMachine Learning",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 3,
      "page_label": "iv"
    }
  },
  {
    "page_content": "Christopher M. Bishop F.R.Eng.\nAssistant Director\nMicrosoft Research Ltd\nCambridge CB3 0FB, U.K.\ncmbishop@microsoft.com\nhttp://research.microsoft.com//H11011cmbishop\nSeries Editors\nMichael Jordan\nDepartment of Computer\nScience and Department\nof Statistics\nUniversity of California,\nBerkeley\nBerkeley, CA 94720\nUSA\nProfessor Jon Kleinberg\nDepartment of Computer\nScience\nCornell University\nIthaca, NY 14853\nUSA\nBernhard Scho¨lkopf\nMax Planck Institute for\nBiological Cybernetics\nSpemannstrasse 38\n72076 Tu¨bingen\nGermany\nLibrary of Congress Control Number: 2006922522\nISBN-10: 0-387-31073-8\nISBN-13: 978-0387-31073-2\nPrinted on acid-free paper.\n© 2006 Springer Science+Business Media, LLC\nAll rights reserved. This work may not be translated or copied in whole or in part without the written permission of the publisher\n(Springer Science+Business Media, LLC, 233 Spring Street, New York, NY 10013, USA), except for brief excerpts in connection\nwith reviews or scholarly analysis. Use in connection with any form of information storage and retrieval, electronic adaptation,\ncomputer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such,\nis not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.\nPrinted in Singapore. (KYO)\n987654321\nspringer.com",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 4,
      "page_label": "v"
    }
  },
  {
    "page_content": "This book is dedicated to my family:\nJenna, Mark, and Hugh\nTotal eclipse of the sun, Antalya, Turkey, 29 March 2006.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 5,
      "page_label": "vi"
    }
  },
  {
    "page_content": "Preface\nPattern recognition has its origins in engineering, whereas machine learning grew\nout of computer science. However, these activities can be viewed as two facets of\nthe same ﬁeld, and together they have undergone substantial development over the\npast ten years. In particular, Bayesian methods have grown from a specialist niche to\nbecome mainstream, while graphical models have emerged as a general framework\nfor describing and applying probabilistic models. Also, the practical applicability of\nBayesian methods has been greatly enhanced through the development of a range of\napproximate inference algorithms such as variational Bayes and expectation propa-\ngation. Similarly, new models based on kernels have had signiﬁcant impact on both\nalgorithms and applications.\nThis new textbook reﬂects these recent developments while providing a compre-\nhensive introduction to the ﬁelds of pattern recognition and machine learning. It is\naimed at advanced undergraduates or ﬁrst year PhD students, as well as researchers\nand practitioners, and assumes no previous knowledge of pattern recognition or ma-\nchine learning concepts. Knowledge of multivariate calculus and basic linear algebra\nis required, and some familiarity with probabilities would be helpful though not es-\nsential as the book includes a self-contained introduction to basic probability theory.\nBecause this book has broad scope, it is impossible to provide a complete list of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 6,
      "page_label": "vii"
    }
  },
  {
    "page_content": "Because this book has broad scope, it is impossible to provide a complete list of\nreferences, and in particular no attempt has been made to provide accurate historical\nattribution of ideas. Instead, the aim has been to give references that offer greater\ndetail than is possible here and that hopefully provide entry points into what, in some\ncases, is a very extensive literature. For this reason, the references are often to more\nrecent textbooks and review articles rather than to original sources.\nThe book is supported by a great deal of additional material, including lecture\nslides as well as the complete set of ﬁgures used in the book, and the reader is\nencouraged to visit the book web site for the latest information:\nhttp://research.microsoft.com/∼cmbishop/PRML\nvii",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 6,
      "page_label": "vii"
    }
  },
  {
    "page_content": "viii PREFACE\nExercises\nThe exercises that appear at the end of every chapter form an important com-\nponent of the book. Each exercise has been carefully chosen to reinforce concepts\nexplained in the text or to develop and generalize them in signiﬁcant ways, and each\nis graded according to difﬁculty ranging from(⋆), which denotes a simple exercise\ntaking a few minutes to complete, through to(⋆⋆⋆ ), which denotes a signiﬁcantly\nmore complex exercise.\nIt has been difﬁcult to know to what extent these solutions should be made\nwidely available. Those engaged in self study will ﬁnd worked solutions very ben-\neﬁcial, whereas many course tutors request that solutions be available only via the\npublisher so that the exercises may be used in class. In order to try to meet these\nconﬂicting requirements, those exercises that help amplify key points in the text, or\nthat ﬁll in important details, have solutions that are available as a PDF ﬁle from the\nbook web site. Such exercises are denoted bywww . Solutions for the remaining\nexercises are available to course tutors by contacting the publisher (contact details\nare given on the book web site). Readers are strongly encouraged to work through\nthe exercises unaided, and to turn to the solutions only as required.\nAlthough this book focuses on concepts and principles, in a taught course the\nstudents should ideally have the opportunity to experiment with some of the key",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 7,
      "page_label": "viii"
    }
  },
  {
    "page_content": "students should ideally have the opportunity to experiment with some of the key\nalgorithms using appropriate data sets. A companion volume (Bishop and Nabney,\n2008) will deal with practical aspects of pattern recognition and machine learning,\nand will be accompanied by Matlab software implementing most of the algorithms\ndiscussed in this book.\nAcknowledgements\nFirst of all I would like to express my sincere thanks to Markus Svens´en who\nhas provided immense help with preparation of ﬁgures and with the typesetting of\nthe book in LATEX. His assistance has been invaluable.\nI am very grateful to Microsoft Research for providing a highly stimulating re-\nsearch environment and for giving me the freedom to write this book (the views and\nopinions expressed in this book, however, are my own and are therefore not neces-\nsarily the same as those of Microsoft or its afﬁliates).\nSpringer has provided excellent support throughout the ﬁnal stages of prepara-\ntion of this book, and I would like to thank my commissioning editor John Kimmel\nfor his support and professionalism, as well as Joseph Piliero for his help in design-\ning the cover and the text format and MaryAnn Brickner for her numerous contribu-\ntions during the production phase. The inspiration for the cover design came from a\ndiscussion with Antonio Criminisi.\nI also wish to thank Oxford University Press for permission to reproduce ex-\ncerpts from an earlier textbook,Neural Networks for Pattern Recognition(Bishop,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 7,
      "page_label": "viii"
    }
  },
  {
    "page_content": "cerpts from an earlier textbook,Neural Networks for Pattern Recognition(Bishop,\n1995a). The images of the Mark 1 perceptron and of Frank Rosenblatt are repro-\nduced with the permission of Arvin Calspan Advanced Technology Center. I would\nalso like to thank Asela Gunawardana for plotting the spectrogram in Figure 13.1,\nand Bernhard Sch¨olkopf for permission to use his kernel PCA code to plot Fig-\nure 12.17.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 7,
      "page_label": "viii"
    }
  },
  {
    "page_content": "PREFACE ix\nMany people have helped by proofreading draft material and providing com-\nments and suggestions, including Shivani Agarwal, C´edric Archambeau, Arik Azran,\nAndrew Blake, Hakan Cevikalp, Michael Fourman, Brendan Frey, Zoubin Ghahra-\nmani, Thore Graepel, Katherine Heller, Ralf Herbrich, Geoffrey Hinton, Adam Jo-\nhansen, Matthew Johnson, Michael Jordan, Eva Kalyvianaki, Anitha Kannan, Julia\nLasserre, David Liu, Tom Minka, Ian Nabney, Tonatiuh Pena, Y uan Qi, Sam Roweis,\nBalaji Sanjiya, Toby Sharp, Ana Costa e Silva, David Spiegelhalter, Jay Stokes, Tara\nSymeonides, Martin Szummer, Marshall Tappen, Ilkay Ulusoy, Chris Williams, John\nWinn, and Andrew Zisserman.\nFinally, I would like to thank my wife Jenna who has been hugely supportive\nthroughout the several years it has taken to write this book.\nChris Bishop\nCambridge\nFebruary 2006",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 8,
      "page_label": "ix"
    }
  },
  {
    "page_content": "Mathematical notation\nI have tried to keep the mathematical content of the book to the minimum neces-\nsary to achieve a proper understanding of the ﬁeld. However, this minimum level is\nnonzero, and it should be emphasized that a good grasp of calculus, linear algebra,\nand probability theory is essential for a clear understanding of modern pattern recog-\nnition and machine learning techniques. Nevertheless, the emphasis in this book is\non conveying the underlying concepts rather than on mathematical rigour.\nI have tried to use a consistent notation throughout the book, although at times\nthis means departing from some of the conventions used in the corresponding re-\nsearch literature. V ectors are denoted by lower case bold Roman letters such as\nx, and all vectors are assumed to be column vectors. A superscriptT denotes the\ntranspose of a matrix or vector, so thatxT will be a row vector. Uppercase bold\nroman letters, such as M, denote matrices. The notation (w1,...,w M ) denotes a\nrow vector with M elements, while the corresponding column vector is written as\nw =( w1,...,w M )T.\nThe notation [a, b] is used to denote theclosed interval from a to b, that is the\ninterval including the valuesa and b themselves, while(a, b) denotes the correspond-\ning open interval, that is the interval excludinga and b. Similarly, [a, b) denotes an\ninterval that includes a but excludes b. For the most part, however, there will be",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 10,
      "page_label": "xi"
    }
  },
  {
    "page_content": "interval that includes a but excludes b. For the most part, however, there will be\nlittle need to dwell on such reﬁnements as whether the end points of an interval are\nincluded or not.\nThe M × M identity matrix (also known as the unit matrix) is denoted IM ,\nwhich will be abbreviated toI where there is no ambiguity about it dimensionality.\nIt has elementsIij that equal 1 if i = j and 0 if i ̸=j.\nA functional is denoted f[y] where y(x) is some function. The concept of a\nfunctional is discussed in Appendix D.\nThe notation g(x)= O(f(x)) denotes that |f(x)/g(x)| is bounded asx →∞ .\nFor instance ifg(x)=3 x2 +2 , then g(x)= O(x2).\nThe expectation of a functionf(x, y) with respect to a random variablex is de-\nnoted by Ex[f(x, y)]. In situations where there is no ambiguity as to which variable\nis being averaged over, this will be simpliﬁed by omitting the sufﬁx, for instance\nxi",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 10,
      "page_label": "xi"
    }
  },
  {
    "page_content": "xii MATHEMATICAL NOTATION\nE[x]. If the distribution of x is conditioned on another variablez, then the corre-\nsponding conditional expectation will be writtenEx[f(x)|z]. Similarly, the variance\nis denotedvar[f(x)], and for vector variables the covariance is writtencov[x,y].W e\nshall also usecov[x] as a shorthand notation forcov[x,x]. The concepts of expecta-\ntions and covariances are introduced in Section 1.2.2.\nIf we haveN values x1,..., xN of aD-dimensional vectorx =( x1,...,x D)T,\nwe can combine the observations into a data matrixX in which the nth row of X\ncorresponds to the row vectorxT\nn. Thus the n, i element of X corresponds to the\nith element of thenth observation xn. For the case of one-dimensional variables we\nshall denote such a matrix byx, which is a column vector whosenth element is xn.\nNote that x (which has dimensionalityN) uses a different typeface to distinguish it\nfrom x (which has dimensionalityD).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 11,
      "page_label": "xii"
    }
  },
  {
    "page_content": "Contents\nPreface vii\nMathematical notation xi\nContents xiii\n1 Introduction 1\n1.1 Example: Polynomial Curve Fitting . . . . ............. 4\n1.2 Probability Theory . . ........................ 1 2\n1.2.1 Probability densities . .................... 1 7\n1.2.2 Expectations and covariances ................ 1 9\n1.2.3 Bayesian probabilities .................... 2 1\n1.2.4 The Gaussian distribution . . . . . ............. 2 4\n1.2.5 Curve ﬁtting re-visited .................... 2 8\n1.2.6 Bayesian curve ﬁtting .................... 3 0\n1.3 Model Selection . . . ........................ 3 2\n1.4 The Curse of Dimensionality . .................... 3 3\n1.5 Decision Theory . . . ........................ 3 8\n1.5.1 Minimizing the misclassiﬁcation rate . . . ......... 3 9\n1.5.2 Minimizing the expected loss . . .............. 4 1\n1.5.3 The reject option . . . .................... 4 2\n1.5.4 Inference and decision .................... 4 2\n1.5.5 Loss functions for regression . . . . ............. 4 6\n1.6 Information Theory . . ........................ 4 8\n1.6.1 Relative entropy and mutual information . ......... 5 5\nExercises . . . ............................... 5 8\nxiii",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 12,
      "page_label": "xiii"
    }
  },
  {
    "page_content": "xiv CONTENTS\n2 Probability Distributions 67\n2.1 Binary V ariables . . . ........................ 6 8\n2.1.1 The beta distribution . .................... 7 1\n2.2 Multinomial V ariables ........................ 7 4\n2.2.1 The Dirichlet distribution . . . . . . ............. 7 6\n2.3 The Gaussian Distribution . . .................... 7 8\n2.3.1 Conditional Gaussian distributions . ............. 8 5\n2.3.2 Marginal Gaussian distributions . . ............. 8 8\n2.3.3 Bayes’ theorem for Gaussian variables . . . ......... 9 0\n2.3.4 Maximum likelihood for the Gaussian . . . ......... 9 3\n2.3.5 Sequential estimation . .................... 9 4\n2.3.6 Bayesian inference for the Gaussian ............. 9 7\n2.3.7 Student’s t-distribution .................... 1 0 2\n2.3.8 Periodic variables . . . .................... 1 0 5\n2.3.9 Mixtures of Gaussians .................... 1 1 0\n2.4 The Exponential Family . . . .................... 1 1 3\n2.4.1 Maximum likelihood and sufﬁcient statistics . . . . . . . . 116\n2.4.2 Conjugate priors . . . .................... 1 1 7\n2.4.3 Noninformative priors .................... 1 1 7\n2.5 Nonparametric Methods . . . .................... 1 2 0\n2.5.1 Kernel density estimators . . . . . . ............. 1 2 2\n2.5.2 Nearest-neighbour methods . . . . ............. 1 2 4\nExercises . . . ............................... 1 2 7\n3 Linear Models for Regression 137\n3.1 Linear Basis Function Models .................... 1 3 8",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 13,
      "page_label": "xiv"
    }
  },
  {
    "page_content": "Exercises . . . ............................... 1 2 7\n3 Linear Models for Regression 137\n3.1 Linear Basis Function Models .................... 1 3 8\n3.1.1 Maximum likelihood and least squares . . . ......... 1 4 0\n3.1.2 Geometry of least squares . . . . . ............. 1 4 3\n3.1.3 Sequential learning . . .................... 1 4 3\n3.1.4 Regularized least squares . . . . . . ............. 1 4 4\n3.1.5 Multiple outputs . . . .................... 1 4 6\n3.2 The Bias-V ariance Decomposition . . . . . ............. 1 4 7\n3.3 Bayesian Linear Regression . .................... 1 5 2\n3.3.1 Parameter distribution .................... 1 5 2\n3.3.2 Predictive distribution .................... 1 5 6\n3.3.3 Equivalent kernel . . . .................... 1 5 9\n3.4 Bayesian Model Comparison . .................... 1 6 1\n3.5 The Evidence Approximation .................... 1 6 5\n3.5.1 Evaluation of the evidence function ............. 1 6 6\n3.5.2 Maximizing the evidence function . ............. 1 6 8\n3.5.3 Effective number of parameters . . ............. 1 7 0\n3.6 Limitations of Fixed Basis Functions . . . ............. 1 7 2\nExercises . . . ............................... 1 7 3",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 13,
      "page_label": "xiv"
    }
  },
  {
    "page_content": "CONTENTS xv\n4 Linear Models for Classiﬁcation 179\n4.1 Discriminant Functions ........................ 1 8 1\n4.1.1 Two classes . . ........................ 1 8 1\n4.1.2 Multiple classes ........................ 1 8 2\n4.1.3 Least squares for classiﬁcation ................ 1 8 4\n4.1.4 Fisher’s linear discriminant . . . . . ............. 1 8 6\n4.1.5 Relation to least squares . . . . . . ............. 1 8 9\n4.1.6 Fisher’s discriminant for multiple classes . ......... 1 9 1\n4.1.7 The perceptron algorithm . . . . . . ............. 1 9 2\n4.2 Probabilistic Generative Models . . . ................ 1 9 6\n4.2.1 Continuous inputs . . .................... 1 9 8\n4.2.2 Maximum likelihood solution . . . ............. 2 0 0\n4.2.3 Discrete features . . . .................... 2 0 2\n4.2.4 Exponential family . . .................... 2 0 2\n4.3 Probabilistic Discriminative Models . . . . ............. 2 0 3\n4.3.1 Fixed basis functions . .................... 2 0 4\n4.3.2 Logistic regression . . .................... 2 0 5\n4.3.3 Iterative reweighted least squares .............. 2 0 7\n4.3.4 Multiclass logistic regression . . . .............. 2 0 9\n4.3.5 Probit regression . . . .................... 2 1 0\n4.3.6 Canonical link functions . . . . . . ............. 2 1 2\n4.4 The Laplace Approximation . .................... 2 1 3\n4.4.1 Model comparison and BIC . . . .............. 2 1 6\n4.5 Bayesian Logistic Regression .................... 2 1 7\n4.5.1 Laplace approximation .................... 2 1 7",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 14,
      "page_label": "xv"
    }
  },
  {
    "page_content": "4.5 Bayesian Logistic Regression .................... 2 1 7\n4.5.1 Laplace approximation .................... 2 1 7\n4.5.2 Predictive distribution .................... 2 1 8\nExercises . . . ............................... 2 2 0\n5 Neural Networks 225\n5.1 Feed-forward Network Functions . . . . . ............. 2 2 7\n5.1.1 Weight-space symmetries . . . . .............. 2 3 1\n5.2 Network Training . . . ........................ 2 3 2\n5.2.1 Parameter optimization .................... 2 3 6\n5.2.2 Local quadratic approximation . . . ............. 2 3 7\n5.2.3 Use of gradient information . . . .............. 2 3 9\n5.2.4 Gradient descent optimization . . . ............. 2 4 0\n5.3 Error Backpropagation ........................ 2 4 1\n5.3.1 Evaluation of error-function derivatives . . ......... 2 4 2\n5.3.2 A simple example ...................... 2 4 5\n5.3.3 Efﬁciency of backpropagation . . . ............. 2 4 6\n5.3.4 The Jacobian matrix . .................... 2 4 7\n5.4 The Hessian Matrix . . ........................ 2 4 9\n5.4.1 Diagonal approximation . . . . . .............. 2 5 0\n5.4.2 Outer product approximation . ................ 2 5 1\n5.4.3 Inverse Hessian ........................ 2 5 2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 14,
      "page_label": "xv"
    }
  },
  {
    "page_content": "xvi CONTENTS\n5.4.4 Finite difference s....................... 2 5 2\n5.4.5 Exact evaluation of the Hessian . . ............. 2 5 3\n5.4.6 Fast multiplication by the Hessian . ............. 2 5 4\n5.5 Regularization in Neural Networks . . . . ............. 2 5 6\n5.5.1 Consistent Gaussian priors . . . . .............. 2 5 7\n5.5.2 Early stopping ........................ 2 5 9\n5.5.3 Invariances . . ........................ 2 6 1\n5.5.4 Tangent propagation . .................... 2 6 3\n5.5.5 Training with transformed data . . . ............. 2 6 5\n5.5.6 Convolutional networks . . . . . . ............. 2 6 7\n5.5.7 Soft weight sharing . . .................... 2 6 9\n5.6 Mixture Density Networks . . .................... 2 7 2\n5.7 Bayesian Neural Networks . . .................... 2 7 7\n5.7.1 Posterior parameter distribution . . ............. 2 7 8\n5.7.2 Hyperparameter optimization . . . ............. 2 8 0\n5.7.3 Bayesian neural networks for classiﬁcation ......... 2 8 1\nExercises . . . ............................... 2 8 4\n6 Kernel Methods 291\n6.1 Dual Representations . ........................ 2 9 3\n6.2 Constructing Kernels . ........................ 2 9 4\n6.3 Radial Basis Function Networks . . . ................ 2 9 9\n6.3.1 Nadaraya-Watson model . . . . . . ............. 3 0 1\n6.4 Gaussian Processes . . ........................ 3 0 3\n6.4.1 Linear regression revisited . . . . . ............. 3 0 4\n6.4.2 Gaussian processes for regression . ............. 3 0 6",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 15,
      "page_label": "xvi"
    }
  },
  {
    "page_content": "6.4.1 Linear regression revisited . . . . . ............. 3 0 4\n6.4.2 Gaussian processes for regression . ............. 3 0 6\n6.4.3 Learning the hyperparameters . . . ............. 3 1 1\n6.4.4 Automatic relevance determination ............. 3 1 2\n6.4.5 Gaussian processes for classiﬁcation ............. 3 1 3\n6.4.6 Laplace approximation .................... 3 1 5\n6.4.7 Connection to neural networks . . . ............. 3 1 9\nExercises . . . ............................... 3 2 0\n7 Sparse Kernel Machines 325\n7.1 Maximum Margin Classiﬁers .................... 3 2 6\n7.1.1 Overlapping class distributions . . .............. 3 3 1\n7.1.2 Relation to logistic regression . . .............. 3 3 6\n7.1.3 Multiclass SVMs . . . .................... 3 3 8\n7.1.4 SVMs for regression . .................... 3 3 9\n7.1.5 Computational learning theory . . . ............. 3 4 4\n7.2 Relevance V ector Machines . .................... 3 4 5\n7.2.1 RVM for regression . . .................... 3 4 5\n7.2.2 Analysis of sparsity . . .................... 3 4 9\n7.2.3 RVM for classiﬁcation .................... 3 5 3\nExercises . . . ............................... 3 5 7",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 15,
      "page_label": "xvi"
    }
  },
  {
    "page_content": "CONTENTS xvii\n8 Graphical Models 359\n8.1 Bayesian Networks . . ........................ 3 6 0\n8.1.1 Example: Polynomial regression . . ............. 3 6 2\n8.1.2 Generative models . . .................... 3 6 5\n8.1.3 Discrete variables . . . .................... 3 6 6\n8.1.4 Linear-Gaussian models . . . . . . ............. 3 7 0\n8.2 Conditional Independence . . .................... 3 7 2\n8.2.1 Three example graphs .................... 3 7 3\n8.2.2 D-separation . ........................ 3 7 8\n8.3 Markov Random Fields . . . .................... 3 8 3\n8.3.1 Conditional independence properties ............. 3 8 3\n8.3.2 Factorization properties . . . . . . ............. 3 8 4\n8.3.3 Illustration: Image de-noising . . . ............. 3 8 7\n8.3.4 Relation to directed graphs . . . . . ............. 3 9 0\n8.4 Inference in Graphical Models .................... 3 9 3\n8.4.1 Inference on a chain . .................... 3 9 4\n8.4.2 Trees ............................. 3 9 8\n8.4.3 Factor graphs . ........................ 3 9 9\n8.4.4 The sum-product algorithm . . . . .............. 4 0 2\n8.4.5 The max-sum algorithm . . . . . . ............. 4 1 1\n8.4.6 Exact inference in general graphs . ............. 4 1 6\n8.4.7 Loopy belief propagation . . . . . . ............. 4 1 7\n8.4.8 Learning the graph structure . . . .............. 4 1 8\nExercises . . . ............................... 4 1 8\n9 Mixture Models and EM 423\n9.1 K-means Clustering . ........................ 4 2 4",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 16,
      "page_label": "xvii"
    }
  },
  {
    "page_content": "Exercises . . . ............................... 4 1 8\n9 Mixture Models and EM 423\n9.1 K-means Clustering . ........................ 4 2 4\n9.1.1 Image segmentation and compression . . . ......... 4 2 8\n9.2 Mixtures of Gaussians ........................ 4 3 0\n9.2.1 Maximum likelihood . .................... 4 3 2\n9.2.2 EM for Gaussian mixtures . . . . . ............. 4 3 5\n9.3 An Alternative View of EM . .................... 4 3 9\n9.3.1 Gaussian mixtures revisited . . . .............. 4 4 1\n9.3.2 Relation to K-means . .................... 4 4 3\n9.3.3 Mixtures of Bernoulli distributions . ............. 4 4 4\n9.3.4 EM for Bayesian linear regression . ............. 4 4 8\n9.4 The EM Algorithm in General .................... 4 5 0\nExercises . . . ............................... 4 5 5\n10 Approximate Inference 461\n10.1 V ariational Inference . ........................ 4 6 2\n10.1.1 Factorized distributions .................... 4 6 4\n10.1.2 Properties of factorized approximations . . ......... 4 6 6\n10.1.3 Example: The univariate Gaussian . ............. 4 7 0\n10.1.4 Model comparison . . .................... 4 7 3\n10.2 Illustration: V ariational Mixture of Gaussians . . . ......... 4 7 4",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 16,
      "page_label": "xvii"
    }
  },
  {
    "page_content": "xviii CONTENTS\n10.2.1 V ariational distribution .................... 4 7 5\n10.2.2 V ariational lower bound . . . . . . ............. 4 8 1\n10.2.3 Predictive density . . . .................... 4 8 2\n10.2.4 Determining the number of components . . ......... 4 8 3\n10.2.5 Induced factorizations .................... 4 8 5\n10.3 V ariational Linear Regression .................... 4 8 6\n10.3.1 V ariational distribution .................... 4 8 6\n10.3.2 Predictive distribution .................... 4 8 8\n10.3.3 Lower bound . ........................ 4 8 9\n10.4 Exponential Family Distributions . . . . . ............. 4 9 0\n10.4.1 V ariational message passing . . . .............. 4 9 1\n10.5 Local V ariational Methods . . .................... 4 9 3\n10.6 V ariational Logistic Regression . . . ................ 4 9 8\n10.6.1 V ariational posterior distribution . . ............. 4 9 8\n10.6.2 Optimizing the variational parameters . . . ......... 5 0 0\n10.6.3 Inference of hyperparameters ................ 5 0 2\n10.7 Expectation Propagation . . . .................... 5 0 5\n10.7.1 Example: The clutter problem . . . ............. 5 1 1\n10.7.2 Expectation propagation on graphs . ............. 5 1 3\nExercises . . . ............................... 5 1 7\n11 Sampling Methods 523\n11.1 Basic Sampling Algorithms . .................... 5 2 6\n11.1.1 Standard distributions .................... 5 2 6\n11.1.2 Rejection sampling . . .................... 5 2 8",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 17,
      "page_label": "xviii"
    }
  },
  {
    "page_content": "11.1.1 Standard distributions .................... 5 2 6\n11.1.2 Rejection sampling . . .................... 5 2 8\n11.1.3 Adaptive rejection sampling . . . . ............. 5 3 0\n11.1.4 Importance sampling . .................... 5 3 2\n11.1.5 Sampling-importance-resampling . ............. 5 3 4\n11.1.6 Sampling and the EM algorithm . . ............. 5 3 6\n11.2 Markov Chain Monte Carlo . .................... 5 3 7\n11.2.1 Markov chains ........................ 5 3 9\n11.2.2 The Metropolis-Hastings algorithm ............. 5 4 1\n11.3 Gibbs Sampling . . . ........................ 5 4 2\n11.4 Slice Sampling . . . . ........................ 5 4 6\n11.5 The Hybrid Monte Carlo Algorithm . . . . ............. 5 4 8\n11.5.1 Dynamical systems . . .................... 5 4 8\n11.5.2 Hybrid Monte Carlo . .................... 5 5 2\n11.6 Estimating the Partition Function . . . . . ............. 5 5 4\nExercises . . . ............................... 5 5 6\n12 Continuous Latent Variables 559\n12.1 Principal Component Analysis .................... 5 6 1\n12.1.1 Maximum variance formulation . . ............. 5 6 1\n12.1.2 Minimum-error formulation . . . . ............. 5 6 3\n12.1.3 Applications of PCA . .................... 5 6 5\n12.1.4 PCA for high-dimensional data . . ............. 5 6 9",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 17,
      "page_label": "xviii"
    }
  },
  {
    "page_content": "CONTENTS xix\n12.2 Probabilistic PCA . . ........................ 5 7 0\n12.2.1 Maximum likelihood PCA . . . . . ............. 5 7 4\n12.2.2 EM algorithm for PCA .................... 5 7 7\n12.2.3 Bayesian PCA ........................ 5 8 0\n12.2.4 Factor analysis ........................ 5 8 3\n12.3 Kernel PCA .............................. 5 8 6\n12.4 Nonlinear Latent V ariable Models . . . . . ............. 5 9 1\n12.4.1 Independent component analysis . . ............. 5 9 1\n12.4.2 Autoassociative neural networks . . ............. 5 9 2\n12.4.3 Modelling nonlinear manifolds . . . ............. 5 9 5\nExercises . . . ............................... 5 9 9\n13 Sequential Data 605\n13.1 Markov Models . . . . ........................ 6 0 7\n13.2 Hidden Markov Models . . . .................... 6 1 0\n13.2.1 Maximum likelihood for the HMM ............. 6 1 5\n13.2.2 The forward-backward algorithm . ............. 6 1 8\n13.2.3 The sum-product algorithm for the HMM . ......... 6 2 5\n13.2.4 Scaling factors ........................ 6 2 7\n13.2.5 The Viterbi algorithm . .................... 6 2 9\n13.2.6 Extensions of the hidden Markov model . . ......... 6 3 1\n13.3 Linear Dynamical Systems . . .................... 6 3 5\n13.3.1 Inference in LDS . . . .................... 6 3 8\n13.3.2 Learning in LDS . . . .................... 6 4 2\n13.3.3 Extensions of LDS . . .................... 6 4 4\n13.3.4 Particle ﬁlters . ........................ 6 4 5\nExercises . . . ............................... 6 4 6",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 18,
      "page_label": "xix"
    }
  },
  {
    "page_content": "13.3.4 Particle ﬁlters . ........................ 6 4 5\nExercises . . . ............................... 6 4 6\n14 Combining Models 653\n14.1 Bayesian Model Averaging . . .................... 6 5 4\n14.2 Committees . . . . . . ........................ 6 5 5\n14.3 Boosting ............................... 6 5 7\n14.3.1 Minimizing exponential error . . . ............. 6 5 9\n14.3.2 Error functions for boosting . . . .............. 6 6 1\n14.4 Tree-based Models . . ........................ 6 6 3\n14.5 Conditional Mixture Models . .................... 6 6 6\n14.5.1 Mixtures of linear regression models............. 6 6 7\n14.5.2 Mixtures of logistic models . . . . ............. 6 7 0\n14.5.3 Mixtures of experts . . .................... 6 7 2\nExercises . . . ............................... 6 7 4\nAppendix A Data Sets 677\nAppendix B Probability Distributions 685\nAppendix C Properties of Matrices 695",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 18,
      "page_label": "xix"
    }
  },
  {
    "page_content": "xx CONTENTS\nAppendix D Calculus of Variations 703\nAppendix E Lagrange Multipliers 707\nReferences 711\nIndex 729",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 19,
      "page_label": "xx"
    }
  },
  {
    "page_content": "1\nIntroduction\nThe problem of searching for patterns in data is a fundamental one and has a long and\nsuccessful history. For instance, the extensive astronomical observations of Tycho\nBrahe in the 16th century allowed Johannes Kepler to discover the empirical laws of\nplanetary motion, which in turn provided a springboard for the development of clas-\nsical mechanics. Similarly, the discovery of regularities in atomic spectra played a\nkey role in the development and veriﬁcation of quantum physics in the early twenti-\neth century. The ﬁeld of pattern recognition is concerned with the automatic discov-\nery of regularities in data through the use of computer algorithms and with the use of\nthese regularities to take actions such as classifying the data into different categories.\nConsider the example of recognizing handwritten digits, illustrated in Figure 1.1.\nEach digit corresponds to a28×28 pixel image and so can be represented by a vector\nx comprising 784 real numbers. The goal is to build a machine that will take such a\nvector x as input and that will produce the identity of the digit0,..., 9 as the output.\nThis is a nontrivial problem due to the wide variability of handwriting. It could be\n1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 20,
      "page_label": "1"
    }
  },
  {
    "page_content": "2 1. INTRODUCTION\nFigure 1.1 Examples of hand-written dig-\nits taken from US zip codes.\ntackled using handcrafted rules or heuristics for distinguishing the digits based on\nthe shapes of the strokes, but in practice such an approach leads to a proliferation of\nrules and of exceptions to the rules and so on, and invariably gives poor results.\nFar better results can be obtained by adopting a machine learning approach in\nwhich a large set ofN digits {x1,..., xN } called a training set is used to tune the\nparameters of an adaptive model. The categories of the digits in the training set\nare known in advance, typically by inspecting them individually and hand-labelling\nthem. We can express the category of a digit usingtarget vector t, which represents\nthe identity of the corresponding digit. Suitable techniques for representing cate-\ngories in terms of vectors will be discussed later. Note that there is one such target\nvector t for each digit imagex.\nThe result of running the machine learning algorithm can be expressed as a\nfunction y(x) which takes a new digit imagex as input and that generates an output\nvector y, encoded in the same way as the target vectors. The precise form of the\nfunction y(x) is determined during thetraining phase, also known as thelearning\nphase, on the basis of the training data. Once the model is trained it can then de-\ntermine the identity of new digit images, which are said to comprise atest set. The",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 21,
      "page_label": "2"
    }
  },
  {
    "page_content": "termine the identity of new digit images, which are said to comprise atest set. The\nability to categorize correctly new examples that differ from those used for train-\ning is known asgeneralization. In practical applications, the variability of the input\nvectors will be such that the training data can comprise only a tiny fraction of all\npossible input vectors, and so generalization is a central goal in pattern recognition.\nFor most practical applications, the original input variables are typicallyprepro-\ncessed to transform them into some new space of variables where, it is hoped, the\npattern recognition problem will be easier to solve. For instance, in the digit recogni-\ntion problem, the images of the digits are typically translated and scaled so that each\ndigit is contained within a box of a ﬁxed size. This greatly reduces the variability\nwithin each digit class, because the location and scale of all the digits are now the\nsame, which makes it much easier for a subsequent pattern recognition algorithm\nto distinguish between the different classes. This pre-processing stage is sometimes\nalso calledfeature extraction. Note that new test data must be pre-processed using\nthe same steps as the training data.\nPre-processing might also be performed in order to speed up computation. For\nexample, if the goal is real-time face detection in a high-resolution video stream,\nthe computer must handle huge numbers of pixels per second, and presenting these",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 21,
      "page_label": "2"
    }
  },
  {
    "page_content": "the computer must handle huge numbers of pixels per second, and presenting these\ndirectly to a complex pattern recognition algorithm may be computationally infeasi-\nble. Instead, the aim is to ﬁnd useful features that are fast to compute, and yet that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 21,
      "page_label": "2"
    }
  },
  {
    "page_content": "1. INTRODUCTION 3\nalso preserve useful discriminatory information enabling faces to be distinguished\nfrom non-faces. These features are then used as the inputs to the pattern recognition\nalgorithm. For instance, the average value of the image intensity over a rectangular\nsubregion can be evaluated extremely efﬁciently (Viola and Jones, 2004), and a set of\nsuch features can prove very effective in fast face detection. Because the number of\nsuch features is smaller than the number of pixels, this kind of pre-processing repre-\nsents a form of dimensionality reduction. Care must be taken during pre-processing\nbecause often information is discarded, and if this information is important to the\nsolution of the problem then the overall accuracy of the system can suffer.\nApplications in which the training data comprises examples of the input vectors\nalong with their corresponding target vectors are known assupervised learningprob-\nlems. Cases such as the digit recognition example, in which the aim is to assign each\ninput vector to one of a ﬁnite number of discrete categories, are calledclassiﬁcation\nproblems. If the desired output consists of one or more continuous variables, then\nthe task is calledregression. An example of a regression problem would be the pre-\ndiction of the yield in a chemical manufacturing process in which the inputs consist\nof the concentrations of reactants, the temperature, and the pressure.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 22,
      "page_label": "3"
    }
  },
  {
    "page_content": "of the concentrations of reactants, the temperature, and the pressure.\nIn other pattern recognition problems, the training data consists of a set of input\nvectors x without any corresponding target values. The goal in suchunsupervised\nlearning problems may be to discover groups of similar examples within the data,\nwhere it is calledclustering, or to determine the distribution of data within the input\nspace, known asdensity estimation, or to project the data from a high-dimensional\nspace down to two or three dimensions for the purpose ofvisualization.\nFinally, the technique ofreinforcement learning(Sutton and Barto, 1998) is con-\ncerned with the problem of ﬁnding suitable actions to take in a given situation in\norder to maximize a reward. Here the learning algorithm is not given examples of\noptimal outputs, in contrast to supervised learning, but must instead discover them\nby a process of trial and error. Typically there is a sequence of states and actions in\nwhich the learning algorithm is interacting with its environment. In many cases, the\ncurrent action not only affects the immediate reward but also has an impact on the re-\nward at all subsequent time steps. For example, by using appropriate reinforcement\nlearning techniques a neural network can learn to play the game of backgammon to a\nhigh standard (Tesauro, 1994). Here the network must learn to take a board position\nas input, along with the result of a dice throw, and produce a strong move as the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 22,
      "page_label": "3"
    }
  },
  {
    "page_content": "as input, along with the result of a dice throw, and produce a strong move as the\noutput. This is done by having the network play against a copy of itself for perhaps a\nmillion games. A major challenge is that a game of backgammon can involve dozens\nof moves, and yet it is only at the end of the game that the reward, in the form of\nvictory, is achieved. The reward must then be attributed appropriately to all of the\nmoves that led to it, even though some moves will have been good ones and others\nless so. This is an example of acredit assignment problem. A general feature of re-\ninforcement learning is the trade-off betweenexploration, in which the system tries\nout new kinds of actions to see how effective they are, andexploitation, in which\nthe system makes use of actions that are known to yield a high reward. Too strong\na focus on either exploration or exploitation will yield poor results. Reinforcement\nlearning continues to be an active area of machine learning research. However, a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 22,
      "page_label": "3"
    }
  },
  {
    "page_content": "4 1. INTRODUCTION\nFigure 1.2 Plot of a training data set of N =\n10 points, shown as blue circles,\neach comprising an observation\nof the input variable x along with\nthe corresponding target variable\nt. The green curve shows the\nfunction sin(2πx) used to gener-\nate the data. Our goal is to pre-\ndict the value of t for some new\nvalue of x, without knowledge of\nthe green curve.\nx\nt\n0 1\n−1\n0\n1\ndetailed treatment lies beyond the scope of this book.\nAlthough each of these tasks needs its own tools and techniques, many of the\nkey ideas that underpin them are common to all such problems. One of the main\ngoals of this chapter is to introduce, in a relatively informal way, several of the most\nimportant of these concepts and to illustrate them using simple examples. Later in\nthe book we shall see these same ideas re-emerge in the context of more sophisti-\ncated models that are applicable to real-world pattern recognition applications. This\nchapter also provides a self-contained introduction to three important tools that will\nbe used throughout the book, namely probability theory, decision theory, and infor-\nmation theory. Although these might sound like daunting topics, they are in fact\nstraightforward, and a clear understanding of them is essential if machine learning\ntechniques are to be used to best effect in practical applications.\n1.1. Example: Polynomial Curve Fitting\nWe begin by introducing a simple regression problem, which we shall use as a run-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 23,
      "page_label": "4"
    }
  },
  {
    "page_content": "1.1. Example: Polynomial Curve Fitting\nWe begin by introducing a simple regression problem, which we shall use as a run-\nning example throughout this chapter to motivate a number of key concepts. Sup-\npose we observe a real-valued input variablex and we wish to use this observation to\npredict the value of a real-valued target variablet. For the present purposes, it is in-\nstructive to consider an artiﬁcial example using synthetically generated data because\nwe then know the precise process that generated the data for comparison against any\nlearned model. The data for this example is generated from the functionsin(2πx)\nwith random noise included in the target values, as described in detail in Appendix A.\nNow suppose that we are given a training set comprisingN observations of x,\nwritten x ≡ (x1,...,x N )T, together with corresponding observations of the values\nof t, denotedt ≡ (t1,...,t N )T. Figure 1.2 shows a plot of a training set comprising\nN =1 0 data points. The input data set x in Figure 1.2 was generated by choos-\ning values ofxn, for n =1 ,...,N , spaced uniformly in range[0,1], and the target\ndata set t was obtained by ﬁrst computing the corresponding values of the function",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 23,
      "page_label": "4"
    }
  },
  {
    "page_content": "1.1. Example: Polynomial Curve Fitting 5\nsin(2πx) and then adding a small level of random noise having a Gaussian distri-\nbution (the Gaussian distribution is discussed in Section 1.2.4) to each such point in\norder to obtain the corresponding valuetn. By generating data in this way, we are\ncapturing a property of many real data sets, namely that they possess an underlying\nregularity, which we wish to learn, but that individual observations are corrupted by\nrandom noise. This noise might arise from intrinsically stochastic (i.e. random) pro-\ncesses such as radioactive decay but more typically is due to there being sources of\nvariability that are themselves unobserved.\nOur goal is to exploit this training set in order to make predictions of the value\nˆt of the target variable for some new valueˆx of the input variable. As we shall see\nlater, this involves implicitly trying to discover the underlying functionsin(2πx).\nThis is intrinsically a difﬁcult problem as we have to generalize from a ﬁnite data\nset. Furthermore the observed data are corrupted with noise, and so for a givenˆx\nthere is uncertainty as to the appropriate value forˆt. Probability theory, discussed\nin Section 1.2, provides a framework for expressing such uncertainty in a precise\nand quantitative manner, and decision theory, discussed in Section 1.5, allows us to\nexploit this probabilistic representation in order to make predictions that are optimal\naccording to appropriate criteria.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 24,
      "page_label": "5"
    }
  },
  {
    "page_content": "exploit this probabilistic representation in order to make predictions that are optimal\naccording to appropriate criteria.\nFor the moment, however, we shall proceed rather informally and consider a\nsimple approach based on curve ﬁtting. In particular, we shall ﬁt the data using a\npolynomial function of the form\ny(x,w)= w0 + w1x + w2x2 + ... + wM xM =\nM∑\nj=0\nwjxj (1.1)\nwhere M is the order of the polynomial, andxj denotes x raised to the power ofj.\nThe polynomial coefﬁcients w0,...,w M are collectively denoted by the vectorw.\nNote that, although the polynomial functiony(x,w) is a nonlinear function ofx,i t\nis a linear function of the coefﬁcientsw. Functions, such as the polynomial, which\nare linear in the unknown parameters have important properties and are calledlinear\nmodels and will be discussed extensively in Chapters 3 and 4.\nThe values of the coefﬁcients will be determined by ﬁtting the polynomial to the\ntraining data. This can be done by minimizing anerror function that measures the\nmisﬁt between the functiony(x,w), for any given value ofw, and the training set\ndata points. One simple choice of error function, which is widely used, is given by\nthe sum of the squares of the errors between the predictionsy(xn, w) for each data\npoint xn and the corresponding target valuestn, so that we minimize\nE(w)= 1\n2\nN∑\nn=1\n{y(xn, w) − tn}2 (1.2)\nwhere the factor of1/2 is included for later convenience. We shall discuss the mo-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 24,
      "page_label": "5"
    }
  },
  {
    "page_content": "E(w)= 1\n2\nN∑\nn=1\n{y(xn, w) − tn}2 (1.2)\nwhere the factor of1/2 is included for later convenience. We shall discuss the mo-\ntivation for this choice of error function later in this chapter. For the moment we\nsimply note that it is a nonnegative quantity that would be zero if, and only if, the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 24,
      "page_label": "5"
    }
  },
  {
    "page_content": "6 1. INTRODUCTION\nFigure 1.3 The error function (1.2) corre-\nsponds to (one half of) the sum of\nthe squares of the displacements\n(shown by the vertical green bars)\nof each data point from the function\ny(x, w).\nt\nx\ny(xn,w)\ntn\nxn\nfunction y(x,w) were to pass exactly through each training data point. The geomet-\nrical interpretation of the sum-of-squares error function is illustrated in Figure 1.3.\nWe can solve the curve ﬁtting problem by choosing the value ofw for which\nE(w) is as small as possible. Because the error function is a quadratic function of\nthe coefﬁcientsw, its derivatives with respect to the coefﬁcients will be linear in the\nelements of w, and so the minimization of the error function has a unique solution,\ndenoted by w⋆ , which can be found in closed form. The resulting polynomial isExercise 1.1\ngiven by the functiony(x,w⋆ ).\nThere remains the problem of choosing the orderM of the polynomial, and as\nwe shall see this will turn out to be an example of an important concept calledmodel\ncomparison or model selection. In Figure 1.4, we show four examples of the results\nof ﬁtting polynomials having orders M =0 , 1, 3,a n d9 to the data set shown in\nFigure 1.2.\nWe notice that the constant (M =0 ) and ﬁrst order (M =1 ) polynomials\ngive rather poor ﬁts to the data and consequently rather poor representations of the\nfunction sin(2πx). The third order (M =3 ) polynomial seems to give the best ﬁt\nto the function sin(2πx) of the examples shown in Figure 1.4. When we go to a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 25,
      "page_label": "6"
    }
  },
  {
    "page_content": "to the function sin(2πx) of the examples shown in Figure 1.4. When we go to a\nmuch higher order polynomial (M =9 ), we obtain an excellent ﬁt to the training\ndata. In fact, the polynomial passes exactly through each data point andE(w⋆ )=0 .\nHowever, the ﬁtted curve oscillates wildly and gives a very poor representation of\nthe function sin(2πx). This latter behaviour is known asover-ﬁtting.\nAs we have noted earlier, the goal is to achieve good generalization by making\naccurate predictions for new data. We can obtain some quantitative insight into the\ndependence of the generalization performance onM by considering a separate test\nset comprising 100 data points generated using exactly the same procedure used\nto generate the training set points but with new choices for the random noise values\nincluded in the target values. For each choice ofM, we can then evaluate the residual\nvalue ofE(w⋆ ) given by (1.2) for the training data, and we can also evaluateE(w⋆ )\nfor the test data set. It is sometimes more convenient to use the root-mean-square",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 25,
      "page_label": "6"
    }
  },
  {
    "page_content": "1.1. Example: Polynomial Curve Fitting 7\nx\nt\nM =0\n0 1\n−1\n0\n1\nx\nt\nM =1\n0 1\n−1\n0\n1\nx\nt\nM =3\n0 1\n−1\n0\n1\nx\nt\nM =9\n0 1\n−1\n0\n1\nFigure 1.4 Plots of polynomials having various orders M, shown as red curves, ﬁtted to the data set shown in\nFigure 1.2.\n(RMS) error deﬁned by\nERMS =\n√\n2E(w⋆ )/N (1.3)\nin which the division by N allows us to compare different sizes of data sets on\nan equal footing, and the square root ensures thatERMS is measured on the same\nscale (and in the same units) as the target variablet. Graphs of the training and\ntest set RMS errors are shown, for various values ofM, in Figure 1.5. The test\nset error is a measure of how well we are doing in predicting the values oft for\nnew data observations ofx. We note from Figure 1.5 that small values ofM give\nrelatively large values of the test set error, and this can be attributed to the fact that\nthe corresponding polynomials are rather inﬂexible and are incapable of capturing\nthe oscillations in the function sin(2πx). V alues ofM in the range 3 ⩽ M ⩽ 8\ngive small values for the test set error, and these also give reasonable representations\nof the generating functionsin(2πx), as can be seen, for the case ofM =3 , from\nFigure 1.4.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 26,
      "page_label": "7"
    }
  },
  {
    "page_content": "8 1. INTRODUCTION\nFigure 1.5 Graphs of the root-mean-square\nerror, deﬁned by (1.3), evaluated\non the training set and on an inde-\npendent test set for various values\nof M.\nM\nERMS\n0 3 6 9\n0\n0.5\n1\nTraining\nTest\nFor M =9 , the training set error goes to zero, as we might expect because\nthis polynomial contains10 degrees of freedom corresponding to the10 coefﬁcients\nw0,...,w 9, and so can be tuned exactly to the10 data points in the training set.\nHowever, the test set error has become very large and, as we saw in Figure 1.4, the\ncorresponding function y(x,w⋆ ) exhibits wild oscillations.\nThis may seem paradoxical because a polynomial of given order contains all\nlower order polynomials as special cases. TheM =9 polynomial is therefore capa-\nble of generating results at least as good as theM =3 polynomial. Furthermore, we\nmight suppose that the best predictor of new data would be the functionsin(2πx)\nfrom which the data was generated (and we shall see later that this is indeed the\ncase). We know that a power series expansion of the function sin(2πx) contains\nterms of all orders, so we might expect that results should improve monotonically as\nwe increase M.\nWe can gain some insight into the problem by examining the values of the co-\nefﬁcients w⋆ obtained from polynomials of various order, as shown in Table 1.1.\nWe see that, asM increases, the magnitude of the coefﬁcients typically gets larger.\nIn particular for theM =9 polynomial, the coefﬁcients have become ﬁnely tuned",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 27,
      "page_label": "8"
    }
  },
  {
    "page_content": "In particular for theM =9 polynomial, the coefﬁcients have become ﬁnely tuned\nto the data by developing large positive and negative values so that the correspond-\nTable 1.1 Table of the coefﬁcients w⋆ for\npolynomials of various order.\nObserve how the typical mag-\nnitude of the coefﬁcients in-\ncreases dramatically as the or-\nder of the polynomial increases.\nM =0 M =1 M =6 M =9\nw⋆\n0 0.19 0.82 0.31 0.35\nw⋆\n1 -1.27 7.99 232.37\nw⋆\n2 -25.43 -5321.83\nw⋆\n3 17.37 48568.31\nw⋆\n4 -231639.30\nw⋆\n5 640042.26\nw⋆\n6 -1061800.52\nw⋆\n7 1042400.18\nw⋆\n8 -557682.99\nw⋆\n9 125201.43",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 27,
      "page_label": "8"
    }
  },
  {
    "page_content": "1.1. Example: Polynomial Curve Fitting 9\nx\nt\nN =1 5\n0 1\n−1\n0\n1\nx\nt\nN = 100\n0 1\n−1\n0\n1\nFigure 1.6 Plots of the solutions obtained by minimizing the sum-of-squares error function using the M =9\npolynomial for N =1 5 data points (left plot) and N = 100 data points (right plot). We see that increasing the\nsize of the data set reduces the over-ﬁtting problem.\ning polynomial function matches each of the data points exactly, but between data\npoints (particularly near the ends of the range) the function exhibits the large oscilla-\ntions observed in Figure 1.4. Intuitively, what is happening is that the more ﬂexible\npolynomials with larger values ofM are becoming increasingly tuned to the random\nnoise on the target values.\nIt is also interesting to examine the behaviour of a given model as the size of the\ndata set is varied, as shown in Figure 1.6. We see that, for a given model complexity,\nthe over-ﬁtting problem become less severe as the size of the data set increases.\nAnother way to say this is that the larger the data set, the more complex (in other\nwords more ﬂexible) the model that we can afford to ﬁt to the data. One rough\nheuristic that is sometimes advocated is that the number of data points should be\nno less than some multiple (say 5 or 10) of the number of adaptive parameters in\nthe model. However, as we shall see in Chapter 3, the number of parameters is not\nnecessarily the most appropriate measure of model complexity.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 28,
      "page_label": "9"
    }
  },
  {
    "page_content": "the model. However, as we shall see in Chapter 3, the number of parameters is not\nnecessarily the most appropriate measure of model complexity.\nAlso, there is something rather unsatisfying about having to limit the number of\nparameters in a model according to the size of the available training set. It would\nseem more reasonable to choose the complexity of the model according to the com-\nplexity of the problem being solved. We shall see that the least squares approach\nto ﬁnding the model parameters represents a speciﬁc case ofmaximum likelihood\n(discussed in Section 1.2.5), and that the over-ﬁtting problem can be understood as\na general property of maximum likelihood. By adopting aBayesian approach, theSection 3.4\nover-ﬁtting problem can be avoided. We shall see that there is no difﬁculty from\na Bayesian perspective in employing models for which the number of parameters\ngreatly exceeds the number of data points. Indeed, in a Bayesian model theeffective\nnumber of parameters adapts automatically to the size of the data set.\nFor the moment, however, it is instructive to continue with the current approach\nand to consider how in practice we can apply it to data sets of limited size where we",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 28,
      "page_label": "9"
    }
  },
  {
    "page_content": "10 1. INTRODUCTION\nx\nt\nlnλ = −18\n0 1\n−1\n0\n1\nx\nt\nlnλ =0\n0 1\n−1\n0\n1\nFigure 1.7 Plots of M =9 polynomials ﬁtted to the data set shown in Figure 1.2 using the regularized error\nfunction (1.4) for two values of the regularization parameter λ corresponding to ln λ = −18 and ln λ =0 . The\ncase of no regularizer, i.e., λ =0 , corresponding to ln λ = −∞, is shown at the bottom right of Figure 1.4.\nmay wish to use relatively complex and ﬂexible models. One technique that is often\nused to control the over-ﬁtting phenomenon in such cases is that ofregularization,\nwhich involves adding a penalty term to the error function (1.2) in order to discourage\nthe coefﬁcients from reaching large values. The simplest such penalty term takes the\nform of a sum of squares of all of the coefﬁcients, leading to a modiﬁed error function\nof the form\n˜E(w)= 1\n2\nN∑\nn=1\n{y(xn, w) − tn}2 + λ\n2∥w∥2 (1.4)\nwhere ∥w∥2 ≡ wTw = w2\n0 + w2\n1 + ... + w2\nM , and the coefﬁcientλ governs the rel-\native importance of the regularization term compared with the sum-of-squares error\nterm. Note that often the coefﬁcientw0 is omitted from the regularizer because its\ninclusion causes the results to depend on the choice of origin for the target variable\n(Hastie et al., 2001), or it may be included but with its own regularization coefﬁcient\n(we shall discuss this topic in more detail in Section 5.5.1). Again, the error function\nin (1.4) can be minimized exactly in closed form. Techniques such as this are knownExercise 1.2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 29,
      "page_label": "10"
    }
  },
  {
    "page_content": "in (1.4) can be minimized exactly in closed form. Techniques such as this are knownExercise 1.2\nin the statistics literature asshrinkage methods because they reduce the value of the\ncoefﬁcients. The particular case of a quadratic regularizer is called ridge regres-\nsion (Hoerl and Kennard, 1970). In the context of neural networks, this approach is\nknown asweight decay.\nFigure 1.7 shows the results of ﬁtting the polynomial of orderM =9 to the\nsame data set as before but now using the regularized error function given by (1.4).\nWe see that, for a value oflnλ = −18, the over-ﬁtting has been suppressed and we\nnow obtain a much closer representation of the underlying functionsin(2πx). If,\nhowever, we use too large a value forλ then we again obtain a poor ﬁt, as shown in\nFigure 1.7 forlnλ =0 . The corresponding coefﬁcients from the ﬁtted polynomials\nare given in Table 1.2, showing that regularization has the desired effect of reducing",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 29,
      "page_label": "10"
    }
  },
  {
    "page_content": "1.1. Example: Polynomial Curve Fitting 11\nTable 1.2 Table of the coefﬁcients w⋆ for M =\n9 polynomials with various values for\nthe regularization parameterλ. Note\nthat ln λ = −∞ corresponds to a\nmodel with no regularization, i.e., to\nthe graph at the bottom right in Fig-\nure 1.4. We see that, as the value of\nλ increases, the typical magnitude of\nthe coefﬁcients gets smaller.\nln λ = −∞ lnλ = −18 ln λ =0\nw⋆\n0 0.35 0.35 0.13\nw⋆\n1 232.37 4.74 -0.05\nw⋆\n2 -5321.83 -0.77 -0.06\nw⋆\n3 48568.31 -31.97 -0.05\nw⋆\n4 -231639.30 -3.89 -0.03\nw⋆\n5 640042.26 55.28 -0.02\nw⋆\n6 -1061800.52 41.32 -0.01\nw⋆\n7 1042400.18 -45.95 -0.00\nw⋆\n8 -557682.99 -91.53 0.00\nw⋆\n9 125201.43 72.68 0.01\nthe magnitude of the coefﬁcients.\nThe impact of the regularization term on the generalization error can be seen by\nplotting the value of the RMS error (1.3) for both training and test sets againstlnλ,\nas shown in Figure 1.8. We see that in effectλ now controls the effective complexity\nof the model and hence determines the degree of over-ﬁtting.\nThe issue of model complexity is an important one and will be discussed at\nlength in Section 1.3. Here we simply note that, if we were trying to solve a practical\napplication using this approach of minimizing an error function, we would have to\nﬁnd a way to determine a suitable value for the model complexity. The results above\nsuggest a simple way of achieving this, namely by taking the available data and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 30,
      "page_label": "11"
    }
  },
  {
    "page_content": "suggest a simple way of achieving this, namely by taking the available data and\npartitioning it into a training set, used to determine the coefﬁcientsw, and a separate\nvalidation set, also called a hold-out set, used to optimize the model complexity\n(either M or λ). In many cases, however, this will prove to be too wasteful of\nvaluable training data, and we have to seek more sophisticated approaches.Section 1.3\nSo far our discussion of polynomial curve ﬁtting has appealed largely to in-\ntuition. We now seek a more principled approach to solving problems in pattern\nrecognition by turning to a discussion of probability theory. As well as providing the\nfoundation for nearly all of the subsequent developments in this book, it will also\nFigure 1.8 Graph of the root-mean-square er-\nror (1.3) versus ln λ for the M =9\npolynomial.\nERMS\nlnλ−35 −30 −25 −20\n0\n0.5\n1\nTraining\nTest",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 30,
      "page_label": "11"
    }
  },
  {
    "page_content": "12 1. INTRODUCTION\ngive us some important insights into the concepts we have introduced in the con-\ntext of polynomial curve ﬁtting and will allow us to extend these to more complex\nsituations.\n1.2. Probability Theory\nA key concept in the ﬁeld of pattern recognition is that of uncertainty. It arises both\nthrough noise on measurements, as well as through the ﬁnite size of data sets. Prob-\nability theory provides a consistent framework for the quantiﬁcation and manipula-\ntion of uncertainty and forms one of the central foundations for pattern recognition.\nWhen combined with decision theory, discussed in Section 1.5, it allows us to make\noptimal predictions given all the information available to us, even though that infor-\nmation may be incomplete or ambiguous.\nWe will introduce the basic concepts of probability theory by considering a sim-\nple example. Imagine we have two boxes, one red and one blue, and in the red box\nwe have 2 apples and 6 oranges, and in the blue box we have 3 apples and 1 orange.\nThis is illustrated in Figure 1.9. Now suppose we randomly pick one of the boxes\nand from that box we randomly select an item of fruit, and having observed which\nsort of fruit it is we replace it in the box from which it came. We could imagine\nrepeating this process many times. Let us suppose that in so doing we pick the red\nbox 40% of the time and we pick the blue box 60% of the time, and that when we",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 31,
      "page_label": "12"
    }
  },
  {
    "page_content": "box 40% of the time and we pick the blue box 60% of the time, and that when we\nremove an item of fruit from a box we are equally likely to select any of the pieces\nof fruit in the box.\nIn this example, the identity of the box that will be chosen is a random variable,\nwhich we shall denote by B. This random variable can take one of two possible\nvalues, namely r (corresponding to the red box) or b (corresponding to the blue\nbox). Similarly, the identity of the fruit is also a random variable and will be denoted\nby F. It can take either of the valuesa (for apple) oro (for orange).\nTo begin with, we shall deﬁne the probability of an event to be the fraction\nof times that event occurs out of the total number of trials, in the limit that the total\nnumber of trials goes to inﬁnity. Thus the probability of selecting the red box is4/10\nFigure 1.9 We use a simple example of two\ncoloured boxes each containing fruit\n(apples shown in green and or-\nanges shown in orange) to intro-\nduce the basic ideas of probability.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 31,
      "page_label": "12"
    }
  },
  {
    "page_content": "1.2. Probability Theory 13\nFigure 1.10 We can derive the sum and product rules of probability by\nconsidering two random variables,X, which takes the values{xi} where\ni =1 ,...,M , and Y , which takes the values {yj} where j =1 ,...,L .\nIn this illustration we have M =5 and L =3 . If we consider a total\nnumber N of instances of these variables, then we denote the number\nof instances where X = xi and Y = yj by nij, which is the number of\npoints in the corresponding cell of the array. The number of points in\ncolumn i, corresponding to X = xi, is denoted by ci, and the number of\npoints in row j, corresponding to Y = yj, is denoted by rj.\n}\n}\nci\nrjyj\nxi\nnij\nand the probability of selecting the blue box is6/10. We write these probabilities\nas p(B = r)=4 /10 and p(B = b)=6 /10. Note that, by deﬁnition, probabilities\nmust lie in the interval[0,1]. Also, if the events are mutually exclusive and if they\ninclude all possible outcomes (for instance, in this example the box must be either\nred or blue), then we see that the probabilities for those events must sum to one.\nWe can now ask questions such as: “what is the overall probability that the se-\nlection procedure will pick an apple?”, or “given that we have chosen an orange,\nwhat is the probability that the box we chose was the blue one?”. We can answer\nquestions such as these, and indeed much more complex questions associated with\nproblems in pattern recognition, once we have equipped ourselves with the two el-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 32,
      "page_label": "13"
    }
  },
  {
    "page_content": "problems in pattern recognition, once we have equipped ourselves with the two el-\nementary rules of probability, known as thesum rule and the product rule. Having\nobtained these rules, we shall then return to our boxes of fruit example.\nIn order to derive the rules of probability, consider the slightly more general ex-\nample shown in Figure 1.10 involving two random variablesX and Y (which could\nfor instance be the Box and Fruit variables considered above). We shall suppose that\nX can take any of the valuesxi where i =1 ,...,M , and Y can take the valuesyj\nwhere j =1 ,...,L . Consider a total of N trials in which we sample both of the\nvariables X and Y , and let the number of such trials in whichX = xi and Y = yj\nbe nij. Also, let the number of trials in which X takes the value xi (irrespective\nof the value thatY takes) be denoted byci, and similarly let the number of trials in\nwhich Y takes the valueyj be denoted byrj.\nThe probability that X will take the value xi and Y will take the value yj is\nwritten p(X = xi,Y = yj) and is called the joint probability of X = xi and\nY = yj. It is given by the number of points falling in the celli,j as a fraction of the\ntotal number of points, and hence\np(X = xi,Y = yj)= nij\nN . (1.5)\nHere we are implicitly considering the limitN →∞ . Similarly, the probability that\nX takes the valuexi irrespective of the value ofY is written as p(X = xi) and is\ngiven by the fraction of the total number of points that fall in columni, so that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 32,
      "page_label": "13"
    }
  },
  {
    "page_content": "given by the fraction of the total number of points that fall in columni, so that\np(X = xi)= ci\nN . (1.6)\nBecause the number of instances in columni in Figure 1.10 is just the sum of the\nnumber of instances in each cell of that column, we haveci = ∑\nj nij and therefore,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 32,
      "page_label": "13"
    }
  },
  {
    "page_content": "14 1. INTRODUCTION\nfrom (1.5) and (1.6), we have\np(X = xi)=\nL∑\nj=1\np(X = xi,Y = yj) (1.7)\nwhich is thesum rule of probability. Note thatp(X = xi) is sometimes called the\nmarginal probability, because it is obtained by marginalizing, or summing out, the\nother variables (in this caseY ).\nIf we consider only those instances for which X = xi, then the fraction of\nsuch instances for which Y = yj is written p(Y = yj|X = xi) and is called the\nconditional probability of Y = yj given X = xi. It is obtained by ﬁnding the\nfraction of those points in columni that fall in celli,j and hence is given by\np(Y = yj|X = xi)= nij\nci\n. (1.8)\nFrom (1.5), (1.6), and (1.8), we can then derive the following relationship\np(X = xi,Y = yj)= nij\nN = nij\nci\n· ci\nN\n= p(Y = yj|X = xi)p(X = xi) (1.9)\nwhich is theproduct ruleof probability.\nSo far we have been quite careful to make a distinction between a random vari-\nable, such as the boxB in the fruit example, and the values that the random variable\ncan take, for exampler if the box were the red one. Thus the probability thatB takes\nthe value r is denoted p(B = r). Although this helps to avoid ambiguity, it leads\nto a rather cumbersome notation, and in many cases there will be no need for such\npedantry. Instead, we may simply writep(B) to denote a distribution over the ran-\ndom variable B,o r p(r) to denote the distribution evaluated for the particular value\nr, provided that the interpretation is clear from the context.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 33,
      "page_label": "14"
    }
  },
  {
    "page_content": "dom variable B,o r p(r) to denote the distribution evaluated for the particular value\nr, provided that the interpretation is clear from the context.\nWith this more compact notation, we can write the two fundamental rules of\nprobability theory in the following form.\nThe Rules of Probability\nsum rule p(X)=\n∑\nY\np(X,Y ) (1.10)\nproduct rule p(X,Y )= p(Y |X)p(X). (1.11)\nHere p(X,Y ) is a joint probability and is verbalized as “the probability ofX and\nY ”. Similarly, the quantityp(Y |X) is a conditional probability and is verbalized as\n“the probability ofY given X”, whereas the quantityp(X) is a marginal probability",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 33,
      "page_label": "14"
    }
  },
  {
    "page_content": "1.2. Probability Theory 15\nand is simply “the probability ofX”. These two simple rules form the basis for all\nof the probabilistic machinery that we use throughout this book.\nFrom the product rule, together with the symmetry propertyp(X,Y )= p(Y,X ),\nwe immediately obtain the following relationship between conditional probabilities\np(Y |X)= p(X|Y )p(Y )\np(X) (1.12)\nwhich is calledBayes’ theoremand which plays a central role in pattern recognition\nand machine learning. Using the sum rule, the denominator in Bayes’ theorem can\nbe expressed in terms of the quantities appearing in the numerator\np(X)=\n∑\nY\np(X|Y )p(Y ). (1.13)\nWe can view the denominator in Bayes’ theorem as being the normalization constant\nrequired to ensure that the sum of the conditional probability on the left-hand side of\n(1.12) over all values ofY equals one.\nIn Figure 1.11, we show a simple example involving a joint distribution over two\nvariables to illustrate the concept of marginal and conditional distributions. Here\na ﬁnite sample of N =6 0 data points has been drawn from the joint distribution\nand is shown in the top left. In the top right is a histogram of the fractions of data\npoints having each of the two values ofY . From the deﬁnition of probability, these\nfractions would equal the corresponding probabilitiesp(Y ) in the limitN →∞ .W e\ncan view the histogram as a simple way to model a probability distribution given only",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 34,
      "page_label": "15"
    }
  },
  {
    "page_content": "can view the histogram as a simple way to model a probability distribution given only\na ﬁnite number of points drawn from that distribution. Modelling distributions from\ndata lies at the heart of statistical pattern recognition and will be explored in great\ndetail in this book. The remaining two plots in Figure 1.11 show the corresponding\nhistogram estimates ofp(X) and p(X|Y =1 ).\nLet us now return to our example involving boxes of fruit. For the moment, we\nshall once again be explicit about distinguishing between the random variables and\ntheir instantiations. We have seen that the probabilities of selecting either the red or\nthe blue boxes are given by\np(B = r)=4 /10 (1.14)\np(B = b)=6 /10 (1.15)\nrespectively. Note that these satisfyp(B = r)+ p(B = b)=1 .\nNow suppose that we pick a box at random, and it turns out to be the blue box.\nThen the probability of selecting an apple is just the fraction of apples in the blue\nbox which is3/4, and sop(F = a|B = b)=3 /4. In fact, we can write out all four\nconditional probabilities for the type of fruit, given the selected box\np(F = a|B = r)=1 /4 (1.16)\np(F = o|B = r)=3 /4 (1.17)\np(F = a|B = b)=3 /4 (1.18)\np(F = o|B = b)=1 /4. (1.19)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 34,
      "page_label": "15"
    }
  },
  {
    "page_content": "16 1. INTRODUCTION\np(X,Y )\nX\nY =2\nY =1\np(Y )\np(X)\nX X\np(X|Y =1 )\nFigure 1.11 An illustration of a distribution over two variables, X, which takes 9 possible values, and Y , which\ntakes two possible values. The top left ﬁgure shows a sample of 60 points drawn from a joint probability distri-\nbution over these variables. The remaining ﬁgures show histogram estimates of the marginal distributions p(X)\nand p(Y ), as well as the conditional distribution p(X|Y =1 ) corresponding to the bottom row in the top left\nﬁgure.\nAgain, note that these probabilities are normalized so that\np(F = a|B = r)+ p(F = o|B = r)=1 (1.20)\nand similarly\np(F = a|B = b)+ p(F = o|B = b)=1 . (1.21)\nWe can now use the sum and product rules of probability to evaluate the overall\nprobability of choosing an apple\np(F = a)= p(F = a|B = r)p(B = r)+ p(F = a|B = b)p(B = b)\n= 1\n4 × 4\n10 + 3\n4 × 6\n10 = 11\n20 (1.22)\nfrom which it follows, using the sum rule, thatp(F = o)=1 − 11/20 = 9/20.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 35,
      "page_label": "16"
    }
  },
  {
    "page_content": "1.2. Probability Theory 17\nSuppose instead we are told that a piece of fruit has been selected and it is an\norange, and we would like to know which box it came from. This requires that\nwe evaluate the probability distribution over boxes conditioned on the identity of\nthe fruit, whereas the probabilities in (1.16)–(1.19) give the probability distribution\nover the fruit conditioned on the identity of the box. We can solve the problem of\nreversing the conditional probability by using Bayes’ theorem to give\np(B = r|F = o)= p(F = o|B = r)p(B = r)\np(F = o) = 3\n4 × 4\n10 × 20\n9 = 2\n3. (1.23)\nFrom the sum rule, it then follows thatp(B = b|F = o)=1 − 2/3=1 /3.\nWe can provide an important interpretation of Bayes’ theorem as follows. If\nwe had been asked which box had been chosen before being told the identity of\nthe selected item of fruit, then the most complete information we have available is\nprovided by the probabilityp(B). We call this theprior probabilitybecause it is the\nprobability availablebefore we observe the identity of the fruit. Once we are told that\nthe fruit is an orange, we can then use Bayes’ theorem to compute the probability\np(B|F), which we shall call theposterior probability because it is the probability\nobtained after we have observedF. Note that in this example, the prior probability\nof selecting the red box was4/10, so that we were more likely to select the blue box\nthan the red one. However, once we have observed that the piece of selected fruit is",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 36,
      "page_label": "17"
    }
  },
  {
    "page_content": "than the red one. However, once we have observed that the piece of selected fruit is\nan orange, we ﬁnd that the posterior probability of the red box is now2/3, so that\nit is now more likely that the box we selected was in fact the red one. This result\naccords with our intuition, as the proportion of oranges is much higher in the red box\nthan it is in the blue box, and so the observation that the fruit was an orange provides\nsigniﬁcant evidence favouring the red box. In fact, the evidence is sufﬁciently strong\nthat it outweighs the prior and makes it more likely that the red box was chosen\nrather than the blue one.\nFinally, we note that if the joint distribution of two variables factorizes into the\nproduct of the marginals, so thatp(X,Y )= p(X)p(Y ), then X and Y are said to\nbe independent. From the product rule, we see that p(Y |X)= p(Y ), and so the\nconditional distribution ofY given X is indeed independent of the value ofX.F o r\ninstance, in our boxes of fruit example, if each box contained the same fraction of\napples and oranges, thenp(F|B)= P(F), so that the probability of selecting, say,\nan apple is independent of which box is chosen.\n1.2.1 Probability densities\nAs well as considering probabilities deﬁned over discrete sets of events, we\nalso wish to consider probabilities with respect to continuous variables. We shall\nlimit ourselves to a relatively informal discussion. If the probability of a real-valued",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 36,
      "page_label": "17"
    }
  },
  {
    "page_content": "limit ourselves to a relatively informal discussion. If the probability of a real-valued\nvariable x falling in the interval(x, x+ δx) is given by p(x)δx for δx → 0, then\np(x) is called theprobability density over x. This is illustrated in Figure 1.12. The\nprobability that x will lie in an interval(a, b) is then given by\np(x ∈ (a, b)) =\n∫ b\na\np(x)d x. (1.24)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 36,
      "page_label": "17"
    }
  },
  {
    "page_content": "18 1. INTRODUCTION\nFigure 1.12 The concept of probability for\ndiscrete variables can be ex-\ntended to that of a probability\ndensity p(x) over a continuous\nvariable x and is such that the\nprobability of x lying in the inter-\nval (x, x+δx) is given by p(x)δx\nfor δx → 0. The probability\ndensity can be expressed as the\nderivative of a cumulative distri-\nbution function P(x).\nxδx\np(x) P(x)\nBecause probabilities are nonnegative, and because the value ofx must lie some-\nwhere on the real axis, the probability densityp(x) must satisfy the two conditions\np(x) ⩾ 0 (1.25)∫ ∞\n−∞\np(x)d x =1 . (1.26)\nUnder a nonlinear change of variable, a probability density transforms differently\nfrom a simple function, due to the Jacobian factor. For instance, if we consider\na change of variables x = g(y), then a function f(x) becomes ˜f(y)= f(g(y)).\nNow consider a probability densitypx(x) that corresponds to a densitypy(y) with\nrespect to the new variabley, where the sufﬁces denote the fact thatpx(x) and py(y)\nare different densities. Observations falling in the range(x, x+ δx) will, for small\nvalues of δx, be transformed into the range(y,y + δy) where px(x)δx ≃ py(y)δy,\nand hence\npy(y)= px(x)\n⏐⏐⏐⏐\ndx\ndy\n⏐⏐⏐⏐\n= px(g(y)) |g′(y)|. (1.27)\nOne consequence of this property is that the concept of the maximum of a probability\ndensity is dependent on the choice of variable.Exercise 1.4\nThe probability that x lies in the interval (−∞,z ) is given by the cumulative\ndistribution functiondeﬁned by\nP(z)=\n∫ z\n−∞",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 37,
      "page_label": "18"
    }
  },
  {
    "page_content": "The probability that x lies in the interval (−∞,z ) is given by the cumulative\ndistribution functiondeﬁned by\nP(z)=\n∫ z\n−∞\np(x)d x (1.28)\nwhich satisﬁes P′(x)= p(x), as shown in Figure 1.12.\nIf we have several continuous variablesx1,...,x D, denoted collectively by the\nvector x, then we can deﬁne a joint probability densityp(x)= p(x1,...,x D) such",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 37,
      "page_label": "18"
    }
  },
  {
    "page_content": "1.2. Probability Theory 19\nthat the probability ofx falling in an inﬁnitesimal volumeδx containing the pointx\nis given byp(x)δx. This multivariate probability density must satisfy\np(x) ⩾ 0 (1.29)∫\np(x)dx =1 (1.30)\nin which the integral is taken over the whole ofx space. We can also consider joint\nprobability distributions over a combination of discrete and continuous variables.\nNote that if x is a discrete variable, thenp(x) is sometimes called aprobability\nmass functionbecause it can be regarded as a set of ‘probability masses’ concentrated\nat the allowed values ofx.\nThe sum and product rules of probability, as well as Bayes’ theorem, apply\nequally to the case of probability densities, or to combinations of discrete and con-\ntinuous variables. For instance, ifx and y are two real variables, then the sum and\nproduct rules take the form\np(x)=\n∫\np(x, y)d y (1.31)\np(x, y)= p(y|x)p(x). (1.32)\nA formal justiﬁcation of the sum and product rules for continuous variables (Feller,\n1966) requires a branch of mathematics calledmeasure theory and lies outside the\nscope of this book. Its validity can be seen informally, however, by dividing each\nreal variable into intervals of width∆ and considering the discrete probability dis-\ntribution over these intervals. Taking the limit∆ → 0 then turns sums into integrals\nand gives the desired result.\n1.2.2 Expectations and covariances\nOne of the most important operations involving probabilities is that of ﬁnding",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 38,
      "page_label": "19"
    }
  },
  {
    "page_content": "and gives the desired result.\n1.2.2 Expectations and covariances\nOne of the most important operations involving probabilities is that of ﬁnding\nweighted averages of functions. The average value of some functionf(x) under a\nprobability distributionp(x) is called theexpectation of f(x) and will be denoted by\nE[f]. For a discrete distribution, it is given by\nE[f]=\n∑\nx\np(x)f(x) (1.33)\nso that the average is weighted by the relative probabilities of the different values\nof x. In the case of continuous variables, expectations are expressed in terms of an\nintegration with respect to the corresponding probability density\nE[f]=\n∫\np(x)f(x)d x. (1.34)\nIn either case, if we are given a ﬁnite numberN of points drawn from the probability\ndistribution or probability density, then the expectation can be approximated as a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 38,
      "page_label": "19"
    }
  },
  {
    "page_content": "20 1. INTRODUCTION\nﬁnite sum over these points\nE[f] ≃ 1\nN\nN∑\nn=1\nf(xn). (1.35)\nWe shall make extensive use of this result when we discuss sampling methods in\nChapter 11. The approximation in (1.35) becomes exact in the limitN →∞ .\nSometimes we will be considering expectations of functions of several variables,\nin which case we can use a subscript to indicate which variable is being averaged\nover, so that for instance\nEx[f(x, y)] (1.36)\ndenotes the average of the functionf(x, y) with respect to the distribution ofx. Note\nthat Ex[f(x, y)] will be a function ofy.\nWe can also consider a conditional expectation with respect to a conditional\ndistribution, so that\nEx[f|y]=\n∑\nx\np(x|y)f(x) (1.37)\nwith an analogous deﬁnition for continuous variables.\nThe variance of f(x) is deﬁned by\nvar[f]= E\n[\n(f(x) − E[f(x)])2]\n(1.38)\nand provides a measure of how much variability there is inf(x) around its mean\nvalue E[f(x)]. Expanding out the square, we see that the variance can also be written\nin terms of the expectations off(x) and f(x)2Exercise 1.5\nvar[f]= E[f(x)2] − E[f(x)]2. (1.39)\nIn particular, we can consider the variance of the variablex itself, which is given by\nvar[x]= E[x2] − E[x]2. (1.40)\nFor two random variablesx and y, the covariance is deﬁned by\ncov[x, y]= Ex,y [{x − E[x]}{y − E[y]}]\n= Ex,y[xy] − E[x]E[y] (1.41)\nwhich expresses the extent to whichx and y vary together. If x and y are indepen-\ndent, then their covariance vanishes.Exercise 1.6",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 39,
      "page_label": "20"
    }
  },
  {
    "page_content": "which expresses the extent to whichx and y vary together. If x and y are indepen-\ndent, then their covariance vanishes.Exercise 1.6\nIn the case of two vectors of random variablesx and y, the covariance is a matrix\ncov[x,y]= Ex,y\n[\n{x − E[x]}{yT − E[yT]}\n]\n= Ex,y[xyT] − E[x]E[yT]. (1.42)\nIf we consider the covariance of the components of a vectorx with each other, then\nwe use a slightly simpler notationcov[x] ≡ cov[x,x].",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 39,
      "page_label": "20"
    }
  },
  {
    "page_content": "1.2. Probability Theory 21\n1.2.3 Bayesian probabilities\nSo far in this chapter, we have viewed probabilities in terms of the frequencies\nof random, repeatable events. We shall refer to this as theclassical or frequentist\ninterpretation of probability. Now we turn to the more general Bayesian view, in\nwhich probabilities provide a quantiﬁcation of uncertainty.\nConsider an uncertain event, for example whether the moon was once in its own\norbit around the sun, or whether the Arctic ice cap will have disappeared by the end\nof the century. These are not events that can be repeated numerous times in order\nto deﬁne a notion of probability as we did earlier in the context of boxes of fruit.\nNevertheless, we will generally have some idea, for example, of how quickly we\nthink the polar ice is melting. If we now obtain fresh evidence, for instance from a\nnew Earth observation satellite gathering novel forms of diagnostic information, we\nmay revise our opinion on the rate of ice loss. Our assessment of such matters will\naffect the actions we take, for instance the extent to which we endeavour to reduce\nthe emission of greenhouse gasses. In such circumstances, we would like to be able\nto quantify our expression of uncertainty and make precise revisions of uncertainty in\nthe light of new evidence, as well as subsequently to be able to take optimal actions\nor decisions as a consequence. This can all be achieved through the elegant, and very\ngeneral, Bayesian interpretation of probability.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 40,
      "page_label": "21"
    }
  },
  {
    "page_content": "or decisions as a consequence. This can all be achieved through the elegant, and very\ngeneral, Bayesian interpretation of probability.\nThe use of probability to represent uncertainty, however, is not an ad-hoc choice,\nbut is inevitable if we are to respect common sense while making rational coherent\ninferences. For instance, Cox (1946) showed that if numerical values are used to\nrepresent degrees of belief, then a simple set of axioms encoding common sense\nproperties of such beliefs leads uniquely to a set of rules for manipulating degrees of\nbelief that are equivalent to the sum and product rules of probability. This provided\nthe ﬁrst rigorous proof that probability theory could be regarded as an extension of\nBoolean logic to situations involving uncertainty (Jaynes, 2003). Numerous other\nauthors have proposed different sets of properties or axioms that such measures of\nuncertainty should satisfy (Ramsey, 1931; Good, 1950; Savage, 1961; deFinetti,\n1970; Lindley, 1982). In each case, the resulting numerical quantities behave pre-\ncisely according to the rules of probability. It is therefore natural to refer to these\nquantities as (Bayesian) probabilities.\nIn the ﬁeld of pattern recognition, too, it is helpful to have a more general no-\nThomas Bayes\n1701–1761\nThomas Bayes was born in Tun-\nbridge Wells and was a clergyman\nas well as an amateur scientist and\na mathematician. He studied logic\nand theology at Edinburgh Univer-\nsity and was elected Fellow of the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 40,
      "page_label": "21"
    }
  },
  {
    "page_content": "as well as an amateur scientist and\na mathematician. He studied logic\nand theology at Edinburgh Univer-\nsity and was elected Fellow of the\nRoyal Society in 1742. During the 18 th century, is-\nsues regarding probability arose in connection with\ngambling and with the new concept of insurance. One\nparticularly important problem concerned so-called in-\nverse probability. A solution was proposed by Thomas\nBayes in his paper ‘Essay towards solving a problem\nin the doctrine of chances’, which was published in\n1764, some three years after his death, in thePhilo-\nsophical Transactions of the Royal Society . In fact,\nBayes only formulated his theory for the case of a uni-\nform prior, and it was Pierre-Simon Laplace who inde-\npendently rediscovered the theory in general form and\nwho demonstrated its broad applicability.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 40,
      "page_label": "21"
    }
  },
  {
    "page_content": "22 1. INTRODUCTION\ntion of probability. Consider the example of polynomial curve ﬁtting discussed in\nSection 1.1. It seems reasonable to apply the frequentist notion of probability to the\nrandom values of the observed variablestn. However, we would like to address and\nquantify the uncertainty that surrounds the appropriate choice for the model param-\neters w. We shall see that, from a Bayesian perspective, we can use the machinery\nof probability theory to describe the uncertainty in model parameters such asw,o r\nindeed in the choice of model itself.\nBayes’ theorem now acquires a new signiﬁcance. Recall that in the boxes of fruit\nexample, the observation of the identity of the fruit provided relevant information\nthat altered the probability that the chosen box was the red one. In that example,\nBayes’ theorem was used to convert a prior probability into a posterior probability\nby incorporating the evidence provided by the observed data. As we shall see in\ndetail later, we can adopt a similar approach when making inferences about quantities\nsuch as the parametersw in the polynomial curve ﬁtting example. We capture our\nassumptions about w, before observing the data, in the form of a prior probability\ndistribution p(w). The effect of the observed dataD = {t1,...,t N } is expressed\nthrough the conditional probabilityp(D|w), and we shall see later, in Section 1.2.5,\nhow this can be represented explicitly. Bayes’ theorem, which takes the form\np(w|D)= p(D|w)p(w)\np(D) (1.43)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 41,
      "page_label": "22"
    }
  },
  {
    "page_content": "how this can be represented explicitly. Bayes’ theorem, which takes the form\np(w|D)= p(D|w)p(w)\np(D) (1.43)\nthen allows us to evaluate the uncertainty inw after we have observedD in the form\nof the posterior probabilityp(w|D).\nThe quantity p(D|w) on the right-hand side of Bayes’ theorem is evaluated for\nthe observed data set D and can be viewed as a function of the parameter vector\nw, in which case it is called thelikelihood function. It expresses how probable the\nobserved data set is for different settings of the parameter vectorw. Note that the\nlikelihood is not a probability distribution overw, and its integral with respect tow\ndoes not (necessarily) equal one.\nGiven this deﬁnition of likelihood, we can state Bayes’ theorem in words\nposterior ∝ likelihood × prior (1.44)\nwhere all of these quantities are viewed as functions of w. The denominator in\n(1.43) is the normalization constant, which ensures that the posterior distribution\non the left-hand side is a valid probability density and integrates to one. Indeed,\nintegrating both sides of (1.43) with respect tow, we can express the denominator\nin Bayes’ theorem in terms of the prior distribution and the likelihood function\np(D)=\n∫\np(D|w)p(w)d w. (1.45)\nIn both the Bayesian and frequentist paradigms, the likelihood functionp(D|w)\nplays a central role. However, the manner in which it is used is fundamentally dif-\nferent in the two approaches. In a frequentist setting,w is considered to be a ﬁxed",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 41,
      "page_label": "22"
    }
  },
  {
    "page_content": "ferent in the two approaches. In a frequentist setting,w is considered to be a ﬁxed\nparameter, whose value is determined by some form of ‘estimator’, and error bars",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 41,
      "page_label": "22"
    }
  },
  {
    "page_content": "1.2. Probability Theory 23\non this estimate are obtained by considering the distribution of possible data setsD.\nBy contrast, from the Bayesian viewpoint there is only a single data setD (namely\nthe one that is actually observed), and the uncertainty in the parameters is expressed\nthrough a probability distribution overw.\nA widely used frequentist estimator ismaximum likelihood, in which w is set\nto the value that maximizes the likelihood functionp(D|w). This corresponds to\nchoosing the value ofw for which the probability of the observed data set is maxi-\nmized. In the machine learning literature, the negative log of the likelihood function\nis called anerror function. Because the negative logarithm is a monotonically de-\ncreasing function, maximizing the likelihood is equivalent to minimizing the error.\nOne approach to determining frequentist error bars is thebootstrap (Efron, 1979;\nHastie et al., 2001), in which multiple data sets are created as follows. Suppose our\noriginal data set consists ofN data points X = {x1,..., xN }. We can create a new\ndata setXB by drawingN points at random fromX, with replacement, so that some\npoints inX may be replicated inXB, whereas other points inX may be absent from\nXB. This process can be repeatedL times to generateL data sets each of sizeN and\neach obtained by sampling from the original data setX. The statistical accuracy of\nparameter estimates can then be evaluated by looking at the variability of predictions",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 42,
      "page_label": "23"
    }
  },
  {
    "page_content": "parameter estimates can then be evaluated by looking at the variability of predictions\nbetween the different bootstrap data sets.\nOne advantage of the Bayesian viewpoint is that the inclusion of prior knowl-\nedge arises naturally. Suppose, for instance, that a fair-looking coin is tossed three\ntimes and lands heads each time. A classical maximum likelihood estimate of the\nprobability of landing heads would give 1, implying that all future tosses will landSection 2.1\nheads! By contrast, a Bayesian approach with any reasonable prior will lead to a\nmuch less extreme conclusion.\nThere has been much controversy and debate associated with the relative mer-\nits of the frequentist and Bayesian paradigms, which have not been helped by the\nfact that there is no unique frequentist, or even Bayesian, viewpoint. For instance,\none common criticism of the Bayesian approach is that the prior distribution is of-\nten selected on the basis of mathematical convenience rather than as a reﬂection of\nany prior beliefs. Even the subjective nature of the conclusions through their de-\npendence on the choice of prior is seen by some as a source of difﬁculty. Reducing\nthe dependence on the prior is one motivation for so-callednoninformative priors.Section 2.4.3\nHowever, these lead to difﬁculties when comparing different models, and indeed\nBayesian methods based on poor choices of prior can give poor results with high\nconﬁdence. Frequentist evaluation methods offer some protection from such prob-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 42,
      "page_label": "23"
    }
  },
  {
    "page_content": "conﬁdence. Frequentist evaluation methods offer some protection from such prob-\nlems, and techniques such as cross-validation remain useful in areas such as modelSection 1.3\ncomparison.\nThis book places a strong emphasis on the Bayesian viewpoint, reﬂecting the\nhuge growth in the practical importance of Bayesian methods in the past few years,\nwhile also discussing useful frequentist concepts as required.\nAlthough the Bayesian framework has its origins in the 18th century, the prac-\ntical application of Bayesian methods was for a long time severely limited by the\ndifﬁculties in carrying through the full Bayesian procedure, particularly the need to\nmarginalize (sum or integrate) over the whole of parameter space, which, as we shall",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 42,
      "page_label": "23"
    }
  },
  {
    "page_content": "24 1. INTRODUCTION\nsee, is required in order to make predictions or to compare different models. The\ndevelopment of sampling methods, such as Markov chain Monte Carlo (discussed in\nChapter 11) along with dramatic improvements in the speed and memory capacity\nof computers, opened the door to the practical use of Bayesian techniques in an im-\npressive range of problem domains. Monte Carlo methods are very ﬂexible and can\nbe applied to a wide range of models. However, they are computationally intensive\nand have mainly been used for small-scale problems.\nMore recently, highly efﬁcient deterministic approximation schemes such as\nvariational Bayes and expectation propagation (discussed in Chapter 10) have been\ndeveloped. These offer a complementary alternative to sampling methods and have\nallowed Bayesian techniques to be used in large-scale applications (Bleiet al., 2003).\n1.2.4 The Gaussian distribution\nWe shall devote the whole of Chapter 2 to a study of various probability dis-\ntributions and their key properties. It is convenient, however, to introduce here one\nof the most important probability distributions for continuous variables, called the\nnormal or Gaussian distribution. We shall make extensive use of this distribution in\nthe remainder of this chapter and indeed throughout much of the book.\nFor the case of a single real-valued variablex, the Gaussian distribution is de-\nﬁned by\nN\n(\nx|µ, σ2)\n= 1\n(2πσ2)1/2 exp\n{\n− 1\n2σ2 (x − µ)2\n}\n(1.46)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 43,
      "page_label": "24"
    }
  },
  {
    "page_content": "For the case of a single real-valued variablex, the Gaussian distribution is de-\nﬁned by\nN\n(\nx|µ, σ2)\n= 1\n(2πσ2)1/2 exp\n{\n− 1\n2σ2 (x − µ)2\n}\n(1.46)\nwhich is governed by two parameters:µ, called the mean, and σ2, called the vari-\nance. The square root of the variance, given byσ, is called thestandard deviation,\nand the reciprocal of the variance, written asβ =1 /σ2, is called theprecision.W e\nshall see the motivation for these terms shortly. Figure 1.13 shows a plot of the\nGaussian distribution.\nFrom the form of (1.46) we see that the Gaussian distribution satisﬁes\nN(x|µ, σ2) > 0. (1.47)\nAlso it is straightforward to show that the Gaussian is normalized, so thatExercise 1.7\nPierre-Simon Laplace\n1749–1827\nIt is said that Laplace was seri-\nously lacking in modesty and at one\npoint declared himself to be the\nbest mathematician in France at the\ntime, a claim that was arguably true.\nAs well as being proliﬁc in mathe-\nmatics, he also made numerous contributions to as-\ntronomy, including the nebular hypothesis by which the\nearth is thought to have formed from the condensa-\ntion and cooling of a large rotating disk of gas and\ndust. In 1812 he published the ﬁrst edition ofTh´eorie\nAnalytique des Probabilit ´es, in which Laplace states\nthat “probability theory is nothing but common sense\nreduced to calculation”. This work included a discus-\nsion of the inverse probability calculation (later termed\nBayes’ theorem by Poincar´e), which he used to solve",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 43,
      "page_label": "24"
    }
  },
  {
    "page_content": "sion of the inverse probability calculation (later termed\nBayes’ theorem by Poincar´e), which he used to solve\nproblems in life expectancy, jurisprudence, planetary\nmasses, triangulation, and error estimation.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 43,
      "page_label": "24"
    }
  },
  {
    "page_content": "1.2. Probability Theory 25\nFigure 1.13 Plot of the univariate Gaussian\nshowing the mean µ and the\nstandard deviation σ.\nN(x|µ, σ2)\nx\n2σ\nµ\n∫ ∞\n−∞\nN\n(\nx|µ, σ2)\ndx =1 . (1.48)\nThus (1.46) satisﬁes the two requirements for a valid probability density.\nWe can readily ﬁnd expectations of functions ofx under the Gaussian distribu-\ntion. In particular, the average value ofx is given byExercise 1.8\nE[x]=\n∫ ∞\n−∞\nN\n(\nx|µ, σ2)\nxdx = µ. (1.49)\nBecause the parameterµ represents the average value ofx under the distribution, it\nis referred to as the mean. Similarly, for the second order moment\nE[x2]=\n∫ ∞\n−∞\nN\n(\nx|µ, σ2)\nx2 dx = µ2 + σ2. (1.50)\nFrom (1.49) and (1.50), it follows that the variance ofx is given by\nvar[x]= E[x2] − E[x]2 = σ2 (1.51)\nand henceσ2 is referred to as the variance parameter. The maximum of a distribution\nis known as its mode. For a Gaussian, the mode coincides with the mean.Exercise 1.9\nWe are also interested in the Gaussian distribution deﬁned over aD-dimensional\nvector x of continuous variables, which is given by\nN(x|µ,Σ)= 1\n(2π)D/2\n1\n|Σ|1/2 exp\n{\n−1\n2(x − µ)TΣ−1(x − µ)\n}\n(1.52)\nwhere theD-dimensional vectorµ is called the mean, theD × D matrix Σ is called\nthe covariance, and |Σ| denotes the determinant of Σ. We shall make use of the\nmultivariate Gaussian distribution brieﬂy in this chapter, although its properties will\nbe studied in detail in Section 2.3.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 44,
      "page_label": "25"
    }
  },
  {
    "page_content": "26 1. INTRODUCTION\nFigure 1.14 Illustration of the likelihood function for\na Gaussian distribution, shown by the\nred curve. Here the black points de-\nnote a data set of values {xn}, and\nthe likelihood function given by (1.53)\ncorresponds to the product of the blue\nvalues. Maximizing the likelihood in-\nvolves adjusting the mean and vari-\nance of the Gaussian so as to maxi-\nmize this product.\nx\np(x)\nxn\nN(xn|µ, σ2)\nNow suppose that we have a data set of observationsx =( x1,...,x N )T, rep-\nresenting N observations of the scalar variablex. Note that we are using the type-\nface x to distinguish this from a single observation of the vector-valued variable\n(x1,...,x D)T, which we denote byx. We shall suppose that the observations are\ndrawn independently from a Gaussian distribution whose meanµ and variance σ2\nare unknown, and we would like to determine these parameters from the data set.\nData points that are drawn independently from the same distribution are said to be\nindependent and identically distributed, which is often abbreviated to i.i.d. We have\nseen that the joint probability of two independent events is given by the product of\nthe marginal probabilities for each event separately. Because our data setx is i.i.d.,\nwe can therefore write the probability of the data set, givenµ and σ2, in the form\np(x|µ, σ2)=\nN∏\nn=1\nN\n(\nxn|µ, σ2)\n. (1.53)\nWhen viewed as a function ofµ and σ2, this is the likelihood function for the Gaus-\nsian and is interpreted diagrammatically in Figure 1.14.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 45,
      "page_label": "26"
    }
  },
  {
    "page_content": ". (1.53)\nWhen viewed as a function ofµ and σ2, this is the likelihood function for the Gaus-\nsian and is interpreted diagrammatically in Figure 1.14.\nOne common criterion for determining the parameters in a probability distribu-\ntion using an observed data set is to ﬁnd the parameter values that maximize the\nlikelihood function. This might seem like a strange criterion because, from our fore-\ngoing discussion of probability theory, it would seem more natural to maximize the\nprobability of the parameters given the data, not the probability of the data given the\nparameters. In fact, these two criteria are related, as we shall discuss in the context\nof curve ﬁtting.Section 1.2.5\nFor the moment, however, we shall determine values for the unknown parame-\nters µ and σ2 in the Gaussian by maximizing the likelihood function (1.53). In prac-\ntice, it is more convenient to maximize the log of the likelihood function. Because\nthe logarithm is a monotonically increasing function of its argument, maximization\nof the log of a function is equivalent to maximization of the function itself. Taking\nthe log not only simpliﬁes the subsequent mathematical analysis, but it also helps\nnumerically because the product of a large number of small probabilities can easily\nunderﬂow the numerical precision of the computer, and this is resolved by computing\ninstead the sum of the log probabilities. From (1.46) and (1.53), the log likelihood",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 45,
      "page_label": "26"
    }
  },
  {
    "page_content": "1.2. Probability Theory 27\nfunction can be written in the form\nlnp\n(\nx|µ, σ2)\n= − 1\n2σ2\nN∑\nn=1\n(xn − µ)2 − N\n2 lnσ2 − N\n2 ln(2π). (1.54)\nMaximizing (1.54) with respect to µ, we obtain the maximum likelihood solution\ngiven byExercise 1.11\nµML = 1\nN\nN∑\nn=1\nxn (1.55)\nwhich is the sample mean, i.e., the mean of the observed values{xn}. Similarly,\nmaximizing (1.54) with respect toσ2, we obtain the maximum likelihood solution\nfor the variance in the form\nσ2\nML = 1\nN\nN∑\nn=1\n(xn − µML)2 (1.56)\nwhich is thesample variance measured with respect to the sample meanµML. Note\nthat we are performing a joint maximization of (1.54) with respect toµ and σ2,b u t\nin the case of the Gaussian distribution the solution forµ decouples from that forσ2\nso that we can ﬁrst evaluate (1.55) and then subsequently use this result to evaluate\n(1.56).\nLater in this chapter, and also in subsequent chapters, we shall highlight the sig-\nniﬁcant limitations of the maximum likelihood approach. Here we give an indication\nof the problem in the context of our solutions for the maximum likelihood param-\neter settings for the univariate Gaussian distribution. In particular, we shall show\nthat the maximum likelihood approach systematically underestimates the variance\nof the distribution. This is an example of a phenomenon calledbias and is related\nto the problem of over-ﬁtting encountered in the context of polynomial curve ﬁtting.Section 1.1\nWe ﬁrst note that the maximum likelihood solutionsµML and σ2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 46,
      "page_label": "27"
    }
  },
  {
    "page_content": "We ﬁrst note that the maximum likelihood solutionsµML and σ2\nML are functions of\nthe data set values x1,...,x N . Consider the expectations of these quantities with\nrespect to the data set values, which themselves come from a Gaussian distribution\nwith parameters µ and σ2. It is straightforward to show thatExercise 1.12\nE[µML]= µ (1.57)\nE[σ2\nML]=\n( N − 1\nN\n)\nσ2 (1.58)\nso that on average the maximum likelihood estimate will obtain the correct mean but\nwill underestimate the true variance by a factor(N − 1)/N. The intuition behind\nthis result is given by Figure 1.15.\nFrom (1.58) it follows that the following estimate for the variance parameter is\nunbiased\n˜σ2 = N\nN − 1σ2\nML = 1\nN − 1\nN∑\nn=1\n(xn − µML)2. (1.59)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 46,
      "page_label": "27"
    }
  },
  {
    "page_content": "28 1. INTRODUCTION\nFigure 1.15 Illustration of how bias arises in using max-\nimum likelihood to determine the variance\nof a Gaussian. The green curve shows\nthe true Gaussian distribution from which\ndata is generated, and the three red curves\nshow the Gaussian distributions obtained\nby ﬁtting to three data sets, each consist-\ning of two data points shown in blue, us-\ning the maximum likelihood results (1.55)\nand (1.56). Averaged across the three data\nsets, the mean is correct, but the variance\nis systematically under-estimated because\nit is measured relative to the sample mean\nand not relative to the true mean.\n(a)\n(b)\n(c)\nIn Section 10.1.3, we shall see how this result arises automatically when we adopt a\nBayesian approach.\nNote that the bias of the maximum likelihood solution becomes less signiﬁcant\nas the number N of data points increases, and in the limitN →∞ the maximum\nlikelihood solution for the variance equals the true variance of the distribution that\ngenerated the data. In practice, for anything other than smallN, this bias will not\nprove to be a serious problem. However, throughout this book we shall be interested\nin more complex models with many parameters, for which the bias problems asso-\nciated with maximum likelihood will be much more severe. In fact, as we shall see,\nthe issue of bias in maximum likelihood lies at the root of the over-ﬁtting problem\nthat we encountered earlier in the context of polynomial curve ﬁtting.\n1.2.5 Curve ﬁtting re-visited",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 47,
      "page_label": "28"
    }
  },
  {
    "page_content": "that we encountered earlier in the context of polynomial curve ﬁtting.\n1.2.5 Curve ﬁtting re-visited\nWe have seen how the problem of polynomial curve ﬁtting can be expressed in\nterms of error minimization. Here we return to the curve ﬁtting example and view itSection 1.1\nfrom a probabilistic perspective, thereby gaining some insights into error functions\nand regularization, as well as taking us towards a full Bayesian treatment.\nThe goal in the curve ﬁtting problem is to be able to make predictions for the\ntarget variablet given some new value of the input variablex on the basis of a set of\ntraining data comprisingN input valuesx =( x1,...,x N )T and their corresponding\ntarget values t =( t1,...,t N )T. We can express our uncertainty over the value of\nthe target variable using a probability distribution. For this purpose, we shall assume\nthat, given the value ofx, the corresponding value oft has a Gaussian distribution\nwith a mean equal to the valuey(x,w) of the polynomial curve given by (1.1). Thus\nwe have\np(t|x, w,β )= N\n(\nt|y(x,w),β −1)\n(1.60)\nwhere, for consistency with the notation in later chapters, we have deﬁned a preci-\nsion parameter β corresponding to the inverse variance of the distribution. This is\nillustrated schematically in Figure 1.16.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 47,
      "page_label": "28"
    }
  },
  {
    "page_content": "1.2. Probability Theory 29\nFigure 1.16 Schematic illustration of a Gaus-\nsian conditional distribution for t given x given by\n(1.60), in which the mean is given by the polyno-\nmial function y(x, w), and the precision is given\nby the parameter β, which is related to the vari-\nance by β−1 = σ2.\nt\nxx0\n2σy(x0,w)\ny(x,w)\np(t|x0, w,β )\nWe now use the training data {x, t} to determine the values of the unknown\nparameters w and β by maximum likelihood. If the data are assumed to be drawn\nindependently from the distribution (1.60), then the likelihood function is given by\np(t|x, w,β )=\nN∏\nn=1\nN\n(\ntn|y(xn, w),β −1)\n. (1.61)\nAs we did in the case of the simple Gaussian distribution earlier, it is convenient to\nmaximize the logarithm of the likelihood function. Substituting for the form of the\nGaussian distribution, given by (1.46), we obtain the log likelihood function in the\nform\nlnp(t|x, w,β )= −β\n2\nN∑\nn=1\n{y(xn, w) − tn}2 + N\n2 ln β − N\n2 ln(2π). (1.62)\nConsider ﬁrst the determination of the maximum likelihood solution for the polyno-\nmial coefﬁcients, which will be denoted bywML. These are determined by maxi-\nmizing (1.62) with respect tow. For this purpose, we can omit the last two terms\non the right-hand side of (1.62) because they do not depend onw. Also, we note\nthat scaling the log likelihood by a positive constant coefﬁcient does not alter the\nlocation of the maximum with respect tow, and so we can replace the coefﬁcient",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 48,
      "page_label": "29"
    }
  },
  {
    "page_content": "location of the maximum with respect tow, and so we can replace the coefﬁcient\nβ/2 with 1/2. Finally, instead of maximizing the log likelihood, we can equivalently\nminimize the negative log likelihood. We therefore see that maximizing likelihood is\nequivalent, so far as determiningw is concerned, to minimizing thesum-of-squares\nerror functiondeﬁned by (1.2). Thus the sum-of-squares error function has arisen as\na consequence of maximizing likelihood under the assumption of a Gaussian noise\ndistribution.\nWe can also use maximum likelihood to determine the precision parameterβ of\nthe Gaussian conditional distribution. Maximizing (1.62) with respect toβ gives\n1\nβML\n= 1\nN\nN∑\nn=1\n{y(xn, wML) − tn}2 . (1.63)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 48,
      "page_label": "29"
    }
  },
  {
    "page_content": "30 1. INTRODUCTION\nAgain we can ﬁrst determine the parameter vectorwML governing the mean and sub-\nsequently use this to ﬁnd the precisionβML as was the case for the simple Gaussian\ndistribution.Section 1.2.4\nHaving determined the parametersw and β, we can now make predictions for\nnew values ofx. Because we now have a probabilistic model, these are expressed\nin terms of thepredictive distribution that gives the probability distribution overt,\nrather than simply a point estimate, and is obtained by substituting the maximum\nlikelihood parameters into (1.60) to give\np(t|x, wML,βML)= N\n(\nt|y(x,wML),β −1\nML\n)\n. (1.64)\nNow let us take a step towards a more Bayesian approach and introduce a prior\ndistribution over the polynomial coefﬁcients w. For simplicity, let us consider a\nGaussian distribution of the form\np(w|α)= N(w|0,α −1I)=\n( α\n2π\n)(M+1)/2\nexp\n{\n−α\n2 wTw\n}\n(1.65)\nwhere α is the precision of the distribution, andM+1 is the total number of elements\nin the vector w for an Mth order polynomial. V ariables such asα, which control\nthe distribution of model parameters, are called hyperparameters. Using Bayes’\ntheorem, the posterior distribution forw is proportional to the product of the prior\ndistribution and the likelihood function\np(w|x, t,α ,β) ∝ p(t|x, w,β )p(w|α). (1.66)\nWe can now determinew by ﬁnding the most probable value ofw given the data,\nin other words by maximizing the posterior distribution. This technique is called",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 49,
      "page_label": "30"
    }
  },
  {
    "page_content": "in other words by maximizing the posterior distribution. This technique is called\nmaximum posterior, or simply MAP. Taking the negative logarithm of (1.66) and\ncombining with (1.62) and (1.65), we ﬁnd that the maximum of the posterior is\ngiven by the minimum of\nβ\n2\nN∑\nn=1\n{y(xn, w) − tn}2 + α\n2 wTw. (1.67)\nThus we see that maximizing the posterior distribution is equivalent to minimizing\nthe regularized sum-of-squares error function encountered earlier in the form (1.4),\nwith a regularization parameter given byλ = α/β.\n1.2.6 Bayesian curve ﬁtting\nAlthough we have included a prior distributionp(w|α), we are so far still mak-\ning a point estimate ofw and so this does not yet amount to a Bayesian treatment. In\na fully Bayesian approach, we should consistently apply the sum and product rules\nof probability, which requires, as we shall see shortly, that we integrate over all val-\nues ofw. Such marginalizations lie at the heart of Bayesian methods for pattern\nrecognition.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 49,
      "page_label": "30"
    }
  },
  {
    "page_content": "1.2. Probability Theory 31\nIn the curve ﬁtting problem, we are given the training datax and t, along with\na new test point x, and our goal is to predict the value of t. We therefore wish\nto evaluate the predictive distribution p(t|x, x, t). Here we shall assume that the\nparameters α and β are ﬁxed and known in advance (in later chapters we shall discuss\nhow such parameters can be inferred from data in a Bayesian setting).\nA Bayesian treatment simply corresponds to a consistent application of the sum\nand product rules of probability, which allow the predictive distribution to be written\nin the form\np(t|x, x, t)=\n∫\np(t|x,w)p(w|x, t)d w. (1.68)\nHere p(t|x, w) is given by (1.60), and we have omitted the dependence onα and\nβ to simplify the notation. Herep(w|x,t) is the posterior distribution over param-\neters, and can be found by normalizing the right-hand side of (1.66). We shall see\nin Section 3.3 that, for problems such as the curve-ﬁtting example, this posterior\ndistribution is a Gaussian and can be evaluated analytically. Similarly, the integra-\ntion in (1.68) can also be performed analytically with the result that the predictive\ndistribution is given by a Gaussian of the form\np(t|x, x, t)= N\n(\nt|m(x),s 2(x)\n)\n(1.69)\nwhere the mean and variance are given by\nm(x)= βφ(x)TS\nN∑\nn=1\nφ(xn)tn (1.70)\ns2(x)= β−1 + φ(x)TSφ(x). (1.71)\nHere the matrixS is given by\nS−1 = αI + β\nN∑\nn=1\nφ(xn)φ(x)T (1.72)\nwhere I is the unit matrix, and we have deﬁned the vector φ(x) with elements",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 50,
      "page_label": "31"
    }
  },
  {
    "page_content": "Here the matrixS is given by\nS−1 = αI + β\nN∑\nn=1\nφ(xn)φ(x)T (1.72)\nwhere I is the unit matrix, and we have deﬁned the vector φ(x) with elements\nφi(x)= xi for i =0 ,...,M .\nWe see that the variance, as well as the mean, of the predictive distribution in\n(1.69) is dependent onx. The ﬁrst term in (1.71) represents the uncertainty in the\npredicted value oft due to the noise on the target variables and was expressed already\nin the maximum likelihood predictive distribution (1.64) throughβ−1\nML. However, the\nsecond term arises from the uncertainty in the parametersw and is a consequence\nof the Bayesian treatment. The predictive distribution for the synthetic sinusoidal\nregression problem is illustrated in Figure 1.17.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 50,
      "page_label": "31"
    }
  },
  {
    "page_content": "32 1. INTRODUCTION\nFigure 1.17 The predictive distribution result-\ning from a Bayesian treatment of\npolynomial curve ﬁtting using an\nM =9 polynomial, with the ﬁxed\nparameters α =5 ×10−3 and β =\n11.1 (corresponding to the known\nnoise variance), in which the red\ncurve denotes the mean of the\npredictive distribution and the red\nregion corresponds to ±1 stan-\ndard deviation around the mean.\nx\nt\n0 1\n−1\n0\n1\n1.3. Model Selection\nIn our example of polynomial curve ﬁtting using least squares, we saw that there was\nan optimal order of polynomial that gave the best generalization. The order of the\npolynomial controls the number of free parameters in the model and thereby governs\nthe model complexity. With regularized least squares, the regularization coefﬁcient\nλ also controls the effective complexity of the model, whereas for more complex\nmodels, such as mixture distributions or neural networks there may be multiple pa-\nrameters governing complexity. In a practical application, we need to determine\nthe values of such parameters, and the principal objective in doing so is usually to\nachieve the best predictive performance on new data. Furthermore, as well as ﬁnd-\ning the appropriate values for complexity parameters within a given model, we may\nwish to consider a range of different types of model in order to ﬁnd the best one for\nour particular application.\nWe have already seen that, in the maximum likelihood approach, the perfor-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 51,
      "page_label": "32"
    }
  },
  {
    "page_content": "our particular application.\nWe have already seen that, in the maximum likelihood approach, the perfor-\nmance on the training set is not a good indicator of predictive performance on un-\nseen data due to the problem of over-ﬁtting. If data is plentiful, then one approach is\nsimply to use some of the available data to train a range of models, or a given model\nwith a range of values for its complexity parameters, and then to compare them on\nindependent data, sometimes called avalidation set, and select the one having the\nbest predictive performance. If the model design is iterated many times using a lim-\nited size data set, then some over-ﬁtting to the validation data can occur and so it may\nbe necessary to keep aside a thirdtest set on which the performance of the selected\nmodel is ﬁnally evaluated.\nIn many applications, however, the supply of data for training and testing will be\nlimited, and in order to build good models, we wish to use as much of the available\ndata as possible for training. However, if the validation set is small, it will give a\nrelatively noisy estimate of predictive performance. One solution to this dilemma is\nto use cross-validation, which is illustrated in Figure 1.18. This allows a proportion\n(S −1)/S of the available data to be used for training while making use of all of the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 51,
      "page_label": "32"
    }
  },
  {
    "page_content": "1.4. The Curse of Dimensionality 33\nFigure 1.18 The technique of S-fold cross-validation, illus-\ntrated here for the case of S =4 , involves tak-\ning the available data and partitioning it into S\ngroups (in the simplest case these are of equal\nsize). Then S − 1 of the groups are used to train\na set of models that are then evaluated on the re-\nmaining group. This procedure is then repeated\nfor all S possible choices for the held-out group,\nindicated here by the red blocks, and the perfor-\nmance scores from the S runs are then averaged.\nrun 1\nrun 2\nrun 3\nrun 4\ndata to assess performance. When data is particularly scarce, it may be appropriate\nto consider the caseS = N, whereN is the total number of data points, which gives\nthe leave-one-out technique.\nOne major drawback of cross-validation is that the number of training runs that\nmust be performed is increased by a factor ofS, and this can prove problematic for\nmodels in which the training is itself computationally expensive. A further problem\nwith techniques such as cross-validation that use separate data to assess performance\nis that we might have multiple complexity parameters for a single model (for in-\nstance, there might be several regularization parameters). Exploring combinations\nof settings for such parameters could, in the worst case, require a number of training\nruns that is exponential in the number of parameters. Clearly, we need a better ap-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 52,
      "page_label": "33"
    }
  },
  {
    "page_content": "runs that is exponential in the number of parameters. Clearly, we need a better ap-\nproach. Ideally, this should rely only on the training data and should allow multiple\nhyperparameters and model types to be compared in a single training run. We there-\nfore need to ﬁnd a measure of performance which depends only on the training data\nand which does not suffer from bias due to over-ﬁtting.\nHistorically various ‘information criteria’ have been proposed that attempt to\ncorrect for the bias of maximum likelihood by the addition of a penalty term to\ncompensate for the over-ﬁtting of more complex models. For example, theAkaike\ninformation criterion, or AIC (Akaike, 1974), chooses the model for which the quan-\ntity\nlnp(D|wML) − M (1.73)\nis largest. Here p(D|wML) is the best-ﬁt log likelihood, andM is the number of\nadjustable parameters in the model. A variant of this quantity, called theBayesian\ninformation criterion,o r BIC, will be discussed in Section 4.4.1. Such criteria do\nnot take account of the uncertainty in the model parameters, however, and in practice\nthey tend to favour overly simple models. We therefore turn in Section 3.4 to a fully\nBayesian approach where we shall see how complexity penalties arise in a natural\nand principled way.\n1.4. The Curse of Dimensionality\nIn the polynomial curve ﬁtting example we had just one input variablex. For prac-\ntical applications of pattern recognition, however, we will have to deal with spaces",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 52,
      "page_label": "33"
    }
  },
  {
    "page_content": "34 1. INTRODUCTION\nFigure 1.19 Scatter plot of the oil ﬂow data\nfor input variables x6 and x7,i n\nwhich red denotes the ‘homoge-\nnous’ class, green denotes the\n‘annular’ class, and blue denotes\nthe ‘laminar’ class. Our goal is\nto classify the new test point de-\nnoted by ‘×’.\nx6\nx7\n0 0.25 0.5 0.75 1\n0\n0.5\n1\n1.5\n2\nof high dimensionality comprising many input variables. As we now discuss, this\nposes some serious challenges and is an important factor inﬂuencing the design of\npattern recognition techniques.\nIn order to illustrate the problem we consider a synthetically generated data set\nrepresenting measurements taken from a pipeline containing a mixture of oil, wa-\nter, and gas (Bishop and James, 1993). These three materials can be present in one\nof three different geometrical conﬁgurations known as ‘homogenous’, ‘annular’, and\n‘laminar’, and the fractions of the three materials can also vary. Each data point com-\nprises a 12-dimensional input vector consisting of measurements taken with gamma\nray densitometers that measure the attenuation of gamma rays passing along nar-\nrow beams through the pipe. This data set is described in detail in Appendix A.\nFigure 1.19 shows100 points from this data set on a plot showing two of the mea-\nsurements x6 and x7 (the remaining ten input values are ignored for the purposes of\nthis illustration). Each data point is labelled according to which of the three geomet-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 53,
      "page_label": "34"
    }
  },
  {
    "page_content": "this illustration). Each data point is labelled according to which of the three geomet-\nrical classes it belongs to, and our goal is to use this data as a training set in order to\nbe able to classify a new observation(x6,x7), such as the one denoted by the cross\nin Figure 1.19. We observe that the cross is surrounded by numerous red points, and\nso we might suppose that it belongs to the red class. However, there are also plenty\nof green points nearby, so we might think that it could instead belong to the green\nclass. It seems unlikely that it belongs to the blue class. The intuition here is that the\nidentity of the cross should be determined more strongly by nearby points from the\ntraining set and less strongly by more distant points. In fact, this intuition turns out\nto be reasonable and will be discussed more fully in later chapters.\nHow can we turn this intuition into a learning algorithm? One very simple ap-\nproach would be to divide the input space into regular cells, as indicated in Fig-\nure 1.20. When we are given a test point and we wish to predict its class, we ﬁrst\ndecide which cell it belongs to, and we then ﬁnd all of the training data points that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 53,
      "page_label": "34"
    }
  },
  {
    "page_content": "1.4. The Curse of Dimensionality 35\nFigure 1.20 Illustration of a simple approach\nto the solution of a classiﬁcation\nproblem in which the input space\nis divided into cells and any new\ntest point is assigned to the class\nthat has a majority number of rep-\nresentatives in the same cell as\nthe test point. As we shall see\nshortly, this simplistic approach\nhas some severe shortcomings.\nx6\nx7\n0 0.25 0.5 0.75 1\n0\n0.5\n1\n1.5\n2\nfall in the same cell. The identity of the test point is predicted as being the same\nas the class having the largest number of training points in the same cell as the test\npoint (with ties being broken at random).\nThere are numerous problems with this naive approach, but one of the most se-\nvere becomes apparent when we consider its extension to problems having larger\nnumbers of input variables, corresponding to input spaces of higher dimensionality.\nThe origin of the problem is illustrated in Figure 1.21, which shows that, if we divide\na region of a space into regular cells, then the number of such cells grows exponen-\ntially with the dimensionality of the space. The problem with an exponentially large\nnumber of cells is that we would need an exponentially large quantity of training data\nin order to ensure that the cells are not empty. Clearly, we have no hope of applying\nsuch a technique in a space of more than a few variables, and so we need to ﬁnd a\nmore sophisticated approach.\nWe can gain further insight into the problems of high-dimensional spaces by",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 54,
      "page_label": "35"
    }
  },
  {
    "page_content": "more sophisticated approach.\nWe can gain further insight into the problems of high-dimensional spaces by\nreturning to the example of polynomial curve ﬁtting and considering how we wouldSection 1.1\nFigure 1.21 Illustration of the\ncurse of dimensionality, showing\nhow the number of regions of a\nregular grid grows exponentially\nwith the dimensionality D of the\nspace. For clarity, only a subset of\nthe cubical regions are shown for\nD =3 .\nx1\nD =1\nx1\nx2\nD =2\nx1\nx2\nx3\nD =3",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 54,
      "page_label": "35"
    }
  },
  {
    "page_content": "36 1. INTRODUCTION\nextend this approach to deal with input spaces having several variables. If we have\nD input variables, then a general polynomial with coefﬁcients up to order3 would\ntake the form\ny(x,w)= w0 +\nD∑\ni=1\nwixi +\nD∑\ni=1\nD∑\nj=1\nwijxixj +\nD∑\ni=1\nD∑\nj=1\nD∑\nk=1\nwijkxixjxk. (1.74)\nAs D increases, so the number of independent coefﬁcients (not all of the coefﬁcients\nare independent due to interchange symmetries amongst thex variables) grows pro-\nportionally toD3. In practice, to capture complex dependencies in the data, we may\nneed to use a higher-order polynomial. For a polynomial of orderM, the growth in\nthe number of coefﬁcients is likeDM . Although this is now a power law growth,Exercise 1.16\nrather than an exponential growth, it still points to the method becoming rapidly\nunwieldy and of limited practical utility.\nOur geometrical intuitions, formed through a life spent in a space of three di-\nmensions, can fail badly when we consider spaces of higher dimensionality. As a\nsimple example, consider a sphere of radiusr =1 in a space ofD dimensions, and\nask what is the fraction of the volume of the sphere that lies between radiusr =1 −ϵ\nand r =1 . We can evaluate this fraction by noting that the volume of a sphere of\nradius r in D dimensions must scale asrD, and so we write\nVD(r)= KDrD (1.75)\nwhere the constantKD depends only onD. Thus the required fraction is given byExercise 1.18\nVD(1) − VD(1 − ϵ)\nVD(1) =1 − (1 − ϵ)D (1.76)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 55,
      "page_label": "36"
    }
  },
  {
    "page_content": "where the constantKD depends only onD. Thus the required fraction is given byExercise 1.18\nVD(1) − VD(1 − ϵ)\nVD(1) =1 − (1 − ϵ)D (1.76)\nwhich is plotted as a function ofϵ for various values ofD in Figure 1.22. We see\nthat, for largeD, this fraction tends to1 even for small values ofϵ. Thus, in spaces\nof high dimensionality, most of the volume of a sphere is concentrated in a thin shell\nnear the surface!\nAs a further example, of direct relevance to pattern recognition, consider the\nbehaviour of a Gaussian distribution in a high-dimensional space. If we transform\nfrom Cartesian to polar coordinates, and then integrate out the directional variables,\nwe obtain an expression for the densityp(r) as a function of radiusr from the origin.Exercise 1.20\nThus p(r)δr is the probability mass inside a thin shell of thicknessδr located at\nradius r. This distribution is plotted, for various values ofD, in Figure 1.23, and we\nsee that for largeD the probability mass of the Gaussian is concentrated in a thin\nshell.\nThe severe difﬁculty that can arise in spaces of many dimensions is sometimes\ncalled the curse of dimensionality(Bellman, 1961). In this book, we shall make ex-\ntensive use of illustrative examples involving input spaces of one or two dimensions,\nbecause this makes it particularly easy to illustrate the techniques graphically. The\nreader should be warned, however, that not all intuitions developed in spaces of low\ndimensionality will generalize to spaces of many dimensions.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 55,
      "page_label": "36"
    }
  },
  {
    "page_content": "1.4. The Curse of Dimensionality 37\nFigure 1.22 Plot of the fraction of the volume of\na sphere lying in the ranger =1 −ϵ\nto r =1 for various values of the\ndimensionality D.\nϵ\nvolume fraction\nD =1\nD =2\nD =5\nD =2 0\n0 0.2 0.4 0.6 0.8 1\n0\n0.2\n0.4\n0.6\n0.8\n1\nAlthough the curse of dimensionality certainly raises important issues for pat-\ntern recognition applications, it does not prevent us from ﬁnding effective techniques\napplicable to high-dimensional spaces. The reasons for this are twofold. First, real\ndata will often be conﬁned to a region of the space having lower effective dimension-\nality, and in particular the directions over which important variations in the target\nvariables occur may be so conﬁned. Second, real data will typically exhibit some\nsmoothness properties (at least locally) so that for the most part small changes in the\ninput variables will produce small changes in the target variables, and so we can ex-\nploit local interpolation-like techniques to allow us to make predictions of the target\nvariables for new values of the input variables. Successful pattern recognition tech-\nniques exploit one or both of these properties. Consider, for example, an application\nin manufacturing in which images are captured of identical planar objects on a con-\nveyor belt, in which the goal is to determine their orientation. Each image is a point\nFigure 1.23 Plot of the probability density with\nrespect to radius r of a Gaus-\nsian distribution for various values",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 56,
      "page_label": "37"
    }
  },
  {
    "page_content": "Figure 1.23 Plot of the probability density with\nrespect to radius r of a Gaus-\nsian distribution for various values\nof the dimensionality D.I n a\nhigh-dimensional space, most of the\nprobability mass of a Gaussian is lo-\ncated within a thin shell at a speciﬁc\nradius.\nD =1\nD =2\nD =2 0\nr\np(r)\n0 2 4\n0\n1\n2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 56,
      "page_label": "37"
    }
  },
  {
    "page_content": "38 1. INTRODUCTION\nin a high-dimensional space whose dimensionality is determined by the number of\npixels. Because the objects can occur at different positions within the image and\nin different orientations, there are three degrees of freedom of variability between\nimages, and a set of images will live on a three dimensionalmanifold embedded\nwithin the high-dimensional space. Due to the complex relationships between the\nobject position or orientation and the pixel intensities, this manifold will be highly\nnonlinear. If the goal is to learn a model that can take an input image and output the\norientation of the object irrespective of its position, then there is only one degree of\nfreedom of variability within the manifold that is signiﬁcant.\n1.5. Decision Theory\nWe have seen in Section 1.2 how probability theory provides us with a consistent\nmathematical framework for quantifying and manipulating uncertainty. Here we\nturn to a discussion of decision theory that, when combined with probability theory,\nallows us to make optimal decisions in situations involving uncertainty such as those\nencountered in pattern recognition.\nSuppose we have an input vectorx together with a corresponding vectort of\ntarget variables, and our goal is to predictt given a new value forx. For regression\nproblems, t will comprise continuous variables, whereas for classiﬁcation problems\nt will represent class labels. The joint probability distribution p(x, t) provides a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 57,
      "page_label": "38"
    }
  },
  {
    "page_content": "t will represent class labels. The joint probability distribution p(x, t) provides a\ncomplete summary of the uncertainty associated with these variables. Determination\nofp(x, t) from a set of training data is an example ofinference and is typically a\nvery difﬁcult problem whose solution forms the subject of much of this book. In\na practical application, however, we must often make a speciﬁc prediction for the\nvalue oft, or more generally take a speciﬁc action based on our understanding of the\nvalues t is likely to take, and this aspect is the subject of decision theory.\nConsider, for example, a medical diagnosis problem in which we have taken an\nX-ray image of a patient, and we wish to determine whether the patient has cancer\nor not. In this case, the input vector x is the set of pixel intensities in the image,\nand output variablet will represent the presence of cancer, which we denote by the\nclass C1, or the absence of cancer, which we denote by the classC2. We might, for\ninstance, chooset to be a binary variable such thatt =0 corresponds to classC1 and\nt =1 corresponds to class C2. We shall see later that this choice of label values is\nparticularly convenient for probabilistic models. The general inference problem then\ninvolves determining the joint distributionp(x, Ck), or equivalently p(x,t ), which\ngives us the most complete probabilistic description of the situation. Although this\ncan be a very useful and informative quantity, in the end we must decide either to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 57,
      "page_label": "38"
    }
  },
  {
    "page_content": "can be a very useful and informative quantity, in the end we must decide either to\ngive treatment to the patient or not, and we would like this choice to be optimal\nin some appropriate sense (Duda and Hart, 1973). This is thedecision step, and\nit is the subject of decision theory to tell us how to make optimal decisions given\nthe appropriate probabilities. We shall see that the decision stage is generally very\nsimple, even trivial, once we have solved the inference problem.\nHere we give an introduction to the key ideas of decision theory as required for",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 57,
      "page_label": "38"
    }
  },
  {
    "page_content": "1.5. Decision Theory 39\nthe rest of the book. Further background, as well as more detailed accounts, can be\nfound in Berger (1985) and Bather (2000).\nBefore giving a more detailed analysis, let us ﬁrst consider informally how we\nmight expect probabilities to play a role in making decisions. When we obtain the\nX-ray image x for a new patient, our goal is to decide which of the two classes to\nassign to the image. We are interested in the probabilities of the two classes given\nthe image, which are given byp(Ck|x). Using Bayes’ theorem, these probabilities\ncan be expressed in the form\np(Ck|x)= p(x|Ck)p(Ck)\np(x) . (1.77)\nNote that any of the quantities appearing in Bayes’ theorem can be obtained from\nthe joint distributionp(x, Ck) by either marginalizing or conditioning with respect to\nthe appropriate variables. We can now interpretp(Ck) as the prior probability for the\nclass Ck, and p(Ck|x) as the corresponding posterior probability. Thusp(C1) repre-\nsents the probability that a person has cancer, before we take the X-ray measurement.\nSimilarly, p(C1|x) is the corresponding probability, revised using Bayes’ theorem in\nlight of the information contained in the X-ray. If our aim is to minimize the chance\nof assigning x to the wrong class, then intuitively we would choose the class having\nthe higher posterior probability. We now show that this intuition is correct, and we\nalso discuss more general criteria for making decisions.\n1.5.1 Minimizing the misclassiﬁcation rate",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 58,
      "page_label": "39"
    }
  },
  {
    "page_content": "also discuss more general criteria for making decisions.\n1.5.1 Minimizing the misclassiﬁcation rate\nSuppose that our goal is simply to make as few misclassiﬁcations as possible.\nWe need a rule that assigns each value ofx to one of the available classes. Such a\nrule will divide the input space into regionsRk called decision regions, one for each\nclass, such that all points inRk are assigned to classCk. The boundaries between\ndecision regions are calleddecision boundaries or decision surfaces. Note that each\ndecision region need not be contiguous but could comprise some number of disjoint\nregions. We shall encounter examples of decision boundaries and decision regions in\nlater chapters. In order to ﬁnd the optimal decision rule, consider ﬁrst of all the case\nof two classes, as in the cancer problem for instance. A mistake occurs when an input\nvector belonging to classC1 is assigned to classC2 or vice versa. The probability of\nthis occurring is given by\np(mistake) = p(x ∈R 1, C2)+ p(x ∈R 2, C1)\n=\n∫\nR1\np(x, C2)d x +\n∫\nR2\np(x, C1)d x. (1.78)\nWe are free to choose the decision rule that assigns each pointx to one of the two\nclasses. Clearly to minimizep(mistake) we should arrange that eachx is assigned to\nwhichever class has the smaller value of the integrand in (1.78). Thus, ifp(x, C1) >\np(x, C2) for a given value ofx, then we should assign thatx to class C1. From the\nproduct rule of probability we havep(x, Ck)= p(Ck|x)p(x). Because the factor",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 58,
      "page_label": "39"
    }
  },
  {
    "page_content": "product rule of probability we havep(x, Ck)= p(Ck|x)p(x). Because the factor\np(x) is common to both terms, we can restate this result as saying that the minimum",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 58,
      "page_label": "39"
    }
  },
  {
    "page_content": "40 1. INTRODUCTION\nR1 R2\nx0 ˆx\np(x,C1)\np(x,C2)\nx\nFigure 1.24 Schematic illustration of the joint probabilities p(x, Ck) for each of two classes plotted\nagainst x, together with the decision boundary x = bx. Values of x ⩾ bx are classiﬁed as\nclass C2 and hence belong to decision region R2, whereas points x< bx are classiﬁed\nas C1 and belong to R1. Errors arise from the blue, green, and red regions, so that for\nx< bx the errors are due to points from classC2 being misclassiﬁed as C1 (represented by\nthe sum of the red and green regions), and conversely for points in the region x ⩾ bx the\nerrors are due to points from class C1 being misclassiﬁed as C2 (represented by the blue\nregion). As we vary the location bx of the decision boundary, the combined areas of the\nblue and green regions remains constant, whereas the size of the red region varies. The\noptimal choice for bx is where the curves for p(x, C1) and p(x, C2) cross, corresponding to\nbx = x0, because in this case the red region disappears. This is equivalent to the minimum\nmisclassiﬁcation rate decision rule, which assigns each value of x to the class having the\nhigher posterior probability p(Ck|x).\nprobability of making a mistake is obtained if each value ofx is assigned to the class\nfor which the posterior probabilityp(Ck|x) is largest. This result is illustrated for\ntwo classes, and a single input variablex, in Figure 1.24.\nFor the more general case of K classes, it is slightly easier to maximize the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 59,
      "page_label": "40"
    }
  },
  {
    "page_content": "two classes, and a single input variablex, in Figure 1.24.\nFor the more general case of K classes, it is slightly easier to maximize the\nprobability of being correct, which is given by\np(correct) =\nK∑\nk=1\np(x ∈R k, Ck)\n=\nK∑\nk=1\n∫\nRk\np(x, Ck)d x (1.79)\nwhich is maximized when the regionsRk are chosen such that eachx is assigned\nto the class for whichp(x, Ck) is largest. Again, using the product rulep(x, Ck)=\np(Ck|x)p(x), and noting that the factor of p(x) is common to all terms, we see\nthat each x should be assigned to the class having the largest posterior probability\np(Ck|x).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 59,
      "page_label": "40"
    }
  },
  {
    "page_content": "1.5. Decision Theory 41\nFigure 1.25 An example of a loss matrix with ele-\nments Lkj for the cancer treatment problem. The rows\ncorrespond to the true class, whereas the columns cor-\nrespond to the assignment of class made by our deci-\nsion criterion.\n( cancer normal\ncancer 0 1000\nnormal 10\n)\n1.5.2 Minimizing the expected loss\nFor many applications, our objective will be more complex than simply mini-\nmizing the number of misclassiﬁcations. Let us consider again the medical diagnosis\nproblem. We note that, if a patient who does not have cancer is incorrectly diagnosed\nas having cancer, the consequences may be some patient distress plus the need for\nfurther investigations. Conversely, if a patient with cancer is diagnosed as healthy,\nthe result may be premature death due to lack of treatment. Thus the consequences\nof these two types of mistake can be dramatically different. It would clearly be better\nto make fewer mistakes of the second kind, even if this was at the expense of making\nmore mistakes of the ﬁrst kind.\nWe can formalize such issues through the introduction of aloss function, also\ncalled a cost function, which is a single, overall measure of loss incurred in taking\nany of the available decisions or actions. Our goal is then to minimize the total loss\nincurred. Note that some authors consider instead a utility function, whose value\nthey aim to maximize. These are equivalent concepts if we take the utility to be",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 60,
      "page_label": "41"
    }
  },
  {
    "page_content": "they aim to maximize. These are equivalent concepts if we take the utility to be\nsimply the negative of the loss, and throughout this text we shall use the loss function\nconvention. Suppose that, for a new value ofx, the true class isCk and that we assign\nx to class Cj (where j may or may not be equal tok). In so doing, we incur some\nlevel of loss that we denote byLkj, which we can view as thek,j element of aloss\nmatrix. For instance, in our cancer example, we might have a loss matrix of the form\nshown in Figure 1.25. This particular loss matrix says that there is no loss incurred\nif the correct decision is made, there is a loss of1 if a healthy patient is diagnosed as\nhaving cancer, whereas there is a loss of1000 if a patient having cancer is diagnosed\nas healthy.\nThe optimal solution is the one which minimizes the loss function. However,\nthe loss function depends on the true class, which is unknown. For a given input\nvector x, our uncertainty in the true class is expressed through the joint probability\ndistribution p(x, Ck) and so we seek instead to minimize the average loss, where the\naverage is computed with respect to this distribution, which is given by\nE[L]=\n∑\nk\n∑\nj\n∫\nRj\nLkjp(x, Ck)d x. (1.80)\nEach x can be assigned independently to one of the decision regionsRj. Our goal\nis to choose the regions Rj in order to minimize the expected loss (1.80), which\nimplies that for eachx we should minimize∑\nk Lkjp(x, Ck). As before, we can use",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 60,
      "page_label": "41"
    }
  },
  {
    "page_content": "implies that for eachx we should minimize∑\nk Lkjp(x, Ck). As before, we can use\nthe product rule p(x, Ck)= p(Ck|x)p(x) to eliminate the common factor ofp(x).\nThus the decision rule that minimizes the expected loss is the one that assigns each",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 60,
      "page_label": "41"
    }
  },
  {
    "page_content": "42 1. INTRODUCTION\nFigure 1.26 Illustration of the reject option. Inputs\nx such that the larger of the two poste-\nrior probabilities is less than or equal to\nsome threshold θ will be rejected.\nx\np(C1|x) p(C2|x)\n0.0\n1.0\nθ\nreject region\nnew x to the classj for which the quantity\n∑\nk\nLkjp(Ck|x) (1.81)\nis a minimum. This is clearly trivial to do, once we know the posterior class proba-\nbilities p(Ck|x).\n1.5.3 The reject option\nWe have seen that classiﬁcation errors arise from the regions of input space\nwhere the largest of the posterior probabilitiesp(Ck|x) is signiﬁcantly less than unity,\nor equivalently where the joint distributionsp(x, Ck) have comparable values. These\nare the regions where we are relatively uncertain about class membership. In some\napplications, it will be appropriate to avoid making decisions on the difﬁcult cases\nin anticipation of a lower error rate on those examples for which a classiﬁcation de-\ncision is made. This is known as thereject option. For example, in our hypothetical\nmedical illustration, it may be appropriate to use an automatic system to classify\nthose X-ray images for which there is little doubt as to the correct class, while leav-\ning a human expert to classify the more ambiguous cases. We can achieve this by\nintroducing a threshold θ and rejecting those inputs x for which the largest of the\nposterior probabilities p(Ck|x) is less than or equal toθ. This is illustrated for the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 61,
      "page_label": "42"
    }
  },
  {
    "page_content": "posterior probabilities p(Ck|x) is less than or equal toθ. This is illustrated for the\ncase of two classes, and a single continuous input variablex, in Figure 1.26. Note\nthat setting θ =1 will ensure that all examples are rejected, whereas if there areK\nclasses then setting θ< 1/K will ensure that no examples are rejected. Thus the\nfraction of examples that get rejected is controlled by the value ofθ.\nWe can easily extend the reject criterion to minimize the expected loss, when\na loss matrix is given, taking account of the loss incurred when a reject decision is\nmade.Exercise 1.24\n1.5.4 Inference and decision\nWe have broken the classiﬁcation problem down into two separate stages, the\ninference stage in which we use training data to learn a model forp(Ck|x), and the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 61,
      "page_label": "42"
    }
  },
  {
    "page_content": "1.5. Decision Theory 43\nsubsequent decision stage in which we use these posterior probabilities to make op-\ntimal class assignments. An alternative possibility would be to solve both problems\ntogether and simply learn a function that maps inputsx directly into decisions. Such\na function is called adiscriminant function.\nIn fact, we can identify three distinct approaches to solving decision problems,\nall of which have been used in practical applications. These are given, in decreasing\norder of complexity, by:\n(a) First solve the inference problem of determining the class-conditional densities\np(x|Ck) for each class Ck individually. Also separately infer the prior class\nprobabilities p(Ck). Then use Bayes’ theorem in the form\np(Ck|x)= p(x|Ck)p(Ck)\np(x) (1.82)\nto ﬁnd the posterior class probabilities p(Ck|x). As usual, the denominator\nin Bayes’ theorem can be found in terms of the quantities appearing in the\nnumerator, because\np(x)=\n∑\nk\np(x|Ck)p(Ck). (1.83)\nEquivalently, we can model the joint distributionp(x, Ck) directly and then\nnormalize to obtain the posterior probabilities. Having found the posterior\nprobabilities, we use decision theory to determine class membership for each\nnew inputx. Approaches that explicitly or implicitly model the distribution of\ninputs as well as outputs are known asgenerative models, because by sampling\nfrom them it is possible to generate synthetic data points in the input space.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 62,
      "page_label": "43"
    }
  },
  {
    "page_content": "from them it is possible to generate synthetic data points in the input space.\n(b) First solve the inference problem of determining the posterior class probabilities\np(Ck|x), and then subsequently use decision theory to assign each newx to\none of the classes. Approaches that model the posterior probabilities directly\nare called discriminative models.\n(c) Find a function f(x), called a discriminant function, which maps each inputx\ndirectly onto a class label. For instance, in the case of two-class problems,\nf(·) might be binary valued and such thatf =0 represents classC1 and f =1\nrepresents class C2. In this case, probabilities play no role.\nLet us consider the relative merits of these three alternatives. Approach (a) is the\nmost demanding because it involves ﬁnding the joint distribution over bothx and\nCk. For many applications, x will have high dimensionality, and consequently we\nmay need a large training set in order to be able to determine the class-conditional\ndensities to reasonable accuracy. Note that the class priorsp(Ck) can often be esti-\nmated simply from the fractions of the training set data points in each of the classes.\nOne advantage of approach (a), however, is that it also allows the marginal density\nof datap(x) to be determined from (1.83). This can be useful for detecting new data\npoints that have low probability under the model and for which the predictions may",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 62,
      "page_label": "43"
    }
  },
  {
    "page_content": "44 1. INTRODUCTION\np(x|C1)\np(x|C2)\nx\nclass densities\n0 0.2 0.4 0.6 0.8 1\n0\n1\n2\n3\n4\n5\nx\np(C1|x) p(C2|x)\n0 0.2 0.4 0.6 0.8 1\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nFigure 1.27 Example of the class-conditional densities for two classes having a single input variable x (left\nplot) together with the corresponding posterior probabilities (right plot). Note that the left-hand mode of the\nclass-conditional density p(x|C1), shown in blue on the left plot, has no effect on the posterior probabilities. The\nvertical green line in the right plot shows the decision boundary in x that gives the minimum misclassiﬁcation\nrate.\nbe of low accuracy, which is known asoutlier detectionor novelty detection(Bishop,\n1994; Tarassenko, 1995).\nHowever, if we only wish to make classiﬁcation decisions, then it can be waste-\nful of computational resources, and excessively demanding of data, to ﬁnd the joint\ndistribution p(x, Ck) when in fact we only really need the posterior probabilities\np(Ck|x), which can be obtained directly through approach (b). Indeed, the class-\nconditional densities may contain a lot of structure that has little effect on the pos-\nterior probabilities, as illustrated in Figure 1.27. There has been much interest in\nexploring the relative merits of generative and discriminative approaches to machine\nlearning, and in ﬁnding ways to combine them (Jebara, 2004; Lasserreet al., 2006).\nAn even simpler approach is (c) in which we use the training data to ﬁnd a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 63,
      "page_label": "44"
    }
  },
  {
    "page_content": "An even simpler approach is (c) in which we use the training data to ﬁnd a\ndiscriminant function f(x) that maps each x directly onto a class label, thereby\ncombining the inference and decision stages into a single learning problem. In the\nexample of Figure 1.27, this would correspond to ﬁnding the value ofx shown by\nthe vertical green line, because this is the decision boundary giving the minimum\nprobability of misclassiﬁcation.\nWith option (c), however, we no longer have access to the posterior probabilities\np(Ck|x). There are many powerful reasons for wanting to compute the posterior\nprobabilities, even if we subsequently use them to make decisions. These include:\nMinimizing risk. Consider a problem in which the elements of the loss matrix are\nsubjected to revision from time to time (such as might occur in a ﬁnancial",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 63,
      "page_label": "44"
    }
  },
  {
    "page_content": "1.5. Decision Theory 45\napplication). If we know the posterior probabilities, we can trivially revise the\nminimum risk decision criterion by modifying (1.81) appropriately. If we have\nonly a discriminant function, then any change to the loss matrix would require\nthat we return to the training data and solve the classiﬁcation problem afresh.\nReject option. Posterior probabilities allow us to determine a rejection criterion that\nwill minimize the misclassiﬁcation rate, or more generally the expected loss,\nfor a given fraction of rejected data points.\nCompensating for class priors.Consider our medical X-ray problem again, and\nsuppose that we have collected a large number of X-ray images from the gen-\neral population for use as training data in order to build an automated screening\nsystem. Because cancer is rare amongst the general population, we might ﬁnd\nthat, say, only 1 in every 1,000 examples corresponds to the presence of can-\ncer. If we used such a data set to train an adaptive model, we could run into\nsevere difﬁculties due to the small proportion of the cancer class. For instance,\na classiﬁer that assigned every point to the normal class would already achieve\n99.9% accuracy and it would be difﬁcult to avoid this trivial solution. Also,\neven a large data set will contain very few examples of X-ray images corre-\nsponding to cancer, and so the learning algorithm will not be exposed to a\nbroad range of examples of such images and hence is not likely to generalize",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 64,
      "page_label": "45"
    }
  },
  {
    "page_content": "broad range of examples of such images and hence is not likely to generalize\nwell. A balanced data set in which we have selected equal numbers of exam-\nples from each of the classes would allow us to ﬁnd a more accurate model.\nHowever, we then have to compensate for the effects of our modiﬁcations to\nthe training data. Suppose we have used such a modiﬁed data set and found\nmodels for the posterior probabilities. From Bayes’ theorem (1.82), we see that\nthe posterior probabilities are proportional to the prior probabilities, which we\ncan interpret as the fractions of points in each class. We can therefore simply\ntake the posterior probabilities obtained from our artiﬁcially balanced data set\nand ﬁrst divide by the class fractions in that data set and then multiply by the\nclass fractions in the population to which we wish to apply the model. Finally,\nwe need to normalize to ensure that the new posterior probabilities sum to one.\nNote that this procedure cannot be applied if we have learned a discriminant\nfunction directly instead of determining posterior probabilities.\nCombining models. For complex applications, we may wish to break the problem\ninto a number of smaller subproblems each of which can be tackled by a sep-\narate module. For example, in our hypothetical medical diagnosis problem,\nwe may have information available from, say, blood tests as well as X-ray im-\nages. Rather than combine all of this heterogeneous information into one huge",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 64,
      "page_label": "45"
    }
  },
  {
    "page_content": "ages. Rather than combine all of this heterogeneous information into one huge\ninput space, it may be more effective to build one system to interpret the X-\nray images and a different one to interpret the blood data. As long as each of\nthe two models gives posterior probabilities for the classes, we can combine\nthe outputs systematically using the rules of probability. One simple way to\ndo this is to assume that, for each class separately, the distributions of inputs\nfor the X-ray images, denoted byxI, and the blood data, denoted byxB,a r e",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 64,
      "page_label": "45"
    }
  },
  {
    "page_content": "46 1. INTRODUCTION\nindependent, so that\np(xI, xB|Ck)= p(xI|Ck)p(xB|Ck). (1.84)\nThis is an example ofconditional independenceproperty, because the indepen-Section 8.2\ndence holds when the distribution is conditioned on the classCk. The posterior\nprobability, given both the X-ray and blood data, is then given by\np(Ck|xI, xB) ∝ p(xI, xB|Ck)p(Ck)\n∝ p(xI|Ck)p(xB|Ck)p(Ck)\n∝ p(Ck|xI)p(Ck|xB)\np(Ck) (1.85)\nThus we need the class prior probabilitiesp(Ck), which we can easily estimate\nfrom the fractions of data points in each class, and then we need to normalize\nthe resulting posterior probabilities so they sum to one. The particular condi-\ntional independence assumption (1.84) is an example of thenaive Bayes model.Section 8.2.2\nNote that the joint marginal distributionp(xI, xB) will typically not factorize\nunder this model. We shall see in later chapters how to construct models for\ncombining data that do not require the conditional independence assumption\n(1.84).\n1.5.5 Loss functions for regression\nSo far, we have discussed decision theory in the context of classiﬁcation prob-\nlems. We now turn to the case of regression problems, such as the curve ﬁtting\nexample discussed earlier. The decision stage consists of choosing a speciﬁc esti-Section 1.1\nmate y(x) of the value oft for each input x. Suppose that in doing so, we incur a\nloss L(t, y(x)). The average, or expected, loss is then given by\nE[L]=\n∫∫\nL(t, y(x))p(x,t)d xdt. (1.86)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 65,
      "page_label": "46"
    }
  },
  {
    "page_content": "loss L(t, y(x)). The average, or expected, loss is then given by\nE[L]=\n∫∫\nL(t, y(x))p(x,t)d xdt. (1.86)\nA common choice of loss function in regression problems is the squared loss given\nby L(t, y(x)) = {y(x) − t}2. In this case, the expected loss can be written\nE[L]=\n∫∫\n{y(x) − t}2p(x,t )d xdt. (1.87)\nOur goal is to choose y(x) so as to minimize E[L]. If we assume a completely\nﬂexible function y(x), we can do this formally using the calculus of variations toAppendix D\ngive\nδE[L]\nδy(x) =2\n∫\n{y(x) − t}p(x,t )d t =0 . (1.88)\nSolving for y(x), and using the sum and product rules of probability, we obtain\ny(x)=\n∫\ntp(x,t )d t\np(x) =\n∫\ntp(t|x)d t = Et[t|x] (1.89)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 65,
      "page_label": "46"
    }
  },
  {
    "page_content": "1.5. Decision Theory 47\nFigure 1.28 The regression function y(x),\nwhich minimizes the expected\nsquared loss, is given by the\nmean of the conditional distri-\nbution p(t|x).\nt\nxx0\ny(x0)\ny(x)\np(t|x0)\nwhich is the conditional average oft conditioned onx and is known as theregression\nfunction. This result is illustrated in Figure 1.28. It can readily be extended to mul-\ntiple target variables represented by the vectort, in which case the optimal solution\nis the conditional averagey(x)= Et[t|x].Exercise 1.25\nWe can also derive this result in a slightly different way, which will also shed\nlight on the nature of the regression problem. Armed with the knowledge that the\noptimal solution is the conditional expectation, we can expand the square term as\nfollows\n{y(x) − t}2 = {y(x) − E[t|x]+ E[t|x] − t}2\n= {y(x) − E[t|x]}2 +2 {y(x) − E[t|x]}{E[t|x] − t} + {E[t|x] − t}2\nwhere, to keep the notation uncluttered, we useE[t|x] to denoteEt[t|x]. Substituting\ninto the loss function and performing the integral overt, we see that the cross-term\nvanishes and we obtain an expression for the loss function in the form\nE[L]=\n∫\n{y(x) − E[t|x]}2 p(x)dx +\n∫\n{E[t|x] − t}2p(x)dx. (1.90)\nThe function y(x) we seek to determine enters only in the ﬁrst term, which will be\nminimized when y(x) is equal to E[t|x], in which case this term will vanish. This\nis simply the result that we derived previously and that shows that the optimal least",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 66,
      "page_label": "47"
    }
  },
  {
    "page_content": "is simply the result that we derived previously and that shows that the optimal least\nsquares predictor is given by the conditional mean. The second term is the variance\nof the distribution of t, averaged over x. It represents the intrinsic variability of\nthe target data and can be regarded as noise. Because it is independent ofy(x),i t\nrepresents the irreducible minimum value of the loss function.\nAs with the classiﬁcation problem, we can either determine the appropriate prob-\nabilities and then use these to make optimal decisions, or we can build models that\nmake decisions directly. Indeed, we can identify three distinct approaches to solving\nregression problems given, in order of decreasing complexity, by:\n(a) First solve the inference problem of determining the joint densityp(x,t ). Then\nnormalize to ﬁnd the conditional densityp(t|x), and ﬁnally marginalize to ﬁnd\nthe conditional mean given by (1.89).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 66,
      "page_label": "47"
    }
  },
  {
    "page_content": "48 1. INTRODUCTION\n(b) First solve the inference problem of determining the conditional densityp(t|x),\nand then subsequently marginalize to ﬁnd the conditional mean given by (1.89).\n(c) Find a regression functiony(x) directly from the training data.\nThe relative merits of these three approaches follow the same lines as for classiﬁca-\ntion problems above.\nThe squared loss is not the only possible choice of loss function for regression.\nIndeed, there are situations in which squared loss can lead to very poor results and\nwhere we need to develop more sophisticated approaches. An important example\nconcerns situations in which the conditional distribution p(t|x) is multimodal, as\noften arises in the solution of inverse problems. Here we consider brieﬂy one simpleSection 5.6\ngeneralization of the squared loss, called theMinkowski loss, whose expectation is\ngiven by\nE[Lq]=\n∫∫\n|y(x) − t|qp(x,t )d xdt (1.91)\nwhich reduces to the expected squared loss for q =2 . The function |y − t|q is\nplotted against y − t for various values ofq in Figure 1.29. The minimum ofE[Lq]\nis given by the conditional mean forq =2 , the conditional median forq =1 , and\nthe conditional mode forq → 0.Exercise 1.27\n1.6. Information Theory\nIn this chapter, we have discussed a variety of concepts from probability theory and\ndecision theory that will form the foundations for much of the subsequent discussion\nin this book. We close this chapter by introducing some additional concepts from",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 67,
      "page_label": "48"
    }
  },
  {
    "page_content": "in this book. We close this chapter by introducing some additional concepts from\nthe ﬁeld of information theory, which will also prove useful in our development of\npattern recognition and machine learning techniques. Again, we shall focus only on\nthe key concepts, and we refer the reader elsewhere for more detailed discussions\n(Viterbi and Omura, 1979; Cover and Thomas, 1991; MacKay, 2003) .\nWe begin by considering a discrete random variablex and we ask how much\ninformation is received when we observe a speciﬁc value for this variable. The\namount of information can be viewed as the ‘degree of surprise’ on learning the\nvalue of x. If we are told that a highly improbable event has just occurred, we will\nhave received more information than if we were told that some very likely event\nhas just occurred, and if we knew that the event was certain to happen we would\nreceive no information. Our measure of information content will therefore depend\non the probability distributionp(x), and we therefore look for a quantityh(x) that\nis a monotonic function of the probabilityp(x) and that expresses the information\ncontent. The form of h(·) can be found by noting that if we have two events x\nand y that are unrelated, then the information gain from observing both of them\nshould be the sum of the information gained from each of them separately, so that\nh(x, y)= h(x)+ h(y). Two unrelated events will be statistically independent and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 67,
      "page_label": "48"
    }
  },
  {
    "page_content": "h(x, y)= h(x)+ h(y). Two unrelated events will be statistically independent and\nso p(x, y)= p(x)p(y). From these two relationships, it is easily shown thath(x)\nmust be given by the logarithm ofp(x) and so we haveExercise 1.28",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 67,
      "page_label": "48"
    }
  },
  {
    "page_content": "1.6. Information Theory 49\ny − t\n|y − t|q\nq =0 .3\n−2 −1 0 1 2\n0\n1\n2\ny − t\n|y − t|q\nq =1\n−2 −1 0 1 2\n0\n1\n2\ny − t\n|y − t|q\nq =2\n−2 −1 0 1 2\n0\n1\n2\ny − t\n|y − t|q\nq =1 0\n−2 −1 0 1 2\n0\n1\n2\nFigure 1.29 Plots of the quantity Lq = |y − t|q for various values of q.\nh(x)= −log2 p(x) (1.92)\nwhere the negative sign ensures that information is positive or zero. Note that low\nprobability events x correspond to high information content. The choice of basis\nfor the logarithm is arbitrary, and for the moment we shall adopt the convention\nprevalent in information theory of using logarithms to the base of2. In this case, as\nwe shall see shortly, the units ofh(x) are bits (‘binary digits’).\nNow suppose that a sender wishes to transmit the value of a random variable to\na receiver. The average amount of information that they transmit in the process is\nobtained by taking the expectation of (1.92) with respect to the distributionp(x) and\nis given by\nH[x]= −\n∑\nx\np(x) log2 p(x). (1.93)\nThis important quantity is called theentropy of the random variable x. Note that\nlimp→0 p lnp =0 and so we shall takep(x)l np(x)=0 whenever we encounter a\nvalue forx such that p(x)=0 .\nSo far we have given a rather heuristic motivation for the deﬁnition of informa-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 68,
      "page_label": "49"
    }
  },
  {
    "page_content": "50 1. INTRODUCTION\ntion (1.92) and the corresponding entropy (1.93). We now show that these deﬁnitions\nindeed possess useful properties. Consider a random variablex having 8 possible\nstates, each of which is equally likely. In order to communicate the value ofx to\na receiver, we would need to transmit a message of length 3 bits. Notice that the\nentropy of this variable is given by\nH[x]= −8 × 1\n8 log2\n1\n8 = 3 bits.\nNow consider an example (Cover and Thomas, 1991) of a variable having8 pos-\nsible states {a, b, c, d, e, f, g, h} for which the respective probabilities are given by\n(1\n2, 1\n4, 1\n8, 1\n16, 1\n64, 1\n64, 1\n64, 1\n64). The entropy in this case is given by\nH[x]= −1\n2 log2\n1\n2 − 1\n4 log2\n1\n4 − 1\n8 log2\n1\n8 − 1\n16 log2\n1\n16 − 4\n64 log2\n1\n64 = 2 bits.\nWe see that the nonuniform distribution has a smaller entropy than the uniform one,\nand we shall gain some insight into this shortly when we discuss the interpretation of\nentropy in terms of disorder. For the moment, let us consider how we would transmit\nthe identity of the variable’s state to a receiver. We could do this, as before, using\na 3-bit number. However, we can take advantage of the nonuniform distribution by\nusing shorter codes for the more probable events, at the expense of longer codes for\nthe less probable events, in the hope of getting a shorter average code length. This\ncan be done by representing the states{a, b, c, d, e, f, g, h} using, for instance, the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 69,
      "page_label": "50"
    }
  },
  {
    "page_content": "can be done by representing the states{a, b, c, d, e, f, g, h} using, for instance, the\nfollowing set of code strings: 0, 10, 110, 1110, 111100, 111101, 111110, 111111.\nThe average length of the code that has to be transmitted is then\naverage code length =1\n2 × 1+ 1\n4 × 2+ 1\n8 × 3+ 1\n16 × 4+4 × 1\n64 × 6 = 2 bits\nwhich again is the same as the entropy of the random variable. Note that shorter code\nstrings cannot be used because it must be possible to disambiguate a concatenation\nof such strings into its component parts. For instance, 11001110 decodes uniquely\ninto the state sequencec, a, d.\nThis relation between entropy and shortest coding length is a general one. The\nnoiseless coding theorem (Shannon, 1948) states that the entropy is a lower bound\non the number of bits needed to transmit the state of a random variable.\nFrom now on, we shall switch to the use of natural logarithms in deﬁning en-\ntropy, as this will provide a more convenient link with ideas elsewhere in this book.\nIn this case, the entropy is measured in units of ‘nats’ instead of bits, which differ\nsimply by a factor ofln 2.\nWe have introduced the concept of entropy in terms of the average amount of\ninformation needed to specify the state of a random variable. In fact, the concept of\nentropy has much earlier origins in physics where it was introduced in the context\nof equilibrium thermodynamics and later given a deeper interpretation as a measure",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 69,
      "page_label": "50"
    }
  },
  {
    "page_content": "of equilibrium thermodynamics and later given a deeper interpretation as a measure\nof disorder through developments in statistical mechanics. We can understand this\nalternative view of entropy by considering a set ofN identical objects that are to be\ndivided amongst a set of bins, such that there areni objects in theith bin. Consider",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 69,
      "page_label": "50"
    }
  },
  {
    "page_content": "1.6. Information Theory 51\nthe number of different ways of allocating the objects to the bins. There are N\nways to choose the ﬁrst object, (N − 1) ways to choose the second object, and\nso on, leading to a total ofN! ways to allocate allN objects to the bins, whereN!\n(pronounced ‘factorialN’) denotes the productN ×(N −1)×···× 2×1. However,\nwe don’t wish to distinguish between rearrangements of objects within each bin. In\ntheith bin there are ni! ways of reordering the objects, and so the total number of\nways of allocating theN objects to the bins is given by\nW = N!∏\ni ni! (1.94)\nwhich is called themultiplicity. The entropy is then deﬁned as the logarithm of the\nmultiplicity scaled by an appropriate constant\nH= 1\nN lnW = 1\nN lnN! − 1\nN\n∑\ni\nlnni!. (1.95)\nWe now consider the limitN →∞ , in which the fractionsni/N are held ﬁxed, and\napply Stirling’s approximation\nlnN! ≃ N lnN − N (1.96)\nwhich gives\nH= − lim\nN→∞\n∑\ni\n(ni\nN\n)\nln\n(ni\nN\n)\n= −\n∑\ni\npi lnpi (1.97)\nwhere we have used ∑\ni ni = N. Here pi = limN→∞(ni/N) is the probability\nof an object being assigned to theith bin. In physics terminology, the speciﬁc ar-\nrangements of objects in the bins is called amicrostate, and the overall distribution\nof occupation numbers, expressed through the ratiosni/N, is called amacrostate.\nThe multiplicity W is also known as theweight of the macrostate.\nWe can interpret the bins as the statesxi of a discrete random variableX, where\np(X = xi)= pi. The entropy of the random variableX is then\nH[p]= −",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 70,
      "page_label": "51"
    }
  },
  {
    "page_content": "We can interpret the bins as the statesxi of a discrete random variableX, where\np(X = xi)= pi. The entropy of the random variableX is then\nH[p]= −\n∑\ni\np(xi)l np(xi). (1.98)\nDistributions p(xi) that are sharply peaked around a few values will have a relatively\nlow entropy, whereas those that are spread more evenly across many values will\nhave higher entropy, as illustrated in Figure 1.30. Because0 ⩽ pi ⩽ 1, the entropy\nis nonnegative, and it will equal its minimum value of 0 when one of the pi =\n1 and all other pj̸=i =0 . The maximum entropy conﬁguration can be found by\nmaximizing H using a Lagrange multiplier to enforce the normalization constraintAppendix E\non the probabilities. Thus we maximize\n˜H= −\n∑\ni\np(xi)l np(xi)+ λ\n( ∑\ni\np(xi) − 1\n)\n(1.99)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 70,
      "page_label": "51"
    }
  },
  {
    "page_content": "52 1. INTRODUCTION\nprobabilities\nH = 1.77\n0\n0.25\n0.5\nprobabilities\nH = 3.09\n0\n0.25\n0.5\nFigure 1.30 Histograms of two probability distributions over 30 bins illustrating the higher value of the entropy\nH for the broader distribution. The largest entropy would arise from a uniform distribution that would give H=\n−ln(1/30) = 3.40.\nfrom which we ﬁnd that all of thep(xi) are equal and are given byp(xi)=1 /M\nwhere M is the total number of statesxi. The corresponding value of the entropy\nis then H=l n M. This result can also be derived from Jensen’s inequality (to be\ndiscussed shortly). To verify that the stationary point is indeed a maximum, we canExercise 1.29\nevaluate the second derivative of the entropy, which gives\n∂˜H\n∂p(xi)∂p(xj) = −Iij\n1\npi\n(1.100)\nwhere Iij are the elements of the identity matrix.\nWe can extend the deﬁnition of entropy to include distributionsp(x) over con-\ntinuous variables x as follows. First dividex into bins of width∆. Then, assuming\np(x) is continuous, themean value theorem(Weisstein, 1999) tells us that, for each\nsuch bin, there must exist a valuexi such that\n∫ (i+1)∆\ni∆\np(x)d x = p(xi)∆. (1.101)\nWe can now quantize the continuous variablex by assigning any valuex to the value\nxi whenever x falls in theith bin. The probability of observing the valuexi is then\np(xi)∆. This gives a discrete distribution for which the entropy takes the form\nH∆ = −\n∑\ni\np(xi)∆ ln (p(xi)∆) = −\n∑\ni\np(xi)∆ lnp(xi) − ln ∆ (1.102)\nwhere we have used∑",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 71,
      "page_label": "52"
    }
  },
  {
    "page_content": "H∆ = −\n∑\ni\np(xi)∆ ln (p(xi)∆) = −\n∑\ni\np(xi)∆ lnp(xi) − ln ∆ (1.102)\nwhere we have used∑\ni p(xi)∆ = 1 , which follows from (1.101). We now omit\nthe second term−ln ∆on the right-hand side of (1.102) and then consider the limit",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 71,
      "page_label": "52"
    }
  },
  {
    "page_content": "1.6. Information Theory 53\n∆ → 0. The ﬁrst term on the right-hand side of (1.102) will approach the integral of\np(x)l np(x) in this limit so that\nlim\n∆→0\n{∑\ni\np(xi)∆ lnp(xi)\n}\n= −\n∫\np(x)l np(x)d x (1.103)\nwhere the quantity on the right-hand side is called thedifferential entropy. We see\nthat the discrete and continuous forms of the entropy differ by a quantityln ∆, which\ndiverges in the limit ∆ → 0. This reﬂects the fact that to specify a continuous\nvariable very precisely requires a large number of bits. For a density deﬁned over\nmultiple continuous variables, denoted collectively by the vectorx, the differential\nentropy is given by\nH[x]= −\n∫\np(x)l np(x)dx. (1.104)\nIn the case of discrete distributions, we saw that the maximum entropy con-\nﬁguration corresponded to an equal distribution of probabilities across the possible\nstates of the variable. Let us now consider the maximum entropy conﬁguration for\na continuous variable. In order for this maximum to be well deﬁned, it will be nec-\nessary to constrain the ﬁrst and second moments ofp(x) as well as preserving the\nnormalization constraint. We therefore maximize the differential entropy with the\nLudwig Boltzmann\n1844–1906\nLudwig Eduard Boltzmann was an\nAustrian physicist who created the\nﬁeld of statistical mechanics. Prior\nto Boltzmann, the concept of en-\ntropy was already known from\nclassical thermodynamics where it\nquantiﬁes the fact that when we take energy from a\nsystem, not all of that energy is typically available",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 72,
      "page_label": "53"
    }
  },
  {
    "page_content": "classical thermodynamics where it\nquantiﬁes the fact that when we take energy from a\nsystem, not all of that energy is typically available\nto do useful work. Boltzmann showed that the ther-\nmodynamic entropyS, a macroscopic quantity, could\nbe related to the statistical properties at the micro-\nscopic level. This is expressed through the famous\nequation S = k ln W in which W represents the\nnumber of possible microstates in a macrostate, and\nk≃ 1.38 × 10−23 (in units of Joules per Kelvin) is\nknown as Boltzmann’s constant. Boltzmann’s ideas\nwere disputed by many scientists of they day. One dif-\nﬁculty they saw arose from the second law of thermo-\ndynamics, which states that the entropy of a closed\nsystem tends to increase with time. By contrast, at\nthe microscopic level the classical Newtonian equa-\ntions of physics are reversible, and so they found it\ndifﬁcult to see how the latter could explain the for-\nmer. They didn’t fully appreciate Boltzmann’s argu-\nments, which were statistical in nature and which con-\ncluded not that entropy could never decrease over\ntime but simply that with overwhelming probability it\nwould generally increase. Boltzmann even had a long-\nrunning dispute with the editor of the leading German\nphysics journal who refused to let him refer to atoms\nand molecules as anything other than convenient the-\noretical constructs. The continued attacks on his work\nlead to bouts of depression, and eventually he com-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 72,
      "page_label": "53"
    }
  },
  {
    "page_content": "oretical constructs. The continued attacks on his work\nlead to bouts of depression, and eventually he com-\nmitted suicide. Shortly after Boltzmann’s death, new\nexperiments by Perrin on colloidal suspensions veri-\nﬁed his theories and conﬁrmed the value of the Boltz-\nmann constant. The equationS = k ln W is carved on\nBoltzmann’s tombstone.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 72,
      "page_label": "53"
    }
  },
  {
    "page_content": "54 1. INTRODUCTION\nthree constraints\n∫ ∞\n−∞\np(x)d x =1 (1.105)\n∫ ∞\n−∞\nxp(x)d x = µ (1.106)\n∫ ∞\n−∞\n(x − µ)2p(x)d x = σ2. (1.107)\nThe constrained maximization can be performed using Lagrange multipliers so thatAppendix E\nwe maximize the following functional with respect top(x)\n−\n∫ ∞\n−∞\np(x)l np(x)d x + λ1\n(∫ ∞\n−∞\np(x)d x − 1\n)\n+λ2\n(∫ ∞\n−∞\nxp(x)d x − µ\n)\n+ λ3\n(∫ ∞\n−∞\n(x − µ)2p(x)d x − σ2\n)\n.\nUsing the calculus of variations, we set the derivative of this functional to zero givingAppendix D\np(x)=e x p\n{\n−1+ λ1 + λ2x + λ3(x − µ)2}\n. (1.108)\nThe Lagrange multipliers can be found by back substitution of this result into the\nthree constraint equations, leading ﬁnally to the resultExercise 1.34\np(x)= 1\n(2πσ2)1/2 exp\n{\n−(x − µ)2\n2σ2\n}\n(1.109)\nand so the distribution that maximizes the differential entropy is the Gaussian. Note\nthat we did not constrain the distribution to be nonnegative when we maximized the\nentropy. However, because the resulting distribution is indeed nonnegative, we see\nwith hindsight that such a constraint is not necessary.\nIf we evaluate the differential entropy of the Gaussian, we obtainExercise 1.35\nH[x]= 1\n2\n{\n1+l n ( 2πσ2)\n}\n. (1.110)\nThus we see again that the entropy increases as the distribution becomes broader,\ni.e., as σ2 increases. This result also shows that the differential entropy, unlike the\ndiscrete entropy, can be negative, becauseH(x) < 0 in (1.110) forσ2 < 1/(2πe).\nSuppose we have a joint distributionp(x, y) from which we draw pairs of values",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 73,
      "page_label": "54"
    }
  },
  {
    "page_content": "Suppose we have a joint distributionp(x, y) from which we draw pairs of values\nof x and y. If a value ofx is already known, then the additional information needed\nto specify the corresponding value ofy is given by−lnp(y|x). Thus the average\nadditional information needed to specifyy can be written as\nH[y|x]= −\n∫∫\np(y, x)l np(y|x)d ydx (1.111)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 73,
      "page_label": "54"
    }
  },
  {
    "page_content": "1.6. Information Theory 55\nwhich is called the conditional entropy of y given x. It is easily seen, using the\nproduct rule, that the conditional entropy satisﬁes the relationExercise 1.37\nH[x,y]=H [ y|x]+H [x] (1.112)\nwhere H[x,y] is the differential entropy ofp(x, y) and H[x] is the differential en-\ntropy of the marginal distributionp(x). Thus the information needed to describex\nand y is given by the sum of the information needed to describex alone plus the\nadditional information required to specifyy given x.\n1.6.1 Relative entropy and mutual information\nSo far in this section, we have introduced a number of concepts from information\ntheory, including the key notion of entropy. We now start to relate these ideas to\npattern recognition. Consider some unknown distributionp(x), and suppose that\nwe have modelled this using an approximating distributionq(x). I fw eu s eq(x) to\nconstruct a coding scheme for the purpose of transmitting values ofx to a receiver,\nthen the averageadditional amount of information (in nats) required to specify the\nvalue ofx (assuming we choose an efﬁcient coding scheme) as a result of usingq(x)\ninstead of the true distributionp(x) is given by\nKL(p∥q)= −\n∫\np(x)l nq(x)dx −\n(\n−\n∫\np(x)l np(x)d x\n)\n= −\n∫\np(x)l n\n{q(x)\np(x)\n}\ndx. (1.113)\nThis is known as therelative entropy or Kullback-Leibler divergence,o r KL diver-\ngence (Kullback and Leibler, 1951), between the distributionsp(x) and q(x). Note",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 74,
      "page_label": "55"
    }
  },
  {
    "page_content": "gence (Kullback and Leibler, 1951), between the distributionsp(x) and q(x). Note\nthat it is not a symmetrical quantity, that is to sayKL(p∥q) ̸≡KL(q∥p).\nWe now show that the Kullback-Leibler divergence satisﬁesKL(p∥q) ⩾ 0 with\nequality if, and only if,p(x)= q(x). To do this we ﬁrst introduce the concept of\nconvex functions. A function f(x) is said to be convex if it has the property that\nevery chord lies on or above the function, as shown in Figure 1.31. Any value ofx\nin the interval fromx = a to x = b can be written in the formλa +( 1− λ)b where\n0 ⩽ λ ⩽ 1. The corresponding point on the chord is given byλf(a)+( 1 − λ)f(b),\nClaude Shannon\n1916–2001\nAfter graduating from Michigan and\nMIT, Shannon joined the AT&T Bell\nTelephone laboratories in 1941. His\npaper ‘A Mathematical Theory of\nCommunication’ published in the\nBell System Technical Journal in\n1948 laid the foundations for modern information the-\nory. This paper introduced the word ‘bit’, and his con-\ncept that information could be sent as a stream of 1s\nand 0s paved the way for the communications revo-\nlution. It is said that von Neumann recommended to\nShannon that he use the term entropy, not only be-\ncause of its similarity to the quantity used in physics,\nbut also because “nobody knows what entropy really\nis, so in any discussion you will always have an advan-\ntage”.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 74,
      "page_label": "55"
    }
  },
  {
    "page_content": "56 1. INTRODUCTION\nFigure 1.31 A convex functionf(x) is one for which ev-\nery chord (shown in blue) lies on or above\nthe function (shown in red).\nxa bxλ\nchord\nxλ\nf(x)\nand the corresponding value of the function isf (λa +( 1− λ)b). Convexity then\nimplies\nf(λa +( 1− λ)b) ⩽ λf(a)+( 1 − λ)f(b). (1.114)\nThis is equivalent to the requirement that the second derivative of the function be\neverywhere positive. Examples of convex functions arexln x (for x> 0) andx2.AExercise 1.36\nfunction is calledstrictly convexif the equality is satisﬁed only forλ =0 and λ =1 .\nIf a function has the opposite property, namely that every chord lies on or below the\nfunction, it is calledconcave, with a corresponding deﬁnition forstrictly concave.I f\na function f(x) is convex, then−f(x) will be concave.\nUsing the technique of proof by induction, we can show from (1.114) that aExercise 1.38\nconvex functionf(x) satisﬁes\nf\n( M∑\ni=1\nλixi\n)\n⩽\nM∑\ni=1\nλif(xi) (1.115)\nwhere λi ⩾ 0 and ∑\ni λi =1 , for any set of points {xi}. The result (1.115) is\nknown as Jensen’s inequality. If we interpret the λi as the probability distribution\nover a discrete variablex taking the values{xi}, then (1.115) can be written\nf (E[x]) ⩽ E[f(x)] (1.116)\nwhere E[·] denotes the expectation. For continuous variables, Jensen’s inequality\ntakes the form\nf\n(∫\nxp(x)dx\n)\n⩽\n∫\nf(x)p(x)dx. (1.117)\nWe can apply Jensen’s inequality in the form (1.117) to the Kullback-Leibler\ndivergence (1.113) to give\nKL(p∥q)= −\n∫\np(x)l n\n{q(x)\np(x)\n}\ndx ⩾ −ln\n∫",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 75,
      "page_label": "56"
    }
  },
  {
    "page_content": "We can apply Jensen’s inequality in the form (1.117) to the Kullback-Leibler\ndivergence (1.113) to give\nKL(p∥q)= −\n∫\np(x)l n\n{q(x)\np(x)\n}\ndx ⩾ −ln\n∫\nq(x)dx =0 (1.118)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 75,
      "page_label": "56"
    }
  },
  {
    "page_content": "1.6. Information Theory 57\nwhere we have used the fact that−lnx is a convex function, together with the nor-\nmalization condition\n∫\nq(x)dx =1 . In fact, −lnx is a strictly convex function,\nso the equality will hold if, and only if,q(x)= p(x) for all x. Thus we can in-\nterpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two\ndistributions p(x) and q(x).\nWe see that there is an intimate relationship between data compression and den-\nsity estimation (i.e., the problem of modelling an unknown probability distribution)\nbecause the most efﬁcient compression is achieved when we know the true distri-\nbution. If we use a distribution that is different from the true one, then we must\nnecessarily have a less efﬁcient coding, and on average the additional information\nthat must be transmitted is (at least) equal to the Kullback-Leibler divergence be-\ntween the two distributions.\nSuppose that data is being generated from an unknown distributionp(x) that we\nwish to model. We can try to approximate this distribution using some parametric\ndistribution q(x|θ), governed by a set of adjustable parameters θ, for example a\nmultivariate Gaussian. One way to determineθ is to minimize the Kullback-Leibler\ndivergence between p(x) and q(x|θ) with respect to θ. We cannot do this directly\nbecause we don’t knowp(x). Suppose, however, that we have observed a ﬁnite set\nof training points xn, for n =1 ,...,N , drawn from p(x). Then the expectation",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 76,
      "page_label": "57"
    }
  },
  {
    "page_content": "of training points xn, for n =1 ,...,N , drawn from p(x). Then the expectation\nwith respect to p(x) can be approximated by a ﬁnite sum over these points, using\n(1.35), so that\nKL(p∥q) ≃\nN∑\nn=1\n{−lnq(xn|θ)+l n p(xn)}. (1.119)\nThe second term on the right-hand side of (1.119) is independent ofθ, and the ﬁrst\nterm is the negative log likelihood function forθ under the distributionq(x|θ) eval-\nuated using the training set. Thus we see that minimizing this Kullback-Leibler\ndivergence is equivalent to maximizing the likelihood function.\nNow consider the joint distribution between two sets of variablesx and y given\nby p(x, y). If the sets of variables are independent, then their joint distribution will\nfactorize into the product of their marginalsp(x, y)= p(x)p(y). If the variables are\nnot independent, we can gain some idea of whether they are ‘close’ to being indepen-\ndent by considering the Kullback-Leibler divergence between the joint distribution\nand the product of the marginals, given by\nI[x,y] ≡ KL(p(x, y)∥p(x)p(y))\n= −\n∫∫\np(x, y)l n\n( p(x)p(y)\np(x, y)\n)\ndxdy (1.120)\nwhich is called the mutual information between the variables x and y. From the\nproperties of the Kullback-Leibler divergence, we see thatI(x,y) ⩾ 0 with equal-\nity if, and only if, x and y are independent. Using the sum and product rules of\nprobability, we see that the mutual information is related to the conditional entropy\nthroughExercise 1.41\nI[x,y]=H [ x] − H[x|y]=H [ y] − H[y|x]. (1.121)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 76,
      "page_label": "57"
    }
  },
  {
    "page_content": "58 1. INTRODUCTION\nThus we can view the mutual information as the reduction in the uncertainty aboutx\nby virtue of being told the value ofy (or vice versa). From a Bayesian perspective,\nwe can viewp(x) as the prior distribution forx and p(x|y) as the posterior distribu-\ntion after we have observed new datay. The mutual information therefore represents\nthe reduction in uncertainty aboutx as a consequence of the new observationy.\nExercises\n1.1 (⋆) www Consider the sum-of-squares error function given by (1.2) in which\nthe function y(x,w) is given by the polynomial (1.1). Show that the coefﬁcients\nw = {wi} that minimize this error function are given by the solution to the following\nset of linear equations\nM∑\nj=0\nAijwj = Ti (1.122)\nwhere\nAij =\nN∑\nn=1\n(xn)i+j,T i =\nN∑\nn=1\n(xn)itn. (1.123)\nHere a sufﬁxi or j denotes the index of a component, whereas(x)i denotes x raised\nto the power ofi.\n1.2 (⋆) Write down the set of coupled linear equations, analogous to (1.122), satisﬁed\nby the coefﬁcientswi which minimize the regularized sum-of-squares error function\ngiven by (1.4).\n1.3 (⋆⋆ ) Suppose that we have three coloured boxesr (red), b (blue), and g (green).\nBox r contains 3 apples, 4 oranges, and 3 limes, boxb contains 1 apple, 1 orange,\nand 0 limes, and boxg contains 3 apples, 3 oranges, and 4 limes. If a box is chosen\nat random with probabilities p(r)=0 .2, p(b)=0 .2, p(g)=0 .6, and a piece of\nfruit is removed from the box (with equal probability of selecting any of the items in",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 77,
      "page_label": "58"
    }
  },
  {
    "page_content": "fruit is removed from the box (with equal probability of selecting any of the items in\nthe box), then what is the probability of selecting an apple? If we observe that the\nselected fruit is in fact an orange, what is the probability that it came from the green\nbox?\n1.4 (⋆⋆ ) www Consider a probability densitypx(x) deﬁned over a continuous vari-\nable x, and suppose that we make a nonlinear change of variable usingx = g(y),\nso that the density transforms according to (1.27). By differentiating (1.27), show\nthat the locationˆy of the maximum of the density iny is not in general related to the\nlocation ˆx of the maximum of the density overx by the simple functional relation\nˆx = g(ˆy) as a consequence of the Jacobian factor. This shows that the maximum\nof a probability density (in contrast to a simple function) is dependent on the choice\nof variable. V erify that, in the case of a linear transformation, the location of the\nmaximum transforms in the same way as the variable itself.\n1.5 (⋆) Using the deﬁnition (1.38) show thatvar[f(x)] satisﬁes (1.39).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 77,
      "page_label": "58"
    }
  },
  {
    "page_content": "Exercises 59\n1.6 (⋆) Show that if two variablesx and y are independent, then their covariance is\nzero.\n1.7 (⋆⋆ ) www In this exercise, we prove the normalization condition (1.48) for the\nunivariate Gaussian. To do this consider, the integral\nI =\n∫ ∞\n−∞\nexp\n(\n− 1\n2σ2 x2\n)\ndx (1.124)\nwhich we can evaluate by ﬁrst writing its square in the form\nI2 =\n∫ ∞\n−∞\n∫ ∞\n−∞\nexp\n(\n− 1\n2σ2 x2 − 1\n2σ2 y2\n)\ndxdy. (1.125)\nNow make the transformation from Cartesian coordinates(x, y) to polar coordinates\n(r, θ) and then substituteu = r2. Show that, by performing the integrals overθ and\nu, and then taking the square root of both sides, we obtain\nI =\n(\n2πσ2)1/2\n. (1.126)\nFinally, use this result to show that the Gaussian distributionN(x|µ, σ2) is normal-\nized.\n1.8 (⋆⋆ ) www By using a change of variables, verify that the univariate Gaussian\ndistribution given by (1.46) satisﬁes (1.49). Next, by differentiating both sides of the\nnormalization condition\n∫ ∞\n−∞\nN\n(\nx|µ, σ2)\ndx =1 (1.127)\nwith respect toσ2, verify that the Gaussian satisﬁes (1.50). Finally, show that (1.51)\nholds.\n1.9 (⋆) www Show that the mode (i.e. the maximum) of the Gaussian distribution\n(1.46) is given by µ. Similarly, show that the mode of the multivariate Gaussian\n(1.52) is given byµ.\n1.10 (⋆) www Suppose that the two variables x and z are statistically independent.\nShow that the mean and variance of their sum satisﬁes\nE[x + z]= E[x]+ E[z] (1.128)\nvar[x + z]=v a r [ x]+v a r [z]. (1.129)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 78,
      "page_label": "59"
    }
  },
  {
    "page_content": "Show that the mean and variance of their sum satisﬁes\nE[x + z]= E[x]+ E[z] (1.128)\nvar[x + z]=v a r [ x]+v a r [z]. (1.129)\n1.11 (⋆) By setting the derivatives of the log likelihood function (1.54) with respect toµ\nand σ2 equal to zero, verify the results (1.55) and (1.56).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 78,
      "page_label": "59"
    }
  },
  {
    "page_content": "60 1. INTRODUCTION\n1.12 (⋆⋆ ) www Using the results (1.49) and (1.50), show that\nE[xnxm]= µ2 + Inmσ2 (1.130)\nwhere xn and xm denote data points sampled from a Gaussian distribution with mean\nµ and variance σ2, and Inm satisﬁes Inm =1 if n = m and Inm =0 otherwise.\nHence prove the results (1.57) and (1.58).\n1.13 (⋆) Suppose that the variance of a Gaussian is estimated using the result (1.56) but\nwith the maximum likelihood estimate µML replaced with the true value µ of the\nmean. Show that this estimator has the property that its expectation is given by the\ntrue varianceσ2.\n1.14 (⋆⋆ ) Show that an arbitrary square matrix with elements wij can be written in\nthe form wij = wS\nij + wA\nij where wS\nij and wA\nij are symmetric and anti-symmetric\nmatrices, respectively, satisfyingwS\nij = wS\nji and wA\nij = −wA\nji for all i and j.N o w\nconsider the second order term in a higher order polynomial inD dimensions, given\nby\nD∑\ni=1\nD∑\nj=1\nwijxixj. (1.131)\nShow that\nD∑\ni=1\nD∑\nj=1\nwijxixj =\nD∑\ni=1\nD∑\nj=1\nwS\nijxixj (1.132)\nso that the contribution from the anti-symmetric matrix vanishes. We therefore see\nthat, without loss of generality, the matrix of coefﬁcientswij can be chosen to be\nsymmetric, and so not all of theD2 elements of this matrix can be chosen indepen-\ndently. Show that the number of independent parameters in the matrixwS\nij is given\nby D(D +1 )/2.\n1.15 (⋆⋆⋆ ) www In this exercise and the next, we explore how the number of indepen-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 79,
      "page_label": "60"
    }
  },
  {
    "page_content": "ij is given\nby D(D +1 )/2.\n1.15 (⋆⋆⋆ ) www In this exercise and the next, we explore how the number of indepen-\ndent parameters in a polynomial grows with the orderM of the polynomial and with\nthe dimensionality D of the input space. We start by writing down theMth order\nterm for a polynomial inD dimensions in the form\nD∑\ni1=1\nD∑\ni2=1\n···\nD∑\niM =1\nwi1i2···iM xi1 xi2 ··· xiM . (1.133)\nThe coefﬁcients wi1i2···iM comprise DM elements, but the number of independent\nparameters is signiﬁcantly fewer due to the many interchange symmetries of the\nfactor xi1 xi2 ··· xiM . Begin by showing that the redundancy in the coefﬁcients can\nbe removed by rewriting thisMth order term in the form\nD∑\ni1=1\ni1∑\ni2=1\n···\niM−1∑\niM =1\n˜wi1i2···iM xi1 xi2 ··· xiM . (1.134)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 79,
      "page_label": "60"
    }
  },
  {
    "page_content": "Exercises 61\nNote that the precise relationship between the˜w coefﬁcients andw coefﬁcients need\nnot be made explicit. Use this result to show that the number ofindependent param-\neters n(D,M ), which appear at orderM, satisﬁes the following recursion relation\nn(D,M )=\nD∑\ni=1\nn(i, M− 1). (1.135)\nNext use proof by induction to show that the following result holds\nD∑\ni=1\n(i + M − 2)!\n(i − 1)! (M − 1)! = (D + M − 1)!\n(D − 1)! M! (1.136)\nwhich can be done by ﬁrst proving the result forD =1 and arbitrary M by making\nuse of the result0 !=1 , then assuming it is correct for dimensionD and verifying\nthat it is correct for dimensionD +1 . Finally, use the two previous results, together\nwith proof by induction, to show\nn(D,M )= (D + M − 1)!\n(D − 1)! M! . (1.137)\nTo do this, ﬁrst show that the result is true forM =2 , and any value ofD ⩾ 1,\nby comparison with the result of Exercise 1.14. Then make use of (1.135), together\nwith (1.136), to show that, if the result holds at orderM −1, then it will also hold at\norder M\n1.16 (⋆⋆⋆ ) In Exercise 1.15, we proved the result (1.135) for the number of independent\nparameters in theMth order term of aD-dimensional polynomial. We now ﬁnd an\nexpression for the total numberN(D,M ) of independent parameters in all of the\nterms up to and including theM6th order. First show thatN(D,M ) satisﬁes\nN(D,M )=\nM∑\nm=0\nn(D,m ) (1.138)\nwhere n(D,m ) is the number of independent parameters in the term of orderm.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 80,
      "page_label": "61"
    }
  },
  {
    "page_content": "N(D,M )=\nM∑\nm=0\nn(D,m ) (1.138)\nwhere n(D,m ) is the number of independent parameters in the term of orderm.\nNow make use of the result (1.137), together with proof by induction, to show that\nN(d, M)= (D + M)!\nD! M! . (1.139)\nThis can be done by ﬁrst proving that the result holds for M =0 and arbitrary\nD ⩾ 1, then assuming that it holds at orderM, and hence showing that it holds at\norder M +1 . Finally, make use of Stirling’s approximation in the form\nn! ≃ nne−n (1.140)\nfor large n to show that, for D ≫ M, the quantity N(D,M ) grows like DM ,\nand for M ≫ D it grows like MD. Consider a cubic (M =3 ) polynomial in D\ndimensions, and evaluate numerically the total number of independent parameters\nfor (i)D =1 0 and (ii) D = 100 , which correspond to typical small-scale and\nmedium-scale machine learning applications.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 80,
      "page_label": "61"
    }
  },
  {
    "page_content": "62 1. INTRODUCTION\n1.17 (⋆⋆ ) www The gamma function is deﬁned by\nΓ(x) ≡\n∫ ∞\n0\nux−1e−u du. (1.141)\nUsing integration by parts, prove the relationΓ(x +1 )= xΓ(x). Show also that\nΓ(1) = 1and hence thatΓ(x +1 )= x! when x is an integer.\n1.18 (⋆⋆ ) www We can use the result (1.126) to derive an expression for the surface\narea SD, and the volumeVD, of a sphere of unit radius inD dimensions. To do this,\nconsider the following result, which is obtained by transforming from Cartesian to\npolar coordinates\nD∏\ni=1\n∫ ∞\n−∞\ne−x2\ni dxi = SD\n∫ ∞\n0\ne−r2\nrD−1 dr. (1.142)\nUsing the deﬁnition (1.141) of the Gamma function, together with (1.126), evaluate\nboth sides of this equation, and hence show that\nSD = 2πD/2\nΓ(D/2). (1.143)\nNext, by integrating with respect to radius from0 to 1, show that the volume of the\nunit sphere inD dimensions is given by\nVD = SD\nD . (1.144)\nFinally, use the results Γ(1) = 1 and Γ(3/2) = √π/2 to show that (1.143) and\n(1.144) reduce to the usual expressions forD =2 and D =3 .\n1.19 (⋆⋆ ) Consider a sphere of radiusa in D-dimensions together with the concentric\nhypercube of side2a, so that the sphere touches the hypercube at the centres of each\nof its sides. By using the results of Exercise 1.18, show that the ratio of the volume\nof the sphere to the volume of the cube is given by\nvolume of sphere\nvolume of cube = πD/2\nD2D−1Γ(D/2). (1.145)\nNow make use of Stirling’s formula in the form\nΓ(x +1 )≃ (2π)1/2e−xxx+1/2 (1.146)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 81,
      "page_label": "62"
    }
  },
  {
    "page_content": "volume of sphere\nvolume of cube = πD/2\nD2D−1Γ(D/2). (1.145)\nNow make use of Stirling’s formula in the form\nΓ(x +1 )≃ (2π)1/2e−xxx+1/2 (1.146)\nwhich is valid forx ≫ 1, to show that, asD →∞ , the ratio (1.145) goes to zero.\nShow also that the ratio of the distance from the centre of the hypercube to one of\nthe corners, divided by the perpendicular distance to one of the sides, is\n√\nD, which\ntherefore goes to∞ as D →∞ . From these results we see that, in a space of high\ndimensionality, most of the volume of a cube is concentrated in the large number of\ncorners, which themselves become very long ‘spikes’!",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 81,
      "page_label": "62"
    }
  },
  {
    "page_content": "Exercises 63\n1.20 (⋆⋆ ) www In this exercise, we explore the behaviour of the Gaussian distribution\nin high-dimensional spaces. Consider a Gaussian distribution inD dimensions given\nby\np(x)= 1\n(2πσ2)D/2 exp\n(\n−∥x∥2\n2σ2\n)\n. (1.147)\nWe wish to ﬁnd the density with respect to radius in polar coordinates in which the\ndirection variables have been integrated out. To do this, show that the integral of\nthe probability density over a thin shell of radiusr and thickness ϵ, where ϵ ≪ 1,i s\ngiven byp(r)ϵ where\np(r)= SDrD−1\n(2πσ2)D/2 exp\n(\n− r2\n2σ2\n)\n(1.148)\nwhere SD is the surface area of a unit sphere inD dimensions. Show that the function\np(r) has a single stationary point located, for largeD,a tˆr ≃\n√\nDσ. By considering\np(ˆr + ϵ) where ϵ ≪ ˆr, show that for largeD,\np(ˆr + ϵ)= p(ˆr)e x p\n(\n− 3ϵ2\n2σ2\n)\n(1.149)\nwhich shows thatˆr is a maximum of the radial probability density and also thatp(r)\ndecays exponentially away from its maximum atˆr with length scale σ.W e h a v e\nalready seen that σ ≪ ˆr for large D, and so we see that most of the probability\nmass is concentrated in a thin shell at large radius. Finally, show that the probability\ndensity p(x) is larger at the origin than at the radiusˆr by a factor of exp(D/2).\nWe therefore see that most of the probability mass in a high-dimensional Gaussian\ndistribution is located at a different radius from the region of high probability density.\nThis property of distributions in spaces of high dimensionality will have important",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 82,
      "page_label": "63"
    }
  },
  {
    "page_content": "This property of distributions in spaces of high dimensionality will have important\nconsequences when we consider Bayesian inference of model parameters in later\nchapters.\n1.21 (⋆⋆ ) Consider two nonnegative numbers a and b, and show that, if a ⩽ b, then\na ⩽ (ab)1/2. Use this result to show that, if the decision regions of a two-class\nclassiﬁcation problem are chosen to minimize the probability of misclassiﬁcation,\nthis probability will satisfy\np(mistake) ⩽\n∫\n{p(x,C1)p(x,C2)}1/2 dx. (1.150)\n1.22 (⋆) www Given a loss matrix with elementsLkj, the expected risk is minimized\nif, for each x, we choose the class that minimizes (1.81). V erify that, when the\nloss matrix is given byLkj =1 − Ikj, where Ikj are the elements of the identity\nmatrix, this reduces to the criterion of choosing the class having the largest posterior\nprobability. What is the interpretation of this form of loss matrix?\n1.23 (⋆) Derive the criterion for minimizing the expected loss when there is a general\nloss matrix and general prior probabilities for the classes.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 82,
      "page_label": "63"
    }
  },
  {
    "page_content": "64 1. INTRODUCTION\n1.24 (⋆⋆ ) www Consider a classiﬁcation problem in which the loss incurred when\nan input vector from classCk is classiﬁed as belonging to classCj is given by the\nloss matrix Lkj, and for which the loss incurred in selecting the reject option isλ.\nFind the decision criterion that will give the minimum expected loss. V erify that this\nreduces to the reject criterion discussed in Section 1.5.3 when the loss matrix is given\nbyLkj =1 − Ikj. What is the relationship betweenλ and the rejection thresholdθ?\n1.25 (⋆) www Consider the generalization of the squared loss function (1.87) for a\nsingle target variablet to the case of multiple target variables described by the vector\nt given by\nE[L(t, y(x))] =\n∫∫\n∥y(x) − t∥2p(x, t)d xdt. (1.151)\nUsing the calculus of variations, show that the functiony(x) for which this expected\nloss is minimized is given byy(x)= Et[t|x]. Show that this result reduces to (1.89)\nfor the case of a single target variablet.\n1.26 (⋆) By expansion of the square in (1.151), derive a result analogous to (1.90) and\nhence show that the functiony(x) that minimizes the expected squared loss for the\ncase of a vectort of target variables is again given by the conditional expectation of\nt.\n1.27 (⋆⋆ ) www Consider the expected loss for regression problems under theLq loss\nfunction given by (1.91). Write down the condition thaty(x) must satisfy in order\nto minimize E[Lq]. Show that, for q =1 , this solution represents the conditional",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 83,
      "page_label": "64"
    }
  },
  {
    "page_content": "to minimize E[Lq]. Show that, for q =1 , this solution represents the conditional\nmedian, i.e., the function y(x) such that the probability mass fort<y (x) is the\nsame as fort ⩾ y(x). Also show that the minimum expectedLq loss for q → 0 is\ngiven by the conditional mode, i.e., by the functiony(x) equal to the value oft that\nmaximizes p(t|x) for each x.\n1.28 (⋆) In Section 1.6, we introduced the idea of entropyh(x) as the information gained\non observing the value of a random variablex having distribution p(x).W e s a w\nthat, for independent variablesx and y for which p(x, y)= p(x)p(y), the entropy\nfunctions are additive, so thath(x, y)= h(x)+ h(y). In this exercise, we derive the\nrelation between h and p in the form of a functionh(p). First show that h(p2)=\n2h(p), and hence by induction thath(pn)= nh(p) where n is a positive integer.\nHence show that h(pn/m)=( n/m)h(p) where m is also a positive integer. This\nimplies that h(px)= xh(p) where x is a positive rational number, and hence by\ncontinuity when it is a positive real number. Finally, show that this impliesh(p)\nmust take the formh(p) ∝ lnp.\n1.29 (⋆) www Consider an M-state discrete random variablex, and use Jensen’s in-\nequality in the form (1.115) to show that the entropy of its distributionp(x) satisﬁes\nH[x] ⩽ ln M.\n1.30 (⋆⋆ ) Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians\np(x)= N(x|µ, σ2) and q(x)= N(x|m, s2).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 83,
      "page_label": "64"
    }
  },
  {
    "page_content": "Exercises 65\nTable 1.3 The joint distributionp(x, y) for two binary variables\nx and y used in Exercise 1.39.\ny\n01\nx 0 1/3 1/3\n1 0 1/3\n1.31 (⋆⋆ ) www Consider two variablesx and y having joint distributionp(x, y). Show\nthat the differential entropy of this pair of variables satisﬁes\nH[x,y] ⩽ H[x]+H [y] (1.152)\nwith equality if, and only if,x and y are statistically independent.\n1.32 (⋆) Consider a vector x of continuous variables with distributionp(x) and corre-\nsponding entropy H[x]. Suppose that we make a nonsingular linear transformation\nof x to obtain a new variabley = Ax. Show that the corresponding entropy is given\nby H[y]=H [ x]+l n |A| where |A| denotes the determinant ofA.\n1.33 (⋆⋆ ) Suppose that the conditional entropy H[y|x] between two discrete random\nvariables x and y is zero. Show that, for all values of x such that p(x) > 0, the\nvariable y must be a function ofx, in other words for eachx there is only one value\nof y such that p(y|x) ̸=0.\n1.34 (⋆⋆ ) www Use the calculus of variations to show that the stationary point of the\nfunctional (1.108) is given by (1.108). Then use the constraints (1.105), (1.106),\nand (1.107) to eliminate the Lagrange multipliers and hence show that the maximum\nentropy solution is given by the Gaussian (1.109).\n1.35 (⋆) www Use the results (1.106) and (1.107) to show that the entropy of the\nunivariate Gaussian (1.109) is given by (1.110).\n1.36 (⋆) A strictly convex function is deﬁned as one for which every chord lies above",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 84,
      "page_label": "65"
    }
  },
  {
    "page_content": "univariate Gaussian (1.109) is given by (1.110).\n1.36 (⋆) A strictly convex function is deﬁned as one for which every chord lies above\nthe function. Show that this is equivalent to the condition that the second derivative\nof the function be positive.\n1.37 (⋆) Using the deﬁnition (1.111) together with the product rule of probability, prove\nthe result (1.112).\n1.38 (⋆⋆ ) www Using proof by induction, show that the inequality (1.114) for convex\nfunctions implies the result (1.115).\n1.39 (⋆⋆⋆ ) Consider two binary variablesx and y having the joint distribution given in\nTable 1.3.\nEvaluate the following quantities\n(a) H[x] (c) H[y|x] (e) H[x, y]\n(b) H[y] (d) H[x|y] (f) I[x, y].\nDraw a diagram to show the relationship between these various quantities.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 84,
      "page_label": "65"
    }
  },
  {
    "page_content": "66 1. INTRODUCTION\n1.40 (⋆) By applying Jensen’s inequality (1.115) withf(x)=l n x, show that the arith-\nmetic mean of a set of real numbers is never less than their geometrical mean.\n1.41 (⋆) www Using the sum and product rules of probability, show that the mutual\ninformation I(x,y) satisﬁes the relation (1.121).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 85,
      "page_label": "66"
    }
  },
  {
    "page_content": "2\nProbability\nDistributions\nIn Chapter 1, we emphasized the central role played by probability theory in the\nsolution of pattern recognition problems. We turn now to an exploration of some\nparticular examples of probability distributions and their properties. As well as be-\ning of great interest in their own right, these distributions can form building blocks\nfor more complex models and will be used extensively throughout the book. The\ndistributions introduced in this chapter will also serve another important purpose,\nnamely to provide us with the opportunity to discuss some key statistical concepts,\nsuch as Bayesian inference, in the context of simple models before we encounter\nthem in more complex situations in later chapters.\nOne role for the distributions discussed in this chapter is to model the prob-\nability distribution p(x) of a random variable x, given a ﬁnite set x1,..., xN of\nobservations. This problem is known as density estimation. For the purposes of\nthis chapter, we shall assume that the data points are independent and identically\ndistributed. It should be emphasized that the problem of density estimation is fun-\n67",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 86,
      "page_label": "67"
    }
  },
  {
    "page_content": "68 2. PROBABILITY DISTRIBUTIONS\ndamentally ill-posed, because there are inﬁnitely many probability distributions that\ncould have given rise to the observed ﬁnite data set. Indeed, any distributionp(x)\nthat is nonzero at each of the data pointsx1,..., xN is a potential candidate. The\nissue of choosing an appropriate distribution relates to the problem of model selec-\ntion that has already been encountered in the context of polynomial curve ﬁtting in\nChapter 1 and that is a central issue in pattern recognition.\nWe begin by considering the binomial and multinomial distributions for discrete\nrandom variables and the Gaussian distribution for continuous random variables.\nThese are speciﬁc examples ofparametric distributions, so-called because they are\ngoverned by a small number of adaptive parameters, such as the mean and variance in\nthe case of a Gaussian for example. To apply such models to the problem of density\nestimation, we need a procedure for determining suitable values for the parameters,\ngiven an observed data set. In a frequentist treatment, we choose speciﬁc values\nfor the parameters by optimizing some criterion, such as the likelihood function. By\ncontrast, in a Bayesian treatment we introduce prior distributions over the parameters\nand then use Bayes’ theorem to compute the corresponding posterior distribution\ngiven the observed data.\nWe shall see that an important role is played byconjugate priors, that lead to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 87,
      "page_label": "68"
    }
  },
  {
    "page_content": "given the observed data.\nWe shall see that an important role is played byconjugate priors, that lead to\nposterior distributions having the same functional form as the prior, and that there-\nfore lead to a greatly simpliﬁed Bayesian analysis. For example, the conjugate prior\nfor the parameters of the multinomial distribution is called theDirichlet distribution,\nwhile the conjugate prior for the mean of a Gaussian is another Gaussian. All of these\ndistributions are examples of theexponential family of distributions, which possess\na number of important properties, and which will be discussed in some detail.\nOne limitation of the parametric approach is that it assumes a speciﬁc functional\nform for the distribution, which may turn out to be inappropriate for a particular\napplication. An alternative approach is given bynonparametric density estimation\nmethods in which the form of the distribution typically depends on the size of the data\nset. Such models still contain parameters, but these control the model complexity\nrather than the form of the distribution. We end this chapter by considering three\nnonparametric methods based respectively on histograms, nearest-neighbours, and\nkernels.\n2.1. Binary Variables\nWe begin by considering a single binary random variablex ∈{ 0, 1}. For example,\nx might describe the outcome of ﬂipping a coin, withx =1 representing ‘heads’,\nand x =0 representing ‘tails’. We can imagine that this is a damaged coin so that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 87,
      "page_label": "68"
    }
  },
  {
    "page_content": "and x =0 representing ‘tails’. We can imagine that this is a damaged coin so that\nthe probability of landing heads is not necessarily the same as that of landing tails.\nThe probability ofx =1 will be denoted by the parameterµ so that\np(x =1 |µ)= µ (2.1)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 87,
      "page_label": "68"
    }
  },
  {
    "page_content": "2.1. Binary Variables 69\nwhere 0 ⩽ µ ⩽ 1, from which it follows thatp(x =0 |µ)=1 − µ. The probability\ndistribution overx can therefore be written in the form\nBern(x|µ)= µx(1 − µ)1−x (2.2)\nwhich is known as theBernoulli distribution. It is easily veriﬁed that this distributionExercise 2.1\nis normalized and that it has mean and variance given by\nE[x]= µ (2.3)\nvar[x]= µ(1 − µ). (2.4)\nNow suppose we have a data setD = {x1,...,x N } of observed values ofx.\nWe can construct the likelihood function, which is a function ofµ, on the assumption\nthat the observations are drawn independently fromp(x|µ), so that\np(D|µ)=\nN∏\nn=1\np(xn|µ)=\nN∏\nn=1\nµxn (1 − µ)1−xn . (2.5)\nIn a frequentist setting, we can estimate a value forµ by maximizing the likelihood\nfunction, or equivalently by maximizing the logarithm of the likelihood. In the case\nof the Bernoulli distribution, the log likelihood function is given by\nlnp(D|µ)=\nN∑\nn=1\nln p(xn|µ)=\nN∑\nn=1\n{xn ln µ +( 1− xn)l n ( 1− µ)}. (2.6)\nAt this point, it is worth noting that the log likelihood function depends on theN\nobservations xn only through their sum∑\nn xn. This sum provides an example of a\nsufﬁcient statistic for the data under this distribution, and we shall study the impor-\ntant role of sufﬁcient statistics in some detail. If we set the derivative oflnp(D|µ)Section 2.4\nwith respect toµ equal to zero, we obtain the maximum likelihood estimator\nµML = 1\nN\nN∑\nn=1\nxn (2.7)\nJacob Bernoulli\n1654–1705\nJacob Bernoulli, also known as",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 88,
      "page_label": "69"
    }
  },
  {
    "page_content": "µML = 1\nN\nN∑\nn=1\nxn (2.7)\nJacob Bernoulli\n1654–1705\nJacob Bernoulli, also known as\nJacques or James Bernoulli, was a\nSwiss mathematician and was the\nﬁrst of many in the Bernoulli family\nto pursue a career in science and\nmathematics. Although compelled\nto study philosophy and theology against his will by\nhis parents, he travelled extensively after graduating\nin order to meet with many of the leading scientists of\nhis time, including Boyle and Hooke in England. When\nhe returned to Switzerland, he taught mechanics and\nbecame Professor of Mathematics at Basel in 1687.\nUnfortunately, rivalry between Jacob and his younger\nbrother Johann turned an initially productive collabora-\ntion into a bitter and public dispute. Jacob’s most sig-\nniﬁcant contributions to mathematics appeared inThe\nArt of Conjecture published in 1713, eight years after\nhis death, which deals with topics in probability the-\nory including what has become known as the Bernoulli\ndistribution.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 88,
      "page_label": "69"
    }
  },
  {
    "page_content": "70 2. PROBABILITY DISTRIBUTIONS\nFigure 2.1 Histogram plot of the binomial dis-\ntribution (2.9) as a function of m for\nN =1 0 and µ =0 .25.\nm\n0 1 2 3 4 5 6 7 8 9 10\n0\n0.1\n0.2\n0.3\nwhich is also known as thesample mean. If we denote the number of observations\nof x =1 (heads) within this data set bym, then we can write (2.7) in the form\nµML = m\nN (2.8)\nso that the probability of landing heads is given, in this maximum likelihood frame-\nwork, by the fraction of observations of heads in the data set.\nNow suppose we ﬂip a coin, say, 3 times and happen to observe 3 heads. Then\nN = m =3 and µML =1 . In this case, the maximum likelihood result would\npredict that all future observations should give heads. Common sense tells us that\nthis is unreasonable, and in fact this is an extreme example of the over-ﬁtting associ-\nated with maximum likelihood. We shall see shortly how to arrive at more sensible\nconclusions through the introduction of a prior distribution overµ.\nWe can also work out the distribution of the numberm of observations ofx =1 ,\ngiven that the data set has size N. This is called the binomial distribution, and\nfrom (2.5) we see that it is proportional toµm(1 − µ)N−m. In order to obtain the\nnormalization coefﬁcient we note that out of N coin ﬂips, we have to add up all\nof the possible ways of obtainingm heads, so that the binomial distribution can be\nwritten\nBin(m|N,µ )=\n(N\nm\n)\nµm(1 − µ)N−m (2.9)\nwhere (N\nm\n)\n≡ N!\n(N − m)!m! (2.10)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 89,
      "page_label": "70"
    }
  },
  {
    "page_content": "written\nBin(m|N,µ )=\n(N\nm\n)\nµm(1 − µ)N−m (2.9)\nwhere (N\nm\n)\n≡ N!\n(N − m)!m! (2.10)\nis the number of ways of choosingm objects out of a total ofN identical objects.Exercise 2.3\nFigure 2.1 shows a plot of the binomial distribution forN =1 0 and µ =0 .25.\nThe mean and variance of the binomial distribution can be found by using the\nresult of Exercise 1.10, which shows that for independent events the mean of the\nsum is the sum of the means, and the variance of the sum is the sum of the variances.\nBecause m = x1 + ... + xN , and for each observation the mean and variance are",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 89,
      "page_label": "70"
    }
  },
  {
    "page_content": "2.1. Binary Variables 71\ngiven by (2.3) and (2.4), respectively, we have\nE[m] ≡\nN∑\nm=0\nmBin(m|N,µ )= Nµ (2.11)\nvar[m] ≡\nN∑\nm=0\n(m − E[m])2 Bin(m|N,µ )= Nµ(1 − µ). (2.12)\nThese results can also be proved directly using calculus.Exercise 2.4\n2.1.1 The beta distribution\nWe have seen in (2.8) that the maximum likelihood setting for the parameterµ\nin the Bernoulli distribution, and hence in the binomial distribution, is given by the\nfraction of the observations in the data set havingx =1 . As we have already noted,\nthis can give severely over-ﬁtted results for small data sets. In order to develop a\nBayesian treatment for this problem, we need to introduce a prior distributionp(µ)\nover the parameterµ. Here we consider a form of prior distribution that has a simple\ninterpretation as well as some useful analytical properties. To motivate this prior,\nwe note that the likelihood function takes the form of the product of factors of the\nformµx(1 − µ)1−x. If we choose a prior to be proportional to powers of µ and\n(1 − µ), then the posterior distribution, which is proportional to the product of the\nprior and the likelihood function, will have the same functional form as the prior.\nThis property is calledconjugacy and we will see several examples of it later in this\nchapter. We therefore choose a prior, called thebeta distribution, given by\nBeta(µ|a, b)= Γ(a + b)\nΓ(a)Γ(b)µa−1(1 − µ)b−1 (2.13)\nwhere Γ(x) is the gamma function deﬁned by (1.141), and the coefﬁcient in (2.13)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 90,
      "page_label": "71"
    }
  },
  {
    "page_content": "Beta(µ|a, b)= Γ(a + b)\nΓ(a)Γ(b)µa−1(1 − µ)b−1 (2.13)\nwhere Γ(x) is the gamma function deﬁned by (1.141), and the coefﬁcient in (2.13)\nensures that the beta distribution is normalized, so thatExercise 2.5\n∫ 1\n0\nBeta(µ|a, b)d µ =1 . (2.14)\nThe mean and variance of the beta distribution are given byExercise 2.6\nE[µ]= a\na + b (2.15)\nvar[µ]= ab\n(a + b)2(a + b +1 ). (2.16)\nThe parameters a and b are often called hyperparameters because they control the\ndistribution of the parameterµ. Figure 2.2 shows plots of the beta distribution for\nvarious values of the hyperparameters.\nThe posterior distribution of µ is now obtained by multiplying the beta prior\n(2.13) by the binomial likelihood function (2.9) and normalizing. Keeping only the\nfactors that depend onµ, we see that this posterior distribution has the form\np(µ|m, l, a, b) ∝ µm+a−1(1 − µ)l+b−1 (2.17)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 90,
      "page_label": "71"
    }
  },
  {
    "page_content": "72 2. PROBABILITY DISTRIBUTIONS\nµ\na =0 .1\nb =0 .1\n0 0.5 1\n0\n1\n2\n3\nµ\na =1\nb =1\n0 0.5 1\n0\n1\n2\n3\nµ\na =2\nb =3\n0 0.5 1\n0\n1\n2\n3\nµ\na =8\nb =4\n0 0.5 1\n0\n1\n2\n3\nFigure 2.2 Plots of the beta distribution Beta(µ|a, b) given by (2.13) as a function of µ for various values of the\nhyperparameters a and b.\nwhere l = N − m, and therefore corresponds to the number of ‘tails’ in the coin\nexample. We see that (2.17) has the same functional dependence onµ as the prior\ndistribution, reﬂecting the conjugacy properties of the prior with respect to the like-\nlihood function. Indeed, it is simply another beta distribution, and its normalization\ncoefﬁcient can therefore be obtained by comparison with (2.13) to give\np(µ|m, l, a, b)= Γ(m + a + l + b)\nΓ(m + a)Γ(l + b)µm+a−1(1 − µ)l+b−1. (2.18)\nWe see that the effect of observing a data set ofm observations of x =1 and\nl observations of x =0 has been to increase the value ofa by m, and the value of\nb by l, in going from the prior distribution to the posterior distribution. This allows\nus to provide a simple interpretation of the hyperparametersa and b in the prior as\nan effective number of observations of x =1 and x =0 , respectively. Note that\na and b need not be integers. Furthermore, the posterior distribution can act as the\nprior if we subsequently observe additional data. To see this, we can imagine taking\nobservations one at a time and after each observation updating the current posterior",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 91,
      "page_label": "72"
    }
  },
  {
    "page_content": "2.1. Binary Variables 73\nµ\nprior\n0 0.5 1\n0\n1\n2\nµ\nlikelihood function\n0 0.5 1\n0\n1\n2\nµ\nposterior\n0 0.5 1\n0\n1\n2\nFigure 2.3 Illustration of one step of sequential Bayesian inference. The prior is given by a beta distribution\nwith parameters a =2 , b =2 , and the likelihood function, given by (2.9) with N = m =1 , corresponds to a\nsingle observation of x =1 , so that the posterior is given by a beta distribution with parameters a =3 , b =2 .\ndistribution by multiplying by the likelihood function for the new observation and\nthen normalizing to obtain the new, revised posterior distribution. At each stage, the\nposterior is a beta distribution with some total number of (prior and actual) observed\nvalues for x =1 and x =0 given by the parametersa and b. Incorporation of an\nadditional observation ofx =1 simply corresponds to incrementing the value ofa\nby 1, whereas for an observation ofx =0 we incrementb by 1. Figure 2.3 illustrates\none step in this process.\nWe see that thissequential approach to learning arises naturally when we adopt\na Bayesian viewpoint. It is independent of the choice of prior and of the likelihood\nfunction and depends only on the assumption of i.i.d. data. Sequential methods make\nuse of observations one at a time, or in small batches, and then discard them before\nthe next observations are used. They can be used, for example, in real-time learning\nscenarios where a steady stream of data is arriving, and predictions must be made",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 92,
      "page_label": "73"
    }
  },
  {
    "page_content": "scenarios where a steady stream of data is arriving, and predictions must be made\nbefore all of the data is seen. Because they do not require the whole data set to be\nstored or loaded into memory, sequential methods are also useful for large data sets.\nMaximum likelihood methods can also be cast into a sequential framework.Section 2.3.5\nIf our goal is to predict, as best we can, the outcome of the next trial, then we\nmust evaluate the predictive distribution ofx, given the observed data setD. From\nthe sum and product rules of probability, this takes the form\np(x =1 |D)=\n∫ 1\n0\np(x =1 |µ)p(µ|D)d µ =\n∫ 1\n0\nµp(µ|D)d µ = E[µ|D]. (2.19)\nUsing the result (2.18) for the posterior distributionp(µ|D), together with the result\n(2.15) for the mean of the beta distribution, we obtain\np(x =1 |D)= m + a\nm + a + l + b (2.20)\nwhich has a simple interpretation as the total fraction of observations (both real ob-\nservations and ﬁctitious prior observations) that correspond tox =1 . Note that in\nthe limit of an inﬁnitely large data setm, l →∞ the result (2.20) reduces to the\nmaximum likelihood result (2.8). As we shall see, it is a very general property that\nthe Bayesian and maximum likelihood results will agree in the limit of an inﬁnitely",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 92,
      "page_label": "73"
    }
  },
  {
    "page_content": "74 2. PROBABILITY DISTRIBUTIONS\nlarge data set. For a ﬁnite data set, the posterior mean forµ always lies between the\nprior mean and the maximum likelihood estimate forµ corresponding to the relative\nfrequencies of events given by (2.7).Exercise 2.7\nFrom Figure 2.2, we see that as the number of observations increases, so the\nposterior distribution becomes more sharply peaked. This can also be seen from\nthe result (2.16) for the variance of the beta distribution, in which we see that the\nvariance goes to zero fora →∞ or b →∞ . In fact, we might wonder whether it is\na general property of Bayesian learning that, as we observe more and more data, the\nuncertainty represented by the posterior distribution will steadily decrease.\nTo address this, we can take a frequentist view of Bayesian learning and show\nthat, on average, such a property does indeed hold. Consider a general Bayesian\ninference problem for a parameterθ for which we have observed a data setD, de-\nscribed by the joint distributionp(θ, D). The following resultExercise 2.8\nEθ[θ]= ED [Eθ[θ|D]] (2.21)\nwhere\nEθ[θ] ≡\n∫\np(θ)θ dθ (2.22)\nED[Eθ[θ|D]] ≡\n∫ {∫\nθp(θ|D)d θ\n}\np(D)d D (2.23)\nsays that the posterior mean ofθ, averaged over the distribution generating the data,\nis equal to the prior mean ofθ. Similarly, we can show that\nvarθ[θ]= ED [varθ[θ|D]] + varD [Eθ[θ|D]]. (2.24)\nThe term on the left-hand side of (2.24) is the prior variance ofθ. On the right-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 93,
      "page_label": "74"
    }
  },
  {
    "page_content": "varθ[θ]= ED [varθ[θ|D]] + varD [Eθ[θ|D]]. (2.24)\nThe term on the left-hand side of (2.24) is the prior variance ofθ. On the right-\nhand side, the ﬁrst term is the average posterior variance ofθ, and the second term\nmeasures the variance in the posterior mean ofθ. Because this variance is a positive\nquantity, this result shows that, on average, the posterior variance ofθ is smaller than\nthe prior variance. The reduction in variance is greater if the variance in the posterior\nmean is greater. Note, however, that this result only holds on average, and that for a\nparticular observed data set it is possible for the posterior variance to be larger than\nthe prior variance.\n2.2. Multinomial Variables\nBinary variables can be used to describe quantities that can take one of two possible\nvalues. Often, however, we encounter discrete variables that can take on one ofK\npossible mutually exclusive states. Although there are various alternative ways to\nexpress such variables, we shall see shortly that a particularly convenient represen-\ntation is the1-of-K scheme in which the variable is represented by aK-dimensional\nvector x in which one of the elementsxk equals 1, and all remaining elements equal",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 93,
      "page_label": "74"
    }
  },
  {
    "page_content": "2.2. Multinomial Variables 75\n0. So, for instance if we have a variable that can takeK =6 states and a particular\nobservation of the variable happens to correspond to the state wherex3 =1 , then x\nwill be represented by\nx =( 0, 0, 1, 0, 0, 0)T. (2.25)\nNote that such vectors satisfy∑ K\nk=1 xk =1 . If we denote the probability ofxk =1\nby the parameterµk, then the distribution ofx is given\np(x|µ)=\nK∏\nk=1\nµxk\nk (2.26)\nwhere µ =( µ1,...,µ K)T, and the parametersµk are constrained to satisfyµk ⩾ 0\nand ∑\nk µk =1 , because they represent probabilities. The distribution (2.26) can be\nregarded as a generalization of the Bernoulli distribution to more than two outcomes.\nIt is easily seen that the distribution is normalized\n∑\nx\np(x|µ)=\nK∑\nk=1\nµk =1 (2.27)\nand that\nE[x|µ]=\n∑\nx\np(x|µ)x =( µ1,...,µ M )T = µ. (2.28)\nNow consider a data set D of N independent observations x1,..., xN . The\ncorresponding likelihood function takes the form\np(D|µ)=\nN∏\nn=1\nK∏\nk=1\nµxnk\nk =\nK∏\nk=1\nµ(\nP\nn xnk)\nk =\nK∏\nk=1\nµmk\nk . (2.29)\nWe see that the likelihood function depends on theN data points only through the\nK quantities\nmk =\n∑\nn\nxnk (2.30)\nwhich represent the number of observations ofxk =1 . These are called thesufﬁcient\nstatistics for this distribution.Section 2.4\nIn order to ﬁnd the maximum likelihood solution forµ, we need to maximize\nlnp(D|µ) with respect toµk taking account of the constraint that theµk must sum\nto one. This can be achieved using a Lagrange multiplierλ and maximizingAppendix E\nK∑\nk=1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 94,
      "page_label": "75"
    }
  },
  {
    "page_content": "to one. This can be achieved using a Lagrange multiplierλ and maximizingAppendix E\nK∑\nk=1\nmk lnµk + λ\n( K∑\nk=1\nµk − 1\n)\n. (2.31)\nSetting the derivative of (2.31) with respect toµk to zero, we obtain\nµk = −mk/λ. (2.32)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 94,
      "page_label": "75"
    }
  },
  {
    "page_content": "76 2. PROBABILITY DISTRIBUTIONS\nWe can solve for the Lagrange multiplierλ by substituting (2.32) into the constraint∑\nk µk =1 to give λ = −N. Thus we obtain the maximum likelihood solution in\nthe form\nµML\nk = mk\nN (2.33)\nwhich is the fraction of theN observations for whichxk =1 .\nWe can consider the joint distribution of the quantitiesm1,...,m K, conditioned\non the parameters µ and on the total numberN of observations. From (2.29) this\ntakes the form\nMult(m1,m 2,...,m K|µ,N )=\n( N\nm1m2 ...m K\n) K∏\nk=1\nµmk\nk (2.34)\nwhich is known as themultinomial distribution. The normalization coefﬁcient is the\nnumber of ways of partitioningN objects into K groups of sizem1,...,m K and is\ngiven by ( N\nm1m2 ...m K\n)\n= N!\nm1!m2! ...m K!. (2.35)\nNote that the variablesmk are subject to the constraint\nK∑\nk=1\nmk = N. (2.36)\n2.2.1 The Dirichlet distribution\nWe now introduce a family of prior distributions for the parameters{µk} of\nthe multinomial distribution (2.34). By inspection of the form of the multinomial\ndistribution, we see that the conjugate prior is given by\np(µ|α) ∝\nK∏\nk=1\nµαk−1\nk (2.37)\nwhere 0 ⩽ µk ⩽ 1 and ∑\nk µk =1 . Here α1,...,α K are the parameters of the\ndistribution, and α denotes (α1,...,α K)T. Note that, because of the summation\nconstraint, the distribution over the space of the{µk} is conﬁned to a simplex of\ndimensionality K − 1, as illustrated forK =3 in Figure 2.4.\nThe normalized form for this distribution is byExercise 2.9\nDir(µ|α)= Γ(α0)\nΓ(α1) ··· Γ(αK)\nK∏\nk=1\nµαk−1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 95,
      "page_label": "76"
    }
  },
  {
    "page_content": "The normalized form for this distribution is byExercise 2.9\nDir(µ|α)= Γ(α0)\nΓ(α1) ··· Γ(αK)\nK∏\nk=1\nµαk−1\nk (2.38)\nwhich is called theDirichlet distribution. Here Γ(x) is the gamma function deﬁned\nby (1.141) while\nα0 =\nK∑\nk=1\nαk. (2.39)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 95,
      "page_label": "76"
    }
  },
  {
    "page_content": "2.2. Multinomial Variables 77\nFigure 2.4 The Dirichlet distribution over three variables µ1,µ 2,µ 3\nis conﬁned to a simplex (a bounded linear manifold) of\nthe form shown, as a consequence of the constraints\n0 ⩽ µk ⩽ 1 and P\nk µk =1 .\nµ1\nµ2\nµ3\nPlots of the Dirichlet distribution over the simplex, for various settings of the param-\neters αk, are shown in Figure 2.5.\nMultiplying the prior (2.38) by the likelihood function (2.34), we obtain the\nposterior distribution for the parameters{µk} in the form\np(µ|D, α) ∝ p(D|µ)p(µ|α) ∝\nK∏\nk=1\nµαk+mk−1\nk . (2.40)\nWe see that the posterior distribution again takes the form of a Dirichlet distribution,\nconﬁrming that the Dirichlet is indeed a conjugate prior for the multinomial. This\nallows us to determine the normalization coefﬁcient by comparison with (2.38) so\nthat\np(µ|D, α)=D i r ( µ|α + m)\n= Γ(α0 + N)\nΓ(α1 + m1) ··· Γ(αK + mK)\nK∏\nk=1\nµαk+mk−1\nk (2.41)\nwhere we have denoted m =( m1,...,m K)T. As for the case of the binomial\ndistribution with its beta prior, we can interpret the parametersαk of the Dirichlet\nprior as an effective number of observations ofxk =1 .\nNote that two-state quantities can either be represented as binary variables and\nLejeune Dirichlet\n1805–1859\nJohann Peter Gustav Lejeune\nDirichlet was a modest and re-\nserved mathematician who made\ncontributions in number theory, me-\nchanics, and astronomy, and who\ngave the ﬁrst rigorous analysis of\nFourier series. His family originated from Richelet",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 96,
      "page_label": "77"
    }
  },
  {
    "page_content": "chanics, and astronomy, and who\ngave the ﬁrst rigorous analysis of\nFourier series. His family originated from Richelet\nin Belgium, and the name Lejeune Dirichlet comes\nfrom ‘le jeune de Richelet’ (the young person from\nRichelet). Dirichlet’s ﬁrst paper, which was published\nin 1825, brought him instant fame. It concerned Fer-\nmat’s last theorem, which claims that there are no\npositive integer solutions to xn + yn = zn for n> 2.\nDirichlet gave a partial proof for the case n =5 , which\nwas sent to Legendre for review and who in turn com-\npleted the proof. Later, Dirichlet gave a complete proof\nfor n =1 4, although a full proof of Fermat’s last theo-\nrem for arbitrary n had to wait until the work of Andrew\nWiles in the closing years of the 20th century.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 96,
      "page_label": "77"
    }
  },
  {
    "page_content": "78 2. PROBABILITY DISTRIBUTIONS\nFigure 2.5 Plots of the Dirichlet distribution over three variables, where the two horizontal axes are coordinates\nin the plane of the simplex and the vertical axis corresponds to the value of the density . Here{αk} =0 .1 on the\nleft plot, {αk} =1 in the centre plot, and{αk} =1 0 in the right plot.\nmodelled using the binomial distribution (2.9) or as 1-of-2 variables and modelled\nusing the multinomial distribution (2.34) withK =2 .\n2.3.\n The Gaussian Distribution\nThe Gaussian, also known as the normal distribution, is a widely used model for the\ndistribution of continuous variables. In the case of a single variablex, the Gaussian\ndistribution can be written in the form\nN(x|µ, σ2)= 1\n(2πσ2)1/2 exp\n{\n− 1\n2σ2 (x − µ)2\n}\n(2.42)\nwhere µ is the mean and σ2 is the variance. For a D-dimensional vector x,t h e\nmultivariate Gaussian distribution takes the form\nN(x|µ, Σ)= 1\n(2π)D/2\n1\n|Σ|1/2 exp\n{\n−1\n2(x − µ)TΣ−1(x − µ)\n}\n(2.43)\nwhere µ is a D-dimensional mean vector,Σ is a D × D covariance matrix, and|Σ|\ndenotes the determinant ofΣ.\nThe Gaussian distribution arises in many different contexts and can be motivated\nfrom a variety of different perspectives. For example, we have already seen that forSection 1.6\na single real variable, the distribution that maximizes the entropy is the Gaussian.\nThis property applies also to the multivariate Gaussian.Exercise 2.14\nAnother situation in which the Gaussian distribution arises is when we consider",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 97,
      "page_label": "78"
    }
  },
  {
    "page_content": "This property applies also to the multivariate Gaussian.Exercise 2.14\nAnother situation in which the Gaussian distribution arises is when we consider\nthe sum of multiple random variables. The central limit theorem (due to Laplace)\ntells us that, subject to certain mild conditions, the sum of a set of random variables,\nwhich is of course itself a random variable, has a distribution that becomes increas-\ningly Gaussian as the number of terms in the sum increases (Walker, 1969). We can",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 97,
      "page_label": "78"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 79\nN =1\n0 0.5 1\n0\n1\n2\n3 N =2\n0 0.5 1\n0\n1\n2\n3 N =1 0\n0 0.5 1\n0\n1\n2\n3\nFigure 2.6 Histogram plots of the mean of N uniformly distributed numbers for various values of N.W e\nobserve that as N increases, the distribution tends towards a Gaussian.\nillustrate this by considering N variables x1,...,x N each of which has a uniform\ndistribution over the interval [0,1] and then considering the distribution of the mean\n(x1 + ··· + xN )/N. For large N, this distribution tends to a Gaussian, as illustrated\nin Figure 2.6. In practice, the convergence to a Gaussian as N increases can be\nvery rapid. One consequence of this result is that the binomial distribution (2.9),\nwhich is a distribution over m deﬁned by the sum of N observations of the random\nbinary variable x, will tend to a Gaussian as N →∞ (see Figure 2.1 for the case of\nN =1 0).\nThe Gaussian distribution has many important analytical properties, and we shall\nconsider several of these in detail. As a result, this section will be rather more tech-\nnically involved than some of the earlier sections, and will require familiarity with\nvarious matrix identities. However, we strongly encourage the reader to become pro-Appendix C\nﬁcient in manipulating Gaussian distributions using the techniques presented here as\nthis will prove invaluable in understanding the more complex models presented in\nlater chapters.\nWe begin by considering the geometrical form of the Gaussian distribution. The\nCarl Friedrich Gauss",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 98,
      "page_label": "79"
    }
  },
  {
    "page_content": "later chapters.\nWe begin by considering the geometrical form of the Gaussian distribution. The\nCarl Friedrich Gauss\n1777–1855\nIt is said that when Gauss went\nto elementary school at age 7, his\nteacher B ¨uttner, trying to keep the\nclass occupied, asked the pupils to\nsum the integers from 1 to 100. To\nthe teacher’s amazement, Gauss\narrived at the answer in a matter of moments by noting\nthat the sum can be represented as 50 pairs (1 + 100,\n2+99, etc.) each of which added to 101, giving the an-\nswer 5,050. It is now believed that the problem which\nwas actually set was of the same form but somewhat\nharder in that the sequence had a larger starting value\nand a larger increment. Gauss was a German math-\nematician and scientist with a reputation for being a\nhard-working perfectionist. One of his many contribu-\ntions was to show that least squares can be derived\nunder the assumption of normally distributed errors.\nHe also created an early formulation of non-Euclidean\ngeometry (a self-consistent geometrical theory that vi-\nolates the axioms of Euclid) but was reluctant to dis-\ncuss it openly for fear that his reputation might suffer\nif it were seen that he believed in such a geometry.\nAt one point, Gauss was asked to conduct a geodetic\nsurvey of the state of Hanover, which led to his for-\nmulation of the normal distribution, now also known\nas the Gaussian. After his death, a study of his di-\naries revealed that he had discovered several impor-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 98,
      "page_label": "79"
    }
  },
  {
    "page_content": "as the Gaussian. After his death, a study of his di-\naries revealed that he had discovered several impor-\ntant mathematical results years or even decades be-\nfore they were published by others.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 98,
      "page_label": "79"
    }
  },
  {
    "page_content": "80 2. PROBABILITY DISTRIBUTIONS\nfunctional dependence of the Gaussian on x is through the quadratic form\n∆2 =( x − µ)TΣ−1(x − µ) (2.44)\nwhich appears in the exponent. The quantity ∆ is called the Mahalanobis distance\nfrom µ to x and reduces to the Euclidean distance whenΣ is the identity matrix. The\nGaussian distribution will be constant on surfaces inx-space for which this quadratic\nform is constant.\nFirst of all, we note that the matrix Σ can be taken to be symmetric, without\nloss of generality, because any antisymmetric component would disappear from the\nexponent. Now consider the eigenvector equation for the covariance matrixExercise 2.17\nΣui = λiui (2.45)\nwhere i =1 ,...,D . Because Σ is a real, symmetric matrix its eigenvalues will be\nreal, and its eigenvectors can be chosen to form an orthonormal set, so thatExercise 2.18\nuT\ni uj = Iij (2.46)\nwhere Iij is the i, jelement of the identity matrix and satisﬁes\nIij =\n{\n1, if i = j\n0, otherwise. (2.47)\nThe covariance matrix Σ can be expressed as an expansion in terms of its eigenvec-\ntors in the formExercise 2.19\nΣ =\nD∑\ni=1\nλiuiuT\ni (2.48)\nand similarly the inverse covariance matrix Σ−1 can be expressed as\nΣ−1 =\nD∑\ni=1\n1\nλi\nuiuT\ni . (2.49)\nSubstituting (2.49) into (2.44), the quadratic form becomes\n∆2 =\nD∑\ni=1\ny2\ni\nλi\n(2.50)\nwhere we have deﬁned\nyi = uT\ni (x − µ). (2.51)\nWe can interpret{yi} as a new coordinate system deﬁned by the orthonormal vectors",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 99,
      "page_label": "80"
    }
  },
  {
    "page_content": "i=1\ny2\ni\nλi\n(2.50)\nwhere we have deﬁned\nyi = uT\ni (x − µ). (2.51)\nWe can interpret{yi} as a new coordinate system deﬁned by the orthonormal vectors\nui that are shifted and rotated with respect to the original xi coordinates. Forming\nthe vector y =( y1,...,y D)T,w eh a v e\ny = U(x − µ) (2.52)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 99,
      "page_label": "80"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 81\nFigure 2.7 The red curve shows the ellip-\ntical surface of constant proba-\nbility density for a Gaussian in\na two-dimensional space x =\n(x1,x 2) on which the density\nis exp(−1/2) of its value at\nx = µ. The major axes of\nthe ellipse are deﬁned by the\neigenvectors ui of the covari-\nance matrix, with correspond-\ning eigenvalues λi.\nx1\nx2\nλ1/2\n1\nλ1/2\n2\ny1\ny2\nu1\nu2\nµ\nwhere U is a matrix whose rows are given by uT\ni . From (2.46) it follows that U is\nan orthogonal matrix, i.e., it satisﬁes UUT = I, and hence also UTU = I, where IAppendix C\nis the identity matrix.\nThe quadratic form, and hence the Gaussian density, will be constant on surfaces\nfor which (2.51) is constant. If all of the eigenvalues λi are positive, then these\nsurfaces represent ellipsoids, with their centres atµ and their axes oriented alongui,\nand with scaling factors in the directions of the axes given by λ1/2\ni , as illustrated in\nFigure 2.7.\nFor the Gaussian distribution to be well deﬁned, it is necessary for all of the\neigenvalues λi of the covariance matrix to be strictly positive, otherwise the dis-\ntribution cannot be properly normalized. A matrix whose eigenvalues are strictly\npositive is said to be positive deﬁnite. In Chapter 12, we will encounter Gaussian\ndistributions for which one or more of the eigenvalues are zero, in which case the\ndistribution is singular and is conﬁned to a subspace of lower dimensionality. If all",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 100,
      "page_label": "81"
    }
  },
  {
    "page_content": "distribution is singular and is conﬁned to a subspace of lower dimensionality. If all\nof the eigenvalues are nonnegative, then the covariance matrix is said to be positive\nsemideﬁnite.\nNow consider the form of the Gaussian distribution in the new coordinate system\ndeﬁned by theyi. In going from thex to the y coordinate system, we have a Jacobian\nmatrix J with elements given by\nJij = ∂xi\n∂yj\n= Uji (2.53)\nwhere Uji are the elements of the matrix UT. Using the orthonormality property of\nthe matrix U, we see that the square of the determinant of the Jacobian matrix is\n|J|2 =\n⏐⏐UT⏐⏐2\n=\n⏐⏐UT⏐⏐|U| =\n⏐⏐UTU\n⏐⏐= |I| =1 (2.54)\nand hence |J| =1 . Also, the determinant |Σ| of the covariance matrix can be written",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 100,
      "page_label": "81"
    }
  },
  {
    "page_content": "82 2. PROBABILITY DISTRIBUTIONS\nas the product of its eigenvalues, and hence\n|Σ|1/2 =\nD∏\nj=1\nλ1/2\nj . (2.55)\nThus in the yj coordinate system, the Gaussian distribution takes the form\np(y)= p(x)|J| =\nD∏\nj=1\n1\n(2πλj)1/2 exp\n{\n− y2\nj\n2λj\n}\n(2.56)\nwhich is the product of D independent univariate Gaussian distributions. The eigen-\nvectors therefore deﬁne a new set of shifted and rotated coordinates with respect\nto which the joint probability distribution factorizes into a product of independent\ndistributions. The integral of the distribution in the y coordinate system is then\n∫\np(y)d y =\nD∏\nj=1\n∫ ∞\n−∞\n1\n(2πλj)1/2 exp\n{\n− y2\nj\n2λj\n}\ndyj =1 (2.57)\nwhere we have used the result (1.48) for the normalization of the univariate Gaussian.\nThis conﬁrms that the multivariate Gaussian (2.43) is indeed normalized.\nWe now look at the moments of the Gaussian distribution and thereby provide an\ninterpretation of the parameters µ and Σ. The expectation of x under the Gaussian\ndistribution is given by\nE[x]= 1\n(2π)D/2\n1\n|Σ|1/2\n∫\nexp\n{\n−1\n2(x − µ)TΣ−1(x − µ)\n}\nxdx\n= 1\n(2π)D/2\n1\n|Σ|1/2\n∫\nexp\n{\n−1\n2zTΣ−1z\n}\n(z + µ)d z (2.58)\nwhere we have changed variables using z = x − µ. We now note that the exponent\nis an even function of the components of z and, because the integrals over these are\ntaken over the range (−∞, ∞), the term in z in the factor (z + µ) will vanish by\nsymmetry. Thus\nE[x]= µ (2.59)\nand so we refer to µ as the mean of the Gaussian distribution.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 101,
      "page_label": "82"
    }
  },
  {
    "page_content": "symmetry. Thus\nE[x]= µ (2.59)\nand so we refer to µ as the mean of the Gaussian distribution.\nWe now consider second order moments of the Gaussian. In the univariate case,\nwe considered the second order moment given by E[x2]. For the multivariate Gaus-\nsian, there are D2 second order moments given by E[xixj], which we can group\ntogether to form the matrix E[xxT]. This matrix can be written as\nE[xxT]= 1\n(2π)D/2\n1\n|Σ|1/2\n∫\nexp\n{\n−1\n2(x − µ)TΣ−1(x − µ)\n}\nxxT dx\n= 1\n(2π)D/2\n1\n|Σ|1/2\n∫\nexp\n{\n−1\n2zTΣ−1z\n}\n(z + µ)(z + µ)T dz",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 101,
      "page_label": "82"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 83\nwhere again we have changed variables using z = x − µ. Note that the cross-terms\ninvolving µzT and µTz will again vanish by symmetry. The term µµT is constant\nand can be taken outside the integral, which itself is unity because the Gaussian\ndistribution is normalized. Consider the term involving zzT. Again, we can make\nuse of the eigenvector expansion of the covariance matrix given by (2.45), together\nwith the completeness of the set of eigenvectors, to write\nz =\nD∑\nj=1\nyjuj (2.60)\nwhere yj = uT\nj z, which gives\n1\n(2π)D/2\n1\n|Σ|1/2\n∫\nexp\n{\n−1\n2zTΣ−1z\n}\nzzT dz\n= 1\n(2π)D/2\n1\n|Σ|1/2\nD∑\ni=1\nD∑\nj=1\nuiuT\nj\n∫\nexp\n{\n−\nD∑\nk=1\ny2\nk\n2λk\n}\nyiyj dy\n=\nD∑\ni=1\nuiuT\ni λi = Σ (2.61)\nwhere we have made use of the eigenvector equation (2.45), together with the fact\nthat the integral on the right-hand side of the middle line vanishes by symmetry\nunlessi = j, and in the ﬁnal line we have made use of the results (1.50) and (2.55),\ntogether with (2.48). Thus we have\nE[xxT]= µµT + Σ. (2.62)\nFor single random variables, we subtracted the mean before taking second mo-\nments in order to deﬁne a variance. Similarly, in the multivariate case it is again\nconvenient to subtract off the mean, giving rise to thecovariance of a random vector\nx deﬁned by\ncov[x]= E\n[\n(x − E[x])(x − E[x])T]\n. (2.63)\nFor the speciﬁc case of a Gaussian distribution, we can make use of E[x]= µ,\ntogether with the result (2.62), to give\ncov[x]= Σ. (2.64)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 102,
      "page_label": "83"
    }
  },
  {
    "page_content": ". (2.63)\nFor the speciﬁc case of a Gaussian distribution, we can make use of E[x]= µ,\ntogether with the result (2.62), to give\ncov[x]= Σ. (2.64)\nBecause the parameter matrix Σ governs the covariance of x under the Gaussian\ndistribution, it is called the covariance matrix.\nAlthough the Gaussian distribution (2.43) is widely used as a density model, it\nsuffers from some signiﬁcant limitations. Consider the number of free parameters in\nthe distribution. A general symmetric covariance matrix Σ will have D(D +1 )/2\nindependent parameters, and there are another D independent parameters in µ,g i v -Exercise 2.21\ning D(D +3 )/2 parameters in total. For large D, the total number of parameters",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 102,
      "page_label": "83"
    }
  },
  {
    "page_content": "84 2. PROBABILITY DISTRIBUTIONS\nFigure 2.8 Contours of constant\nprobability density for a Gaussian\ndistribution in two dimensions in\nwhich the covariance matrix is (a) of\ngeneral form, (b) diagonal, in which\nthe elliptical contours are aligned\nwith the coordinate axes, and (c)\nproportional to the identity matrix, in\nwhich the contours are concentric\ncircles.\nx1\nx2\n(a)\nx1\nx2\n(b)\nx1\nx2\n(c)\ntherefore grows quadratically with D, and the computational task of manipulating\nand inverting large matrices can become prohibitive. One way to address this prob-\nlem is to use restricted forms of the covariance matrix. If we consider covariance\nmatrices that are diagonal, so that Σ = diag(σ2\ni ), we then have a total of 2D inde-\npendent parameters in the density model. The corresponding contours of constant\ndensity are given by axis-aligned ellipsoids. We could further restrict the covariance\nmatrix to be proportional to the identity matrix, Σ = σ2I, known as an isotropic co-\nvariance, giving D +1 independent parameters in the model and spherical surfaces\nof constant density. The three possibilities of general, diagonal, and isotropic covari-\nance matrices are illustrated in Figure 2.8. Unfortunately, whereas such approaches\nlimit the number of degrees of freedom in the distribution and make inversion of the\ncovariance matrix a much faster operation, they also greatly restrict the form of the\nprobability density and limit its ability to capture interesting correlations in the data.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 103,
      "page_label": "84"
    }
  },
  {
    "page_content": "probability density and limit its ability to capture interesting correlations in the data.\nA further limitation of the Gaussian distribution is that it is intrinsically uni-\nmodal (i.e., has a single maximum) and so is unable to provide a good approximation\nto multimodal distributions. Thus the Gaussian distribution can be both too ﬂexible,\nin the sense of having too many parameters, while also being too limited in the range\nof distributions that it can adequately represent. We will see later that the introduc-\ntion of latent variables, also called hidden variables or unobserved variables, allows\nboth of these problems to be addressed. In particular, a rich family of multimodal\ndistributions is obtained by introducing discrete latent variables leading to mixtures\nof Gaussians, as discussed in Section 2.3.9. Similarly, the introduction of continuous\nlatent variables, as described in Chapter 12, leads to models in which the number of\nfree parameters can be controlled independently of the dimensionality D of the data\nspace while still allowing the model to capture the dominant correlations in the data\nset. Indeed, these two approaches can be combined and further extended to derive\na very rich set of hierarchical models that can be adapted to a broad range of prac-\ntical applications. For instance, the Gaussian version of the Markov random ﬁeld,Section 8.3\nwhich is widely used as a probabilistic model of images, is a Gaussian distribution",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 103,
      "page_label": "84"
    }
  },
  {
    "page_content": "which is widely used as a probabilistic model of images, is a Gaussian distribution\nover the joint space of pixel intensities but rendered tractable through the imposition\nof considerable structure reﬂecting the spatial organization of the pixels. Similarly,\nthe linear dynamical system, used to model time series data for applications suchSection 13.3\nas tracking, is also a joint Gaussian distribution over a potentially large number of\nobserved and latent variables and again is tractable due to the structure imposed on\nthe distribution. A powerful framework for expressing the form and properties of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 103,
      "page_label": "84"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 85\nsuch complex distributions is that of probabilistic graphical models, which will form\nthe subject of Chapter 8.\n2.3.1 Conditional Gaussian distributions\nAn important property of the multivariate Gaussian distribution is that if two\nsets of variables are jointly Gaussian, then the conditional distribution of one set\nconditioned on the other is again Gaussian. Similarly, the marginal distribution of\neither set is also Gaussian.\nConsider ﬁrst the case of conditional distributions. Supposexis a D-dimensional\nvector with Gaussian distribution N(x|µ, Σ) and that we partition x into two dis-\njoint subsets xa and xb. Without loss of generality, we can take xa to form the ﬁrst\nM components of x, with xb comprising the remaining D −M components, so that\nx =\n(\nxa\nxb\n)\n. (2.65)\nWe also deﬁne corresponding partitions of the mean vector µ given by\nµ =\n(\nµa\nµb\n)\n(2.66)\nand of the covariance matrix Σ given by\nΣ =\n(\nΣaa Σab\nΣba Σbb\n)\n. (2.67)\nNote that the symmetry ΣT = Σ of the covariance matrix implies that Σaa and Σbb\nare symmetric, while Σba = ΣT\nab.\nIn many situations, it will be convenient to work with the inverse of the covari-\nance matrix\nΛ ≡ Σ−1 (2.68)\nwhich is known as the precision matrix. In fact, we shall see that some properties\nof Gaussian distributions are most naturally expressed in terms of the covariance,\nwhereas others take a simpler form when viewed in terms of the precision. We",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 104,
      "page_label": "85"
    }
  },
  {
    "page_content": "whereas others take a simpler form when viewed in terms of the precision. We\ntherefore also introduce the partitioned form of the precision matrix\nΛ =\n(\nΛaa Λab\nΛba Λbb\n)\n(2.69)\ncorresponding to the partitioning (2.65) of the vector x. Because the inverse of a\nsymmetric matrix is also symmetric, we see that Λaa and Λbb are symmetric, whileExercise 2.22\nΛT\nab = Λba. It should be stressed at this point that, for instance, Λaa is not simply\ngiven by the inverse of Σaa. In fact, we shall shortly examine the relation between\nthe inverse of a partitioned matrix and the inverses of its partitions.\nLet us begin by ﬁnding an expression for the conditional distribution p(xa|xb).\nFrom the product rule of probability, we see that this conditional distribution can be",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 104,
      "page_label": "85"
    }
  },
  {
    "page_content": "86 2. PROBABILITY DISTRIBUTIONS\nevaluated from the joint distribution p(x)= p(xa, xb) simply by ﬁxing xb to the\nobserved value and normalizing the resulting expression to obtain a valid probability\ndistribution overxa. Instead of performing this normalization explicitly, we can\nobtain the solution more efﬁciently by considering the quadratic form in the exponent\nof the Gaussian distribution given by (2.44) and then reinstating the normalization\ncoefﬁcient at the end of the calculation. If we make use of the partitioning (2.65),\n(2.66), and (2.69), we obtain\n−1\n2(x − µ)TΣ−1(x − µ)=\n−1\n2(xa − µa)TΛaa(xa − µa) − 1\n2(xa − µa)TΛab(xb − µb)\n−1\n2(xb − µb)TΛba(xa − µa) − 1\n2(xb − µb)TΛbb(xb − µb). (2.70)\nWe see that as a function of xa, this is again a quadratic form, and hence the cor-\nresponding conditional distribution p(xa|xb) will be Gaussian. Because this distri-\nbution is completely characterized by its mean and its covariance, our goal will be\nto identify expressions for the mean and covariance ofp(xa|xb) by inspection of\n(2.70).\nThis is an example of a rather common operation associated with Gaussian\ndistributions, sometimes called ‘completing the square’, in which we are given a\nquadratic form deﬁning the exponent terms in a Gaussian distribution, and we need\nto determine the corresponding mean and covariance. Such problems can be solved\nstraightforwardly by noting that the exponent in a general Gaussian distribution\nN(x|µ,Σ) can be written\n−1\n2(x − µ)TΣ−1(x − µ)= −1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 105,
      "page_label": "86"
    }
  },
  {
    "page_content": "straightforwardly by noting that the exponent in a general Gaussian distribution\nN(x|µ,Σ) can be written\n−1\n2(x − µ)TΣ−1(x − µ)= −1\n2xTΣ−1x + xTΣ−1µ +c o n s t (2.71)\nwhere ‘const’ denotes terms which are independent of x, and we have made use of\nthe symmetry of Σ. Thus if we take our general quadratic form and express it in\nthe form given by the right-hand side of (2.71), then we can immediately equate the\nmatrix of coefﬁcients entering the second order term in x to the inverse covariance\nmatrix Σ−1 and the coefﬁcient of the linear term in x to Σ−1µ, from which we can\nobtain µ.\nNow let us apply this procedure to the conditional Gaussian distributionp(xa|xb)\nfor which the quadratic form in the exponent is given by (2.70). We will denote the\nmean and covariance of this distribution byµa|b and Σa|b, respectively. Consider\nthe functional dependence of (2.70) on xa in which xb is regarded as a constant. If\nwe pick out all terms that are second order in xa,w eh a v e\n−1\n2xT\na Λaaxa (2.72)\nfrom which we can immediately conclude that the covariance (inverse precision) of\np(xa|xb) is given by\nΣa|b = Λ−1\naa . (2.73)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 105,
      "page_label": "86"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 87\nNow consider all of the terms in (2.70) that are linear in xa\nxT\na {Λaaµa − Λab(xb − µb)} (2.74)\nwhere we have used ΛT\nba = Λab. From our discussion of the general form (2.71),\nthe coefﬁcient of xa in this expression must equal Σ−1\na|bµa|b and hence\nµa|b = Σa|b {Λaaµa − Λab(xb − µb)}\n= µa − Λ−1\naa Λab(xb − µb) (2.75)\nwhere we have made use of (2.73).\nThe results (2.73) and (2.75) are expressed in terms of the partitioned precision\nmatrix of the original joint distribution p(xa, xb). We can also express these results\nin terms of the corresponding partitioned covariance matrix. To do this, we make use\nof the following identity for the inverse of a partitioned matrixExercise 2.24\n(\nAB\nCD\n)−1\n=\n(\nM −MBD−1\n−D−1CM D −1 + D−1CMBD−1\n)\n(2.76)\nwhere we have deﬁned\nM =( A − BD−1C)−1. (2.77)\nThe quantity M−1 is known as the Schur complementof the matrix on the left-hand\nside of (2.76) with respect to the submatrix D. Using the deﬁnition\n(\nΣaa Σab\nΣba Σbb\n)−1\n=\n(\nΛaa Λab\nΛba Λbb\n)\n(2.78)\nand making use of (2.76), we have\nΛaa =( Σaa − ΣabΣ−1\nbb Σba)−1 (2.79)\nΛab = −(Σaa − ΣabΣ−1\nbb Σba)−1ΣabΣ−1\nbb . (2.80)\nFrom these we obtain the following expressions for the mean and covariance of the\nconditional distribution p(xa|xb)\nµa|b = µa + ΣabΣ−1\nbb (xb − µb) (2.81)\nΣa|b = Σaa − ΣabΣ−1\nbb Σba. (2.82)\nComparing (2.73) and (2.82), we see that the conditional distributionp(xa|xb) takes\na simpler form when expressed in terms of the partitioned precision matrix than",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 106,
      "page_label": "87"
    }
  },
  {
    "page_content": "a simpler form when expressed in terms of the partitioned precision matrix than\nwhen it is expressed in terms of the partitioned covariance matrix. Note that the\nmean of the conditional distributionp(xa|xb), given by (2.81), is a linear function of\nxb and that the covariance, given by (2.82), is independent ofxa. This represents an\nexample of a linear-Gaussian model.Section 8.1.4",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 106,
      "page_label": "87"
    }
  },
  {
    "page_content": "88 2. PROBABILITY DISTRIBUTIONS\n2.3.2 Marginal Gaussian distributions\nWe have seen that if a joint distribution p(xa, xb) is Gaussian, then the condi-\ntional distribution p(xa|xb) will again be Gaussian. Now we turn to a discussion of\nthe marginal distribution given by\np(xa)=\n∫\np(xa, xb)d xb (2.83)\nwhich, as we shall see, is also Gaussian. Once again, our strategy for evaluating this\ndistribution efﬁciently will be to focus on the quadratic form in the exponent of the\njoint distribution and thereby to identify the mean and covariance of the marginal\ndistribution p(xa).\nThe quadratic form for the joint distribution can be expressed, using the par-\ntitioned precision matrix, in the form (2.70). Because our goal is to integrate out\nxb, this is most easily achieved by ﬁrst considering the terms involving xb and then\ncompleting the square in order to facilitate integration. Picking out just those terms\nthat involve xb,w eh a v e\n−1\n2xT\nb Λbbxb+xT\nb m = −1\n2(xb−Λ−1\nbb m)TΛbb(xb−Λ−1\nbb m)+ 1\n2mTΛ−1\nbb m (2.84)\nwhere we have deﬁned\nm = Λbbµb − Λba(xa − µa). (2.85)\nWe see that the dependence onxb has been cast into the standard quadratic form of a\nGaussian distribution corresponding to the ﬁrst term on the right-hand side of (2.84),\nplus a term that does not depend on xb (but that does depend on xa). Thus, when\nwe take the exponential of this quadratic form, we see that the integration over xb\nrequired by (2.83) will take the form\n∫\nexp\n{\n−1\n2(xb − Λ−1\nbb m)TΛbb(xb − Λ−1\nbb m)\n}",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 107,
      "page_label": "88"
    }
  },
  {
    "page_content": "required by (2.83) will take the form\n∫\nexp\n{\n−1\n2(xb − Λ−1\nbb m)TΛbb(xb − Λ−1\nbb m)\n}\ndxb. (2.86)\nThis integration is easily performed by noting that it is the integral over an unnor-\nmalized Gaussian, and so the result will be the reciprocal of the normalization co-\nefﬁcient. We know from the form of the normalized Gaussian given by (2.43), that\nthis coefﬁcient is independent of the mean and depends only on the determinant of\nthe covariance matrix. Thus, by completing the square with respect to xb, we can\nintegrate out xb and the only term remaining from the contributions on the left-hand\nside of (2.84) that depends on xa is the last term on the right-hand side of (2.84) in\nwhich m is given by (2.85). Combining this term with the remaining terms from",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 107,
      "page_label": "88"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 89\n(2.70) that depend on xa, we obtain\n1\n2 [Λbbµb − Λba(xa − µa)]T Λ−1\nbb [Λbbµb − Λba(xa − µa)]\n−1\n2xT\na Λaaxa + xT\na (Λaaµa + Λabµb)+c o n s t\n= −1\n2xT\na (Λaa − ΛabΛ−1\nbb Λba)xa\n+xT\na (Λaa − ΛabΛ−1\nbb Λba)−1µa +c o n s t (2.87)\nwhere ‘const’ denotes quantities independent of xa. Again, by comparison with\n(2.71), we see that the covariance of the marginal distribution of p(xa) is given by\nΣa =( Λaa − ΛabΛ−1\nbb Λba)−1. (2.88)\nSimilarly, the mean is given by\nΣa(Λaa − ΛabΛ−1\nbb Λba)µa = µa (2.89)\nwhere we have used (2.88). The covariance in (2.88) is expressed in terms of the\npartitioned precision matrix given by (2.69). We can rewrite this in terms of the\ncorresponding partitioning of the covariance matrix given by (2.67), as we did for\nthe conditional distribution. These partitioned matrices are related by\n(\nΛaa Λab\nΛba Λbb\n)−1\n=\n(\nΣaa Σab\nΣba Σbb\n)\n(2.90)\nMaking use of (2.76), we then have\n(\nΛaa − ΛabΛ−1\nbb Λba\n)−1\n= Σaa. (2.91)\nThus we obtain the intuitively satisfying result that the marginal distribution p(xa)\nhas mean and covariance given by\nE[xa]= µa (2.92)\ncov[xa]= Σaa. (2.93)\nWe see that for a marginal distribution, the mean and covariance are most simply ex-\npressed in terms of the partitioned covariance matrix, in contrast to the conditional\ndistribution for which the partitioned precision matrix gives rise to simpler expres-\nsions.\nOur results for the marginal and conditional distributions of a partitioned Gaus-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 108,
      "page_label": "89"
    }
  },
  {
    "page_content": "sions.\nOur results for the marginal and conditional distributions of a partitioned Gaus-\nsian are summarized below.\nPartitioned Gaussians\nGiven a joint Gaussian distribution N(x|µ,Σ) with Λ ≡ Σ−1 and\nx =\n(\nxa\nxb\n)\n, µ =\n(\nµa\nµb\n)\n(2.94)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 108,
      "page_label": "89"
    }
  },
  {
    "page_content": "90 2. PROBABILITY DISTRIBUTIONS\nxa\nxb =0 .7\nxb\np(xa,xb)\n0 0.5 1\n0\n0.5\n1\nxa\np(xa)\np(xa|xb =0 .7)\n0 0.5 1\n0\n5\n10\nFigure 2.9 The plot on the left shows the contours of a Gaussian distribution p(xa,x b) over two variables, and\nthe plot on the right shows the marginal distribution p(xa) (blue curve) and the conditional distribution p(xa|xb)\nfor xb =0 .7 (red curve).\nΣ =\n(\nΣaa Σab\nΣba Σbb\n)\n, Λ =\n(\nΛaa Λab\nΛba Λbb\n)\n. (2.95)\nConditional distribution:\np(xa|xb)= N(x|µa|b, Λ−1\naa ) (2.96)\nµa|b = µa − Λ−1\naa Λab(xb − µb). (2.97)\nMarginal distribution:\np(xa)= N(xa|µa, Σaa). (2.98)\nWe illustrate the idea of conditional and marginal distributions associated with\na multivariate Gaussian using an example involving two variables in Figure 2.9.\n2.3.3 Bayes’ theorem for Gaussian variables\nIn Sections 2.3.1 and 2.3.2, we considered a Gaussian p(x) in which we parti-\ntioned the vector x into two subvectors x =( xa, xb) and then found expressions for\nthe conditional distribution p(xa|xb) and the marginal distribution p(xa). We noted\nthat the mean of the conditional distribution p(xa|xb) was a linear function of xb.\nHere we shall suppose that we are given a Gaussian marginal distributionp(x) and a\nGaussian conditional distribution p(y|x) in which p(y|x) has a mean that is a linear\nfunction of x, and a covariance which is independent of x. This is an example of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 109,
      "page_label": "90"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 91\na linear Gaussian model(Roweis and Ghahramani, 1999), which we shall study in\ngreater generality in Section 8.1.4. We wish to ﬁnd the marginal distribution p(y)\nand the conditional distribution p(x|y). This is a problem that will arise frequently\nin subsequent chapters, and it will prove convenient to derive the general results here.\nWe shall take the marginal and conditional distributions to be\np(x)= N\n(\nx|µ,Λ−1)\n(2.99)\np(y|x)= N\n(\ny|Ax + b, L−1)\n(2.100)\nwhere µ, A, and b are parameters governing the means, and Λ and L are precision\nmatrices. If x has dimensionality M and y has dimensionality D, then the matrix A\nhas size D × M.\nFirst we ﬁnd an expression for the joint distribution overx and y. To do this, we\ndeﬁne\nz =\n(\nx\ny\n)\n(2.101)\nand then consider the log of the joint distribution\nln p(z)=l n p(x)+l n p(y|x)\n= −1\n2(x − µ)TΛ(x − µ)\n−1\n2(y − Ax − b)TL(y − Ax − b) + const (2.102)\nwhere ‘const’ denotes terms independent of x and y. As before, we see that this is a\nquadratic function of the components of z, and hence p(z) is Gaussian distribution.\nTo ﬁnd the precision of this Gaussian, we consider the second order terms in (2.102),\nwhich can be written as\n−1\n2xT(Λ + ATLA)x − 1\n2yTLy + 1\n2yTLAx + 1\n2xTATLy\n= −1\n2\n(\nx\ny\n)T (\nΛ + ATLA −ATL\n−LA L\n)(\nx\ny\n)\n= −1\n2zTRz (2.103)\nand so the Gaussian distribution over z has precision (inverse covariance) matrix\ngiven by\nR =\n(\nΛ + ATLA −ATL\n−LA L\n)\n. (2.104)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 110,
      "page_label": "91"
    }
  },
  {
    "page_content": ")\n= −1\n2zTRz (2.103)\nand so the Gaussian distribution over z has precision (inverse covariance) matrix\ngiven by\nR =\n(\nΛ + ATLA −ATL\n−LA L\n)\n. (2.104)\nThe covariance matrix is found by taking the inverse of the precision, which can be\ndone using the matrix inversion formula (2.76) to giveExercise 2.29\ncov[z]= R−1 =\n(\nΛ−1 Λ−1AT\nAΛ−1 L−1 + AΛ−1AT\n)\n. (2.105)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 110,
      "page_label": "91"
    }
  },
  {
    "page_content": "92 2. PROBABILITY DISTRIBUTIONS\nSimilarly, we can ﬁnd the mean of the Gaussian distribution over z by identify-\ning the linear terms in (2.102), which are given by\nxTΛµ − xTATLb + yTLb =\n(\nx\ny\n)T (\nΛµ − ATLb\nLb\n)\n. (2.106)\nUsing our earlier result (2.71) obtained by completing the square over the quadratic\nform of a multivariate Gaussian, we ﬁnd that the mean of z is given by\nE[z]= R−1\n(\nΛµ − ATLb\nLb\n)\n. (2.107)\nMaking use of (2.105), we then obtainExercise 2.30\nE[z]=\n(\nµ\nAµ + b\n)\n. (2.108)\nNext we ﬁnd an expression for the marginal distribution p(y) in which we have\nmarginalized over x. Recall that the marginal distribution over a subset of the com-\nponents of a Gaussian random vector takes a particularly simple form when ex-\npressed in terms of the partitioned covariance matrix. Speciﬁcally, its mean andSection 2.3\ncovariance are given by (2.92) and (2.93), respectively. Making use of (2.105) and\n(2.108) we see that the mean and covariance of the marginal distribution p(y) are\ngiven by\nE[y]= Aµ + b (2.109)\ncov[y]= L−1 + AΛ−1AT. (2.110)\nA special case of this result is when A = I, in which case it reduces to the convolu-\ntion of two Gaussians, for which we see that the mean of the convolution is the sum\nof the mean of the two Gaussians, and the covariance of the convolution is the sum\nof their covariances.\nFinally, we seek an expression for the conditionalp(x|y). Recall that the results\nfor the conditional distribution are most easily expressed in terms of the partitioned",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 111,
      "page_label": "92"
    }
  },
  {
    "page_content": "for the conditional distribution are most easily expressed in terms of the partitioned\nprecision matrix, using (2.73) and (2.75). Applying these results to (2.105) andSection 2.3\n(2.108) we see that the conditional distribution p(x|y) has mean and covariance\ngiven by\nE[x|y]=( Λ + ATLA)−1 {\nATL(y − b)+ Λµ\n}\n(2.111)\ncov[x|y]=( Λ + ATLA)−1. (2.112)\nThe evaluation of this conditional can be seen as an example of Bayes’ theorem.\nWe can interpret the distribution p(x) as a prior distribution over x. If the variable\ny is observed, then the conditional distribution p(x|y) represents the corresponding\nposterior distribution over x. Having found the marginal and conditional distribu-\ntions, we effectively expressed the joint distribution p(z)= p(x)p(y|x) in the form\np(x|y)p(y). These results are summarized below.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 111,
      "page_label": "92"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 93\nMarginal and Conditional Gaussians\nGiven a marginal Gaussian distribution for x and a conditional Gaussian distri-\nbution for y given x in the form\np(x)= N(x|µ,Λ−1) (2.113)\np(y|x)= N(y|Ax + b, L−1) (2.114)\nthe marginal distribution of y and the conditional distribution of x given y are\ngiven by\np(y)= N(y|Aµ + b, L−1 + AΛ−1AT) (2.115)\np(x|y)= N(x|Σ{ATL(y − b)+ Λµ}, Σ) (2.116)\nwhere\nΣ =( Λ + ATLA)−1. (2.117)\n2.3.4 Maximum likelihood for the Gaussian\nGiven a data set X =( x1,..., xN )T in which the observations {xn} are as-\nsumed to be drawn independently from a multivariate Gaussian distribution, we can\nestimate the parameters of the distribution by maximum likelihood. The log likeli-\nhood function is given by\nlnp(X|µ, Σ)= −ND\n2 ln(2π)−N\n2 ln|Σ|−1\n2\nN∑\nn=1\n(xn−µ)TΣ−1(xn−µ). (2.118)\nBy simple rearrangement, we see that the likelihood function depends on the data set\nonly through the two quantities\nN∑\nn=1\nxn,\nN∑\nn=1\nxnxT\nn. (2.119)\nThese are known as the sufﬁcient statistics for the Gaussian distribution. Using\n(C.19), the derivative of the log likelihood with respect to µ is given byAppendix C\n∂\n∂µ ln p(X|µ, Σ)=\nN∑\nn=1\nΣ−1(xn − µ) (2.120)\nand setting this derivative to zero, we obtain the solution for the maximum likelihood\nestimate of the mean given by\nµML = 1\nN\nN∑\nn=1\nxn (2.121)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 112,
      "page_label": "93"
    }
  },
  {
    "page_content": "94 2. PROBABILITY DISTRIBUTIONS\nwhich is the mean of the observed set of data points. The maximization of (2.118)\nwith respect to Σ is rather more involved. The simplest approach is to ignore the\nsymmetry constraint and show that the resulting solution is symmetric as required.Exercise 2.34\nAlternative derivations of this result, which impose the symmetry and positive deﬁ-\nniteness constraints explicitly, can be found in Magnus and Neudecker (1999). The\nresult is as expected and takes the form\nΣML = 1\nN\nN∑\nn=1\n(xn − µML)(xn − µML)T (2.122)\nwhich involves µML because this is the result of a joint maximization with respect\nto µ and Σ. Note that the solution (2.121) for µML does not depend on ΣML, and so\nwe can ﬁrst evaluate µML and then use this to evaluate ΣML.\nIf we evaluate the expectations of the maximum likelihood solutions under the\ntrue distribution, we obtain the following resultsExercise 2.35\nE[µML]= µ (2.123)\nE[ΣML]= N − 1\nN Σ. (2.124)\nWe see that the expectation of the maximum likelihood estimate for the mean is equal\nto the true mean. However, the maximum likelihood estimate for the covariance has\nan expectation that is less than the true value, and hence it is biased. We can correct\nthis bias by deﬁning a different estimator ˜Σ given by\n˜Σ = 1\nN − 1\nN∑\nn=1\n(xn − µML)(xn − µML)T. (2.125)\nClearly from (2.122) and (2.124), the expectation of ˜Σ is equal to Σ.\n2.3.5 Sequential estimation\nOur discussion of the maximum likelihood solution for the parameters of a Gaus-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 113,
      "page_label": "94"
    }
  },
  {
    "page_content": "2.3.5 Sequential estimation\nOur discussion of the maximum likelihood solution for the parameters of a Gaus-\nsian distribution provides a convenient opportunity to give a more general discussion\nof the topic of sequential estimation for maximum likelihood. Sequential methods\nallow data points to be processed one at a time and then discarded and are important\nfor on-line applications, and also where large data sets are involved so that batch\nprocessing of all data points at once is infeasible.\nConsider the result (2.121) for the maximum likelihood estimator of the mean\nµML, which we will denote by µ(N)\nML when it is based on N observations. If we",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 113,
      "page_label": "94"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 95\nFigure 2.10 A schematic illustration of two correlated ran-\ndom variables z and θ, together with the\nregression function f(θ) given by the con-\nditional expectation E[z|θ]. The Robbins-\nMonro algorithm provides a general sequen-\ntial procedure for ﬁnding the root θ⋆ of such\nfunctions. θ\nz\nθ⋆\nf(θ)\ndissect out the contribution from the ﬁnal data point xN , we obtain\nµ(N)\nML = 1\nN\nN∑\nn=1\nxn\n= 1\nN xN + 1\nN\nN−1∑\nn=1\nxn\n= 1\nN xN + N − 1\nN µ(N−1)\nML\n= µ(N−1)\nML + 1\nN (xN − µ(N−1)\nML ). (2.126)\nThis result has a nice interpretation, as follows. After observing N − 1 data points\nwe have estimated µ by µ(N−1)\nML . We now observe data point xN , and we obtain our\nrevised estimate µ(N)\nML by moving the old estimate a small amount, proportional to\n1/N, in the direction of the ‘error signal’(xN −µ(N−1)\nML ). Note that, as N increases,\nso the contribution from successive data points gets smaller.\nThe result (2.126) will clearly give the same answer as the batch result (2.121)\nbecause the two formulae are equivalent. However, we will not always be able to de-\nrive a sequential algorithm by this route, and so we seek a more general formulation\nof sequential learning, which leads us to the Robbins-Monro algorithm. Consider a\npair of random variables θ and z governed by a joint distribution p(z,θ ). The con-\nditional expectation of z given θ deﬁnes a deterministic function f(θ) that is given\nby\nf(θ) ≡ E[z|θ]=\n∫\nzp(z|θ)d z (2.127)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 114,
      "page_label": "95"
    }
  },
  {
    "page_content": "ditional expectation of z given θ deﬁnes a deterministic function f(θ) that is given\nby\nf(θ) ≡ E[z|θ]=\n∫\nzp(z|θ)d z (2.127)\nand is illustrated schematically in Figure 2.10. Functions deﬁned in this way are\ncalled regression functions.\nOur goal is to ﬁnd the root θ⋆ at which f(θ⋆ )=0 . If we had a large data set\nof observations of z and θ, then we could model the regression function directly and\nthen obtain an estimate of its root. Suppose, however, that we observe values of\nz one at a time and we wish to ﬁnd a corresponding sequential estimation scheme\nfor θ⋆ . The following general procedure for solving such problems was given by",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 114,
      "page_label": "95"
    }
  },
  {
    "page_content": "96 2. PROBABILITY DISTRIBUTIONS\nRobbins and Monro (1951). We shall assume that the conditional variance of z is\nﬁnite so that\nE\n[\n(z − f)2 |θ\n]\n< ∞ (2.128)\nand we shall also, without loss of generality, consider the case where f(θ) > 0 for\nθ>θ ⋆ and f(θ) < 0 for θ<θ ⋆ , as is the case in Figure 2.10. The Robbins-Monro\nprocedure then deﬁnes a sequence of successive estimates of the root θ⋆ given by\nθ(N) = θ(N−1) + aN−1z(θ(N−1)) (2.129)\nwhere z(θ(N)) is an observed value ofz when θ takes the valueθ(N). The coefﬁcients\n{aN } represent a sequence of positive numbers that satisfy the conditions\nlim\nN→∞\naN =0 (2.130)\n∞∑\nN=1\naN = ∞ (2.131)\n∞∑\nN=1\na2\nN < ∞. (2.132)\nIt can then be shown (Robbins and Monro, 1951; Fukunaga, 1990) that the sequence\nof estimates given by (2.129) does indeed converge to the root with probability one.\nNote that the ﬁrst condition (2.130) ensures that the successive corrections decrease\nin magnitude so that the process can converge to a limiting value. The second con-\ndition (2.131) is required to ensure that the algorithm does not converge short of the\nroot, and the third condition (2.132) is needed to ensure that the accumulated noise\nhas ﬁnite variance and hence does not spoil convergence.\nNow let us consider how a general maximum likelihood problem can be solved\nsequentially using the Robbins-Monro algorithm. By deﬁnition, the maximum like-\nlihood solutionθML is a stationary point of the log likelihood function and hence\nsatisﬁes\n∂\n∂θ\n{\n1\nN\nN∑\nn=1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 115,
      "page_label": "96"
    }
  },
  {
    "page_content": "lihood solutionθML is a stationary point of the log likelihood function and hence\nsatisﬁes\n∂\n∂θ\n{\n1\nN\nN∑\nn=1\nlnp(xn|θ)\n}⏐⏐⏐⏐⏐\nθML\n=0 . (2.133)\nExchanging the derivative and the summation, and taking the limitN →∞ we have\nlim\nN→∞\n1\nN\nN∑\nn=1\n∂\n∂θ lnp(xn|θ)= Ex\n[∂\n∂θ lnp(x|θ)\n]\n(2.134)\nand so we see that ﬁnding the maximum likelihood solution corresponds to ﬁnd-\ning the root of a regression function. We can therefore apply the Robbins-Monro\nprocedure, which now takes the form\nθ(N) = θ(N−1) + aN−1\n∂\n∂θ(N−1) ln p(xN |θ(N−1)). (2.135)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 115,
      "page_label": "96"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 97\nFigure 2.11 In the case of a Gaussian distribution, with θ\ncorresponding to the mean µ, the regression\nfunction illustrated in Figure 2.10 takes the form\nof a straight line, as shown in red. In this\ncase, the random variable z corresponds to the\nderivative of the log likelihood function and is\ngiven by (x −µML)/σ2, and its expectation that\ndeﬁnes the regression function is a straight line\ngiven by (µ − µML)/σ2. The root of the regres-\nsion function corresponds to the maximum like-\nlihood estimator µML.\nµ\nz\np(z|µ)\nµML\nAs a speciﬁc example, we consider once again the sequential estimation of the\nmean of a Gaussian distribution, in which case the parameter θ(N) is the estimate\nµ(N)\nML of the mean of the Gaussian, and the random variable z is given by\nz = ∂\n∂µML\nlnp(x|µML,σ 2)= 1\nσ2 (x − µML). (2.136)\nThus the distribution of z is Gaussian with mean µ − µML, as illustrated in Fig-\nure 2.11. Substituting (2.136) into (2.135), we obtain the univariate form of (2.126),\nprovided we choose the coefﬁcients aN to have the form aN = σ2/N. Note that\nalthough we have focussed on the case of a single variable, the same technique,\ntogether with the same restrictions (2.130)–(2.132) on the coefﬁcients aN , apply\nequally to the multivariate case (Blum, 1965).\n2.3.6 Bayesian inference for the Gaussian\nThe maximum likelihood framework gave point estimates for the parameters µ\nand Σ. Now we develop a Bayesian treatment by introducing prior distributions",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 116,
      "page_label": "97"
    }
  },
  {
    "page_content": "and Σ. Now we develop a Bayesian treatment by introducing prior distributions\nover these parameters. Let us begin with a simple example in which we consider a\nsingle Gaussian random variable x. We shall suppose that the variance σ2 is known,\nand we consider the task of inferring the mean µ given a set of N observations\nX = {x1,...,x N }. The likelihood function, that is the probability of the observed\ndata given µ, viewed as a function of µ,i sg i v e nb y\np(X|µ)=\nN∏\nn=1\np(xn|µ)= 1\n(2πσ2)N/2 exp\n{\n− 1\n2σ2\nN∑\nn=1\n(xn − µ)2\n}\n. (2.137)\nAgain we emphasize that the likelihood function p(X|µ) is not a probability distri-\nbution over µ and is not normalized.\nWe see that the likelihood function takes the form of the exponential of a quad-\nratic form in µ. Thus if we choose a prior p(µ) given by a Gaussian, it will be a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 116,
      "page_label": "97"
    }
  },
  {
    "page_content": "98 2. PROBABILITY DISTRIBUTIONS\nconjugate distribution for this likelihood function because the corresponding poste-\nrior will be a product of two exponentials of quadratic functions of µ and hence will\nalso be Gaussian. We therefore take our prior distribution to be\np(µ)= N\n(\nµ|µ0,σ 2\n0\n)\n(2.138)\nand the posterior distribution is given by\np(µ|X) ∝ p(X|µ)p(µ). (2.139)\nSimple manipulation involving completing the square in the exponent shows that theExercise 2.38\nposterior distribution is given by\np(µ|X)= N\n(\nµ|µN ,σ 2\nN\n)\n(2.140)\nwhere\nµN = σ2\nNσ2\n0 + σ2 µ0 + Nσ2\n0\nNσ2\n0 + σ2 µML (2.141)\n1\nσ2\nN\n= 1\nσ2\n0\n+ N\nσ2 (2.142)\nin which µML is the maximum likelihood solution for µ given by the sample mean\nµML = 1\nN\nN∑\nn=1\nxn. (2.143)\nIt is worth spending a moment studying the form of the posterior mean and\nvariance. First of all, we note that the mean of the posterior distribution given by\n(2.141) is a compromise between the prior mean µ0 and the maximum likelihood\nsolution µML. If the number of observed data points N =0 , then (2.141) reduces\nto the prior mean as expected. For N →∞ , the posterior mean is given by the\nmaximum likelihood solution. Similarly, consider the result (2.142) for the variance\nof the posterior distribution. We see that this is most naturally expressed in terms\nof the inverse variance, which is called the precision. Furthermore, the precisions\nare additive, so that the precision of the posterior is given by the precision of the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 117,
      "page_label": "98"
    }
  },
  {
    "page_content": "are additive, so that the precision of the posterior is given by the precision of the\nprior plus one contribution of the data precision from each of the observed data\npoints. As we increase the number of observed data points, the precision steadily\nincreases, corresponding to a posterior distribution with steadily decreasing variance.\nWith no observed data points, we have the prior variance, whereas if the number of\ndata points N →∞ , the variance σ2\nN goes to zero and the posterior distribution\nbecomes inﬁnitely peaked around the maximum likelihood solution. We therefore\nsee that the maximum likelihood result of a point estimate forµ given by (2.143) is\nrecovered precisely from the Bayesian formalism in the limit of an inﬁnite number\nof observations. Note also that for ﬁniteN, if we take the limitσ2\n0 →∞ in which the\nprior has inﬁnite variance then the posterior mean (2.141) reduces to the maximum\nlikelihood result, while from (2.142) the posterior variance is given byσ2\nN = σ2/N.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 117,
      "page_label": "98"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 99\nFigure 2.12 Illustration of Bayesian inference for\nthe mean µ of a Gaussian distri-\nbution, in which the variance is as-\nsumed to be known. The curves\nshow the prior distribution over µ\n(the curve labelled N =0 ), which\nin this case is itself Gaussian, along\nwith the posterior distribution given\nby (2.140) for increasing numbers N\nof data points. The data points are\ngenerated from a Gaussian of mean\n0.8 and variance 0.1, and the prior is\nchosen to have mean 0. In both the\nprior and the likelihood function, the\nvariance is set to the true value.\nN =0\nN =1\nN =2\nN =1 0\n−1 0 1\n0\n5\nWe illustrate our analysis of Bayesian inference for the mean of a Gaussian\ndistribution in Figure 2.12. The generalization of this result to the case of a D-\ndimensional Gaussian random variablex with known covariance and unknown mean\nis straightforward.Exercise 2.40\nWe have already seen how the maximum likelihood expression for the mean of\na Gaussian can be re-cast as a sequential update formula in which the mean afterSection 2.3.5\nobserving N data points was expressed in terms of the mean after observing N − 1\ndata points together with the contribution from data point xN . In fact, the Bayesian\nparadigm leads very naturally to a sequential view of the inference problem. To see\nthis in the context of the inference of the mean of a Gaussian, we write the posterior\ndistribution with the contribution from the ﬁnal data point xN separated out so that\np(µ|D) ∝\n[\np(µ)\nN−1∏",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 118,
      "page_label": "99"
    }
  },
  {
    "page_content": "distribution with the contribution from the ﬁnal data point xN separated out so that\np(µ|D) ∝\n[\np(µ)\nN−1∏\nn=1\np(xn|µ)\n]\np(xN |µ). (2.144)\nThe term in square brackets is (up to a normalization coefﬁcient) just the posterior\ndistribution after observing N − 1 data points. We see that this can be viewed as\na prior distribution, which is combined using Bayes’ theorem with the likelihood\nfunction associated with data point xN to arrive at the posterior distribution after\nobserving N data points. This sequential view of Bayesian inference is very general\nand applies to any problem in which the observed data are assumed to be independent\nand identically distributed.\nSo far, we have assumed that the variance of the Gaussian distribution over the\ndata is known and our goal is to infer the mean. Now let us suppose that the mean\nis known and we wish to infer the variance. Again, our calculations will be greatly\nsimpliﬁed if we choose a conjugate form for the prior distribution. It turns out to be\nmost convenient to work with the precisionλ ≡ 1/σ2. The likelihood function for λ\ntakes the form\np(X|λ)=\nN∏\nn=1\nN(xn|µ, λ−1) ∝ λN/2 exp\n{\n−λ\n2\nN∑\nn=1\n(xn − µ)2\n}\n. (2.145)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 118,
      "page_label": "99"
    }
  },
  {
    "page_content": "100 2. PROBABILITY DISTRIBUTIONS\nλ\na =0 .1\nb =0 .1\n0 1 2\n0\n1\n2\nλ\na =1\nb =1\n0 1 2\n0\n1\n2\nλ\na =4\nb =6\n0 1 2\n0\n1\n2\nFigure 2.13 Plot of the gamma distribution Gam(λ|a, b) deﬁned by (2.146) for various values of the parameters\na and b.\nThe corresponding conjugate prior should therefore be proportional to the product\nof a power of λ and the exponential of a linear function of λ. This corresponds to\nthe gamma distribution which is deﬁned by\nGam(λ|a, b)= 1\nΓ(a)baλa−1 exp(−bλ). (2.146)\nHere Γ(a) is the gamma function that is deﬁned by (1.141) and that ensures that\n(2.146) is correctly normalized. The gamma distribution has a ﬁnite integral ifa> 0,Exercise 2.41\nand the distribution itself is ﬁnite if a ⩾ 1. It is plotted, for various values of a and\nb, in Figure 2.13. The mean and variance of the gamma distribution are given byExercise 2.42\nE[λ]= a\nb (2.147)\nvar[λ]= a\nb2 . (2.148)\nConsider a prior distribution Gam(λ|a0,b 0). If we multiply by the likelihood\nfunction (2.145), then we obtain a posterior distribution\np(λ|X) ∝ λa0−1λN/2 exp\n{\n−b0λ − λ\n2\nN∑\nn=1\n(xn − µ)2\n}\n(2.149)\nwhich we recognize as a gamma distribution of the form Gam(λ|aN ,b N ) where\naN = a0 + N\n2 (2.150)\nbN = b0 + 1\n2\nN∑\nn=1\n(xn − µ)2 = b0 + N\n2 σ2\nML (2.151)\nwhere σ2\nML is the maximum likelihood estimator of the variance. Note that in (2.149)\nthere is no need to keep track of the normalization constants in the prior and the\nlikelihood function because, if required, the correct coefﬁcient can be found at the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 119,
      "page_label": "100"
    }
  },
  {
    "page_content": "likelihood function because, if required, the correct coefﬁcient can be found at the\nend using the normalized form (2.146) for the gamma distribution.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 119,
      "page_label": "100"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 101\nFrom (2.150), we see that the effect of observing N data points is to increase\nthe value of the coefﬁcient a by N/2. Thus we can interpret the parameter a0 in\nthe prior in terms of 2a0 ‘effective’ prior observations. Similarly, from (2.151) we\nsee that the N data points contribute Nσ2\nML/2 to the parameter b, where σ2\nML is\nthe variance, and so we can interpret the parameter b0 in the prior as arising from\nthe 2a0 ‘effective’ prior observations having variance 2b0/(2a0)= b0/a0. Recall\nthat we made an analogous interpretation for the Dirichlet prior. These distributionsSection 2.2\nare examples of the exponential family, and we shall see that the interpretation of\na conjugate prior in terms of effective ﬁctitious data points is a general one for the\nexponential family of distributions.\nInstead of working with the precision, we can consider the variance itself. The\nconjugate prior in this case is called the inverse gamma distribution, although we\nshall not discuss this further because we will ﬁnd it more convenient to work with\nthe precision.\nNow suppose that both the mean and the precision are unknown. To ﬁnd a\nconjugate prior, we consider the dependence of the likelihood function on µ and λ\np(X|µ, λ)=\nN∏\nn=1\n( λ\n2π\n)1/2\nexp\n{\n−λ\n2(xn − µ)2\n}\n∝\n[\nλ1/2 exp\n(\n−λµ2\n2\n)]N\nexp\n{\nλµ\nN∑\nn=1\nxn − λ\n2\nN∑\nn=1\nx2\nn\n}\n. (2.152)\nWe now wish to identify a prior distribution p(µ, λ) that has the same functional",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 120,
      "page_label": "101"
    }
  },
  {
    "page_content": "(\n−λµ2\n2\n)]N\nexp\n{\nλµ\nN∑\nn=1\nxn − λ\n2\nN∑\nn=1\nx2\nn\n}\n. (2.152)\nWe now wish to identify a prior distribution p(µ, λ) that has the same functional\ndependence on µ and λ as the likelihood function and that should therefore take the\nform\np(µ, λ) ∝\n[\nλ1/2 exp\n(\n−λµ2\n2\n)]β\nexp{cλµ − dλ}\n=e x p\n{\n−βλ\n2 (µ − c/β)2\n}\nλβ/2 exp\n{\n−\n(\nd − c2\n2β\n)\nλ\n}\n(2.153)\nwhere c, d, and β are constants. Since we can always write p(µ, λ)= p(µ|λ)p(λ),\nwe can ﬁnd p(µ|λ) and p(λ) by inspection. In particular, we see that p(µ|λ) is a\nGaussian whose precision is a linear function of λ and that p(λ) is a gamma distri-\nbution, so that the normalized prior takes the form\np(µ, λ)= N(µ|µ0, (βλ)−1)Gam(λ|a, b) (2.154)\nwhere we have deﬁned new constants given by µ0 = c/β, a =1 + β/2, b =\nd−c2/2β. The distribution (2.154) is called thenormal-gamma or Gaussian-gamma\ndistribution and is plotted in Figure 2.14. Note that this is not simply the product\nof an independent Gaussian prior over µ and a gamma prior over λ, because the\nprecision of µ is a linear function of λ. Even if we chose a prior in which µ and λ\nwere independent, the posterior distribution would exhibit a coupling between the\nprecision of µ and the value of λ.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 120,
      "page_label": "101"
    }
  },
  {
    "page_content": "102 2. PROBABILITY DISTRIBUTIONS\nFigure 2.14 Contour plot of the normal-gamma\ndistribution (2.154) for parameter\nvalues µ0 =0 , β =2 , a =5 and\nb =6 .\nµ\nλ\n−2 0 2\n0\n1\n2\nIn the case of the multivariate Gaussian distribution N\n(\nx|µ, Λ−1)\nfor a D-\ndimensional variable x, the conjugate prior distribution for the mean µ, assuming\nthe precision is known, is again a Gaussian. For known mean and unknown precision\nmatrix Λ, the conjugate prior is the Wishart distribution given byExercise 2.45\nW(Λ|W,ν )= B|Λ|(ν−D−1)/2 exp\n(\n−1\n2Tr(W−1Λ)\n)\n(2.155)\nwhere ν is called the number ofdegrees of freedomof the distribution, W is a D×D\nscale matrix, and Tr(·) denotes the trace. The normalization constant B is given by\nB(W,ν )= |W|−ν/2\n(\n2νD/2 πD(D−1)/4\nD∏\ni=1\nΓ\n( ν +1 − i\n2\n)) −1\n. (2.156)\nAgain, it is also possible to deﬁne a conjugate prior over the covariance matrix itself,\nrather than over the precision matrix, which leads to the inverse Wishart distribu-\ntion, although we shall not discuss this further. If both the mean and the precision\nare unknown, then, following a similar line of reasoning to the univariate case, the\nconjugate prior is given by\np(µ, Λ|µ0,β, W,ν )= N(µ|µ0, (βΛ)−1) W(Λ|W,ν ) (2.157)\nwhich is known as the normal-Wishart or Gaussian-Wishart distribution.\n2.3.7 Student’s t-distribution\nWe have seen that the conjugate prior for the precision of a Gaussian is given\nby a gamma distribution. If we have a univariate Gaussian N(x|µ, τ−1) togetherSection 2.3.6",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 121,
      "page_label": "102"
    }
  },
  {
    "page_content": "by a gamma distribution. If we have a univariate Gaussian N(x|µ, τ−1) togetherSection 2.3.6\nwith a Gamma prior Gam(τ|a, b) and we integrate out the precision, we obtain the\nmarginal distribution of x in the formExercise 2.46",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 121,
      "page_label": "102"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 103\nFigure 2.15 Plot of Student’s t-distribution (2.159)\nfor µ =0 and λ =1 for various values\nof ν. The limit ν →∞ corresponds\nto a Gaussian distribution with mean\nµ and precision λ.\nν →∞\nν =1 .0\nν =0 .1\n−5 0 5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\np(x|µ, a, b)=\n∫ ∞\n0\nN(x|µ, τ−1)Gam(τ|a, b)d τ (2.158)\n=\n∫ ∞\n0\nbae(−bτ)τa−1\nΓ(a)\n( τ\n2π\n)1/2\nexp\n{\n−τ\n2(x − µ)2\n}\ndτ\n= ba\nΓ(a)\n( 1\n2π\n)1/2 [\nb + (x − µ)2\n2\n]−a−1/2\nΓ(a +1 /2)\nwhere we have made the change of variable z = τ[b +( x − µ)2/2]. By convention\nwe deﬁne new parameters given by ν =2 a and λ = a/b, in terms of which the\ndistribution p(x|µ, a, b) takes the form\nSt(x|µ, λ, ν)= Γ(ν/2+1 /2)\nΓ(ν/2)\n( λ\nπν\n)1/2 [\n1+ λ(x − µ)2\nν\n]−ν/2−1/2\n(2.159)\nwhich is known as Student’s t-distribution. The parameter λ is sometimes called the\nprecision of the t-distribution, even though it is not in general equal to the inverse\nof the variance. The parameter ν is called the degrees of freedom, and its effect is\nillustrated in Figure 2.15. For the particular case of ν =1 , the t-distribution reduces\nto the Cauchy distribution, while in the limit ν →∞ the t-distribution St(x|µ, λ, ν)\nbecomes a Gaussian N(x|µ, λ−1) with mean µ and precision λ.Exercise 2.47\nFrom (2.158), we see that Student’s t-distribution is obtained by adding up an\ninﬁnite number of Gaussian distributions having the same mean but different preci-\nsions. This can be interpreted as an inﬁnite mixture of Gaussians (Gaussian mixtures",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 122,
      "page_label": "103"
    }
  },
  {
    "page_content": "sions. This can be interpreted as an inﬁnite mixture of Gaussians (Gaussian mixtures\nwill be discussed in detail in Section 2.3.9. The result is a distribution that in gen-\neral has longer ‘tails’ than a Gaussian, as was seen in Figure 2.15. This gives the t-\ndistribution an important property calledrobustness, which means that it is much less\nsensitive than the Gaussian to the presence of a few data points which are outliers.\nThe robustness of the t-distribution is illustrated in Figure 2.16, which compares the\nmaximum likelihood solutions for a Gaussian and a t-distribution. Note that the max-\nimum likelihood solution for the t-distribution can be found using the expectation-\nmaximization (EM) algorithm. Here we see that the effect of a small number ofExercise 12.24",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 122,
      "page_label": "103"
    }
  },
  {
    "page_content": "104 2. PROBABILITY DISTRIBUTIONS\n(a)\n−5 0 5 10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n(b)\n−5 0 5 10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nFigure 2.16 Illustration of the robustness of Student’s t-distribution compared to a Gaussian. (a) Histogram\ndistribution of 30 data points drawn from a Gaussian distribution, together with the maximum likelihood ﬁt ob-\ntained from a t-distribution (red curve) and a Gaussian (green curve, largely hidden by the red curve). Because\nthe t-distribution contains the Gaussian as a special case it gives almost the same solution as the Gaussian.\n(b) The same data set but with three additional outlying data points showing how the Gaussian (green curve) is\nstrongly distorted by the outliers, whereas the t-distribution (red curve) is relatively unaffected.\noutliers is much less signiﬁcant for the t-distribution than for the Gaussian. Outliers\ncan arise in practical applications either because the process that generates the data\ncorresponds to a distribution having a heavy tail or simply through mislabelled data.\nRobustness is also an important property for regression problems. Unsurprisingly,\nthe least squares approach to regression does not exhibit robustness, because it cor-\nresponds to maximum likelihood under a (conditional) Gaussian distribution. By\nbasing a regression model on a heavy-tailed distribution such as a t-distribution, we\nobtain a more robust model.\nIf we go back to (2.158) and substitute the alternative parameters ν =2 a, λ =",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 123,
      "page_label": "104"
    }
  },
  {
    "page_content": "obtain a more robust model.\nIf we go back to (2.158) and substitute the alternative parameters ν =2 a, λ =\na/b, and η = τb/a, we see that the t-distribution can be written in the form\nSt(x|µ, λ, ν)=\n∫ ∞\n0\nN\n(\nx|µ,(ηλ)−1)\nGam(η|ν/2,ν/ 2) dη. (2.160)\nWe can then generalize this to a multivariate GaussianN(x|µ,Λ) to obtain the cor-\nresponding multivariate Student’s t-distribution in the form\nSt(x|µ, Λ,ν )=\n∫ ∞\n0\nN(x|µ,(ηΛ)−1)Gam(η|ν/2,ν/ 2) dη. (2.161)\nUsing the same technique as for the univariate case, we can evaluate this integral to\ngiveExercise 2.48",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 123,
      "page_label": "104"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 105\nSt(x|µ,Λ,ν )= Γ(D/2+ ν/2)\nΓ(ν/2)\n|Λ|1/2\n(πν)D/2\n[\n1+ ∆2\nν\n]−D/2−ν/2\n(2.162)\nwhere D is the dimensionality of x, and ∆2 is the squared Mahalanobis distance\ndeﬁned by\n∆2 =( x − µ)TΛ(x − µ). (2.163)\nThis is the multivariate form of Student’s t-distribution and satisﬁes the following\npropertiesExercise 2.49\nE[x]= µ, if ν> 1 (2.164)\ncov[x]= ν\n(ν − 2)Λ−1, if ν> 2 (2.165)\nmode[x]= µ (2.166)\nwith corresponding results for the univariate case.\n2.3.8 Periodic variables\nAlthough Gaussian distributions are of great practical signiﬁcance, both in their\nown right and as building blocks for more complex probabilistic models, there are\nsituations in which they are inappropriate as density models for continuous vari-\nables. One important case, which arises in practical applications, is that of periodic\nvariables.\nAn example of a periodic variable would be the wind direction at a particular\ngeographical location. We might, for instance, measure values of wind direction on a\nnumber of days and wish to summarize this using a parametric distribution. Another\nexample is calendar time, where we may be interested in modelling quantities that\nare believed to be periodic over 24 hours or over an annual cycle. Such quantities\ncan conveniently be represented using an angular (polar) coordinate 0 ⩽ θ< 2π.\nWe might be tempted to treat periodic variables by choosing some direction\nas the origin and then applying a conventional distribution such as the Gaussian.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 124,
      "page_label": "105"
    }
  },
  {
    "page_content": "as the origin and then applying a conventional distribution such as the Gaussian.\nSuch an approach, however, would give results that were strongly dependent on the\narbitrary choice of origin. Suppose, for instance, that we have two observations at\nθ1 =1 ◦ and θ2 = 359◦, and we model them using a standard univariate Gaussian\ndistribution. If we choose the origin at 0◦, then the sample mean of this data set\nwill be 180◦ with standard deviation 179◦, whereas if we choose the origin at 180◦,\nthen the mean will be 0◦ and the standard deviation will be 1◦. We clearly need to\ndevelop a special approach for the treatment of periodic variables.\nLet us consider the problem of evaluating the mean of a set of observations\nD = {θ1,...,θ N } of a periodic variable. From now on, we shall assume that θ is\nmeasured in radians. We have already seen that the simple average(θ1+··· +θN )/N\nwill be strongly coordinate dependent. To ﬁnd an invariant measure of the mean, we\nnote that the observations can be viewed as points on the unit circle and can therefore\nbe described instead by two-dimensional unit vectors x1,..., xN where ∥xn∥=1\nfor n =1 ,...,N , as illustrated in Figure 2.17. We can average the vectors {xn}",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 124,
      "page_label": "105"
    }
  },
  {
    "page_content": "106 2. PROBABILITY DISTRIBUTIONS\nFigure 2.17 Illustration of the representation of val-\nues θn of a periodic variable as two-\ndimensional vectors xn living on the unit\ncircle. Also shown is the average x of\nthose vectors.\nx1\nx2\nx1\nx2\nx3x4\n¯x\n¯r\n¯θ\ninstead to give\nx = 1\nN\nN∑\nn=1\nxn (2.167)\nand then ﬁnd the corresponding angle θ of this average. Clearly, this deﬁnition will\nensure that the location of the mean is independent of the origin of the angular coor-\ndinate. Note that x will typically lie inside the unit circle. The Cartesian coordinates\nof the observations are given by xn =( c o sθn, sinθn), and we can write the Carte-\nsian coordinates of the sample mean in the form x =( r cos θ, r sinθ). Substituting\ninto (2.167) and equating the x1 and x2 components then gives\nr cos θ = 1\nN\nN∑\nn=1\ncos θn, r sin θ = 1\nN\nN∑\nn=1\nsin θn. (2.168)\nTaking the ratio, and using the identity tanθ =s i nθ/ cos θ, we can solve for θ to\ngive\nθ = tan−1\n{∑\nn sinθn∑\nn cos θn\n}\n. (2.169)\nShortly, we shall see how this result arises naturally as the maximum likelihood\nestimator for an appropriately deﬁned distribution over a periodic variable.\nWe now consider a periodic generalization of the Gaussian called thevon Mises\ndistribution. Here we shall limit our attention to univariate distributions, although\nperiodic distributions can also be found over hyperspheres of arbitrary dimension.\nFor an extensive discussion of periodic distributions, see Mardia and Jupp (2000).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 125,
      "page_label": "106"
    }
  },
  {
    "page_content": "For an extensive discussion of periodic distributions, see Mardia and Jupp (2000).\nBy convention, we will consider distributions p(θ) that have period 2π.A n y\nprobability density p(θ) deﬁned over θ must not only be nonnegative and integrate",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 125,
      "page_label": "106"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 107\nFigure 2.18 The von Mises distribution can be derived by considering\na two-dimensional Gaussian of the form (2.173), whose\ndensity contours are shown in blue and conditioning on\nthe unit circle shown in red.\nx1\nx2\np(x)\nr =1\nto one, but it must also be periodic. Thus p(θ) must satisfy the three conditions\np(θ) ⩾ 0 (2.170)\n∫ 2π\n0\np(θ)d θ =1 (2.171)\np(θ +2 π)= p(θ). (2.172)\nFrom (2.172), it follows that p(θ + M2π)= p(θ) for any integer M.\nWe can easily obtain a Gaussian-like distribution that satisﬁes these three prop-\nerties as follows. Consider a Gaussian distribution over two variables x =( x1,x2)\nhaving mean µ =( µ1,µ2) and a covariance matrix Σ = σ2I where I is the 2 × 2\nidentity matrix, so that\np(x1,x2)= 1\n2πσ2 exp\n{\n−(x1 − µ1)2 +( x2 − µ2)2\n2σ2\n}\n. (2.173)\nThe contours of constant p(x) are circles, as illustrated in Figure 2.18. Now suppose\nwe consider the value of this distribution along a circle of ﬁxed radius. Then by con-\nstruction this distribution will be periodic, although it will not be normalized. We can\ndetermine the form of this distribution by transforming from Cartesian coordinates\n(x1,x2) to polar coordinates (r, θ) so that\nx1 = r cos θ, x 2 = r sin θ. (2.174)\nWe also map the mean µ into polar coordinates by writing\nµ1 = r0 cos θ0,µ 2 = r0 sin θ0. (2.175)\nNext we substitute these transformations into the two-dimensional Gaussian distribu-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 126,
      "page_label": "107"
    }
  },
  {
    "page_content": "µ1 = r0 cos θ0,µ 2 = r0 sin θ0. (2.175)\nNext we substitute these transformations into the two-dimensional Gaussian distribu-\ntion (2.173), and then condition on the unit circler =1 , noting that we are interested\nonly in the dependence on θ. Focussing on the exponent in the Gaussian distribution\nwe have\n− 1\n2σ2\n{\n(r cos θ − r0 cos θ0)2 +( r sin θ − r0 sin θ0)2}\n= − 1\n2σ2\n{\n1+ r2\n0 − 2r0 cos θ cos θ0 − 2r0 sinθ sin θ0\n}\n= r0\nσ2 cos(θ − θ0)+c o n s t (2.176)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 126,
      "page_label": "107"
    }
  },
  {
    "page_content": "108 2. PROBABILITY DISTRIBUTIONS\nm =5 , θ0 = π/4\nm =1 , θ0 =3 π/4\n2π\n0\nπ/4\n3π/4\nm =5 , θ0 = π/4\nm =1 , θ0 =3 π/4\nFigure 2.19 The von Mises distribution plotted for two different parameter values, shown as a Cartesian plot\non the left and as the corresponding polar plot on the right.\nwhere ‘const’ denotes terms independent ofθ, and we have made use of the following\ntrigonometrical identitiesExercise 2.51\ncos2 A +s i n2 A =1 (2.177)\ncos A cos B +s i nA sin B =c o s (A − B). (2.178)\nIf we now deﬁne m = r0/σ2, we obtain our ﬁnal expression for the distribution of\np(θ) along the unit circle r =1 in the form\np(θ|θ0,m )= 1\n2πI0(m) exp {m cos(θ − θ0)} (2.179)\nwhich is called the von Mises distribution, or the circular normal. Here the param-\neter θ0 corresponds to the mean of the distribution, while m, which is known as\nthe concentration parameter, is analogous to the inverse variance (precision) for the\nGaussian. The normalization coefﬁcient in (2.179) is expressed in terms of I0(m),\nwhich is the zeroth-order Bessel function of the ﬁrst kind (Abramowitz and Stegun,\n1965) and is deﬁned by\nI0(m)= 1\n2π\n∫ 2π\n0\nexp{m cos θ} dθ. (2.180)\nFor large m, the distribution becomes approximately Gaussian. The von Mises dis-Exercise 2.52\ntribution is plotted in Figure 2.19, and the function I0(m) is plotted in Figure 2.20.\nNow consider the maximum likelihood estimators for the parameters θ0 and m\nfor the von Mises distribution. The log likelihood function is given by",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 127,
      "page_label": "108"
    }
  },
  {
    "page_content": "Now consider the maximum likelihood estimators for the parameters θ0 and m\nfor the von Mises distribution. The log likelihood function is given by\nlnp(D|θ0,m )= −N ln(2π) − N ln I0(m)+ m\nN∑\nn=1\ncos(θn − θ0). (2.181)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 127,
      "page_label": "108"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 109\nI0(m)\nm\n0 5 10\n0\n1000\n2000\n3000\nA(m)\nm\n0 5 10\n0\n0.5\n1\nFigure 2.20 Plot of the Bessel function I0(m) deﬁned by (2.180), together with the function A(m) deﬁned by\n(2.186).\nSetting the derivative with respect to θ0 equal to zero gives\nN∑\nn=1\nsin(θn − θ0)=0 . (2.182)\nTo solve for θ0, we make use of the trigonometric identity\nsin(A − B)=c o sB sinA − cos A sinB (2.183)\nfrom which we obtainExercise 2.53\nθML\n0 = tan−1\n{∑\nn sin θn∑\nn cos θn\n}\n(2.184)\nwhich we recognize as the result (2.169) obtained earlier for the mean of the obser-\nvations viewed in a two-dimensional Cartesian space.\nSimilarly, maximizing (2.181) with respect to m, and making use of I′\n0(m)=\nI1(m) (Abramowitz and Stegun, 1965), we have\nA(m)= 1\nN\nN∑\nn=1\ncos(θn − θML\n0 ) (2.185)\nwhere we have substituted for the maximum likelihood solution for θML\n0 (recalling\nthat we are performing a joint optimization over θ and m), and we have deﬁned\nA(m)= I1(m)\nI0(m). (2.186)\nThe function A(m) is plotted in Figure 2.20. Making use of the trigonometric iden-\ntity (2.178), we can write (2.185) in the form\nA(mML)=\n(\n1\nN\nN∑\nn=1\ncos θn\n)\ncos θML\n0 −\n(\n1\nN\nN∑\nn=1\nsin θn\n)\nsinθML\n0 . (2.187)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 128,
      "page_label": "109"
    }
  },
  {
    "page_content": "110 2. PROBABILITY DISTRIBUTIONS\nFigure 2.21 Plots of the ‘old faith-\nful’ data in which the blue curves\nshow contours of constant proba-\nbility density. On the left is a\nsingle Gaussian distribution which\nhas been ﬁtted to the data us-\ning maximum likelihood. Note that\nthis distribution fails to capture the\ntwo clumps in the data and indeed\nplaces much of its probability mass\nin the central region between the\nclumps where the data are relatively\nsparse. On the right the distribution\nis given by a linear combination of\ntwo Gaussians which has been ﬁtted\nto the data by maximum likelihood\nusing techniques discussed Chap-\nter 9, and which gives a better rep-\nresentation of the data.\n1 2 3 4 5 6\n40\n60\n80\n100\n1 2 3 4 5 6\n40\n60\n80\n100\nThe right-hand side of (2.187) is easily evaluated, and the function A(m) can be\ninverted numerically.\nFor completeness, we mention brieﬂy some alternative techniques for the con-\nstruction of periodic distributions. The simplest approach is to use a histogram of\nobservations in which the angular coordinate is divided into ﬁxed bins. This has the\nvirtue of simplicity and ﬂexibility but also suffers from signiﬁcant limitations, as we\nshall see when we discuss histogram methods in more detail in Section 2.5. Another\napproach starts, like the von Mises distribution, from a Gaussian distribution over a\nEuclidean space but now marginalizes onto the unit circle rather than conditioning",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 129,
      "page_label": "110"
    }
  },
  {
    "page_content": "Euclidean space but now marginalizes onto the unit circle rather than conditioning\n(Mardia and Jupp, 2000). However, this leads to more complex forms of distribution\nand will not be discussed further. Finally, any valid distribution over the real axis\n(such as a Gaussian) can be turned into a periodic distribution by mapping succes-\nsive intervals of width 2π onto the periodic variable (0, 2π), which corresponds to\n‘wrapping’ the real axis around unit circle. Again, the resulting distribution is more\ncomplex to handle than the von Mises distribution.\nOne limitation of the von Mises distribution is that it is unimodal. By forming\nmixtures of von Mises distributions, we obtain a ﬂexible framework for modelling\nperiodic variables that can handle multimodality. For an example of a machine learn-\ning application that makes use of von Mises distributions, see Lawrenceet al.(2002),\nand for extensions to modelling conditional densities for regression problems, see\nBishop and Nabney (1996).\n2.3.9 Mixtures of Gaussians\nWhile the Gaussian distribution has some important analytical properties, it suf-\nfers from signiﬁcant limitations when it comes to modelling real data sets. Consider\nthe example shown in Figure 2.21. This is known as the ‘Old Faithful’ data set,\nand comprises 272 measurements of the eruption of the Old Faithful geyser at Yel-\nlowstone National Park in the USA. Each measurement comprises the duration ofAppendix A",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 129,
      "page_label": "110"
    }
  },
  {
    "page_content": "2.3. The Gaussian Distribution 111\nFigure 2.22 Example of a Gaussian mixture distribution\nin one dimension showing three Gaussians\n(each scaled by a coefﬁcient) in blue and\ntheir sum in red.\nx\np(x)\nthe eruption in minutes (horizontal axis) and the time in minutes to the next erup-\ntion (vertical axis). We see that the data set forms two dominant clumps, and that\na simple Gaussian distribution is unable to capture this structure, whereas a linear\nsuperposition of two Gaussians gives a better characterization of the data set.\nSuch superpositions, formed by taking linear combinations of more basic dis-\ntributions such as Gaussians, can be formulated as probabilistic models known as\nmixture distributions (McLachlan and Basford, 1988; McLachlan and Peel, 2000).\nIn Figure 2.22 we see that a linear combination of Gaussians can give rise to very\ncomplex densities. By using a sufﬁcient number of Gaussians, and by adjusting their\nmeans and covariances as well as the coefﬁcients in the linear combination, almost\nany continuous density can be approximated to arbitrary accuracy.\nWe therefore consider a superposition of K Gaussian densities of the form\np(x)=\nK∑\nk=1\nπkN(x|µk, Σk) (2.188)\nwhich is called a mixture of Gaussians. Each Gaussian density N(x|µk, Σk) is\ncalled a component of the mixture and has its own mean µk and covariance Σk.\nContour and surface plots for a Gaussian mixture having 3 components are shown in\nFigure 2.23.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 130,
      "page_label": "111"
    }
  },
  {
    "page_content": "Contour and surface plots for a Gaussian mixture having 3 components are shown in\nFigure 2.23.\nIn this section we shall consider Gaussian components to illustrate the frame-\nwork of mixture models. More generally, mixture models can comprise linear com-\nbinations of other distributions. For instance, in Section 9.3.3 we shall consider\nmixtures of Bernoulli distributions as an example of a mixture model for discrete\nvariables.Section 9.3.3\nThe parameters πk in (2.188) are called mixing coefﬁcients. If we integrate both\nsides of (2.188) with respect tox, and note that bothp(x) and the individual Gaussian\ncomponents are normalized, we obtain\nK∑\nk=1\nπk =1 . (2.189)\nAlso, the requirement that p(x) ⩾ 0, together with N(x|µk, Σk) ⩾ 0, implies\nπk ⩾ 0 for all k. Combining this with the condition (2.189) we obtain\n0 ⩽ πk ⩽ 1. (2.190)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 130,
      "page_label": "111"
    }
  },
  {
    "page_content": "112 2. PROBABILITY DISTRIBUTIONS\n0.5 0.3\n0.2\n(a)\n0 0.5 1\n0\n0.5\n1 (b)\n0 0.5 1\n0\n0.5\n1\nFigure 2.23 Illustration of a mixture of 3 Gaussians in a two-dimensional space. (a) Contours of constant\ndensity for each of the mixture components, in which the 3 components are denoted red, blue and green, and\nthe values of the mixing coefﬁcients are shown below each component. (b) Contours of the marginal probability\ndensity p(x) of the mixture distribution. (c) A surface plot of the distribution p(x).\nWe therefore see that the mixing coefﬁcients satisfy the requirements to be probabil-\nities.\nFrom the sum and product rules, the marginal density is given by\np(x)=\nK∑\nk=1\np(k)p(x|k) (2.191)\nwhich is equivalent to (2.188) in which we can view πk = p(k) as the prior prob-\nability of picking the kth component, and the density N(x|µk, Σk)= p(x|k) as\nthe probability of x conditioned on k. As we shall see in later chapters, an impor-\ntant role is played by the posterior probabilities p(k|x), which are also known as\nresponsibilities. From Bayes’ theorem these are given by\nγk(x) ≡ p(k|x)\n= p(k)p(x|k)∑\nl p(l)p(x|l)\n= πkN(x|µk, Σk)∑\nl πlN(x|µl, Σl). (2.192)\nWe shall discuss the probabilistic interpretation of the mixture distribution in greater\ndetail in Chapter 9.\nThe form of the Gaussian mixture distribution is governed by the parameters π,\nµ and Σ, where we have used the notation π ≡{ π1,...,π K}, µ ≡{ µ1,..., µK}\nand Σ ≡{ Σ1,... ΣK}. One way to set the values of these parameters is to use",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 131,
      "page_label": "112"
    }
  },
  {
    "page_content": "and Σ ≡{ Σ1,... ΣK}. One way to set the values of these parameters is to use\nmaximum likelihood. From (2.188) the log of the likelihood function is given by\nlnp(X|π, µ, Σ)=\nN∑\nn=1\nln\n{ K∑\nk=1\nπkN(xn|µk, Σk)\n}\n(2.193)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 131,
      "page_label": "112"
    }
  },
  {
    "page_content": "2.4. The Exponential Family 113\nwhere X = {x1,..., xN }. We immediately see that the situation is now much\nmore complex than with a single Gaussian, due to the presence of the summation\noverk inside the logarithm. As a result, the maximum likelihood solution for the\nparameters no longer has a closed-form analytical solution. One approach to maxi-\nmizing the likelihood function is to use iterative numerical optimization techniques\n(Fletcher, 1987; Nocedal and Wright, 1999; Bishop and Nabney, 2008). Alterna-\ntively we can employ a powerful framework calledexpectation maximization, which\nwill be discussed at length in Chapter 9.\n2.4. The Exponential Family\nThe probability distributions that we have studied so far in this chapter (with the\nexception of the Gaussian mixture) are speciﬁc examples of a broad class of distri-\nbutions called the exponential family (Duda and Hart, 1973; Bernardo and Smith,\n1994). Members of the exponential family have many important properties in com-\nmon, and it is illuminating to discuss these properties in some generality.\nThe exponential family of distributions overx, given parameters η, is deﬁned to\nbe the set of distributions of the form\np(x|η)= h(x)g(η)e x p\n{\nηTu(x)\n}\n(2.194)\nwhere x may be scalar or vector, and may be discrete or continuous. Here η are\ncalled the natural parameters of the distribution, and u(x) is some function of x.\nThe function g(η) can be interpreted as the coefﬁcient that ensures that the distribu-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 132,
      "page_label": "113"
    }
  },
  {
    "page_content": "The function g(η) can be interpreted as the coefﬁcient that ensures that the distribu-\ntion is normalized and therefore satisﬁes\ng(η)\n∫\nh(x)e x p\n{\nηTu(x)\n}\ndx =1 (2.195)\nwhere the integration is replaced by summation if x is a discrete variable.\nWe begin by taking some examples of the distributions introduced earlier in\nthe chapter and showing that they are indeed members of the exponential family.\nConsider ﬁrst the Bernoulli distribution\np(x|µ)=B e r n (x|µ)= µx(1 − µ)1−x. (2.196)\nExpressing the right-hand side as the exponential of the logarithm, we have\np(x|µ) = exp {xlnµ +( 1− x)l n ( 1− µ)}\n=( 1 − µ)e x p\n{\nln\n( µ\n1 − µ\n)\nx\n}\n. (2.197)\nComparison with (2.194) allows us to identify\nη =l n\n( µ\n1 − µ\n)\n(2.198)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 132,
      "page_label": "113"
    }
  },
  {
    "page_content": "114 2. PROBABILITY DISTRIBUTIONS\nwhich we can solve for µ to give µ = σ(η), where\nσ(η)= 1\n1+e x p (−η) (2.199)\nis called the logistic sigmoid function. Thus we can write the Bernoulli distribution\nusing the standard representation (2.194) in the form\np(x|η)= σ(−η) exp(ηx) (2.200)\nwhere we have used 1 − σ(η)= σ(−η), which is easily proved from (2.199). Com-\nparison with (2.194) shows that\nu(x)= x (2.201)\nh(x)=1 (2.202)\ng(η)= σ(−η). (2.203)\nNext consider the multinomial distribution that, for a single observationx, takes\nthe form\np(x|µ)=\nM∏\nk=1\nµxk\nk =e x p\n{ M∑\nk=1\nxk ln µk\n}\n(2.204)\nwhere x =( x1,...,x N )T. Again, we can write this in the standard representation\n(2.194) so that\np(x|η)=e x p (ηTx) (2.205)\nwhere ηk =l nµk, and we have deﬁned η =( η1,...,η M )T. Again, comparing with\n(2.194) we have\nu(x)= x (2.206)\nh(x)=1 (2.207)\ng(η)=1 . (2.208)\nNote that the parameters ηk are not independent because the parameters µk are sub-\nject to the constraint\nM∑\nk=1\nµk =1 (2.209)\nso that, given any M −1 of the parameters µk, the value of the remaining parameter\nis ﬁxed. In some circumstances, it will be convenient to remove this constraint by\nexpressing the distribution in terms of onlyM −1 parameters. This can be achieved\nby using the relationship (2.209) to eliminate µM by expressing it in terms of the\nremaining {µk} where k =1 ,...,M − 1, thereby leaving M − 1 parameters. Note\nthat these remaining parameters are still subject to the constraints\n0 ⩽ µk ⩽ 1,\nM−1∑\nk=1\nµk ⩽ 1. (2.210)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 133,
      "page_label": "114"
    }
  },
  {
    "page_content": "2.4. The Exponential Family 115\nMaking use of the constraint (2.209), the multinomial distribution in this representa-\ntion then becomes\nexp\n{ M∑\nk=1\nxk ln µk\n}\n=e x p\n{M−1∑\nk=1\nxk lnµk +\n(\n1 −\nM−1∑\nk=1\nxk\n)\nln\n(\n1 −\nM−1∑\nk=1\nµk\n)}\n=e x p\n{M−1∑\nk=1\nxk ln\n(\nµk\n1 − ∑ M−1\nj=1 µj\n)\n+l n\n(\n1 −\nM−1∑\nk=1\nµk\n)}\n. (2.211)\nWe now identify\nln\n(\nµk\n1 − ∑\nj µj\n)\n= ηk (2.212)\nwhich we can solve for µk by ﬁrst summing both sides over k and then rearranging\nand back-substituting to give\nµk = exp(ηk)\n1+ ∑\nj exp(ηj). (2.213)\nThis is called the softmax function, or the normalized exponential. In this represen-\ntation, the multinomial distribution therefore takes the form\np(x|η)=\n(\n1+\nM−1∑\nk=1\nexp(ηk)\n) −1\nexp(ηTx). (2.214)\nThis is the standard form of the exponential family, with parameter vector η =\n(η1,...,η M−1)T in which\nu(x)= x (2.215)\nh(x)=1 (2.216)\ng(η)=\n(\n1+\nM−1∑\nk=1\nexp(ηk)\n) −1\n. (2.217)\nFinally, let us consider the Gaussian distribution. For the univariate Gaussian,\nwe have\np(x|µ, σ2)= 1\n(2πσ2)1/2 exp\n{\n− 1\n2σ2 (x − µ)2\n}\n(2.218)\n= 1\n(2πσ2)1/2 exp\n{\n− 1\n2σ2 x2 + µ\nσ2 x − 1\n2σ2 µ2\n}\n(2.219)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 134,
      "page_label": "115"
    }
  },
  {
    "page_content": "116 2. PROBABILITY DISTRIBUTIONS\nwhich, after some simple rearrangement, can be cast in the standard exponential\nfamily form (2.194) withExercise 2.57\nη =\n(\nµ/σ2\n−1/2σ2\n)\n(2.220)\nu(x)=\n(\nx\nx2\n)\n(2.221)\nh(x)=( 2 π)−1/2 (2.222)\ng(η)=( −2η2)1/2 exp\n( η2\n1\n4η2\n)\n. (2.223)\n2.4.1 Maximum likelihood and sufﬁcient statistics\nLet us now consider the problem of estimating the parameter vectorη in the gen-\neral exponential family distribution (2.194) using the technique of maximum likeli-\nhood. Taking the gradient of both sides of (2.195) with respect to η,w eh a v e\n∇g(η)\n∫\nh(x)e x p\n{\nηTu(x)\n}\ndx\n+ g(η)\n∫\nh(x)e x p\n{\nηTu(x)\n}\nu(x)dx =0 . (2.224)\nRearranging, and making use again of (2.195) then gives\n− 1\ng(η)∇g(η)= g(η)\n∫\nh(x)e x p\n{\nηTu(x)\n}\nu(x)dx = E[u(x)] (2.225)\nwhere we have used (2.194). We therefore obtain the result\n−∇lng(η)= E[u(x)]. (2.226)\nNote that the covariance of u(x) can be expressed in terms of the second derivatives\nof g(η), and similarly for higher order moments. Thus, provided we can normalize aExercise 2.58\ndistribution from the exponential family, we can always ﬁnd its moments by simple\ndifferentiation.\nNow consider a set of independent identically distributed data denoted by X =\n{x1,..., xn}, for which the likelihood function is given by\np(X|η)=\n( N∏\nn=1\nh(xn)\n)\ng(η)N exp\n{\nηT\nN∑\nn=1\nu(xn)\n}\n. (2.227)\nSetting the gradient of ln p(X|η) with respect to η to zero, we get the following\ncondition to be satisﬁed by the maximum likelihood estimator ηML\n−∇lng(ηML)= 1\nN",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 135,
      "page_label": "116"
    }
  },
  {
    "page_content": "condition to be satisﬁed by the maximum likelihood estimator ηML\n−∇lng(ηML)= 1\nN\nN∑\nn=1\nu(xn) (2.228)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 135,
      "page_label": "116"
    }
  },
  {
    "page_content": "2.4. The Exponential Family 117\nwhich can in principle be solved to obtain ηML. We see that the solution for the\nmaximum likelihood estimator depends on the data only through ∑\nn u(xn), which\nis therefore called the sufﬁcient statistic of the distribution (2.194). We do not need\nto store the entire data set itself but only the value of the sufﬁcient statistic. For\nthe Bernoulli distribution, for example, the function u(x) is given just by x and\nso we need only keep the sum of the data points {xn}, whereas for the Gaussian\nu(x)=( x, x2)T, and so we should keep both the sum of{xn} and the sum of {x2\nn}.\nIf we consider the limit N →∞ , then the right-hand side of (2.228) becomes\nE[u(x)], and so by comparing with (2.226) we see that in this limit ηML will equal\nthe true value η.\nIn fact, this sufﬁciency property holds also for Bayesian inference, although\nwe shall defer discussion of this until Chapter 8 when we have equipped ourselves\nwith the tools of graphical models and can thereby gain a deeper insight into these\nimportant concepts.\n2.4.2 Conjugate priors\nWe have already encountered the concept of a conjugate prior several times, for\nexample in the context of the Bernoulli distribution (for which the conjugate prior\nis the beta distribution) or the Gaussian (where the conjugate prior for the mean is\na Gaussian, and the conjugate prior for the precision is the Wishart distribution). In\ngeneral, for a given probability distributionp(x|η), we can seek a prior p(η) that is",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 136,
      "page_label": "117"
    }
  },
  {
    "page_content": "general, for a given probability distributionp(x|η), we can seek a prior p(η) that is\nconjugate to the likelihood function, so that the posterior distribution has the same\nfunctional form as the prior. For any member of the exponential family (2.194), there\nexists a conjugate prior that can be written in the form\np(η|χ,ν )= f(χ,ν )g(η)ν exp\n{\nνηTχ\n}\n(2.229)\nwhere f(χ,ν ) is a normalization coefﬁcient, and g(η) is the same function as ap-\npears in (2.194). To see that this is indeed conjugate, let us multiply the prior (2.229)\nby the likelihood function (2.227) to obtain the posterior distribution, up to a nor-\nmalization coefﬁcient, in the form\np(η|X, χ,ν ) ∝ g(η)ν+N exp\n{\nηT\n( N∑\nn=1\nu(xn)+ νχ\n)}\n. (2.230)\nThis again takes the same functional form as the prior (2.229), conﬁrming conjugacy.\nFurthermore, we see that the parameter ν can be interpreted as a effective number of\npseudo-observations in the prior, each of which has a value for the sufﬁcient statistic\nu(x) given by χ.\n2.4.3 Noninformative priors\nIn some applications of probabilistic inference, we may have prior knowledge\nthat can be conveniently expressed through the prior distribution. For example, if\nthe prior assigns zero probability to some value of variable, then the posterior dis-\ntribution will necessarily also assign zero probability to that value, irrespective of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 136,
      "page_label": "117"
    }
  },
  {
    "page_content": "118 2. PROBABILITY DISTRIBUTIONS\nany subsequent observations of data. In many cases, however, we may have little\nidea of what form the distribution should take. We may then seek a form of prior\ndistribution, called anoninformative prior, which is intended to have as little inﬂu-\nence on the posterior distribution as possible (Jeffries, 1946; Box and Tao, 1973;\nBernardo and Smith, 1994). This is sometimes referred to as ‘letting the data speak\nfor themselves’.\nIf we have a distributionp(x|λ) governed by a parameterλ, we might be tempted\nto propose a prior distribution p(λ) = const as a suitable prior. If λ is a discrete\nvariable with K states, this simply amounts to setting the prior probability of each\nstate to 1/K. In the case of continuous parameters, however, there are two potential\ndifﬁculties with this approach. The ﬁrst is that, if the domain of λ is unbounded,\nthis prior distribution cannot be correctly normalized because the integral over λ\ndiverges. Such priors are called improper. In practice, improper priors can often\nbe used provided the corresponding posterior distribution is proper, i.e., that it can\nbe correctly normalized. For instance, if we put a uniform prior distribution over\nthe mean of a Gaussian, then the posterior distribution for the mean, once we have\nobserved at least one data point, will be proper.\nA second difﬁculty arises from the transformation behaviour of a probability",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 137,
      "page_label": "118"
    }
  },
  {
    "page_content": "observed at least one data point, will be proper.\nA second difﬁculty arises from the transformation behaviour of a probability\ndensity under a nonlinear change of variables, given by (1.27). If a function h(λ)\nis constant, and we change variables to λ = η2, then ˆh(η)= h(η2) will also be\nconstant. However, if we choose the density pλ(λ) to be constant, then the density\nof η will be given, from (1.27), by\npη(η)= pλ(λ)\n⏐⏐⏐⏐\ndλ\ndη\n⏐⏐⏐⏐= pλ(η2)2η ∝ η (2.231)\nand so the density over η will not be constant. This issue does not arise when we use\nmaximum likelihood, because the likelihood function p(x|λ) is a simple function of\nλ and so we are free to use any convenient parameterization. If, however, we are to\nchoose a prior distribution that is constant, we must take care to use an appropriate\nrepresentation for the parameters.\nHere we consider two simple examples of noninformative priors (Berger, 1985).\nFirst of all, if a density takes the form\np(x|µ)= f(x − µ) (2.232)\nthen the parameter µ is known as a location parameter. This family of densities\nexhibits translation invariancebecause if we shift x by a constant to giveˆx = x+c,\nthen\np(ˆx|ˆµ)= f(ˆx − ˆµ) (2.233)\nwhere we have deﬁned ˆµ = µ + c. Thus the density takes the same form in the\nnew variable as in the original one, and so the density is independent of the choice\nof origin. We would like to choose a prior distribution that reﬂects this translation",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 137,
      "page_label": "118"
    }
  },
  {
    "page_content": "of origin. We would like to choose a prior distribution that reﬂects this translation\ninvariance property, and so we choose a prior that assigns equal probability mass to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 137,
      "page_label": "118"
    }
  },
  {
    "page_content": "2.4. The Exponential Family 119\nan interval A ⩽ µ ⩽ B as to the shifted interval A − c ⩽ µ ⩽ B − c. This implies\n∫ B\nA\np(µ)d µ =\n∫ B−c\nA−c\np(µ)d µ =\n∫ B\nA\np(µ − c)dµ (2.234)\nand because this must hold for all choices of A and B,w eh a v e\np(µ − c)= p(µ) (2.235)\nwhich implies that p(µ) is constant. An example of a location parameter would be\nthe mean µ of a Gaussian distribution. As we have seen, the conjugate prior distri-\nbution for µ in this case is a Gaussian p(µ|µ0,σ 2\n0)= N(µ|µ0,σ 2\n0), and we obtain a\nnoninformative prior by taking the limit σ2\n0 →∞ . Indeed, from (2.141) and (2.142)\nwe see that this gives a posterior distribution over µ in which the contributions from\nthe prior vanish.\nAs a second example, consider a density of the form\np(x|σ)= 1\nσf\n(x\nσ\n)\n(2.236)\nwhere σ> 0. Note that this will be a normalized density provided f(x) is correctly\nnormalized. The parameter σ is known as ascale parameter, and the density exhibitsExercise 2.59\nscale invariancebecause if we scale x by a constant to give ˆx = cx, then\np(ˆx|ˆσ)= 1\nˆσf\n( ˆx\nˆσ\n)\n(2.237)\nwhere we have deﬁned ˆσ = cσ. This transformation corresponds to a change of\nscale, for example from meters to kilometers if x is a length, and we would like\nto choose a prior distribution that reﬂects this scale invariance. If we consider an\nintervalA ⩽ σ ⩽ B, and a scaled interval A/c ⩽ σ ⩽ B/c, then the prior should\nassign equal probability mass to these two intervals. Thus we have\n∫ B\nA\np(σ)d σ =\n∫ B/c\nA/c\np(σ)d σ =\n∫ B\nA\np",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 138,
      "page_label": "119"
    }
  },
  {
    "page_content": "assign equal probability mass to these two intervals. Thus we have\n∫ B\nA\np(σ)d σ =\n∫ B/c\nA/c\np(σ)d σ =\n∫ B\nA\np\n( 1\ncσ\n) 1\nc dσ (2.238)\nand because this must hold for choices of A and B,w eh a v e\np(σ)= p\n( 1\ncσ\n) 1\nc (2.239)\nand hence p(σ) ∝ 1/σ. Note that again this is an improper prior because the integral\nof the distribution over 0 ⩽ σ ⩽ ∞ is divergent. It is sometimes also convenient\nto think of the prior distribution for a scale parameter in terms of the density of the\nlog of the parameter. Using the transformation rule (1.27) for densities we see that\np(lnσ) = const. Thus, for this prior there is the same probability mass in the range\n1 ⩽ σ ⩽ 10 as in the range 10 ⩽ σ ⩽ 100 and in 100 ⩽ σ ⩽ 1000.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 138,
      "page_label": "119"
    }
  },
  {
    "page_content": "120 2. PROBABILITY DISTRIBUTIONS\nAn example of a scale parameter would be the standard deviationσ of a Gaussian\ndistribution, after we have taken account of the location parameter µ, because\nN(x|µ, σ2) ∝ σ−1 exp\n{\n−(˜x/σ)2}\n(2.240)\nwhere ˜x = x − µ. As discussed earlier, it is often more convenient to work in terms\nof the precision λ =1 /σ2 rather than σ itself. Using the transformation rule for\ndensities, we see that a distribution p(σ) ∝ 1/σ corresponds to a distribution over λ\nof the form p(λ) ∝ 1/λ. We have seen that the conjugate prior forλ was the gamma\ndistribution Gam(λ|a0,b 0) given by (2.146). The noninformative prior is obtainedSection 2.3\nas the special casea0 = b0 =0 . Again, if we examine the results (2.150) and (2.151)\nfor the posterior distribution ofλ, we see that for a0 = b0 =0 , the posterior depends\nonly on terms arising from the data and not from the prior.\n2.5. Nonparametric Methods\nThroughout this chapter, we have focussed on the use of probability distributions\nhaving speciﬁc functional forms governed by a small number of parameters whose\nvalues are to be determined from a data set. This is called theparametric approach\nto density modelling. An important limitation of this approach is that the chosen\ndensity might be a poor model of the distribution that generates the data, which can\nresult in poor predictive performance. For instance, if the process that generates the\ndata is multimodal, then this aspect of the distribution can never be captured by a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 139,
      "page_label": "120"
    }
  },
  {
    "page_content": "data is multimodal, then this aspect of the distribution can never be captured by a\nGaussian, which is necessarily unimodal.\nIn this ﬁnal section, we consider some nonparametric approaches to density es-\ntimation that make few assumptions about the form of the distribution. Here we shall\nfocus mainly on simple frequentist methods. The reader should be aware, however,\nthat nonparametric Bayesian methods are attracting increasing interest (Walkeret al.,\n1999; Neal, 2000; M¨uller and Quintana, 2004; Teh et al., 2006).\nLet us start with a discussion of histogram methods for density estimation, which\nwe have already encountered in the context of marginal and conditional distributions\nin Figure 1.11 and in the context of the central limit theorem in Figure 2.6. Here we\nexplore the properties of histogram density models in more detail, focussing on the\ncase of a single continuous variable x. Standard histograms simply partition x into\ndistinct bins of width ∆i and then count the number ni of observations of x falling\nin bin i. In order to turn this count into a normalized probability density, we simply\ndivide by the total number N of observations and by the width ∆i of the bins to\nobtain probability values for each bin given by\npi = ni\nN∆i\n(2.241)\nfor which it is easily seen that\n∫\np(x)d x =1 . This gives a model for the density\np(x) that is constant over the width of each bin, and often the bins are chosen to have\nthe same width ∆i =∆ .",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 139,
      "page_label": "120"
    }
  },
  {
    "page_content": "2.5. Nonparametric Methods 121\nFigure 2.24 An illustration of the histogram approach\nto density estimation, in which a data set\nof 50 data points is generated from the\ndistribution shown by the green curve.\nHistogram density estimates, based on\n(2.241), with a common bin width ∆ are\nshown for various values of ∆.\n∆=0 .04\n0 0.5 1\n0\n5\n∆=0 .08\n0 0.5 1\n0\n5\n∆=0 .25\n0 0.5 1\n0\n5\nIn Figure 2.24, we show an example of histogram density estimation. Here\nthe data is drawn from the distribution, corresponding to the green curve, which is\nformed from a mixture of two Gaussians. Also shown are three examples of his-\ntogram density estimates corresponding to three different choices for the bin width\n∆. We see that when ∆ is very small (top ﬁgure), the resulting density model is very\nspiky, with a lot of structure that is not present in the underlying distribution that\ngenerated the data set. Conversely, if ∆ is too large (bottom ﬁgure) then the result is\na model that is too smooth and that consequently fails to capture the bimodal prop-\nerty of the green curve. The best results are obtained for some intermediate value\nof ∆ (middle ﬁgure). In principle, a histogram density model is also dependent on\nthe choice of edge location for the bins, though this is typically much less signiﬁcant\nthan the value of ∆.\nNote that the histogram method has the property (unlike the methods to be dis-\ncussed shortly) that, once the histogram has been computed, the data set itself can",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 140,
      "page_label": "121"
    }
  },
  {
    "page_content": "cussed shortly) that, once the histogram has been computed, the data set itself can\nbe discarded, which can be advantageous if the data set is large. Also, the histogram\napproach is easily applied if the data points are arriving sequentially.\nIn practice, the histogram technique can be useful for obtaining a quick visual-\nization of data in one or two dimensions but is unsuited to most density estimation\napplications. One obvious problem is that the estimated density has discontinuities\nthat are due to the bin edges rather than any property of the underlying distribution\nthat generated the data. Another major limitation of the histogram approach is its\nscaling with dimensionality. If we divide each variable in a D-dimensional space\ninto M bins, then the total number of bins will be MD. This exponential scaling\nwith D is an example of the curse of dimensionality. In a space of high dimensional-Section 1.4\nity, the quantity of data needed to provide meaningful estimates of local probability\ndensity would be prohibitive.\nThe histogram approach to density estimation does, however, teach us two im-\nportant lessons. First, to estimate the probability density at a particular location,\nwe should consider the data points that lie within some local neighbourhood of that\npoint. Note that the concept of locality requires that we assume some form of dis-\ntance measure, and here we have been assuming Euclidean distance. For histograms,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 140,
      "page_label": "121"
    }
  },
  {
    "page_content": "122 2. PROBABILITY DISTRIBUTIONS\nthis neighbourhood property was deﬁned by the bins, and there is a natural ‘smooth-\ning’ parameter describing the spatial extent of the local region, in this case the bin\nwidth. Second, the value of the smoothing parameter should be neither too large nor\ntoo small in order to obtain good results. This is reminiscent of the choice of model\ncomplexity in polynomial curve ﬁtting discussed in Chapter 1 where the degree M\nof the polynomial, or alternatively the value α of the regularization parameter, was\noptimal for some intermediate value, neither too large nor too small. Armed with\nthese insights, we turn now to a discussion of two widely used nonparametric tech-\nniques for density estimation, kernel estimators and nearest neighbours, which have\nbetter scaling with dimensionality than the simple histogram model.\n2.5.1 Kernel density estimators\nLet us suppose that observations are being drawn from some unknown probabil-\nity density p(x) in some D-dimensional space, which we shall take to be Euclidean,\nand we wish to estimate the value of p(x). From our earlier discussion of locality,\nlet us consider some small region R containing x. The probability mass associated\nwith this region is given by\nP =\n∫\nR\np(x)dx. (2.242)\nNow suppose that we have collected a data set comprising N observations drawn\nfrom p(x). Because each data point has a probability P of falling within R, the total",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 141,
      "page_label": "122"
    }
  },
  {
    "page_content": "from p(x). Because each data point has a probability P of falling within R, the total\nnumber K of points that lie inside R will be distributed according to the binomial\ndistributionSection 2.1\nBin(K|N,P )= N!\nK!(N − K)!PK(1 − P)1−K. (2.243)\nUsing (2.11), we see that the mean fraction of points falling inside the region is\nE[K/N]= P, and similarly using (2.12) we see that the variance around this mean\nis var[K/N]= P(1 − P)/N. For large N, this distribution will be sharply peaked\naround the mean and so\nK ≃ NP. (2.244)\nIf, however, we also assume that the regionR is sufﬁciently small that the probability\ndensity p(x) is roughly constant over the region, then we have\nP ≃ p(x)V (2.245)\nwhere V is the volume of R. Combining (2.244) and (2.245), we obtain our density\nestimate in the form\np(x)= K\nNV . (2.246)\nNote that the validity of (2.246) depends on two contradictory assumptions, namely\nthat the regionR be sufﬁciently small that the density is approximately constant over\nthe region and yet sufﬁciently large (in relation to the value of that density) that the\nnumber K of points falling inside the region is sufﬁcient for the binomial distribution\nto be sharply peaked.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 141,
      "page_label": "122"
    }
  },
  {
    "page_content": "2.5. Nonparametric Methods 123\nWe can exploit the result (2.246) in two different ways. Either we can ﬁxK and\ndetermine the value of V from the data, which gives rise to theK-nearest-neighbour\ntechnique discussed shortly, or we can ﬁx V and determine K from the data, giv-\ning rise to the kernel approach. It can be shown that both the K-nearest-neighbour\ndensity estimator and the kernel density estimator converge to the true probability\ndensity in the limitN →∞ provided V shrinks suitably with N, and K grows with\nN (Duda and Hart, 1973).\nWe begin by discussing the kernel method in detail, and to start with we take\nthe region R to be a small hypercube centred on the point x at which we wish to\ndetermine the probability density. In order to count the number K of points falling\nwithin this region, it is convenient to deﬁne the following function\nk(u)=\n{\n1, |ui| ⩽ 1/2, i =1 ,...,D ,\n0, otherwise (2.247)\nwhich represents a unit cube centred on the origin. The function k(u) is an example\nof a kernel function, and in this context is also called aParzen window. From (2.247),\nthe quantity k((x−xn)/h) will be one if the data point xn lies inside a cube of side\nh centred on x, and zero otherwise. The total number of data points lying inside this\ncube will therefore be\nK =\nN∑\nn=1\nk\n(x − xn\nh\n)\n. (2.248)\nSubstituting this expression into (2.246) then gives the following result for the esti-\nmated density at x\np(x)= 1\nN\nN∑\nn=1\n1\nhD k\n(x − xn\nh\n)\n(2.249)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 142,
      "page_label": "123"
    }
  },
  {
    "page_content": "mated density at x\np(x)= 1\nN\nN∑\nn=1\n1\nhD k\n(x − xn\nh\n)\n(2.249)\nwhere we have used V = hD for the volume of a hypercube of side h in D di-\nmensions. Using the symmetry of the function k(u), we can now re-interpret this\nequation, not as a single cube centred on x but as the sum over N cubes centred on\nthe N data points xn.\nAs it stands, the kernel density estimator (2.249) will suffer from one of the same\nproblems that the histogram method suffered from, namely the presence of artiﬁcial\ndiscontinuities, in this case at the boundaries of the cubes. We can obtain a smoother\ndensity model if we choose a smoother kernel function, and a common choice is the\nGaussian, which gives rise to the following kernel density model\np(x)= 1\nN\nN∑\nn=1\n1\n(2πh2)1/2 exp\n{\n−∥x − xn∥2\n2h2\n}\n(2.250)\nwhere h represents the standard deviation of the Gaussian components. Thus our\ndensity model is obtained by placing a Gaussian over each data point and then adding\nup the contributions over the whole data set, and then dividing byN so that the den-\nsity is correctly normalized. In Figure 2.25, we apply the model (2.250) to the data",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 142,
      "page_label": "123"
    }
  },
  {
    "page_content": "124 2. PROBABILITY DISTRIBUTIONS\nFigure 2.25 Illustration of the kernel density model\n(2.250) applied to the same data set used\nto demonstrate the histogram approach in\nFigure 2.24. We see that h acts as a\nsmoothing parameter and that if it is set\ntoo small (top panel), the result is a very\nnoisy density model, whereas if it is set\ntoo large (bottom panel), then the bimodal\nnature of the underlying distribution from\nwhich the data is generated (shown by the\ngreen curve) is washed out. The best den-\nsity model is obtained for some intermedi-\nate value of h (middle panel).\nh =0 .005\n0 0.5 1\n0\n5\nh =0 .07\n0 0.5 1\n0\n5\nh =0 .2\n0 0.5 1\n0\n5\nset used earlier to demonstrate the histogram technique. We see that, as expected,\nthe parameter h plays the role of a smoothing parameter, and there is a trade-off\nbetween sensitivity to noise at small h and over-smoothing at large h. Again, the\noptimization of h is a problem in model complexity, analogous to the choice of bin\nwidth in histogram density estimation, or the degree of the polynomial used in curve\nﬁtting.\nWe can choose any other kernel function k(u) in (2.249) subject to the condi-\ntions\nk(u) ⩾ 0, (2.251)∫\nk(u)d u =1 (2.252)\nwhich ensure that the resulting probability distribution is nonnegative everywhere\nand integrates to one. The class of density model given by (2.249) is called a kernel\ndensity estimator, or Parzen estimator. It has a great merit that there is no compu-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 143,
      "page_label": "124"
    }
  },
  {
    "page_content": "density estimator, or Parzen estimator. It has a great merit that there is no compu-\ntation involved in the ‘training’ phase because this simply requires storage of the\ntraining set. However, this is also one of its great weaknesses because the computa-\ntional cost of evaluating the density grows linearly with the size of the data set.\n2.5.2 Nearest-neighbour methods\nOne of the difﬁculties with the kernel approach to density estimation is that the\nparameter h governing the kernel width is ﬁxed for all kernels. In regions of high\ndata density, a large value of h may lead to over-smoothing and a washing out of\nstructure that might otherwise be extracted from the data. However, reducing h may\nlead to noisy estimates elsewhere in data space where the density is smaller. Thus\nthe optimal choice for h may be dependent on location within the data space. This\nissue is addressed by nearest-neighbour methods for density estimation.\nWe therefore return to our general result (2.246) for local density estimation,\nand instead of ﬁxing V and determining the value of K from the data, we consider\na ﬁxed value of K and use the data to ﬁnd an appropriate value for V . To do this,\nwe consider a small sphere centred on the point x at which we wish to estimate the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 143,
      "page_label": "124"
    }
  },
  {
    "page_content": "2.5. Nonparametric Methods 125\nFigure 2.26 Illustration of K-nearest-neighbour den-\nsity estimation using the same data set\nas in Figures 2.25 and 2.24. We see\nthat the parameter K governs the degree\nof smoothing, so that a small value of\nK leads to a very noisy density model\n(top panel), whereas a large value (bot-\ntom panel) smoothes out the bimodal na-\nture of the true distribution (shown by the\ngreen curve) from which the data set was\ngenerated.\nK =1\n0 0.5 1\n0\n5\nK =5\n0 0.5 1\n0\n5\nK =3 0\n0 0.5 1\n0\n5\ndensity p(x), and we allow the radius of the sphere to grow until it contains precisely\nK data points. The estimate of the densityp(x) is then given by (2.246) withV set to\nthe volume of the resulting sphere. This technique is known asK nearest neighbours\nand is illustrated in Figure 2.26, for various choices of the parameter K, using the\nsame data set as used in Figure 2.24 and Figure 2.25. We see that the value of K\nnow governs the degree of smoothing and that again there is an optimum choice for\nK that is neither too large nor too small. Note that the model produced byK nearest\nneighbours is not a true density model because the integral over all space diverges.Exercise 2.61\nWe close this chapter by showing how the K-nearest-neighbour technique for\ndensity estimation can be extended to the problem of classiﬁcation. To do this, we\napply the K-nearest-neighbour density estimation technique to each class separately",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 144,
      "page_label": "125"
    }
  },
  {
    "page_content": "apply the K-nearest-neighbour density estimation technique to each class separately\nand then make use of Bayes’ theorem. Let us suppose that we have a data set com-\nprising Nk points in class Ck with N points in total, so that ∑\nk Nk = N.I f w e\nwish to classify a new point x, we draw a sphere centred on x containing precisely\nK points irrespective of their class. Suppose this sphere has volume V and contains\nKk points from class Ck. Then (2.246) provides an estimate of the density associated\nwith each class\np(x|Ck)= Kk\nNkV . (2.253)\nSimilarly, the unconditional density is given by\np(x)= K\nNV (2.254)\nwhile the class priors are given by\np(Ck)= Nk\nN . (2.255)\nWe can now combine (2.253), (2.254), and (2.255) using Bayes’ theorem to obtain\nthe posterior probability of class membership\np(Ck|x)= p(x|Ck)p(Ck)\np(x) = Kk\nK . (2.256)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 144,
      "page_label": "125"
    }
  },
  {
    "page_content": "126 2. PROBABILITY DISTRIBUTIONS\nFigure 2.27 (a) In the K-nearest-\nneighbour classiﬁer, a new point,\nshown by the black diamond, is clas-\nsiﬁed according to the majority class\nmembership of the K closest train-\ning data points, in this case K =\n3. (b) In the nearest-neighbour\n(K =1 ) approach to classiﬁcation,\nthe resulting decision boundary is\ncomposed of hyperplanes that form\nperpendicular bisectors of pairs of\npoints from different classes.\nx1\nx2\n(a)\nx1\nx2\n(b)\nIf we wish to minimize the probability of misclassiﬁcation, this is done by assigning\nthe test point x to the class having the largest posterior probability, corresponding to\nthe largest value of Kk/K. Thus to classify a new point, we identify the K nearest\npoints from the training data set and then assign the new point to the class having the\nlargest number of representatives amongst this set. Ties can be broken at random.\nThe particular case of K =1 is called the nearest-neighbour rule, because a test\npoint is simply assigned to the same class as the nearest point from the training set.\nThese concepts are illustrated in Figure 2.27.\nIn Figure 2.28, we show the results of applying the K-nearest-neighbour algo-\nrithm to the oil ﬂow data, introduced in Chapter 1, for various values of K.A s\nexpected, we see that K controls the degree of smoothing, so that small K produces\nmany small regions of each class, whereas large K leads to fewer larger regions.\nx6\nx7\nK =1\n0 1 2\n0\n1\n2\nx6\nx7\nK =3\n0 1 2\n0\n1\n2\nx6\nx7\nK =31\n0 1 2\n0\n1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 145,
      "page_label": "126"
    }
  },
  {
    "page_content": "many small regions of each class, whereas large K leads to fewer larger regions.\nx6\nx7\nK =1\n0 1 2\n0\n1\n2\nx6\nx7\nK =3\n0 1 2\n0\n1\n2\nx6\nx7\nK =31\n0 1 2\n0\n1\n2\nFigure 2.28 Plot of 200 data points from the oil data set showing values of x6 plotted against x7, where the\nred, green, and blue points correspond to the ‘laminar’, ‘annular’, and ‘homogeneous’ classes, respectively. Also\nshown are the classiﬁcations of the input space given by the K-nearest-neighbour algorithm for various values\nof K.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 145,
      "page_label": "126"
    }
  },
  {
    "page_content": "Exercises 127\nAn interesting property of the nearest-neighbour (K =1 ) classiﬁer is that, in the\nlimit N →∞ , the error rate is never more than twice the minimum achievable error\nrate of an optimal classiﬁer, i.e., one that uses the true class distributions (Cover and\nHart, 1967) .\nAs discussed so far, both the K-nearest-neighbour method, and the kernel den-\nsity estimator, require the entire training data set to be stored, leading to expensive\ncomputation if the data set is large. This effect can be offset, at the expense of some\nadditional one-off computation, by constructing tree-based search structures to allow\n(approximate) near neighbours to be found efﬁciently without doing an exhaustive\nsearch of the data set. Nevertheless, these nonparametric methods are still severely\nlimited. On the other hand, we have seen that simple parametric models are very\nrestricted in terms of the forms of distribution that they can represent. We therefore\nneed to ﬁnd density models that are very ﬂexible and yet for which the complexity\nof the models can be controlled independently of the size of the training set, and we\nshall see in subsequent chapters how to achieve this.\nExercises\n2.1 (⋆) www Verify that the Bernoulli distribution (2.2) satisﬁes the following prop-\nerties\n1∑\nx=0\np(x|µ)=1 (2.257)\nE[x]= µ (2.258)\nvar[x]= µ(1 − µ). (2.259)\nShow that the entropy H[x] of a Bernoulli distributed random binary variable x is\ngiven by\nH[x]= −µln µ − (1 − µ)l n ( 1− µ). (2.260)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 146,
      "page_label": "127"
    }
  },
  {
    "page_content": "Show that the entropy H[x] of a Bernoulli distributed random binary variable x is\ngiven by\nH[x]= −µln µ − (1 − µ)l n ( 1− µ). (2.260)\n2.2 (⋆⋆ ) The form of the Bernoulli distribution given by (2.2) is not symmetric be-\ntween the two values of x. In some situations, it will be more convenient to use an\nequivalent formulation for which x ∈{ −1, 1}, in which case the distribution can be\nwritten\np(x|µ)=\n( 1 − µ\n2\n)(1−x)/2 ( 1+ µ\n2\n)(1+x)/2\n(2.261)\nwhere µ ∈ [−1,1]. Show that the distribution (2.261) is normalized, and evaluate its\nmean, variance, and entropy.\n2.3 (⋆⋆ ) www In this exercise, we prove that the binomial distribution (2.9) is nor-\nmalized. First use the deﬁnition (2.10) of the number of combinations ofm identical\nobjects chosen from a total of N to show that\n(N\nm\n)\n+\n( N\nm − 1\n)\n=\n(N +1\nm\n)\n. (2.262)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 146,
      "page_label": "127"
    }
  },
  {
    "page_content": "128 2. PROBABILITY DISTRIBUTIONS\nUse this result to prove by induction the following result\n(1 +x)N =\nN∑\nm=0\n(N\nm\n)\nxm (2.263)\nwhich is known as the binomial theorem, and which is valid for all real values of x.\nFinally, show that the binomial distribution is normalized, so that\nN∑\nm=0\n(N\nm\n)\nµm(1 − µ)N−m =1 (2.264)\nwhich can be done by ﬁrst pulling out a factor (1 − µ)N out of the summation and\nthen making use of the binomial theorem.\n2.4 (⋆⋆ ) Show that the mean of the binomial distribution is given by (2.11). To do this,\ndifferentiate both sides of the normalization condition (2.264) with respect to µ and\nthen rearrange to obtain an expression for the mean ofn. Similarly, by differentiating\n(2.264) twice with respect to µ and making use of the result (2.11) for the mean of\nthe binomial distribution prove the result (2.12) for the variance of the binomial.\n2.5 (⋆⋆ ) www In this exercise, we prove that the beta distribution, given by (2.13), is\ncorrectly normalized, so that (2.14) holds. This is equivalent to showing that\n∫ 1\n0\nµa−1(1 − µ)b−1 dµ = Γ(a)Γ(b)\nΓ(a + b) . (2.265)\nFrom the deﬁnition (1.141) of the gamma function, we have\nΓ(a)Γ(b)=\n∫ ∞\n0\nexp(−x)xa−1 dx\n∫ ∞\n0\nexp(−y)yb−1 dy. (2.266)\nUse this expression to prove (2.265) as follows. First bring the integral overy inside\nthe integrand of the integral over x, next make the change of variable t = y + x\nwhere x is ﬁxed, then interchange the order of the x and t integrations, and ﬁnally",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 147,
      "page_label": "128"
    }
  },
  {
    "page_content": "where x is ﬁxed, then interchange the order of the x and t integrations, and ﬁnally\nmake the change of variable x = tµ where t is ﬁxed.\n2.6 (⋆) Make use of the result (2.265) to show that the mean, variance, and mode of the\nbeta distribution (2.13) are given respectively by\nE[µ]= a\na + b (2.267)\nvar[µ]= ab\n(a + b)2(a + b +1 ) (2.268)\nmode[µ]= a − 1\na + b − 2. (2.269)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 147,
      "page_label": "128"
    }
  },
  {
    "page_content": "Exercises 129\n2.7 (⋆⋆ ) Consider a binomial random variable x given by (2.9), with prior distribution\nfor µ given by the beta distribution (2.13), and suppose we have observed m occur-\nrences of x =1 and l occurrences of x =0 . Show that the posterior mean value ofx\nlies between the prior mean and the maximum likelihood estimate for µ. To do this,\nshow that the posterior mean can be written as λ times the prior mean plus (1 − λ)\ntimes the maximum likelihood estimate, where 0 ⩽ λ ⩽ 1. This illustrates the con-\ncept of the posterior distribution being a compromise between the prior distribution\nand the maximum likelihood solution.\n2.8 (⋆) Consider two variables x and y with joint distribution p(x, y). Prove the follow-\ning two results\nE[x]= Ey [Ex[x|y]] (2.270)\nvar[x]= Ey [varx[x|y]] + vary [Ex[x|y]]. (2.271)\nHere Ex[x|y] denotes the expectation of x under the conditional distribution p(x|y),\nwith a similar notation for the conditional variance.\n2.9 (⋆⋆⋆ ) www . In this exercise, we prove the normalization of the Dirichlet dis-\ntribution (2.38) using induction. We have already shown in Exercise 2.5 that the\nbeta distribution, which is a special case of the Dirichlet forM =2 , is normalized.\nWe now assume that the Dirichlet distribution is normalized for M − 1 variables\nand prove that it is normalized for M variables. To do this, consider the Dirichlet\ndistribution over M variables, and take account of the constraint ∑ M\nk=1 µk =1 by",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 148,
      "page_label": "129"
    }
  },
  {
    "page_content": "distribution over M variables, and take account of the constraint ∑ M\nk=1 µk =1 by\neliminating µM , so that the Dirichlet is written\npM (µ1,...,µ M−1)= CM\nM−1∏\nk=1\nµαk−1\nk\n(\n1 −\nM−1∑\nj=1\nµj\n) αM −1\n(2.272)\nand our goal is to ﬁnd an expression forCM . To do this, integrate overµM−1, taking\ncare over the limits of integration, and then make a change of variable so that this\nintegral has limits 0 and 1. By assuming the correct result for CM−1 and making use\nof (2.265), derive the expression for CM .\n2.10 (⋆⋆ ) Using the property Γ(x +1 ) = xΓ(x) of the gamma function, derive the\nfollowing results for the mean, variance, and covariance of the Dirichlet distribution\ngiven by (2.38)\nE[µj]= αj\nα0\n(2.273)\nvar[µj]= αj(α0 − αj)\nα2\n0(α0 +1 ) (2.274)\ncov[µjµl]= − αjαl\nα2\n0(α0 +1 ),j ̸=l (2.275)\nwhere α0 is deﬁned by (2.39).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 148,
      "page_label": "129"
    }
  },
  {
    "page_content": "130 2. PROBABILITY DISTRIBUTIONS\n2.11 (⋆) www By expressing the expectation of lnµj under the Dirichlet distribution\n(2.38) as a derivative with respect to αj, show that\nE[lnµj]= ψ(αj) − ψ(α0) (2.276)\nwhere α0 is given by (2.39) and\nψ(a) ≡ d\nda ln Γ(a) (2.277)\nis the digamma function.\n2.12 (⋆) The uniform distribution for a continuous variable x is deﬁned by\nU(x|a, b)= 1\nb − a,a ⩽ x ⩽ b. (2.278)\nVerify that this distribution is normalized, and ﬁnd expressions for its mean and\nvariance.\n2.13 (⋆⋆ ) Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians\np(x)= N(x|µ,Σ) and q(x)= N(x|m, L).\n2.14 (⋆⋆ ) www This exercise demonstrates that the multivariate distribution with max-\nimum entropy, for a given covariance, is a Gaussian. The entropy of a distribution\np(x) is given by\nH[x]= −\n∫\np(x)l np(x)dx. (2.279)\nWe wish to maximize H[x] over all distributions p(x) subject to the constraints that\np(x) be normalized and that it have a speciﬁc mean and covariance, so that\n∫\np(x)dx =1 (2.280)\n∫\np(x)xdx = µ (2.281)\n∫\np(x)(x − µ)(x − µ)T dx = Σ. (2.282)\nBy performing a variational maximization of (2.279) and using Lagrange multipliers\nto enforce the constraints (2.280), (2.281), and (2.282), show that the maximum\nlikelihood distribution is given by the Gaussian (2.43).\n2.15 (⋆⋆ ) Show that the entropy of the multivariate Gaussian N(x|µ, Σ) is given by\nH[x]= 1\n2 ln|Σ| + D\n2 (1 + ln(2π)) (2.283)\nwhere D is the dimensionality of x.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 149,
      "page_label": "130"
    }
  },
  {
    "page_content": "Exercises 131\n2.16 (⋆⋆⋆ ) www Consider two random variables x1 and x2 having Gaussian distri-\nbutions with means µ1,µ2 and precisions τ1, τ2 respectively. Derive an expression\nfor the differential entropy of the variable x = x1 + x2. To do this, ﬁrst ﬁnd the\ndistribution of x by using the relation\np(x)=\n∫ ∞\n−∞\np(x|x2)p(x2)d x2 (2.284)\nand completing the square in the exponent. Then observe that this represents the\nconvolution of two Gaussian distributions, which itself will be Gaussian, and ﬁnally\nmake use of the result (1.110) for the entropy of the univariate Gaussian.\n2.17 (⋆) www Consider the multivariate Gaussian distribution given by (2.43). By\nwriting the precision matrix (inverse covariance matrix) Σ−1 as the sum of a sym-\nmetric and an anti-symmetric matrix, show that the anti-symmetric term does not\nappear in the exponent of the Gaussian, and hence that the precision matrix may be\ntaken to be symmetric without loss of generality. Because the inverse of a symmetric\nmatrix is also symmetric (see Exercise 2.22), it follows that the covariance matrix\nmay also be chosen to be symmetric without loss of generality.\n2.18 (⋆⋆⋆ ) Consider a real, symmetric matrix Σ whose eigenvalue equation is given\nby (2.45). By taking the complex conjugate of this equation and subtracting the\noriginal equation, and then forming the inner product with eigenvectorui, show that\nthe eigenvalues λi are real. Similarly, use the symmetry property of Σ to show that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 150,
      "page_label": "131"
    }
  },
  {
    "page_content": "the eigenvalues λi are real. Similarly, use the symmetry property of Σ to show that\ntwo eigenvectors ui and uj will be orthogonal provided λj ̸=λi. Finally, show that\nwithout loss of generality, the set of eigenvectors can be chosen to be orthonormal,\nso that they satisfy (2.46), even if some of the eigenvalues are zero.\n2.19 (⋆⋆ ) Show that a real, symmetric matrix Σ having the eigenvector equation (2.45)\ncan be expressed as an expansion in the eigenvectors, with coefﬁcients given by the\neigenvalues, of the form (2.48). Similarly, show that the inverse matrix Σ−1 has a\nrepresentation of the form (2.49).\n2.20 (⋆⋆ ) www A positive deﬁnite matrix Σ can be deﬁned as one for which the\nquadratic form\naTΣa (2.285)\nis positive for any real value of the vector a. Show that a necessary and sufﬁcient\ncondition for Σ to be positive deﬁnite is that all of the eigenvalues λi of Σ, deﬁned\nby (2.45), are positive.\n2.21 (⋆) Show that a real, symmetric matrix of sizeD ×D has D(D+1)/2 independent\nparameters.\n2.22 (⋆) www Show that the inverse of a symmetric matrix is itself symmetric.\n2.23 (⋆⋆ ) By diagonalizing the coordinate system using the eigenvector expansion (2.45),\nshow that the volume contained within the hyperellipsoid corresponding to a constant",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 150,
      "page_label": "131"
    }
  },
  {
    "page_content": "132 2. PROBABILITY DISTRIBUTIONS\nMahalanobis distance ∆ is given by\nVD|Σ|1/2∆D (2.286)\nwhere VD is the volume of the unit sphere in D dimensions, and the Mahalanobis\ndistance is deﬁned by (2.44).\n2.24 (⋆⋆ ) www Prove the identity (2.76) by multiplying both sides by the matrix\n(\nAB\nCD\n)\n(2.287)\nand making use of the deﬁnition (2.77).\n2.25 (⋆⋆ ) In Sections 2.3.1 and 2.3.2, we considered the conditional and marginal distri-\nbutions for a multivariate Gaussian. More generally, we can consider a partitioning\nof the components ofx into three groups xa, xb, and xc, with a corresponding par-\ntitioning of the mean vector µ and of the covariance matrix Σ in the form\nµ =\n( µa\nµb\nµc\n)\n, Σ =\n( Σaa Σab Σac\nΣba Σbb Σbc\nΣca Σcb Σcc\n)\n. (2.288)\nBy making use of the results of Section 2.3, ﬁnd an expression for the conditional\ndistribution p(xa|xb) in which xc has been marginalized out.\n2.26 (⋆⋆ ) A very useful result from linear algebra is the Woodbury matrix inversion\nformula given by\n(A + BCD)−1 = A−1 − A−1B(C−1 + DA−1B)−1DA−1. (2.289)\nBy multiplying both sides by (A + BCD) prove the correctness of this result.\n2.27 (⋆) Let x and z be two independent random vectors, so that p(x, z)= p(x)p(z).\nShow that the mean of their sumy = x+z is given by the sum of the means of each\nof the variable separately. Similarly, show that the covariance matrix ofy is given by\nthe sum of the covariance matrices of x and z. Conﬁrm that this result agrees with\nthat of Exercise 1.10.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 151,
      "page_label": "132"
    }
  },
  {
    "page_content": "the sum of the covariance matrices of x and z. Conﬁrm that this result agrees with\nthat of Exercise 1.10.\n2.28 (⋆⋆⋆ ) www Consider a joint distribution over the variable\nz =\n(\nx\ny\n)\n(2.290)\nwhose mean and covariance are given by (2.108) and (2.105) respectively. By mak-\ning use of the results (2.92) and (2.93) show that the marginal distribution p(x) is\ngiven (2.99). Similarly, by making use of the results (2.81) and (2.82) show that the\nconditional distributionp(y|x) is given by (2.100).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 151,
      "page_label": "132"
    }
  },
  {
    "page_content": "Exercises 133\n2.29 (⋆⋆ ) Using the partitioned matrix inversion formula (2.76), show that the inverse of\nthe precision matrix (2.104) is given by the covariance matrix (2.105).\n2.30 (⋆) By starting from (2.107) and making use of the result (2.105), verify the result\n(2.108).\n2.31 (⋆⋆ ) Consider two multidimensional random vectors x and z having Gaussian\ndistributions p(x)= N(x|µx, Σx) and p(z)= N(z|µz, Σz) respectively, together\nwith their sumy = x+z. Use the results (2.109) and (2.110) to ﬁnd an expression for\nthe marginal distribution p(y) by considering the linear-Gaussian model comprising\nthe product of the marginal distributionp(x) and the conditional distributionp(y|x).\n2.32 (⋆⋆⋆ ) www This exercise and the next provide practice at manipulating the\nquadratic forms that arise in linear-Gaussian models, as well as giving an indepen-\ndent check of results derived in the main text. Consider a joint distributionp(x, y)\ndeﬁned by the marginal and conditional distributions given by (2.99) and (2.100).\nBy examining the quadratic form in the exponent of the joint distribution, and using\nthe technique of ‘completing the square’ discussed in Section 2.3, ﬁnd expressions\nfor the mean and covariance of the marginal distribution p(y) in which the variable\nx has been integrated out. To do this, make use of the Woodbury matrix inversion\nformula (2.289). Verify that these results agree with (2.109) and (2.110) obtained\nusing the results of Chapter 2.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 152,
      "page_label": "133"
    }
  },
  {
    "page_content": "formula (2.289). Verify that these results agree with (2.109) and (2.110) obtained\nusing the results of Chapter 2.\n2.33 (⋆⋆⋆ ) Consider the same joint distribution as in Exercise 2.32, but now use the\ntechnique of completing the square to ﬁnd expressions for the mean and covariance\nof the conditional distributionp(x|y). Again, verify that these agree with the corre-\nsponding expressions (2.111) and (2.112).\n2.34 (⋆⋆ ) www To ﬁnd the maximum likelihood solution for the covariance matrix\nof a multivariate Gaussian, we need to maximize the log likelihood function (2.118)\nwith respect toΣ, noting that the covariance matrix must be symmetric and positive\ndeﬁnite. Here we proceed by ignoring these constraints and doing a straightforward\nmaximization. Using the results (C.21), (C.26), and (C.28) from Appendix C, show\nthat the covariance matrixΣ that maximizes the log likelihood function (2.118) is\ngiven by the sample covariance (2.122). We note that the ﬁnal result is necessarily\nsymmetric and positive deﬁnite (provided the sample covariance is nonsingular).\n2.35 (⋆⋆ ) Use the result (2.59) to prove (2.62). Now, using the results (2.59), and (2.62),\nshow that\nE[xnxm]= µµT + InmΣ (2.291)\nwhere xn denotes a data point sampled from a Gaussian distribution with mean µ\nand covariance Σ, and Inm denotes the (n, m) element of the identity matrix. Hence\nprove the result (2.124).\n2.36 (⋆⋆ ) www Using an analogous procedure to that used to obtain (2.126), derive",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 152,
      "page_label": "133"
    }
  },
  {
    "page_content": "prove the result (2.124).\n2.36 (⋆⋆ ) www Using an analogous procedure to that used to obtain (2.126), derive\nan expression for the sequential estimation of the variance of a univariate Gaussian",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 152,
      "page_label": "133"
    }
  },
  {
    "page_content": "134 2. PROBABILITY DISTRIBUTIONS\ndistribution, by starting with the maximum likelihood expression\nσ2\nML = 1\nN\nN∑\nn=1\n(xn − µ)2. (2.292)\nVerify that substituting the expression for a Gaussian distribution into the Robbins-\nMonro sequential estimation formula (2.135) gives a result of the same form, and\nhence obtain an expression for the corresponding coefﬁcientsaN .\n2.37 (⋆⋆ ) Using an analogous procedure to that used to obtain (2.126), derive an ex-\npression for the sequential estimation of the covariance of a multivariate Gaussian\ndistribution, by starting with the maximum likelihood expression (2.122). Verify that\nsubstituting the expression for a Gaussian distribution into the Robbins-Monro se-\nquential estimation formula (2.135) gives a result of the same form, and hence obtain\nan expression for the corresponding coefﬁcientsaN .\n2.38 (⋆) Use the technique of completing the square for the quadratic form in the expo-\nnent to derive the results (2.141) and (2.142).\n2.39 (⋆⋆ ) Starting from the results (2.141) and (2.142) for the posterior distribution\nof the mean of a Gaussian random variable, dissect out the contributions from the\nﬁrst N − 1 data points and hence obtain expressions for the sequential update of\nµN and σ2\nN . Now derive the same results starting from the posterior distribution\np(µ|x1,...,x N−1)= N(µ|µN−1,σ 2\nN−1) and multiplying by the likelihood func-\ntion p(xN |µ)= N(xN |µ, σ2) and then completing the square and normalizing to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 153,
      "page_label": "134"
    }
  },
  {
    "page_content": "N−1) and multiplying by the likelihood func-\ntion p(xN |µ)= N(xN |µ, σ2) and then completing the square and normalizing to\nobtain the posterior distribution after N observations.\n2.40 (⋆⋆ ) www Consider a D-dimensional Gaussian random variable x with distribu-\ntion N(x|µ,Σ) in which the covariance Σ is known and for which we wish to infer\nthe mean µ from a set of observationsX = {x1,..., xN }. Given a prior distribution\np(µ)= N(µ|µ0, Σ0), ﬁnd the corresponding posterior distribution p(µ|X).\n2.41 (⋆) Use the deﬁnition of the gamma function (1.141) to show that the gamma dis-\ntribution (2.146) is normalized.\n2.42 (⋆⋆ ) Evaluate the mean, variance, and mode of the gamma distribution (2.146).\n2.43 (⋆) The following distribution\np(x|σ2,q )= q\n2(2σ2)1/qΓ(1/q) exp\n(\n−|x|q\n2σ2\n)\n(2.293)\nis a generalization of the univariate Gaussian distribution. Show that this distribution\nis normalized so that ∫ ∞\n−∞\np(x|σ2,q )d x =1 (2.294)\nand that it reduces to the Gaussian when q =2 . Consider a regression model in\nwhich the target variable is given by t = y(x,w)+ ϵ and ϵ is a random noise",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 153,
      "page_label": "134"
    }
  },
  {
    "page_content": "Exercises 135\nvariable drawn from the distribution (2.293). Show that the log likelihood function\nover w and σ2, for an observed data set of input vectors X = {x1,..., xN } and\ncorresponding target variables t =( t1,...,t N )T,i sg i v e nb y\nln p(t|X, w,σ 2)= − 1\n2σ2\nN∑\nn=1\n|y(xn, w) − tn|q − N\nq ln(2σ2)+c o n s t (2.295)\nwhere ‘const’ denotes terms independent of both w and σ2. Note that, as a function\nof w, this is the Lq error function considered in Section 1.5.5.\n2.44 (⋆⋆ ) Consider a univariate Gaussian distribution N(x|µ, τ−1) having conjugate\nGaussian-gamma prior given by (2.154), and a data set x = {x1,...,x N } of i.i.d.\nobservations. Show that the posterior distribution is also a Gaussian-gamma distri-\nbution of the same functional form as the prior, and write down expressions for the\nparameters of this posterior distribution.\n2.45 (⋆) Verify that the Wishart distribution deﬁned by (2.155) is indeed a conjugate\nprior for the precision matrix of a multivariate Gaussian.\n2.46 (⋆) www Verify that evaluating the integral in (2.158) leads to the result (2.159).\n2.47 (⋆) www Show that in the limit ν →∞ , the t-distribution (2.159) becomes a\nGaussian. Hint: ignore the normalization coefﬁcient, and simply look at the depen-\ndence on x.\n2.48 (⋆) By following analogous steps to those used to derive the univariate Student’s\nt-distribution (2.159), verify the result (2.162) for the multivariate form of the Stu-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 154,
      "page_label": "135"
    }
  },
  {
    "page_content": "t-distribution (2.159), verify the result (2.162) for the multivariate form of the Stu-\ndent’s t-distribution, by marginalizing over the variable η in (2.161). Using the\ndeﬁnition (2.161), show by exchanging integration variables that the multivariate\nt-distribution is correctly normalized.\n2.49 (⋆⋆ ) By using the deﬁnition (2.161) of the multivariate Student’s t-distribution as a\nconvolution of a Gaussian with a gamma distribution, verify the properties (2.164),\n(2.165), and (2.166) for the multivariate t-distribution deﬁned by (2.162).\n2.50 (⋆) Show that in the limit ν →∞ , the multivariate Student’s t-distribution (2.162)\nreduces to a Gaussian with mean µ and precision Λ.\n2.51 (⋆) www The various trigonometric identities used in the discussion of periodic\nvariables in this chapter can be proven easily from the relation\nexp(iA)=c o sA + isinA (2.296)\nin which i is the square root of minus one. By considering the identity\nexp(iA) exp(−iA)=1 (2.297)\nprove the result (2.177). Similarly, using the identity\ncos(A − B)= ℜexp{i(A − B)} (2.298)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 154,
      "page_label": "135"
    }
  },
  {
    "page_content": "136 2. PROBABILITY DISTRIBUTIONS\nwhere ℜ denotes the real part, prove (2.178). Finally, by using sin(A − B)=\nℑexp{i(A − B)}, where ℑ denotes the imaginary part, prove the result (2.183).\n2.52 (⋆⋆ ) For large m, the von Mises distribution (2.179) becomes sharply peaked\naround the mode θ0. By deﬁning ξ = m1/2(θ − θ0) and making the Taylor ex-\npansion of the cosine function given by\ncos α =1 − α2\n2 + O(α4) (2.299)\nshow that as m →∞ , the von Mises distribution tends to a Gaussian.\n2.53 (⋆) Using the trigonometric identity (2.183), show that solution of (2.182) for θ0 is\ngiven by (2.184).\n2.54 (⋆) By computing ﬁrst and second derivatives of the von Mises distribution (2.179),\nand using I0(m) > 0 for m> 0, show that the maximum of the distribution occurs\nwhen θ = θ0 and that the minimum occurs when θ = θ0 + π (mod 2π).\n2.55 (⋆) By making use of the result (2.168), together with (2.184) and the trigonometric\nidentity (2.178), show that the maximum likelihood solutionmML for the concentra-\ntion of the von Mises distribution satisﬁes A(mML)= r where r is the radius of the\nmean of the observations viewed as unit vectors in the two-dimensional Euclidean\nplane, as illustrated in Figure 2.17.\n2.56 (⋆⋆ ) www Express the beta distribution (2.13), the gamma distribution (2.146),\nand the von Mises distribution (2.179) as members of the exponential family (2.194)\nand thereby identify their natural parameters.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 155,
      "page_label": "136"
    }
  },
  {
    "page_content": "and the von Mises distribution (2.179) as members of the exponential family (2.194)\nand thereby identify their natural parameters.\n2.57 (⋆) Verify that the multivariate Gaussian distribution can be cast in exponential\nfamily form (2.194) and derive expressions forη, u(x), h(x) and g(η) analogous to\n(2.220)–(2.223).\n2.58 (⋆) The result (2.226) showed that the negative gradient oflng(η) for the exponen-\ntial family is given by the expectation of u(x). By taking the second derivatives of\n(2.195), show that\n−∇∇lng(η)= E[u(x)u(x)T] − E[u(x)]E[u(x)T]=c o v [u(x)]. (2.300)\n2.59 (⋆) By changing variables using y = x/σ, show that the density (2.236) will be\ncorrectly normalized, provided f(x) is correctly normalized.\n2.60 (⋆⋆ ) www Consider a histogram-like density model in which the space x is di-\nvided into ﬁxed regions for which the density p(x) takes the constant value hi over\nthe ith region, and that the volume of region i is denoted ∆i. Suppose we have a set\nof N observations of x such that ni of these observations fall in region i. Using a\nLagrange multiplier to enforce the normalization constraint on the density, derive an\nexpression for the maximum likelihood estimator for the {hi}.\n2.61 (⋆) Show that the K-nearest-neighbour density model deﬁnes an improper distribu-\ntion whose integral over all space is divergent.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 155,
      "page_label": "136"
    }
  },
  {
    "page_content": "3\nLinear\nModels for\nRegression\nThe focus so far in this book has been on unsupervised learning, including topics\nsuch as density estimation and data clustering. We turn now to a discussion of super-\nvised learning, starting with regression. The goal of regression is to predict the value\nof one or more continuoustarget variables t given the value of aD-dimensional vec-\ntor x of input variables. We have already encountered an example of a regression\nproblem when we considered polynomial curve ﬁtting in Chapter 1. The polynomial\nis a speciﬁc example of a broad class of functions called linear regression models,\nwhich share the property of being linear functions of the adjustable parameters, and\nwhich will form the focus of this chapter. The simplest form of linear regression\nmodels are also linear functions of the input variables. However, we can obtain a\nmuch more useful class of functions by taking linear combinations of a ﬁxed set of\nnonlinear functions of the input variables, known asbasis functions. Such models\nare linear functions of the parameters, which gives them simple analytical properties,\nand yet can be nonlinear with respect to the input variables.\n137",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 156,
      "page_label": "137"
    }
  },
  {
    "page_content": "138 3. LINEAR MODELS FOR REGRESSION\nGiven a training data set comprisingN observations {xn}, where n =1 ,...,N ,\ntogether with corresponding target values {tn}, the goal is to predict the value of t\nfor a new value of x. In the simplest approach, this can be done by directly con-\nstructing an appropriate function y(x) whose values for new inputs x constitute the\npredictions for the corresponding values of t. More generally, from a probabilistic\nperspective, we aim to model the predictive distributionp(t|x) because this expresses\nour uncertainty about the value of t for each value of x. From this conditional dis-\ntribution we can make predictions of t, for any new value of x, in such a way as to\nminimize the expected value of a suitably chosen loss function. As discussed in Sec-\ntion 1.5.5, a common choice of loss function for real-valued variables is the squared\nloss, for which the optimal solution is given by the conditional expectation of t.\nAlthough linear models have signiﬁcant limitations as practical techniques for\npattern recognition, particularly for problems involving input spaces of high dimen-\nsionality, they have nice analytical properties and form the foundation for more so-\nphisticated models to be discussed in later chapters.\n3.1. Linear Basis Function Models\nThe simplest linear model for regression is one that involves a linear combination of\nthe input variables\ny(x,w)= w0 + w1x1 + ... + wDxD (3.1)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 157,
      "page_label": "138"
    }
  },
  {
    "page_content": "The simplest linear model for regression is one that involves a linear combination of\nthe input variables\ny(x,w)= w0 + w1x1 + ... + wDxD (3.1)\nwhere x =( x1,...,x D)T. This is often simply known aslinear regression. The key\nproperty of this model is that it is a linear function of the parametersw0,...,w D.I ti s\nalso, however, a linear function of the input variablesxi, and this imposes signiﬁcant\nlimitations on the model. We therefore extend the class of models by considering\nlinear combinations of ﬁxed nonlinear functions of the input variables, of the form\ny(x,w)= w0 +\nM−1∑\nj=1\nwjφj(x) (3.2)\nwhere φj(x) are known as basis functions. By denoting the maximum value of the\nindex j by M − 1, the total number of parameters in this model will be M.\nThe parameter w0 allows for any ﬁxed offset in the data and is sometimes called\na bias parameter (not to be confused with ‘bias’ in a statistical sense). It is often\nconvenient to deﬁne an additional dummy ‘basis function’ φ0(x)=1 so that\ny(x,w)=\nM−1∑\nj=0\nwjφj(x)= wTφ(x) (3.3)\nwhere w =( w0,...,w M−1)T and φ =( φ0,...,φ M−1)T. In many practical ap-\nplications of pattern recognition, we will apply some form of ﬁxed pre-processing,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 157,
      "page_label": "138"
    }
  },
  {
    "page_content": "3.1. Linear Basis Function Models 139\nor feature extraction, to the original data variables. If the original variables com-\nprise the vector x, then the features can be expressed in terms of the basis functions\n{φj(x)}.\nBy using nonlinear basis functions, we allow the function y(x, w) to be a non-\nlinear function of the input vector x. Functions of the form (3.2) are called linear\nmodels, however, because this function is linear in w. It is this linearity in the pa-\nrameters that will greatly simplify the analysis of this class of models. However, it\nalso leads to some signiﬁcant limitations, as we discuss in Section 3.6.\nThe example of polynomial regression considered in Chapter 1 is a particular\nexample of this model in which there is a single input variablex, and the basis func-\ntions take the form of powers of x so that φj(x)= xj. One limitation of polynomial\nbasis functions is that they are global functions of the input variable, so that changes\nin one region of input space affect all other regions. This can be resolved by dividing\nthe input space up into regions and ﬁt a different polynomial in each region, leading\nto spline functions(Hastie et al., 2001).\nThere are many other possible choices for the basis functions, for example\nφj(x)=e x p\n{\n−(x − µj)2\n2s2\n}\n(3.4)\nwhere the µj govern the locations of the basis functions in input space, and the pa-\nrameter s governs their spatial scale. These are usually referred to as ‘Gaussian’",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 158,
      "page_label": "139"
    }
  },
  {
    "page_content": "rameter s governs their spatial scale. These are usually referred to as ‘Gaussian’\nbasis functions, although it should be noted that they are not required to have a prob-\nabilistic interpretation, and in particular the normalization coefﬁcient is unimportant\nbecause these basis functions will be multiplied by adaptive parameters wj.\nAnother possibility is the sigmoidal basis function of the form\nφj(x)= σ\n(x − µj\ns\n)\n(3.5)\nwhere σ(a) is the logistic sigmoid function deﬁned by\nσ(a)= 1\n1+e x p (−a). (3.6)\nEquivalently, we can use the ‘ tanh’ function because this is related to the logistic\nsigmoid by tanh(a)=2 σ(a) − 1, and so a general linear combination of logistic\nsigmoid functions is equivalent to a general linear combination of ‘tanh’ functions.\nThese various choices of basis function are illustrated in Figure 3.1.\nYet another possible choice of basis function is the Fourier basis, which leads to\nan expansion in sinusoidal functions. Each basis function represents a speciﬁc fre-\nquency and has inﬁnite spatial extent. By contrast, basis functions that are localized\nto ﬁnite regions of input space necessarily comprise a spectrum of different spatial\nfrequencies. In many signal processing applications, it is of interest to consider ba-\nsis functions that are localized in both space and frequency, leading to a class of\nfunctions known aswavelets. These are also deﬁned to be mutually orthogonal, to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 158,
      "page_label": "139"
    }
  },
  {
    "page_content": "functions known aswavelets. These are also deﬁned to be mutually orthogonal, to\nsimplify their application. Wavelets are most applicable when the input values live",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 158,
      "page_label": "139"
    }
  },
  {
    "page_content": "140 3. LINEAR MODELS FOR REGRESSION\n−1 0 1\n−1\n−0.5\n0\n0.5\n1\n−1 0 1\n0\n0.25\n0.5\n0.75\n1\n−1 0 1\n0\n0.25\n0.5\n0.75\n1\nFigure 3.1 Examples of basis functions, showing polynomials on the left, Gaussians of the form (3.4) in the\ncentre, and sigmoidal of the form (3.5) on the right.\non a regular lattice, such as the successive time points in a temporal sequence, or the\npixels in an image. Useful texts on wavelets include Ogden (1997), Mallat (1999),\nand Vidakovic (1999).\nMost of the discussion in this chapter, however, is independent of the particular\nchoice of basis function set, and so for most of our discussion we shall not specify\nthe particular form of the basis functions, except for the purposes of numerical il-\nlustration. Indeed, much of our discussion will be equally applicable to the situation\nin which the vector φ(x) of basis functions is simply the identity φ(x)= x. Fur-\nthermore, in order to keep the notation simple, we shall focus on the case of a single\ntarget variable t. However, in Section 3.1.5, we consider brieﬂy the modiﬁcations\nneeded to deal with multiple target variables.\n3.1.1 Maximum likelihood and least squares\nIn Chapter 1, we ﬁtted polynomial functions to data sets by minimizing a sum-\nof-squares error function. We also showed that this error function could be motivated\nas the maximum likelihood solution under an assumed Gaussian noise model. Let\nus return to this discussion and consider the least squares approach, and its relation",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 159,
      "page_label": "140"
    }
  },
  {
    "page_content": "us return to this discussion and consider the least squares approach, and its relation\nto maximum likelihood, in more detail.\nAs before, we assume that the target variable t is given by a deterministic func-\ntion y(x,w) with additive Gaussian noise so that\nt = y(x,w)+ ϵ (3.7)\nwhere ϵ is a zero mean Gaussian random variable with precision (inverse variance)\nβ. Thus we can write\np(t|x, w,β )= N(t|y(x,w),β −1). (3.8)\nRecall that, if we assume a squared loss function, then the optimal prediction, for a\nnew value of x, will be given by the conditional mean of the target variable. In theSection 1.5.5\ncase of a Gaussian conditional distribution of the form (3.8), the conditional mean",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 159,
      "page_label": "140"
    }
  },
  {
    "page_content": "3.1. Linear Basis Function Models 141\nwill be simply\nE[t|x]=\n∫\ntp(t|x)d t = y(x,w). (3.9)\nNote that the Gaussian noise assumption implies that the conditional distribution of\nt given x is unimodal, which may be inappropriate for some applications. An ex-\ntension to mixtures of conditional Gaussian distributions, which permit multimodal\nconditional distributions, will be discussed in Section 14.5.1.\nNow consider a data set of inputs X = {x1,..., xN } with corresponding target\nvalues t1,...,t N . We group the target variables {tn} into a column vector that we\ndenote by t where the typeface is chosen to distinguish it from a single observation\nof a multivariate target, which would be denoted t. Making the assumption that\nthese data points are drawn independently from the distribution (3.8), we obtain the\nfollowing expression for the likelihood function, which is a function of the adjustable\nparameters w and β, in the form\np(t|X, w,β )=\nN∏\nn=1\nN(tn|wTφ(xn),β −1) (3.10)\nwhere we have used (3.3). Note that in supervised learning problems such as regres-\nsion (and classiﬁcation), we are not seeking to model the distribution of the input\nvariables. Thusx will always appear in the set of conditioning variables, and so\nfrom now on we will drop the explicitx from expressions such as p(t|x,w,β ) in or-\nder to keep the notation uncluttered. Taking the logarithm of the likelihood function,\nand making use of the standard form (1.46) for the univariate Gaussian, we have\nlnp(t|w,β )=\nN∑\nn=1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 160,
      "page_label": "141"
    }
  },
  {
    "page_content": "and making use of the standard form (1.46) for the univariate Gaussian, we have\nlnp(t|w,β )=\nN∑\nn=1\nlnN(tn|wTφ(xn),β −1)\n= N\n2 ln β − N\n2 ln(2π) − βED(w) (3.11)\nwhere the sum-of-squares error function is deﬁned by\nED(w)= 1\n2\nN∑\nn=1\n{tn − wTφ(xn)}2. (3.12)\nHaving written down the likelihood function, we can use maximum likelihood to\ndetermine w and β. Consider ﬁrst the maximization with respect to w. As observed\nalready in Section 1.2.5, we see that maximization of the likelihood function under a\nconditional Gaussian noise distribution for a linear model is equivalent to minimizing\na sum-of-squares error function given by ED(w). The gradient of the log likelihood\nfunction (3.11) takes the form\n∇lnp(t|w,β )=\nN∑\nn=1\n{\ntn − wTφ(xn)\n}\nφ(xn)T. (3.13)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 160,
      "page_label": "141"
    }
  },
  {
    "page_content": "142 3. LINEAR MODELS FOR REGRESSION\nSetting this gradient to zero gives\n0=\nN∑\nn=1\ntnφ(xn)T − wT\n( N∑\nn=1\nφ(xn)φ(xn)T\n)\n. (3.14)\nSolving for w we obtain\nwML =\n(\nΦTΦ\n)−1\nΦTt (3.15)\nwhich are known as thenormal equationsfor the least squares problem. HereΦ is an\nN×M matrix, called thedesign matrix, whose elements are given byΦnj = φj(xn),\nso that\nΦ =\n⎛\n⎜⎜⎝\nφ0(x1) φ1(x1) ··· φM−1(x1)\nφ0(x2) φ1(x2) ··· φM−1(x2)\n... ... ... ...\nφ0(xN ) φ1(xN ) ··· φM−1(xN )\n⎞\n⎟⎟⎠. (3.16)\nThe quantity\nΦ† ≡\n(\nΦTΦ\n)−1\nΦT (3.17)\nis known as the Moore-Penrose pseudo-inverse of the matrix Φ (Rao and Mitra,\n1971; Golub and Van Loan, 1996). It can be regarded as a generalization of the\nnotion of matrix inverse to nonsquare matrices. Indeed, ifΦ is square and invertible,\nthen using the property (AB)−1 = B−1A−1 we see that Φ† ≡ Φ−1.\nAt this point, we can gain some insight into the role of the bias parameterw0.I f\nwe make the bias parameter explicit, then the error function (3.12) becomes\nED(w)= 1\n2\nN∑\nn=1\n{tn − w0 −\nM−1∑\nj=1\nwjφj(xn)}2. (3.18)\nSetting the derivative with respect tow0 equal to zero, and solving for w0, we obtain\nw0 = t −\nM−1∑\nj=1\nwjφj (3.19)\nwhere we have deﬁned\nt = 1\nN\nN∑\nn=1\ntn, φj = 1\nN\nN∑\nn=1\nφj(xn). (3.20)\nThus the bias w0 compensates for the difference between the averages (over the\ntraining set) of the target values and the weighted sum of the averages of the basis\nfunction values.\nWe can also maximize the log likelihood function (3.11) with respect to the noise",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 161,
      "page_label": "142"
    }
  },
  {
    "page_content": "function values.\nWe can also maximize the log likelihood function (3.11) with respect to the noise\nprecision parameter β, giving\n1\nβML\n= 1\nN\nN∑\nn=1\n{tn − wT\nMLφ(xn)}2 (3.21)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 161,
      "page_label": "142"
    }
  },
  {
    "page_content": "3.1. Linear Basis Function Models 143\nFigure 3.2 Geometrical interpretation of the least-squares\nsolution, in anN-dimensional space whose axes\nare the values of t1,...,t N . The least-squares\nregression function is obtained by ﬁnding the or-\nthogonal projection of the data vector t onto the\nsubspace spanned by the basis functions φj(x)\nin which each basis function is viewed as a vec-\ntor ϕj of length N with elements φj(xn).\nS\nt\nyϕ1\nϕ2\nand so we see that the inverse of the noise precision is given by the residual variance\nof the target values around the regression function.\n3.1.2 Geometry of least squares\nAt this point, it is instructive to consider the geometrical interpretation of the\nleast-squares solution. To do this we consider an N-dimensional space whose axes\nare given by the tn, so that t =( t1,...,t N )T is a vector in this space. Each basis\nfunction φj(xn), evaluated at theN data points, can also be represented as a vector in\nthe same space, denoted byϕj, as illustrated in Figure 3.2. Note thatϕj corresponds\nto the jth column of Φ, whereas φ(xn) corresponds to the nth row of Φ. If the\nnumber M of basis functions is smaller than the number N of data points, then the\nM vectors φj(xn) will span a linear subspace S of dimensionality M. We deﬁne\ny to be an N-dimensional vector whose nth element is given by y(xn, w), where\nn =1 ,...,N . Because y is an arbitrary linear combination of the vectors ϕj, it can",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 162,
      "page_label": "143"
    }
  },
  {
    "page_content": "n =1 ,...,N . Because y is an arbitrary linear combination of the vectors ϕj, it can\nlive anywhere in the M-dimensional subspace. The sum-of-squares error (3.12) is\nthen equal (up to a factor of 1/2) to the squared Euclidean distance between y and\nt. Thus the least-squares solution for w corresponds to that choice of y that lies in\nsubspace S and that is closest to t. Intuitively, from Figure 3.2, we anticipate that\nthis solution corresponds to the orthogonal projection of t onto the subspace S. This\nis indeed the case, as can easily be veriﬁed by noting that the solution for y is given\nby ΦwML, and then conﬁrming that this takes the form of an orthogonal projection.Exercise 3.2\nIn practice, a direct solution of the normal equations can lead to numerical difﬁ-\nculties when ΦTΦ is close to singular. In particular, when two or more of the basis\nvectors ϕj are co-linear, or nearly so, the resulting parameter values can have large\nmagnitudes. Such near degeneracies will not be uncommon when dealing with real\ndata sets. The resulting numerical difﬁculties can be addressed using the technique\nof singular value decomposition,o r SVD (Press et al., 1992; Bishop and Nabney,\n2008). Note that the addition of a regularization term ensures that the matrix is non-\nsingular, even in the presence of degeneracies.\n3.1.3 Sequential learning\nBatch techniques, such as the maximum likelihood solution (3.15), which in-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 162,
      "page_label": "143"
    }
  },
  {
    "page_content": "singular, even in the presence of degeneracies.\n3.1.3 Sequential learning\nBatch techniques, such as the maximum likelihood solution (3.15), which in-\nvolve processing the entire training set in one go, can be computationally costly for\nlarge data sets. As we have discussed in Chapter 1, if the data set is sufﬁciently large,\nit may be worthwhile to usesequential algorithms, also known ason-line algorithms,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 162,
      "page_label": "143"
    }
  },
  {
    "page_content": "144 3. LINEAR MODELS FOR REGRESSION\nin which the data points are considered one at a time, and the model parameters up-\ndated after each such presentation. Sequential learning is also appropriate for real-\ntime applications in which the data observations are arriving in a continuous stream,\nand predictions must be made before all of the data points are seen.\nWe can obtain a sequential learning algorithm by applying the technique of\nstochastic gradient descent, also known assequential gradient descent, as follows. If\nthe error function comprises a sum over data points E = ∑\nn En, then after presen-\ntation of pattern n, the stochastic gradient descent algorithm updates the parameter\nvector w using\nw(τ+1) = w(τ) − η∇En (3.22)\nwhere τ denotes the iteration number, and η is a learning rate parameter. We shall\ndiscuss the choice of value forη shortly. The value ofw is initialized to some starting\nvector w(0). For the case of the sum-of-squares error function (3.12), this gives\nw(τ+1) = w(τ) + η(tn − w(τ)Tφn)φn (3.23)\nwhere φn = φ(xn). This is known as least-mean-squares or the LMS algorithm.\nThe value of η needs to be chosen with care to ensure that the algorithm converges\n(Bishop and Nabney, 2008).\n3.1.4 Regularized least squares\nIn Section 1.1, we introduced the idea of adding a regularization term to an\nerror function in order to control over-ﬁtting, so that the total error function to be\nminimized takes the form\nED(w)+ λEW (w) (3.24)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 163,
      "page_label": "144"
    }
  },
  {
    "page_content": "error function in order to control over-ﬁtting, so that the total error function to be\nminimized takes the form\nED(w)+ λEW (w) (3.24)\nwhere λ is the regularization coefﬁcient that controls the relative importance of the\ndata-dependent error ED(w) and the regularization term EW (w). One of the sim-\nplest forms of regularizer is given by the sum-of-squares of the weight vector ele-\nments\nEW (w)= 1\n2wTw. (3.25)\nIf we also consider the sum-of-squares error function given by\nE(w)= 1\n2\nN∑\nn=1\n{tn − wTφ(xn)}2 (3.26)\nthen the total error function becomes\n1\n2\nN∑\nn=1\n{tn − wTφ(xn)}2 + λ\n2wTw. (3.27)\nThis particular choice of regularizer is known in the machine learning literature as\nweight decaybecause in sequential learning algorithms, it encourages weight values\nto decay towards zero, unless supported by the data. In statistics, it provides an ex-\nample of a parameter shrinkagemethod because it shrinks parameter values towards",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 163,
      "page_label": "144"
    }
  },
  {
    "page_content": "3.1. Linear Basis Function Models 145\nq =0 .5 q =1 q =2 q =4\nFigure 3.3 Contours of the regularization term in (3.29) for various values of the parameter q.\nzero. It has the advantage that the error function remains a quadratic function of\nw, and so its exact minimizer can be found in closed form. Speciﬁcally, setting the\ngradient of (3.27) with respect to w to zero, and solving for w as before, we obtain\nw =\n(\nλI + ΦTΦ\n)−1\nΦTt. (3.28)\nThis represents a simple extension of the least-squares solution (3.15).\nA more general regularizer is sometimes used, for which the regularized error\ntakes the form\n1\n2\nN∑\nn=1\n{tn − wTφ(xn)}2 + λ\n2\nM∑\nj=1\n|wj|q (3.29)\nwhere q =2 corresponds to the quadratic regularizer (3.27). Figure 3.3 shows con-\ntours of the regularization function for different values of q.\nThe case of q =1 is know as the lasso in the statistics literature (Tibshirani,\n1996). It has the property that if λ is sufﬁciently large, some of the coefﬁcients\nwj are driven to zero, leading to a sparse model in which the corresponding basis\nfunctions play no role. To see this, we ﬁrst note that minimizing (3.29) is equivalent\nto minimizing the unregularized sum-of-squares error (3.12) subject to the constraintExercise 3.5\nM∑\nj=1\n|wj|q ⩽ η (3.30)\nfor an appropriate value of the parameterη, where the two approaches can be related\nusing Lagrange multipliers. The origin of the sparsity can be seen from Figure 3.4,Appendix E",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 164,
      "page_label": "145"
    }
  },
  {
    "page_content": "using Lagrange multipliers. The origin of the sparsity can be seen from Figure 3.4,Appendix E\nwhich shows that the minimum of the error function, subject to the constraint (3.30).\nAs λ is increased, so an increasing number of parameters are driven to zero.\nRegularization allows complex models to be trained on data sets of limited size\nwithout severe over-ﬁtting, essentially by limiting the effective model complexity.\nHowever, the problem of determining the optimal model complexity is then shifted\nfrom one of ﬁnding the appropriate number of basis functions to one of determining\na suitable value of the regularization coefﬁcient λ. We shall return to the issue of\nmodel complexity later in this chapter.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 164,
      "page_label": "145"
    }
  },
  {
    "page_content": "146 3. LINEAR MODELS FOR REGRESSION\nFigure 3.4 Plot of the contours\nof the unregularized error function\n(blue) along with the constraint re-\ngion (3.30) for the quadratic regular-\nizer q =2 on the left and the lasso\nregularizer q =1 on the right, in\nwhich the optimum value for the pa-\nrameter vector w is denoted by w⋆ .\nThe lasso gives a sparse solution in\nwhich w⋆\n1 =0 .\nw1\nw2\nw⋆\nw1\nw2\nw⋆\nFor the remainder of this chapter we shall focus on the quadratic regularizer\n(3.27) both for its practical importance and its analytical tractability.\n3.1.5 Multiple outputs\nSo far, we have considered the case of a single target variablet. In some applica-\ntions, we may wish to predict K> 1 target variables, which we denote collectively\nby the target vectort. This could be done by introducing a different set of basis func-\ntions for each component oft, leading to multiple, independent regression problems.\nHowever, a more interesting, and more common, approach is to use the same set of\nbasis functions to model all of the components of the target vector so that\ny(x,w)= WTφ(x) (3.31)\nwhere y is a K-dimensional column vector, W is an M × K matrix of parameters,\nand φ(x) is an M-dimensional column vector with elements φj(x), with φ0(x)=1\nas before. Suppose we take the conditional distribution of the target vector to be an\nisotropic Gaussian of the form\np(t|x, W,β )= N(t|WTφ(x),β −1I). (3.32)\nIf we have a set of observations t1,..., tN , we can combine these into a matrix T",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 165,
      "page_label": "146"
    }
  },
  {
    "page_content": "p(t|x, W,β )= N(t|WTφ(x),β −1I). (3.32)\nIf we have a set of observations t1,..., tN , we can combine these into a matrix T\nof size N × K such that the nth row is given by tT\nn. Similarly, we can combine the\ninput vectors x1,..., xN into a matrix X. The log likelihood function is then given\nby\nlnp(T|X, W,β )=\nN∑\nn=1\nlnN(tn|WTφ(xn),β −1I)\n= NK\n2 ln\n( β\n2π\n)\n− β\n2\nN∑\nn=1\ntn − WTφ(xn)\n2\n. (3.33)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 165,
      "page_label": "146"
    }
  },
  {
    "page_content": "3.2. The Bias-Variance Decomposition 147\nAs before, we can maximize this function with respect to W, giving\nWML =\n(\nΦTΦ\n)−1\nΦTT. (3.34)\nIf we examine this result for each target variable tk,w eh a v e\nwk =\n(\nΦTΦ\n)−1\nΦTtk = Φ†tk (3.35)\nwhere tk is an N-dimensional column vector with components tnk for n =1 ,...N .\nThus the solution to the regression problem decouples between the different target\nvariables, and we need only compute a single pseudo-inverse matrix Φ†, which is\nshared by all of the vectors wk.\nThe extension to general Gaussian noise distributions having arbitrary covari-\nance matrices is straightforward. Again, this leads to a decoupling into K inde-Exercise 3.6\npendent regression problems. This result is unsurprising because the parameters W\ndeﬁne only the mean of the Gaussian noise distribution, and we know from Sec-\ntion 2.3.4 that the maximum likelihood solution for the mean of a multivariate Gaus-\nsian is independent of the covariance. From now on, we shall therefore consider a\nsingle target variablet for simplicity.\n3.2. The Bias-Variance Decomposition\nSo far in our discussion of linear models for regression, we have assumed that the\nform and number of basis functions are both ﬁxed. As we have seen in Chapter 1,\nthe use of maximum likelihood, or equivalently least squares, can lead to severe\nover-ﬁtting if complex models are trained using data sets of limited size. However,\nlimiting the number of basis functions in order to avoid over-ﬁtting has the side",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 166,
      "page_label": "147"
    }
  },
  {
    "page_content": "limiting the number of basis functions in order to avoid over-ﬁtting has the side\neffect of limiting the ﬂexibility of the model to capture interesting and important\ntrends in the data. Although the introduction of regularization terms can control\nover-ﬁtting for models with many parameters, this raises the question of how to\ndetermine a suitable value for the regularization coefﬁcient λ. Seeking the solution\nthat minimizes the regularized error function with respect to both the weight vector\nwand the regularization coefﬁcient λ is clearly not the right approach since this\nleads to the unregularized solution with λ =0 .\nAs we have seen in earlier chapters, the phenomenon of over-ﬁtting is really an\nunfortunate property of maximum likelihood and does not arise when we marginalize\nover parameters in a Bayesian setting. In this chapter, we shall consider the Bayesian\nview of model complexity in some depth. Before doing so, however, it is instructive\nto consider a frequentist viewpoint of the model complexity issue, known as thebias-\nvariance trade-off. Although we shall introduce this concept in the context of linear\nbasis function models, where it is easy to illustrate the ideas using simple examples,\nthe discussion has more general applicability.\nIn Section 1.5.5, when we discussed decision theory for regression problems,\nwe considered various loss functions each of which leads to a corresponding optimal",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 166,
      "page_label": "147"
    }
  },
  {
    "page_content": "we considered various loss functions each of which leads to a corresponding optimal\nprediction once we are given the conditional distributionp(t|x). A popular choice is",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 166,
      "page_label": "147"
    }
  },
  {
    "page_content": "148 3. LINEAR MODELS FOR REGRESSION\nthe squared loss function, for which the optimal prediction is given by the conditional\nexpectation, which we denote by h(x) and which is given by\nh(x)= E[t|x]=\n∫\ntp(t|x)d t. (3.36)\nAt this point, it is worth distinguishing between the squared loss function arising\nfrom decision theory and the sum-of-squares error function that arose in the maxi-\nmum likelihood estimation of model parameters. We might use more sophisticated\ntechniques than least squares, for example regularization or a fully Bayesian ap-\nproach, to determine the conditional distribution p(t|x). These can all be combined\nwith the squared loss function for the purpose of making predictions.\nWe showed in Section 1.5.5 that the expected squared loss can be written in the\nform\nE[L]=\n∫\n{y(x) − h(x)}2 p(x)dx +\n∫\n{h(x) − t}2p(x,t )d xdt. (3.37)\nRecall that the second term, which is independent of y(x), arises from the intrinsic\nnoise on the data and represents the minimum achievable value of the expected loss.\nThe ﬁrst term depends on our choice for the functiony(x), and we will seek a so-\nlution for y(x) which makes this term a minimum. Because it is nonnegative, the\nsmallest that we can hope to make this term is zero. If we had an unlimited supply of\ndata (and unlimited computational resources), we could in principle ﬁnd the regres-\nsion function h(x) to any desired degree of accuracy, and this would represent the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 167,
      "page_label": "148"
    }
  },
  {
    "page_content": "sion function h(x) to any desired degree of accuracy, and this would represent the\noptimal choice for y(x). However, in practice we have a data set D containing only\na ﬁnite number N of data points, and consequently we do not know the regression\nfunction h(x) exactly.\nIf we model the h(x) using a parametric function y(x,w) governed by a pa-\nrameter vector w, then from a Bayesian perspective the uncertainty in our model is\nexpressed through a posterior distribution overw. A frequentist treatment, however,\ninvolves making a point estimate of w based on the data set D, and tries instead\nto interpret the uncertainty of this estimate through the following thought experi-\nment. Suppose we had a large number of data sets each of size N and each drawn\nindependently from the distribution p(t, x). For any given data set D, we can run\nour learning algorithm and obtain a prediction function y(x; D). Different data sets\nfrom the ensemble will give different functions and consequently different values of\nthe squared loss. The performance of a particular learning algorithm is then assessed\nby taking the average over this ensemble of data sets.\nConsider the integrand of the ﬁrst term in (3.37), which for a particular data set\nD takes the form\n{y(x;D) − h(x)}2. (3.38)\nBecause this quantity will be dependent on the particular data setD, we take its aver-\nage over the ensemble of data sets. If we add and subtract the quantity ED[y(x;D)]",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 167,
      "page_label": "148"
    }
  },
  {
    "page_content": "3.2. The Bias-Variance Decomposition 149\ninside the braces, and then expand, we obtain\n{y(x;D) − ED[y(x;D)] +ED[y(x;D)] − h(x)}2\n= {y(x;D) − ED[y(x;D)]}2 + {ED[y(x;D)] − h(x)}2\n+2{y(x;D) − ED[y(x; D)]}{ED[y(x; D)] − h(x)}. (3.39)\nWe now take the expectation of this expression with respect to D and note that the\nﬁnal term will vanish, giving\nED\n[\n{y(x;D) − h(x)}2]\n= {ED[y(x;D)] − h(x)}2\n  \n(bias)2\n+ ED\n[\n{y(x;D) − ED[y(x;D)]}2]\n  \nvariance\n. (3.40)\nWe see that the expected squared difference between y(x;D) and the regression\nfunction h(x) can be expressed as the sum of two terms. The ﬁrst term, called the\nsquared bias, represents the extent to which the average prediction over all data sets\ndiffers from the desired regression function. The second term, called the variance,\nmeasures the extent to which the solutions for individual data sets vary around their\naverage, and hence this measures the extent to which the functiony(x; D) is sensitive\nto the particular choice of data set. We shall provide some intuition to support these\ndeﬁnitions shortly when we consider a simple example.\nSo far, we have considered a single input valuex. If we substitute this expansion\nback into (3.37), we obtain the following decomposition of the expected squared loss\nexpected loss =( bias)2 + variance + noise (3.41)\nwhere\n(bias)2 =\n∫\n{ED[y(x;D)] − h(x)}2p(x)dx (3.42)\nvariance =\n∫\nED\n[\n{y(x;D) − ED[y(x;D)]}2]\np(x)dx (3.43)\nnoise =\n∫\n{h(x) − t}2p(x,t )d xdt (3.44)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 168,
      "page_label": "149"
    }
  },
  {
    "page_content": "where\n(bias)2 =\n∫\n{ED[y(x;D)] − h(x)}2p(x)dx (3.42)\nvariance =\n∫\nED\n[\n{y(x;D) − ED[y(x;D)]}2]\np(x)dx (3.43)\nnoise =\n∫\n{h(x) − t}2p(x,t )d xdt (3.44)\nand the bias and variance terms now refer to integrated quantities.\nOur goal is to minimize the expected loss, which we have decomposed into the\nsum of a (squared) bias, a variance, and a constant noise term. As we shall see, there\nis a trade-off between bias and variance, with very ﬂexible models having low bias\nand high variance, and relatively rigid models having high bias and low variance.\nThe model with the optimal predictive capability is the one that leads to the best\nbalance between bias and variance. This is illustrated by considering the sinusoidal\ndata set from Chapter 1. Here we generate 100 data sets, each containing N =2 5Appendix A\ndata points, independently from the sinusoidal curve h(x)=s i n ( 2πx). The data\nsets are indexed by l =1 ,...,L , where L = 100, and for each data set D(l) we",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 168,
      "page_label": "149"
    }
  },
  {
    "page_content": "150 3. LINEAR MODELS FOR REGRESSION\nx\nt\nlnλ =2 .6\n0 1\n−1\n0\n1\nx\nt\n0 1\n−1\n0\n1\nx\nt\nlnλ = −0.31\n0 1\n−1\n0\n1\nx\nt\n0 1\n−1\n0\n1\nx\nt\nlnλ = −2.4\n0 1\n−1\n0\n1\nx\nt\n0 1\n−1\n0\n1\nFigure 3.5 Illustration of the dependence of bias and variance on model complexity, governed by a regulariza-\ntion parameter λ, using the sinusoidal data set from Chapter 1. There areL = 100data sets, each havingN =2 5\ndata points, and there are 24 Gaussian basis functions in the model so that the total number of parameters is\nM =2 5 including the bias parameter. The left column shows the result of ﬁtting the model to the data sets for\nvarious values of ln λ (for clarity, only 20 of the 100 ﬁts are shown). The right column shows the corresponding\naverage of the 100 ﬁts (red) along with the sinusoidal function from which the data sets were generated (green).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 169,
      "page_label": "150"
    }
  },
  {
    "page_content": "3.2. The Bias-Variance Decomposition 151\nFigure 3.6 Plot of squared bias and variance,\ntogether with their sum, correspond-\ning to the results shown in Fig-\nure 3.5. Also shown is the average\ntest set error for a test data set size\nof 1000 points. The minimum value\nof (bias)2 + variance occurs around\nln λ = −0.31, which is close to the\nvalue that gives the minimum error\non the test data.\nlnλ\n−3 −2 −1 0 1 2\n0\n0.03\n0.06\n0.09\n0.12\n0.15\n(bias)2\nvariance\n(bias)2 + variance\ntest error\nﬁt a model with 24 Gaussian basis functions by minimizing the regularized error\nfunction (3.27) to give a prediction function y(l)(x) as shown in Figure 3.5. The\ntop row corresponds to a large value of the regularization coefﬁcientλ that gives low\nvariance (because the red curves in the left plot look similar) but high bias (because\nthe two curves in the right plot are very different). Conversely on the bottom row, for\nwhich λ is small, there is large variance (shown by the high variability between the\nred curves in the left plot) but low bias (shown by the good ﬁt between the average\nmodel ﬁt and the original sinusoidal function). Note that the result of averaging many\nsolutions for the complex model with M =2 5 is a very good ﬁt to the regression\nfunction, which suggests that averaging may be a beneﬁcial procedure. Indeed, a\nweighted averaging of multiple solutions lies at the heart of a Bayesian approach,\nalthough the averaging is with respect to the posterior distribution of parameters, not",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 170,
      "page_label": "151"
    }
  },
  {
    "page_content": "although the averaging is with respect to the posterior distribution of parameters, not\nwith respect to multiple data sets.\nWe can also examine the bias-variance trade-off quantitatively for this example.\nThe average prediction is estimated from\ny(x)= 1\nL\nL∑\nl=1\ny(l)(x) (3.45)\nand the integrated squared bias and integrated variance are then given by\n(bias)2 = 1\nN\nN∑\nn=1\n{y(xn) − h(xn)}2 (3.46)\nvariance = 1\nN\nN∑\nn=1\n1\nL\nL∑\nl=1\n{\ny(l)(xn) − y(xn)\n}2\n(3.47)\nwhere the integral over x weighted by the distribution p(x) is approximated by a\nﬁnite sum over data points drawn from that distribution. These quantities, along\nwith their sum, are plotted as a function of lnλ in Figure 3.6. We see that small\nvalues of λ allow the model to become ﬁnely tuned to the noise on each individual",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 170,
      "page_label": "151"
    }
  },
  {
    "page_content": "152 3. LINEAR MODELS FOR REGRESSION\ndata set leading to large variance. Conversely, a large value of λ pulls the weight\nparameters towards zero leading to large bias.\nAlthough the bias-variance decomposition may provide some interesting in-\nsights into the model complexity issue from a frequentist perspective, it is of lim-\nited practical value, because the bias-variance decomposition is based on averages\nwith respect to ensembles of data sets, whereas in practice we have only the single\nobserved data set. If we had a large number of independent training sets of a given\nsize, we would be better off combining them into a single large training set, which\nof course would reduce the level of over-ﬁtting for a given model complexity.\nGiven these limitations, we turn in the next section to a Bayesian treatment of\nlinear basis function models, which not only provides powerful insights into the\nissues of over-ﬁtting but which also leads to practical techniques for addressing the\nquestion model complexity.\n3.3. Bayesian Linear Regression\nIn our discussion of maximum likelihood for setting the parameters of a linear re-\ngression model, we have seen that the effective model complexity, governed by the\nnumber of basis functions, needs to be controlled according to the size of the data\nset. Adding a regularization term to the log likelihood function means the effective\nmodel complexity can then be controlled by the value of the regularization coefﬁ-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 171,
      "page_label": "152"
    }
  },
  {
    "page_content": "model complexity can then be controlled by the value of the regularization coefﬁ-\ncient, although the choice of the number and form of the basis functions is of course\nstill important in determining the overall behaviour of the model.\nThis leaves the issue of deciding the appropriate model complexity for the par-\nticular problem, which cannot be decided simply by maximizing the likelihood func-\ntion, because this always leads to excessively complex models and over-ﬁtting. In-\ndependent hold-out data can be used to determine model complexity, as discussed\nin Section 1.3, but this can be both computationally expensive and wasteful of valu-\nable data. We therefore turn to a Bayesian treatment of linear regression, which will\navoid the over-ﬁtting problem of maximum likelihood, and which will also lead to\nautomatic methods of determining model complexity using the training data alone.\nAgain, for simplicity we will focus on the case of a single target variable t. Ex-\ntension to multiple target variables is straightforward and follows the discussion of\nSection 3.1.5.\n3.3.1 Parameter distribution\nWe begin our discussion of the Bayesian treatment of linear regression by in-\ntroducing a prior probability distribution over the model parameters w. For the mo-\nment, we shall treat the noise precision parameter β as a known constant. First note\nthat the likelihood functionp(t|w) deﬁned by (3.10) is the exponential of a quadratic",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 171,
      "page_label": "152"
    }
  },
  {
    "page_content": "that the likelihood functionp(t|w) deﬁned by (3.10) is the exponential of a quadratic\nfunction of w. The corresponding conjugate prior is therefore given by a Gaussian\ndistribution of the form\np(w)= N(w|m0, S0) (3.48)\nhaving mean m0 and covariance S0.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 171,
      "page_label": "152"
    }
  },
  {
    "page_content": "3.3. Bayesian Linear Regression 153\nNext we compute the posterior distribution, which is proportional to the product\nof the likelihood function and the prior. Due to the choice of a conjugate Gaus-\nsian prior distribution, the posterior will also be Gaussian. We can evaluate this\ndistribution by the usual procedure of completing the square in the exponential, and\nthen ﬁnding the normalization coefﬁcient using the standard result for a normalized\nGaussian. However, we have already done the necessary work in deriving the gen-Exercise 3.7\neral result (2.116), which allows us to write down the posterior distribution directly\nin the form\np(w|t)= N(w|mN , SN ) (3.49)\nwhere\nmN = SN\n(\nS−1\n0 m0 + βΦTt\n)\n(3.50)\nS−1\nN = S−1\n0 + βΦTΦ. (3.51)\nNote that because the posterior distribution is Gaussian, its mode coincides with its\nmean. Thus the maximum posterior weight vector is simply given bywMAP = mN .\nIf we consider an inﬁnitely broad prior S0 = α−1I with α → 0, the mean mN\nof the posterior distribution reduces to the maximum likelihood value wML given\nby (3.15). Similarly, if N =0 , then the posterior distribution reverts to the prior.\nFurthermore, if data points arrive sequentially, then the posterior distribution at any\nstage acts as the prior distribution for the subsequent data point, such that the new\nposterior distribution is again given by (3.49).Exercise 3.8\nFor the remainder of this chapter, we shall consider a particular form of Gaus-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 172,
      "page_label": "153"
    }
  },
  {
    "page_content": "posterior distribution is again given by (3.49).Exercise 3.8\nFor the remainder of this chapter, we shall consider a particular form of Gaus-\nsian prior in order to simplify the treatment. Speciﬁcally, we consider a zero-mean\nisotropic Gaussian governed by a single precision parameterα so that\np(w|α)= N(w|0,α −1I) (3.52)\nand the corresponding posterior distribution over w is then given by (3.49) with\nmN = βSN ΦTt (3.53)\nS−1\nN = αI + βΦTΦ. (3.54)\nThe log of the posterior distribution is given by the sum of the log likelihood and\nthe log of the prior and, as a function of w, takes the form\nlnp(w|t)= −β\n2\nN∑\nn=1\n{tn − wTφ(xn)}2 − α\n2 wTw +c o n s t. (3.55)\nMaximization of this posterior distribution with respect to w is therefore equiva-\nlent to the minimization of the sum-of-squares error function with the addition of a\nquadratic regularization term, corresponding to (3.27) with λ = α/β.\nWe can illustrate Bayesian learning in a linear basis function model, as well as\nthe sequential update of a posterior distribution, using a simple example involving\nstraight-line ﬁtting. Consider a single input variable x, a single target variable t and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 172,
      "page_label": "153"
    }
  },
  {
    "page_content": "154 3. LINEAR MODELS FOR REGRESSION\na linear model of the form y(x,w)= w0 + w1x. Because this has just two adap-\ntive parameters, we can plot the prior and posterior distributions directly in parameter\nspace. We generate synthetic data from the functionf(x,a)= a0 +a1x with param-\neter values a0 = −0.3 and a1 =0 .5 by ﬁrst choosing values of xn from the uniform\ndistribution U(x|−1, 1), then evaluatingf(xn, a), and ﬁnally adding Gaussian noise\nwith standard deviation of 0.2 to obtain the target values tn. Our goal is to recover\nthe values of a0 and a1 from such data, and we will explore the dependence on the\nsize of the data set. We assume here that the noise variance is known and hence we\nset the precision parameter to its true valueβ =( 1/0.2)2 =2 5. Similarly, we ﬁx\nthe parameter α to 2.0. We shall shortly discuss strategies for determining α and\nβ from the training data. Figure 3.7 shows the results of Bayesian learning in this\nmodel as the size of the data set is increased and demonstrates the sequential nature\nof Bayesian learning in which the current posterior distribution forms the prior when\na new data point is observed. It is worth taking time to study this ﬁgure in detail as\nit illustrates several important aspects of Bayesian inference. The ﬁrst row of this\nﬁgure corresponds to the situation before any data points are observed and shows a\nplot of the prior distribution inw space together with six samples of the function",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 173,
      "page_label": "154"
    }
  },
  {
    "page_content": "plot of the prior distribution inw space together with six samples of the function\ny(x,w) in which the values of w are drawn from the prior. In the second row, we\nsee the situation after observing a single data point. The location (x, t) of the data\npoint is shown by a blue circle in the right-hand column. In the left-hand column is a\nplot of the likelihood function p(t|x, w) for this data point as a function of w. Note\nthat the likelihood function provides a soft constraint that the line must pass close to\nthe data point, where close is determined by the noise precisionβ. For comparison,\nthe true parameter values a0 = −0.3 and a1 =0 .5 used to generate the data set\nare shown by a white cross in the plots in the left column of Figure 3.7. When we\nmultiply this likelihood function by the prior from the top row, and normalize, we\nobtain the posterior distribution shown in the middle plot on the second row. Sam-\nples of the regression function y(x,w) obtained by drawing samples of w from this\nposterior distribution are shown in the right-hand plot. Note that these sample lines\nall pass close to the data point. The third row of this ﬁgure shows the effect of ob-\nserving a second data point, again shown by a blue circle in the plot in the right-hand\ncolumn. The corresponding likelihood function for this second data point alone is\nshown in the left plot. When we multiply this likelihood function by the posterior",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 173,
      "page_label": "154"
    }
  },
  {
    "page_content": "shown in the left plot. When we multiply this likelihood function by the posterior\ndistribution from the second row, we obtain the posterior distribution shown in the\nmiddle plot of the third row. Note that this is exactly the same posterior distribution\nas would be obtained by combining the original prior with the likelihood function\nfor the two data points. This posterior has now been inﬂuenced by two data points,\nand because two points are sufﬁcient to deﬁne a line this already gives a relatively\ncompact posterior distribution. Samples from this posterior distribution give rise to\nthe functions shown in red in the third column, and we see that these functions pass\nclose to both of the data points. The fourth row shows the effect of observing a total\nof 20 data points. The left-hand plot shows the likelihood function for the 20th data\npoint alone, and the middle plot shows the resulting posterior distribution that has\nnow absorbed information from all 20 observations. Note how the posterior is much\nsharper than in the third row. In the limit of an inﬁnite number of data points, the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 173,
      "page_label": "154"
    }
  },
  {
    "page_content": "3.3. Bayesian Linear Regression 155\nFigure 3.7 Illustration of sequential Bayesian learning for a simple linear model of the formy(x, w)=\nw0 + w1x. A detailed description of this ﬁgure is given in the text.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 174,
      "page_label": "155"
    }
  },
  {
    "page_content": "156 3. LINEAR MODELS FOR REGRESSION\nposterior distribution would become a delta function centred on the true parameter\nvalues, shown by the white cross.\nOther forms of prior over the parameters can be considered. For instance, we\ncan generalize the Gaussian prior to give\np(w|α)=\n[q\n2\n(α\n2\n)1/q 1\nΓ(1/q)\n]M\nexp\n(\n−α\n2\nM∑\nj=1\n|wj|q\n)\n(3.56)\nin which q =2 corresponds to the Gaussian distribution, and only in this case is the\nprior conjugate to the likelihood function (3.10). Finding the maximum of the poste-\nrior distribution overw corresponds to minimization of the regularized error function\n(3.29). In the case of the Gaussian prior, the mode of the posterior distribution was\nequal to the mean, although this will no longer hold if q ̸=2.\n3.3.2 Predictive distribution\nIn practice, we are not usually interested in the value of w itself but rather in\nmaking predictions of t for new values of x. This requires that we evaluate the\npredictive distributiondeﬁned by\np(t|t,α ,β)=\n∫\np(t|w,β )p(w|t,α ,β)d w (3.57)\nin which t is the vector of target values from the training set, and we have omitted the\ncorresponding input vectors from the right-hand side of the conditioning statements\nto simplify the notation. The conditional distribution p(t|x, w,β ) of the target vari-\nable is given by (3.8), and the posterior weight distribution is given by (3.49). We\nsee that (3.57) involves the convolution of two Gaussian distributions, and so making",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 175,
      "page_label": "156"
    }
  },
  {
    "page_content": "see that (3.57) involves the convolution of two Gaussian distributions, and so making\nuse of the result (2.115) from Section 8.1.4, we see that the predictive distribution\ntakes the formExercise 3.10\np(t|x, t,α ,β)= N(t|mT\nN φ(x),σ 2\nN (x)) (3.58)\nwhere the variance σ2\nN (x) of the predictive distribution is given by\nσ2\nN (x)= 1\nβ + φ(x)TSN φ(x). (3.59)\nThe ﬁrst term in (3.59) represents the noise on the data whereas the second term\nreﬂects the uncertainty associated with the parameters w. Because the noise process\nand the distribution of w are independent Gaussians, their variances are additive.\nNote that, as additional data points are observed, the posterior distribution becomes\nnarrower. As a consequence it can be shown (Qazazet al., 1997) that σ2\nN+1(x) ⩽\nσ2\nN (x). In the limit N →∞ , the second term in (3.59) goes to zero, and the varianceExercise 3.11\nof the predictive distribution arises solely from the additive noise governed by the\nparameterβ.\nAs an illustration of the predictive distribution for Bayesian linear regression\nmodels, let us return to the synthetic sinusoidal data set of Section 1.1. In Figure 3.8,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 175,
      "page_label": "156"
    }
  },
  {
    "page_content": "3.3. Bayesian Linear Regression 157\nx\nt\n0 1\n−1\n0\n1\nx\nt\n0 1\n−1\n0\n1\nx\nt\n0 1\n−1\n0\n1\nx\nt\n0 1\n−1\n0\n1\nFigure 3.8 Examples of the predictive distribution (3.58) for a model consisting of 9 Gaussian basis functions\nof the form (3.4) using the synthetic sinusoidal data set of Section 1.1. See the text for a detailed discussion.\nwe ﬁt a model comprising a linear combination of Gaussian basis functions to data\nsets of various sizes and then look at the corresponding posterior distributions. Here\nthe green curves correspond to the function sin(2πx) from which the data points\nwere generated (with the addition of Gaussian noise). Data sets of size N =1 ,\nN =2 , N =4 , and N =2 5 are shown in the four plots by the blue circles. For\neach plot, the red curve shows the mean of the corresponding Gaussian predictive\ndistribution, and the red shaded region spans one standard deviation either side of\nthe mean. Note that the predictive uncertainty depends on x and is smallest in the\nneighbourhood of the data points. Also note that the level of uncertainty decreases\nas more data points are observed.\nThe plots in Figure 3.8 only show the point-wise predictive variance as a func-\ntion of x. In order to gain insight into the covariance between the predictions at\ndifferent values of x, we can draw samples from the posterior distribution over w,\nand then plot the corresponding functions y(x,w), as shown in Figure 3.9.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 176,
      "page_label": "157"
    }
  },
  {
    "page_content": "158 3. LINEAR MODELS FOR REGRESSION\nx\nt\n0 1\n−1\n0\n1\nx\nt\n0 1\n−1\n0\n1\nx\nt\n0 1\n−1\n0\n1\nx\nt\n0 1\n−1\n0\n1\nFigure 3.9 Plots of the function y(x, w) using samples from the posterior distributions overw corresponding to\nthe plots in Figure 3.8.\nIf we used localized basis functions such as Gaussians, then in regions away\nfrom the basis function centres, the contribution from the second term in the predic-\ntive variance (3.59) will go to zero, leaving only the noise contribution β−1. Thus,\nthe model becomes very conﬁdent in its predictions when extrapolating outside the\nregion occupied by the basis functions, which is generally an undesirable behaviour.\nThis problem can be avoided by adopting an alternative Bayesian approach to re-\ngression known as a Gaussian process.Section 6.4\nNote that, if both w and β are treated as unknown, then we can introduce a\nconjugate prior distribution p(w,β ) that, from the discussion in Section 2.3.6, will\nbe given by a Gaussian-gamma distribution (Denison et al., 2002). In this case, theExercise 3.12\npredictive distribution is a Student’s t-distribution.Exercise 3.13",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 177,
      "page_label": "158"
    }
  },
  {
    "page_content": "3.3. Bayesian Linear Regression 159\nFigure 3.10 The equivalent ker-\nnel k(x, x′) for the Gaussian basis\nfunctions in Figure 3.1, shown as\na plot of x versus x′, together with\nthree slices through this matrix cor-\nresponding to three different values\nofx. The data set used to generate\nthis kernel comprised 200 values of\nx equally spaced over the interval\n(−1, 1).\n3.3.3 Equivalent kernel\nThe posterior mean solution (3.53) for the linear basis function model has an in-\nteresting interpretation that will set the stage for kernel methods, including Gaussian\nprocesses. If we substitute (3.53) into the expression (3.3), we see that the predictiveChapter 6\nmean can be written in the form\ny(x,mN )= mT\nN φ(x)= βφ(x)TSN ΦTt =\nN∑\nn=1\nβφ(x)TSN φ(xn)tn (3.60)\nwhere SN is deﬁned by (3.51). Thus the mean of the predictive distribution at a point\nx is given by a linear combination of the training set target variables tn, so that we\ncan write\ny(x,mN )=\nN∑\nn=1\nk(x,xn)tn (3.61)\nwhere the function\nk(x,x′)= βφ(x)TSN φ(x′) (3.62)\nis known as thesmoother matrixor the equivalent kernel. Regression functions, such\nas this, which make predictions by taking linear combinations of the training set\ntarget values are known aslinear smoothers. Note that the equivalent kernel depends\non the input values xn from the data set because these appear in the deﬁnition of\nSN . The equivalent kernel is illustrated for the case of Gaussian basis functions in",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 178,
      "page_label": "159"
    }
  },
  {
    "page_content": "SN . The equivalent kernel is illustrated for the case of Gaussian basis functions in\nFigure 3.10 in which the kernel functions k(x, x′) have been plotted as a function of\nx′for three different values of x. We see that they are localized around x, and so the\nmean of the predictive distribution at x, given by y(x,mN ), is obtained by forming\na weighted combination of the target values in which data points close tox are given\nhigher weight than points further removed from x. Intuitively, it seems reasonable\nthat we should weight local evidence more strongly than distant evidence. Note that\nthis localization property holds not only for the localized Gaussian basis functions\nbut also for the nonlocal polynomial and sigmoidal basis functions, as illustrated in\nFigure 3.11.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 178,
      "page_label": "159"
    }
  },
  {
    "page_content": "160 3. LINEAR MODELS FOR REGRESSION\nFigure 3.11 Examples of equiva-\nlent kernels k(x, x′) for x =0\nplotted as a function of x′, corre-\nsponding (left) to the polynomial ba-\nsis functions and (right) to the sig-\nmoidal basis functions shown in Fig-\nure 3.1. Note that these are local-\nized functions of x′ even though the\ncorresponding basis functions are\nnonlocal. −1 0 1\n0\n0.02\n0.04\n−1 0 1\n0\n0.02\n0.04\nFurther insight into the role of the equivalent kernel can be obtained by consid-\nering the covariance between y(x) and y(x′), which is given by\ncov[y(x),y (x′) ]=c o v [ φ(x)Tw, wTφ(x′)]\n= φ(x)TSN φ(x′)= β−1k(x, x′) (3.63)\nwhere we have made use of (3.49) and (3.62). From the form of the equivalent\nkernel, we see that the predictive mean at nearby points will be highly correlated,\nwhereas for more distant pairs of points the correlation will be smaller.\nThe predictive distribution shown in Figure 3.8 allows us to visualize the point-\nwise uncertainty in the predictions, governed by (3.59). However, by drawing sam-\nples from the posterior distribution over w, and plotting the corresponding model\nfunctions y(x,w) as in Figure 3.9, we are visualizing the joint uncertainty in the\nposterior distribution between the y values at two (or more)x values, as governed by\nthe equivalent kernel.\nThe formulation of linear regression in terms of a kernel function suggests an\nalternative approach to regression as follows. Instead of introducing a set of basis",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 179,
      "page_label": "160"
    }
  },
  {
    "page_content": "alternative approach to regression as follows. Instead of introducing a set of basis\nfunctions, which implicitly determines an equivalent kernel, we can instead deﬁne\na localized kernel directly and use this to make predictions for new input vectors x,\ngiven the observed training set. This leads to a practical framework for regression\n(and classiﬁcation) called Gaussian processes, which will be discussed in detail in\nSection 6.4.\nWe have seen that the effective kernel deﬁnes the weights by which the training\nset target values are combined in order to make a prediction at a new value ofx, and\nit can be shown that these weights sum to one, in other words\nN∑\nn=1\nk(x,xn)=1 (3.64)\nfor all values of x. This intuitively pleasing result can easily be proven informallyExercise 3.14\nby noting that the summation is equivalent to considering the predictive mean ˆy(x)\nfor a set of target data in which tn =1 for all n. Provided the basis functions are\nlinearly independent, that there are more data points than basis functions, and that\none of the basis functions is constant (corresponding to the bias parameter), then it is\nclear that we can ﬁt the training data exactly and hence that the predictive mean will",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 179,
      "page_label": "160"
    }
  },
  {
    "page_content": "3.4. Bayesian Model Comparison 161\nbe simply ˆy(x)=1 , from which we obtain (3.64). Note that the kernel function can\nbe negative as well as positive, so although it satisﬁes a summation constraint, the\ncorresponding predictions are not necessarily convex combinations of the training\nset target variables.\nFinally, we note that the equivalent kernel (3.62) satisﬁes an important property\nshared by kernel functions in general, namely that it can be expressed in the form anChapter 6\ninner product with respect to a vector ψ(x) of nonlinear functions, so that\nk(x,z)= ψ(x)Tψ(z) (3.65)\nwhere ψ(x)= β1/2S1/2\nN φ(x).\n3.4. Bayesian Model Comparison\nIn Chapter 1, we highlighted the problem of over-ﬁtting as well as the use of cross-\nvalidation as a technique for setting the values of regularization parameters or for\nchoosing between alternative models. Here we consider the problem of model se-\nlection from a Bayesian perspective. In this section, our discussion will be very\ngeneral, and then in Section 3.5 we shall see how these ideas can be applied to the\ndetermination of regularization parameters in linear regression.\nAs we shall see, the over-ﬁtting associated with maximum likelihood can be\navoided by marginalizing (summing or integrating) over the model parameters in-\nstead of making point estimates of their values. Models can then be compared di-\nrectly on the training data, without the need for a validation set. This allows all",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 180,
      "page_label": "161"
    }
  },
  {
    "page_content": "rectly on the training data, without the need for a validation set. This allows all\navailable data to be used for training and avoids the multiple training runs for each\nmodel associated with cross-validation. It also allows multiple complexity parame-\nters to be determined simultaneously as part of the training process. For example,\nin Chapter 7 we shall introduce the relevance vector machine, which is a Bayesian\nmodel having one complexity parameter for every training data point.\nThe Bayesian view of model comparison simply involves the use of probabilities\nto represent uncertainty in the choice of model, along with a consistent application\nof the sum and product rules of probability. Suppose we wish to compare a set ofL\nmodels {Mi} where i =1 ,...,L . Here a model refers to a probability distribution\nover the observed data D. In the case of the polynomial curve-ﬁtting problem, the\ndistribution is deﬁned over the set of target values t, while the set of input values X\nis assumed to be known. Other types of model deﬁne a joint distributions over X\nand t. We shall suppose that the data is generated from one of these models but weSection 1.5.4\nare uncertain which one. Our uncertainty is expressed through a prior probability\ndistribution p(Mi). Given a training set D, we then wish to evaluate the posterior\ndistribution\np(Mi|D) ∝ p(Mi)p(D|Mi). (3.66)\nThe prior allows us to express a preference for different models. Let us simply",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 180,
      "page_label": "161"
    }
  },
  {
    "page_content": "distribution\np(Mi|D) ∝ p(Mi)p(D|Mi). (3.66)\nThe prior allows us to express a preference for different models. Let us simply\nassume that all models are given equal prior probability. The interesting term is\nthe model evidencep(D|Mi) which expresses the preference shown by the data for",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 180,
      "page_label": "161"
    }
  },
  {
    "page_content": "162 3. LINEAR MODELS FOR REGRESSION\ndifferent models, and we shall examine this term in more detail shortly. The model\nevidence is sometimes also called the marginal likelihood because it can be viewed\nas a likelihood function over the space of models, in which the parameters have been\nmarginalized out. The ratio of model evidencesp(D|Mi)/p(D|Mj) for two models\nis known as a Bayes factor (Kass and Raftery, 1995).\nOnce we know the posterior distribution over models, the predictive distribution\nis given, from the sum and product rules, by\np(t|x, D)=\nL∑\ni=1\np(t|x, Mi, D)p(Mi|D). (3.67)\nThis is an example of a mixture distributionin which the overall predictive distribu-\ntion is obtained by averaging the predictive distributionsp(t|x, Mi, D) of individual\nmodels, weighted by the posterior probabilities p(Mi|D) of those models. For in-\nstance, if we have two models that are a-posteriori equally likely and one predicts\na narrow distribution around t = a while the other predicts a narrow distribution\naround t = b, the overall predictive distribution will be a bimodal distribution with\nmodes at t = a and t = b, not a single model at t =( a + b)/2.\nA simple approximation to model averaging is to use the single most probable\nmodel alone to make predictions. This is known as model selection.\nFor a model governed by a set of parameters w, the model evidence is given,\nfrom the sum and product rules of probability, by\np(D|Mi)=\n∫\np(D|w, Mi)p(w|Mi)d w. (3.68)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 181,
      "page_label": "162"
    }
  },
  {
    "page_content": "from the sum and product rules of probability, by\np(D|Mi)=\n∫\np(D|w, Mi)p(w|Mi)d w. (3.68)\nFrom a sampling perspective, the marginal likelihood can be viewed as the proba-Chapter 11\nbility of generating the data set D from a model whose parameters are sampled at\nrandom from the prior. It is also interesting to note that the evidence is precisely the\nnormalizing term that appears in the denominator in Bayes’ theorem when evaluating\nthe posterior distribution over parameters because\np(w|D, Mi)= p(D|w, Mi)p(w|Mi)\np(D|Mi) . (3.69)\nWe can obtain some insight into the model evidence by making a simple approx-\nimation to the integral over parameters. Consider ﬁrst the case of a model having a\nsingle parameter w. The posterior distribution over parameters is proportional to\np(D|w)p(w), where we omit the dependence on the model Mi to keep the notation\nuncluttered. If we assume that the posterior distribution is sharply peaked around the\nmost probable value wMAP, with width ∆wposterior, then we can approximate the in-\ntegral by the value of the integrand at its maximum times the width of the peak. If we\nfurther assume that the prior is ﬂat with width ∆wprior so that p(w)=1 /∆wprior,\nthen we have\np(D)=\n∫\np(D|w)p(w)d w ≃ p(D|wMAP)∆wposterior\n∆wprior\n(3.70)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 181,
      "page_label": "162"
    }
  },
  {
    "page_content": "3.4. Bayesian Model Comparison 163\nFigure 3.12 We can obtain a rough approximation to\nthe model evidence if we assume that\nthe posterior distribution over parame-\nters is sharply peaked around its mode\nwMAP.\n∆wposterior\n∆wprior\nwMAP w\nand so taking logs we obtain\nlnp(D) ≃ ln p(D|wMAP)+l n\n( ∆wposterior\n∆wprior\n)\n. (3.71)\nThis approximation is illustrated in Figure 3.12. The ﬁrst term represents the ﬁt to\nthe data given by the most probable parameter values, and for a ﬂat prior this would\ncorrespond to the log likelihood. The second term penalizes the model according to\nits complexity. Because ∆wposterior < ∆wprior this term is negative, and it increases\nin magnitude as the ratio ∆wposterior/∆wprior gets smaller. Thus, if parameters are\nﬁnely tuned to the data in the posterior distribution, then the penalty term is large.\nFor a model having a set ofM parameters, we can make a similar approximation\nfor each parameter in turn. Assuming that all parameters have the same ratio of\n∆wposterior/∆wprior, we obtain\nlnp(D) ≃ lnp(D|wMAP)+ M ln\n( ∆wposterior\n∆wprior\n)\n. (3.72)\nThus, in this very simple approximation, the size of the complexity penalty increases\nlinearly with the number M of adaptive parameters in the model. As we increase\nthe complexity of the model, the ﬁrst term will typically decrease, because a more\ncomplex model is better able to ﬁt the data, whereas the second term will increase\ndue to the dependence on M. The optimal model complexity, as determined by",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 182,
      "page_label": "163"
    }
  },
  {
    "page_content": "due to the dependence on M. The optimal model complexity, as determined by\nthe maximum evidence, will be given by a trade-off between these two competing\nterms. We shall later develop a more reﬁned version of this approximation, based on\na Gaussian approximation to the posterior distribution.Section 4.4.1\nWe can gain further insight into Bayesian model comparison and understand\nhow the marginal likelihood can favour models of intermediate complexity by con-\nsidering Figure 3.13. Here the horizontal axis is a one-dimensional representation\nof the space of possible data sets, so that each point on this axis corresponds to a\nspeciﬁc data set. We now consider three models M1, M2 and M3 of successively\nincreasing complexity. Imagine running these models generatively to produce exam-\nple data sets, and then looking at the distribution of data sets that result. Any given",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 182,
      "page_label": "163"
    }
  },
  {
    "page_content": "164 3. LINEAR MODELS FOR REGRESSION\nFigure 3.13 Schematic illustration of the\ndistribution of data sets for\nthree models of different com-\nplexity, in which M1 is the\nsimplest and M3 is the most\ncomplex. Note that the dis-\ntributions are normalized. In\nthis example, for the partic-\nular observed data set D0,\nthe model M2 with intermedi-\nate complexity has the largest\nevidence.\np(D)\nDD0\nM1\nM2\nM3\nmodel can generate a variety of different data sets since the parameters are governed\nby a prior probability distribution, and for any choice of the parameters there may\nbe random noise on the target variables. To generate a particular data set from a spe-\nciﬁc model, we ﬁrst choose the values of the parameters from their prior distribution\np(w), and then for these parameter values we sample the data from p(D|w). A sim-\nple model (for example, based on a ﬁrst order polynomial) has little variability and\nso will generate data sets that are fairly similar to each other. Its distribution p(D)\nis therefore conﬁned to a relatively small region of the horizontal axis. By contrast,\na complex model (such as a ninth order polynomial) can generate a great variety of\ndifferent data sets, and so its distribution p(D) is spread over a large region of the\nspace of data sets. Because the distributions p(D|Mi) are normalized, we see that\nthe particular data set D0 can have the highest value of the evidence for the model",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 183,
      "page_label": "164"
    }
  },
  {
    "page_content": "the particular data set D0 can have the highest value of the evidence for the model\nof intermediate complexity. Essentially, the simpler model cannot ﬁt the data well,\nwhereas the more complex model spreads its predictive probability over too broad a\nrange of data sets and so assigns relatively small probability to any one of them.\nImplicit in the Bayesian model comparison framework is the assumption that\nthe true distribution from which the data are generated is contained within the set of\nmodels under consideration. Provided this is so, we can show that Bayesian model\ncomparison will on average favour the correct model. To see this, consider two\nmodels M1 and M2 in which the truth corresponds to M1. For a given ﬁnite data\nset, it is possible for the Bayes factor to be larger for the incorrect model. However, if\nwe average the Bayes factor over the distribution of data sets, we obtain the expected\nBayes factor in the form\n∫\np(D|M1)l np(D|M1)\np(D|M2) dD (3.73)\nwhere the average has been taken with respect to the true distribution of the data.\nThis quantity is an example of theKullback-Leibler divergence and satisﬁes the prop-Section 1.6.1\nerty of always being positive unless the two distributions are equal in which case it\nis zero. Thus on average the Bayes factor will always favour the correct model.\nWe have seen that the Bayesian framework avoids the problem of over-ﬁtting\nand allows models to be compared on the basis of the training data alone. However,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 183,
      "page_label": "164"
    }
  },
  {
    "page_content": "3.5. The Evidence Approximation 165\na Bayesian approach, like any approach to pattern recognition, needs to make as-\nsumptions about the form of the model, and if these are invalid then the results can\nbe misleading. In particular, we see from Figure 3.12 that the model evidence can\nbe sensitive to many aspects of the prior, such as the behaviour in the tails. Indeed,\nthe evidence is not deﬁned if the prior is improper, as can be seen by noting that\nan improper prior has an arbitrary scaling factor (in other words, the normalization\ncoefﬁcient is not deﬁned because the distribution cannot be normalized). If we con-\nsider a proper prior and then take a suitable limit in order to obtain an improper prior\n(for example, a Gaussian prior in which we take the limit of inﬁnite variance) then\nthe evidence will go to zero, as can be seen from (3.70) and Figure 3.12. It may,\nhowever, be possible to consider the evidence ratio between two models ﬁrst and\nthen take a limit to obtain a meaningful answer.\nIn a practical application, therefore, it will be wise to keep aside an independent\ntest set of data on which to evaluate the overall performance of the ﬁnal system.\n3.5. The Evidence Approximation\nIn a fully Bayesian treatment of the linear basis function model, we would intro-\nduce prior distributions over the hyperparametersα and β and make predictions by\nmarginalizing with respect to these hyperparameters as well as with respect to the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 184,
      "page_label": "165"
    }
  },
  {
    "page_content": "marginalizing with respect to these hyperparameters as well as with respect to the\nparameters w. However, although we can integrate analytically over either w or\nover the hyperparameters, the complete marginalization over all of these variables\nis analytically intractable. Here we discuss an approximation in which we set the\nhyperparameters to speciﬁc values determined by maximizing the marginal likeli-\nhood function obtained by ﬁrst integrating over the parameters w. This framework\nis known in the statistics literature as empirical Bayes (Bernardo and Smith, 1994;\nGelman et al., 2004), or type 2 maximum likelihood(Berger, 1985), or generalized\nmaximum likelihood (Wahba, 1975), and in the machine learning literature is also\ncalled the evidence approximation(Gull, 1989; MacKay, 1992a).\nIf we introduce hyperpriors over α and β, the predictive distribution is obtained\nby marginalizing over w, α and β so that\np(t|t)=\n∫∫∫\np(t|w,β )p(w|t,α ,β)p(α, β|t)d wdα dβ (3.74)\nwhere p(t|w,β ) is given by (3.8) and p(w|t,α ,β) is given by (3.49) with mN and\nSN deﬁned by (3.53) and (3.54) respectively. Here we have omitted the dependence\non the input variable x to keep the notation uncluttered. If the posterior distribution\np(α, β|t) is sharply peaked around values ˆα and ˆβ, then the predictive distribution is\nobtained simply by marginalizing over w in which α and β are ﬁxed to the values ˆα\nand ˆβ, so that\np(t|t) ≃ p(t|t, ˆα, ˆβ)=\n∫\np(t|w, ˆβ)p(w|t, ˆα, ˆβ)d w. (3.75)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 184,
      "page_label": "165"
    }
  },
  {
    "page_content": "166 3. LINEAR MODELS FOR REGRESSION\nFrom Bayes’ theorem, the posterior distribution for α and β is given by\np(α, β|t) ∝ p(t|α, β)p(α, β). (3.76)\nIf the prior is relatively ﬂat, then in the evidence framework the values of ˆα and\nˆβ are obtained by maximizing the marginal likelihood function p(t|α, β). We shall\nproceed by evaluating the marginal likelihood for the linear basis function model and\nthen ﬁnding its maxima. This will allow us to determine values for these hyperpa-\nrameters from the training data alone, without recourse to cross-validation. Recall\nthat the ratio α/β is analogous to a regularization parameter.\nAs an aside it is worth noting that, if we deﬁne conjugate (Gamma) prior distri-\nbutions over α and β, then the marginalization over these hyperparameters in (3.74)\ncan be performed analytically to give a Student’s t-distribution over w (see Sec-\ntion 2.3.7). Although the resulting integral overw is no longer analytically tractable,\nit might be thought that approximating this integral, for example using the Laplace\napproximation discussed (Section 4.4) which is based on a local Gaussian approxi-\nmation centred on the mode of the posterior distribution, might provide a practical\nalternative to the evidence framework (Buntine and Weigend, 1991). However, the\nintegrand as a function ofw typically has a strongly skewed mode so that the Laplace\napproximation fails to capture the bulk of the probability mass, leading to poorer re-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 185,
      "page_label": "166"
    }
  },
  {
    "page_content": "approximation fails to capture the bulk of the probability mass, leading to poorer re-\nsults than those obtained by maximizing the evidence (MacKay, 1999).\nReturning to the evidence framework, we note that there are two approaches that\nwe can take to the maximization of the log evidence. We can evaluate the evidence\nfunction analytically and then set its derivative equal to zero to obtain re-estimation\nequations for α and β, which we shall do in Section 3.5.2. Alternatively we use a\ntechnique called the expectation maximization (EM) algorithm, which will be dis-\ncussed in Section 9.3.4 where we shall also show that these two approaches converge\nto the same solution.\n3.5.1 Evaluation of the evidence function\nThe marginal likelihood function p(t|α, β) is obtained by integrating over the\nweight parameters w, so that\np(t|α, β)=\n∫\np(t|w,β )p(w|α)d w. (3.77)\nOne way to evaluate this integral is to make use once again of the result (2.115)\nfor the conditional distribution in a linear-Gaussian model. Here we shall evaluateExercise 3.16\nthe integral instead by completing the square in the exponent and making use of the\nstandard form for the normalization coefﬁcient of a Gaussian.\nFrom (3.11), (3.12), and (3.52), we can write the evidence function in the formExercise 3.17\np(t|α, β)=\n( β\n2π\n)N/2 ( α\n2π\n)M/2 ∫\nexp {−E(w)} dw (3.78)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 185,
      "page_label": "166"
    }
  },
  {
    "page_content": "3.5. The Evidence Approximation 167\nwhere M is the dimensionality of w, and we have deﬁned\nE(w)= βED(w)+ αEW (w)\n= β\n2 ∥t − Φw∥2 + α\n2 wTw. (3.79)\nWe recognize (3.79) as being equal, up to a constant of proportionality, to the reg-\nularized sum-of-squares error function (3.27). We now complete the square over wExercise 3.18\ngiving\nE(w)= E(mN )+ 1\n2(w − mN )TA(w − mN ) (3.80)\nwhere we have introduced\nA = αI + βΦTΦ (3.81)\ntogether with\nE(mN )= β\n2 ∥t − ΦmN ∥2 + α\n2 mT\nN mN . (3.82)\nNote that A corresponds to the matrix of second derivatives of the error function\nA = ∇∇E(w) (3.83)\nand is known as the Hessian matrix. Here we have also deﬁned mN given by\nmN = βA−1ΦTt. (3.84)\nUsing (3.54), we see that A = S−1\nN , and hence (3.84) is equivalent to the previous\ndeﬁnition (3.53), and therefore represents the mean of the posterior distribution.\nThe integral over w can now be evaluated simply by appealing to the standard\nresult for the normalization coefﬁcient of a multivariate Gaussian, givingExercise 3.19\n∫\nexp {−E(w)} dw\n=e x p {−E(mN )}\n∫\nexp\n{\n−1\n2(w − mN )TA(w − mN )\n}\ndw\n=e x p {−E(mN )}(2π)M/2|A|−1/2. (3.85)\nUsing (3.78) we can then write the log of the marginal likelihood in the form\nlnp(t|α, β)= M\n2 ln α + N\n2 lnβ − E(mN ) − 1\n2 ln|A|− N\n2 ln(2π) (3.86)\nwhich is the required expression for the evidence function.\nReturning to the polynomial regression problem, we can plot the model evidence\nagainst the order of the polynomial, as shown in Figure 3.14. Here we have assumed",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 186,
      "page_label": "167"
    }
  },
  {
    "page_content": "against the order of the polynomial, as shown in Figure 3.14. Here we have assumed\na prior of the form (1.65) with the parameter α ﬁxed at α =5 × 10−3. The form\nof this plot is very instructive. Referring back to Figure 1.4, we see that the M =0\npolynomial has very poor ﬁt to the data and consequently gives a relatively low value",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 186,
      "page_label": "167"
    }
  },
  {
    "page_content": "168 3. LINEAR MODELS FOR REGRESSION\nFigure 3.14 Plot of the model evidence versus\nthe order M, for the polynomial re-\ngression model, showing that the\nevidence favours the model with\nM =3 .\nM\n0 2 4 6 8\n−26\n−24\n−22\n−20\n−18\nfor the evidence. Going to the M =1 polynomial greatly improves the data ﬁt, and\nhence the evidence is signiﬁcantly higher. However, in going to M =2 , the data\nﬁt is improved only very marginally, due to the fact that the underlying sinusoidal\nfunction from which the data is generated is an odd function and so has no even terms\nin a polynomial expansion. Indeed, Figure 1.5 shows that the residual data error is\nreduced only slightly in going from M =1 to M =2 . Because this richer model\nsuffers a greater complexity penalty, the evidence actually falls in going fromM =1\nto M =2 . When we go to M =3 we obtain a signiﬁcant further improvement in\ndata ﬁt, as seen in Figure 1.4, and so the evidence is increased again, giving the\nhighest overall evidence for any of the polynomials. Further increases in the value\nof M produce only small improvements in the ﬁt to the data but suffer increasing\ncomplexity penalty, leading overall to a decrease in the evidence values. Looking\nagain at Figure 1.5, we see that the generalization error is roughly constant between\nM =3 and M =8 , and it would be difﬁcult to choose between these models on\nthe basis of this plot alone. The evidence values, however, show a clear preference",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 187,
      "page_label": "168"
    }
  },
  {
    "page_content": "the basis of this plot alone. The evidence values, however, show a clear preference\nfor M =3 , since this is the simplest model which gives a good explanation for the\nobserved data.\n3.5.2 Maximizing the evidence function\nLet us ﬁrst consider the maximization of p(t|α, β) with respect to α. This can\nbe done by ﬁrst deﬁning the following eigenvector equation\n(\nβΦTΦ\n)\nui = λiui. (3.87)\nFrom (3.81), it then follows thatA has eigenvalues α+λi. Now consider the deriva-\ntive of the term involving ln |A| in (3.86) with respect to α.W eh a v e\nd\ndα ln|A| = d\ndα ln\n∏\ni\n(λi + α)= d\ndα\n∑\ni\nln(λi + α)=\n∑\ni\n1\nλi + α. (3.88)\nThus the stationary points of (3.86) with respect to α satisfy\n0= M\n2α − 1\n2mT\nN mN − 1\n2\n∑\ni\n1\nλi + α. (3.89)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 187,
      "page_label": "168"
    }
  },
  {
    "page_content": "3.5. The Evidence Approximation 169\nMultiplying through by 2α and rearranging, we obtain\nαmT\nN mN = M − α\n∑\ni\n1\nλi + α = γ. (3.90)\nSince there are M terms in the sum over i, the quantity γ can be written\nγ =\n∑\ni\nλi\nα + λi\n. (3.91)\nThe interpretation of the quantity γ will be discussed shortly. From (3.90) we see\nthat the value of α that maximizes the marginal likelihood satisﬁesExercise 3.20\nα = γ\nmT\nN mN\n. (3.92)\nNote that this is an implicit solution for α not only because γ depends on α, but also\nbecause the mode mN of the posterior distribution itself depends on the choice of\nα. We therefore adopt an iterative procedure in which we make an initial choice for\nα and use this to ﬁnd mN , which is given by (3.53), and also to evaluate γ, which\nis given by (3.91). These values are then used to re-estimate α using (3.92), and the\nprocess repeated until convergence. Note that because the matrix ΦTΦ is ﬁxed, we\ncan compute its eigenvalues once at the start and then simply multiply these by β to\nobtain the λi.\nIt should be emphasized that the value ofα has been determined purely by look-\ning at the training data. In contrast to maximum likelihood methods, no independent\ndata set is required in order to optimize the model complexity.\nWe can similarly maximize the log marginal likelihood (3.86) with respect toβ.\nTo do this, we note that the eigenvalues λi deﬁned by (3.87) are proportional to β,\nand hence dλi/dβ = λi/β giving\nd\ndβ ln|A| = d\ndβ\n∑\ni\nln(λi + α)= 1\nβ\n∑\ni\nλi\nλi + α = γ",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 188,
      "page_label": "169"
    }
  },
  {
    "page_content": "and hence dλi/dβ = λi/β giving\nd\ndβ ln|A| = d\ndβ\n∑\ni\nln(λi + α)= 1\nβ\n∑\ni\nλi\nλi + α = γ\nβ. (3.93)\nThe stationary point of the marginal likelihood therefore satisﬁes\n0= N\n2β − 1\n2\nN∑\nn=1\n{\ntn − mT\nN φ(xn)\n}2\n− γ\n2β (3.94)\nand rearranging we obtainExercise 3.22\n1\nβ = 1\nN − γ\nN∑\nn=1\n{\ntn − mT\nN φ(xn)\n}2\n. (3.95)\nAgain, this is an implicit solution for β and can be solved by choosing an initial\nvalue for β and then using this to calculate mN and γ and then re-estimate β using\n(3.95), repeating until convergence. If both α and β are to be determined from the\ndata, then their values can be re-estimated together after each update of γ.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 188,
      "page_label": "169"
    }
  },
  {
    "page_content": "170 3. LINEAR MODELS FOR REGRESSION\nFigure 3.15 Contours of the likelihood function (red)\nand the prior (green) in which the axes in parameter\nspace have been rotated to align with the eigenvectors\nui of the Hessian. For α =0 , the mode of the poste-\nrior is given by the maximum likelihood solution wML,\nwhereas for nonzero α the mode is at wMAP = mN .I n\nthe direction w1 the eigenvalue λ1, deﬁned by (3.87), is\nsmall compared with α and so the quantity λ1/(λ1 + α)\nis close to zero, and the corresponding MAP value of\nw1 is also close to zero. By contrast, in the direction w2\nthe eigenvalue λ2 is large compared with α and so the\nquantity λ2/(λ2 +α) is close to unity, and the MAP value\nof w2 is close to its maximum likelihood value.\nu1\nu2\nw1\nw2\nwMAP\nwML\n3.5.3 Effective number of parameters\nThe result (3.92) has an elegant interpretation (MacKay, 1992a), which provides\ninsight into the Bayesian solution forα. To see this, consider the contours of the like-\nlihood function and the prior as illustrated in Figure 3.15. Here we have implicitly\ntransformed to a rotated set of axes in parameter space aligned with the eigenvec-\ntors ui deﬁned in (3.87). Contours of the likelihood function are then axis-aligned\nellipses. The eigenvalues λi measure the curvature of the likelihood function, and\nso in Figure 3.15 the eigenvalue λ1 is small compared with λ2 (because a smaller\ncurvature corresponds to a greater elongation of the contours of the likelihood func-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 189,
      "page_label": "170"
    }
  },
  {
    "page_content": "curvature corresponds to a greater elongation of the contours of the likelihood func-\ntion). Because βΦTΦ is a positive deﬁnite matrix, it will have positive eigenvalues,\nand so the ratio λi/(λi + α) will lie between 0 and 1. Consequently, the quantity γ\ndeﬁned by (3.91) will lie in the range 0 ⩽ γ ⩽ M. For directions in which λi ≫ α,\nthe corresponding parameter wi will be close to its maximum likelihood value, and\nthe ratio λi/(λi + α) will be close to 1. Such parameters are called well determined\nbecause their values are tightly constrained by the data. Conversely, for directions\nin which λi ≪ α, the corresponding parameters wi will be close to zero, as will the\nratios λi/(λi +α). These are directions in which the likelihood function is relatively\ninsensitive to the parameter value and so the parameter has been set to a small value\nby the prior. The quantity γ deﬁned by (3.91) therefore measures the effective total\nnumber of well determined parameters.\nWe can obtain some insight into the result (3.95) for re-estimating β by com-\nparing it with the corresponding maximum likelihood result given by (3.21). Both\nof these formulae express the variance (the inverse precision) as an average of the\nsquared differences between the targets and the model predictions. However, they\ndiffer in that the number of data points N in the denominator of the maximum like-\nlihood result is replaced by N − γ in the Bayesian result. We recall from (1.56) that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 189,
      "page_label": "170"
    }
  },
  {
    "page_content": "lihood result is replaced by N − γ in the Bayesian result. We recall from (1.56) that\nthe maximum likelihood estimate of the variance for a Gaussian distribution over a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 189,
      "page_label": "170"
    }
  },
  {
    "page_content": "3.5. The Evidence Approximation 171\nsingle variable x is given by\nσ2\nML = 1\nN\nN∑\nn=1\n(xn − µML)2 (3.96)\nand that this estimate is biased because the maximum likelihood solution µML for\nthe mean has ﬁtted some of the noise on the data. In effect, this has used up one\ndegree of freedom in the model. The corresponding unbiased estimate is given by\n(1.59) and takes the form\nσ2\nMAP = 1\nN − 1\nN∑\nn=1\n(xn − µML)2. (3.97)\nWe shall see in Section 10.1.3 that this result can be obtained from a Bayesian treat-\nment in which we marginalize over the unknown mean. The factor of N − 1 in the\ndenominator of the Bayesian result takes account of the fact that one degree of free-\ndom has been used in ﬁtting the mean and removes the bias of maximum likelihood.\nNow consider the corresponding results for the linear regression model. The mean\nof the target distribution is now given by the function wTφ(x), which contains M\nparameters. However, not all of these parameters are tuned to the data. The effective\nnumber of parameters that are determined by the data isγ, with the remainingM −γ\nparameters set to small values by the prior. This is reﬂected in the Bayesian result\nfor the variance that has a factorN − γ in the denominator, thereby correcting for\nthe bias of the maximum likelihood result.\nWe can illustrate the evidence framework for setting hyperparameters using the\nsinusoidal synthetic data set from Section 1.1, together with the Gaussian basis func-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 190,
      "page_label": "171"
    }
  },
  {
    "page_content": "sinusoidal synthetic data set from Section 1.1, together with the Gaussian basis func-\ntion model comprising 9 basis functions, so that the total number of parameters in\nthe model is given by M =1 0 including the bias. Here, for simplicity of illustra-\ntion, we have set β to its true value of 11.1 and then used the evidence framework to\ndetermine α, as shown in Figure 3.16.\nWe can also see how the parameter α controls the magnitude of the parameters\n{wi}, by plotting the individual parameters versus the effective number γ of param-\neters, as shown in Figure 3.17.\nIf we consider the limit N ≫ M in which the number of data points is large in\nrelation to the number of parameters, then from (3.87) all of the parameters will be\nwell determined by the data becauseΦTΦ involves an implicit sum over data points,\nand so the eigenvalues λi increase with the size of the data set. In this case, γ = M,\nand the re-estimation equations for α and β become\nα = M\n2EW (mN ) (3.98)\nβ = N\n2ED(mN ) (3.99)\nwhere EW and ED are deﬁned by (3.25) and (3.26), respectively. These results\ncan be used as an easy-to-compute approximation to the full evidence re-estimation",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 190,
      "page_label": "171"
    }
  },
  {
    "page_content": "172 3. LINEAR MODELS FOR REGRESSION\nlnα\n−5 0 5\nlnα\n−5 0 5\nFigure 3.16 The left plot shows γ (red curve) and 2αEW (mN ) (blue curve) versus ln α for the sinusoidal\nsynthetic data set. It is the intersection of these two curves that deﬁnes the optimum value for α given by the\nevidence procedure. The right plot shows the corresponding graph of log evidence ln p(t|α, β) versus lnα (red\ncurve) showing that the peak coincides with the crossing point of the curves in the left plot. Also shown is the\ntest set error (blue curve) showing that the evidence maximum occurs close to the point of best generalization.\nformulae, because they do not require evaluation of the eigenvalue spectrum of the\nHessian.\nFigure 3.17 Plot of the 10 parameters wi\nfrom the Gaussian basis function\nmodel versus the effective num-\nber of parameters γ, in which the\nhyperparameter α is varied in the\nrange 0 ⩽ α ⩽ ∞ causing γ to\nvary in the range 0 ⩽ γ ⩽ M.\n9\n7\n1\n3\n6\n2\n5\n4\n8\n0\nγ\nwi\n0 2 4 6 8 10\n−2\n−1\n0\n1\n2\n3.6. Limitations of Fixed Basis Functions\nThroughout this chapter, we have focussed on models comprising a linear combina-\ntion of ﬁxed, nonlinear basis functions. We have seen that the assumption of linearity\nin the parameters led to a range of useful properties including closed-form solutions\nto the least-squares problem, as well as a tractable Bayesian treatment. Furthermore,\nfor a suitable choice of basis functions, we can model arbitrary nonlinearities in the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 191,
      "page_label": "172"
    }
  },
  {
    "page_content": "Exercises 173\nmapping from input variables to targets. In the next chapter, we shall study an anal-\nogous class of models for classiﬁcation.\nIt might appear, therefore, that such linear models constitute a general purpose\nframework for solving problems in pattern recognition. Unfortunately, there are\nsome signiﬁcant shortcomings with linear models, which will cause us to turn in\nlater chapters to more complex models such as support vector machines and neural\nnetworks.\nThe difﬁculty stems from the assumption that the basis functionsφj(x) are ﬁxed\nbefore the training data set is observed and is a manifestation of the curse of dimen-\nsionality discussed in Section 1.4. As a consequence, the number of basis functions\nneeds to grow rapidly, often exponentially, with the dimensionality D of the input\nspace.\nFortunately, there are two properties of real data sets that we can exploit to help\nalleviate this problem. First of all, the data vectors {xn} typically lie close to a non-\nlinear manifold whose intrinsic dimensionality is smaller than that of the input space\nas a result of strong correlations between the input variables. We will see an example\nof this when we consider images of handwritten digits in Chapter 12. If we are using\nlocalized basis functions, we can arrange that they are scattered in input space only\nin regions containing data. This approach is used in radial basis function networks\nand also in support vector and relevance vector machines. Neural network models,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 192,
      "page_label": "173"
    }
  },
  {
    "page_content": "and also in support vector and relevance vector machines. Neural network models,\nwhich use adaptive basis functions having sigmoidal nonlinearities, can adapt the\nparameters so that the regions of input space over which the basis functions vary\ncorresponds to the data manifold. The second property is that target variables may\nhave signiﬁcant dependence on only a small number of possible directions within the\ndata manifold. Neural networks can exploit this property by choosing the directions\nin input space to which the basis functions respond.\nExercises\n3.1 (⋆) www Show that the ‘ tanh’ function and the logistic sigmoid function (3.6)\nare related by\ntanh(a)=2 σ(2a) − 1. (3.100)\nHence show that a general linear combination of logistic sigmoid functions of the\nform\ny(x,w)= w0 +\nM∑\nj=1\nwjσ\n(x − µj\ns\n)\n(3.101)\nis equivalent to a linear combination of ‘tanh’ functions of the form\ny(x,u)= u0 +\nM∑\nj=1\nuj tanh\n(x − µj\ns\n)\n(3.102)\nand ﬁnd expressions to relate the new parameters {u1,...,u M } to the original pa-\nrameters {w1,...,w M }.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 192,
      "page_label": "173"
    }
  },
  {
    "page_content": "174 3. LINEAR MODELS FOR REGRESSION\n3.2 (⋆⋆ ) Show that the matrix\nΦ(ΦTΦ)−1ΦT (3.103)\ntakes any vector v and projects it onto the space spanned by the columns of Φ. Use\nthis result to show that the least-squares solution (3.15) corresponds to an orthogonal\nprojection of the vectort onto the manifold S as shown in Figure 3.2.\n3.3 (⋆) Consider a data set in which each data point tn is associated with a weighting\nfactor rn > 0, so that the sum-of-squares error function becomes\nED(w)= 1\n2\nN∑\nn=1\nrn\n{\ntn − wTφ(xn)\n}2\n. (3.104)\nFind an expression for the solution w⋆ that minimizes this error function. Give two\nalternative interpretations of the weighted sum-of-squares error function in terms of\n(i) data dependent noise variance and (ii) replicated data points.\n3.4 (⋆) www Consider a linear model of the form\ny(x,w)= w0 +\nD∑\ni=1\nwixi (3.105)\ntogether with a sum-of-squares error function of the form\nED(w)= 1\n2\nN∑\nn=1\n{y(xn, w) − tn}2 . (3.106)\nNow suppose that Gaussian noise ϵi with zero mean and variance σ2 is added in-\ndependently to each of the input variables xi. By making use of E[ϵi]=0 and\nE[ϵiϵj]= δijσ2, show that minimizing ED averaged over the noise distribution is\nequivalent to minimizing the sum-of-squares error for noise-free input variables with\nthe addition of a weight-decay regularization term, in which the bias parameter w0\nis omitted from the regularizer.\n3.5 (⋆) www Using the technique of Lagrange multipliers, discussed in Appendix E,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 193,
      "page_label": "174"
    }
  },
  {
    "page_content": "is omitted from the regularizer.\n3.5 (⋆) www Using the technique of Lagrange multipliers, discussed in Appendix E,\nshow that minimization of the regularized error function (3.29) is equivalent to mini-\nmizing the unregularized sum-of-squares error (3.12) subject to the constraint (3.30).\nDiscuss the relationship between the parameters η and λ.\n3.6 (⋆) www Consider a linear basis function regression model for a multivariate\ntarget variable t having a Gaussian distribution of the form\np(t|W, Σ)= N(t|y(x,W), Σ) (3.107)\nwhere\ny(x,W)= WTφ(x) (3.108)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 193,
      "page_label": "174"
    }
  },
  {
    "page_content": "Exercises 175\ntogether with a training data set comprising input basis vectors φ(xn) and corre-\nsponding target vectors tn, with n =1 ,...,N . Show that the maximum likelihood\nsolution WML for the parameter matrix W has the property that each column is\ngiven by an expression of the form (3.15), which was the solution for an isotropic\nnoise distribution. Note that this is independent of the covariance matrix Σ. Show\nthat the maximum likelihood solution for Σ is given by\nΣ = 1\nN\nN∑\nn=1\n(\ntn − WT\nMLφ(xn)\n)(\ntn − WT\nMLφ(xn)\n)T\n. (3.109)\n3.7 (⋆) By using the technique of completing the square, verify the result (3.49) for the\nposterior distribution of the parametersw in the linear basis function model in which\nmN and SN are deﬁned by (3.50) and (3.51) respectively.\n3.8 (⋆⋆ ) www Consider the linear basis function model in Section 3.1, and suppose\nthat we have already observed N data points, so that the posterior distribution over\nw is given by (3.49). This posterior can be regarded as the prior for the next obser-\nvation. By considering an additional data point (xN+1,t N+1), and by completing\nthe square in the exponential, show that the resulting posterior distribution is again\ngiven by (3.49) but withSN replaced by SN+1 and mN replaced by mN+1.\n3.9 (⋆⋆ ) Repeat the previous exercise but instead of completing the square by hand,\nmake use of the general result for linear-Gaussian models given by (2.116).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 194,
      "page_label": "175"
    }
  },
  {
    "page_content": "make use of the general result for linear-Gaussian models given by (2.116).\n3.10 (⋆⋆ ) www By making use of the result (2.115) to evaluate the integral in (3.57),\nverify that the predictive distribution for the Bayesian linear regression model is\ngiven by (3.58) in which the input-dependent variance is given by (3.59).\n3.11 (⋆⋆ ) We have seen that, as the size of a data set increases, the uncertainty associated\nwith the posterior distribution over model parameters decreases. Make use of the\nmatrix identity (Appendix C)\n(\nM + vvT)−1\n= M−1 − (M−1v)\n(\nvTM−1)\n1+ vTM−1v (3.110)\nto show that the uncertainty σ2\nN (x) associated with the linear regression function\ngiven by (3.59) satisﬁes\nσ2\nN+1(x) ⩽ σ2\nN (x). (3.111)\n3.12 (⋆⋆ ) We saw in Section 2.3.6 that the conjugate prior for a Gaussian distribution\nwith unknown mean and unknown precision (inverse variance) is a normal-gamma\ndistribution. This property also holds for the case of the conditional Gaussian dis-\ntribution p(t|x, w,β ) of the linear regression model. If we consider the likelihood\nfunction (3.10), then the conjugate prior for w and β is given by\np(w,β )= N(w|m0,β −1S0)Gam(β|a0,b 0). (3.112)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 194,
      "page_label": "175"
    }
  },
  {
    "page_content": "176 3. LINEAR MODELS FOR REGRESSION\nShow that the corresponding posterior distribution takes the same functional form,\nso that\np(w,β |t)= N(w|mN ,β −1SN )Gam(β|aN ,b N ) (3.113)\nand ﬁnd expressions for the posterior parameters mN , SN , aN , and bN .\n3.13 (⋆⋆ ) Show that the predictive distribution p(t|x, t) for the model discussed in Ex-\nercise 3.12 is given by a Student’s t-distribution of the form\np(t|x, t)=S t (t|µ, λ, ν) (3.114)\nand obtain expressions for µ, λ and ν.\n3.14 (⋆⋆ ) In this exercise, we explore in more detail the properties of the equivalent\nkernel deﬁned by (3.62), where SN is deﬁned by (3.54). Suppose that the basis\nfunctions φj(x) are linearly independent and that the number N of data points is\ngreater than the number M of basis functions. Furthermore, let one of the basis\nfunctions be constant, say φ0(x)=1 . By taking suitable linear combinations of\nthese basis functions, we can construct a new basis set ψj(x) spanning the same\nspace but that are orthonormal, so that\nN∑\nn=1\nψj(xn)ψk(xn)= Ijk (3.115)\nwhere Ijk is deﬁned to be 1 if j = k and 0 otherwise, and we take ψ0(x)=1 . Show\nthat for α =0 , the equivalent kernel can be written as k(x,x′)= ψ(x)Tψ(x′)\nwhere ψ =( ψ1,...,ψ M )T. Use this result to show that the kernel satisﬁes the\nsummation constraint\nN∑\nn=1\nk(x,xn)=1 . (3.116)\n3.15 (⋆) www Consider a linear basis function model for regression in which the pa-\nrameters α and β are set using the evidence framework. Show that the function",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 195,
      "page_label": "176"
    }
  },
  {
    "page_content": "rameters α and β are set using the evidence framework. Show that the function\nE(mN ) deﬁned by (3.82) satisﬁes the relation 2E(mN )= N.\n3.16 (⋆⋆ ) Derive the result (3.86) for the log evidence function p(t|α, β) of the linear\nregression model by making use of (2.115) to evaluate the integral (3.77) directly.\n3.17 (⋆) Show that the evidence function for the Bayesian linear regression model can\nbe written in the form (3.78) in which E(w) is deﬁned by (3.79).\n3.18 (⋆⋆ ) www By completing the square over w, show that the error function (3.79)\nin Bayesian linear regression can be written in the form (3.80).\n3.19 (⋆⋆ ) Show that the integration overw in the Bayesian linear regression model gives\nthe result (3.85). Hence show that the log marginal likelihood is given by (3.86).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 195,
      "page_label": "176"
    }
  },
  {
    "page_content": "Exercises 177\n3.20 (⋆⋆ ) www Starting from (3.86) verify all of the steps needed to show that maxi-\nmization of the log marginal likelihood function (3.86) with respect toα leads to the\nre-estimation equation (3.92).\n3.21 (⋆⋆ ) An alternative way to derive the result (3.92) for the optimal value of α in the\nevidence framework is to make use of the identity\nd\ndα ln|A| = Tr\n(\nA−1 d\ndαA\n)\n. (3.117)\nProve this identity by considering the eigenvalue expansion of a real, symmetric\nmatrix A, and making use of the standard results for the determinant and trace of\nA expressed in terms of its eigenvalues (Appendix C). Then make use of (3.117) to\nderive (3.92) starting from (3.86).\n3.22 (⋆⋆ ) Starting from (3.86) verify all of the steps needed to show that maximiza-\ntion of the log marginal likelihood function (3.86) with respect to β leads to the\nre-estimation equation (3.95).\n3.23 (⋆⋆ ) www Show that the marginal probability of the data, in other words the\nmodel evidence, for the model described in Exercise 3.12 is given by\np(t)= 1\n(2π)N/2\nba0\n0\nbaN\nN\nΓ(aN )\nΓ(a0)\n|SN |1/2\n|S0|1/2 (3.118)\nby ﬁrst marginalizing with respect to w and then with respect to β.\n3.24 (⋆⋆ ) Repeat the previous exercise but now use Bayes’ theorem in the form\np(t)= p(t|w,β )p(w,β )\np(w,β |t) (3.119)\nand then substitute for the prior and posterior distributions and the likelihood func-\ntion in order to derive the result (3.118).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 196,
      "page_label": "177"
    }
  },
  {
    "page_content": "4\nLinear\nModels for\nClassiﬁcation\nIn the previous chapter, we explored a class of regression models having particularly\nsimple analytical and computational properties. We now discuss an analogous class\nof models for solving classiﬁcation problems. The goal in classiﬁcation is to take an\ninput vector x and to assign it to one of K discrete classes Ck where k =1 ,...,K .\nIn the most common scenario, the classes are taken to be disjoint, so that each input is\nassigned to one and only one class. The input space is thereby divided intodecision\nregions whose boundaries are called decision boundaries or decision surfaces.I n\nthis chapter, we consider linear models for classiﬁcation, by which we mean that the\ndecision surfaces are linear functions of the input vectorx and hence are deﬁned\nby (D − 1)-dimensional hyperplanes within the D-dimensional input space. Data\nsets whose classes can be separated exactly by linear decision surfaces are said to be\nlinearly separable.\nFor regression problems, the target variablet was simply the vector of real num-\nbers whose values we wish to predict. In the case of classiﬁcation, there are various\n179",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 198,
      "page_label": "179"
    }
  },
  {
    "page_content": "180 4. LINEAR MODELS FOR CLASSIFICATION\nways of using target values to represent class labels. For probabilistic models, the\nmost convenient, in the case of two-class problems, is the binary representation in\nwhich there is a single target variablet ∈{ 0, 1} such that t =1 represents class C1\nand t =0 represents class C2. We can interpret the value of t as the probability that\nthe class is C1, with the values of probability taking only the extreme values of 0 and\n1. For K> 2 classes, it is convenient to use a 1-of-K coding scheme in which t is\na vector of length K such that if the class is Cj, then all elements tk of t are zero\nexcept element tj, which takes the value 1. For instance, if we have K =5 classes,\nthen a pattern from class 2 would be given the target vector\nt =( 0, 1, 0, 0, 0)T. (4.1)\nAgain, we can interpret the value of tk as the probability that the class is Ck.F o r\nnonprobabilistic models, alternative choices of target variable representation will\nsometimes prove convenient.\nIn Chapter 1, we identiﬁed three distinct approaches to the classiﬁcation prob-\nlem. The simplest involves constructing a discriminant functionthat directly assigns\neach vector x to a speciﬁc class. A more powerful approach, however, models the\nconditional probability distribution p(Ck|x) in an inference stage, and then subse-\nquently uses this distribution to make optimal decisions. By separating inference\nand decision, we gain numerous beneﬁts, as discussed in Section 1.5.4. There are",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 199,
      "page_label": "180"
    }
  },
  {
    "page_content": "and decision, we gain numerous beneﬁts, as discussed in Section 1.5.4. There are\ntwo different approaches to determining the conditional probabilities p(Ck|x). One\ntechnique is to model them directly, for example by representing them as parametric\nmodels and then optimizing the parameters using a training set. Alternatively, we\ncan adopt a generative approach in which we model the class-conditional densities\ngiven by p(x|Ck), together with the prior probabilitiesp(Ck) for the classes, and then\nwe compute the required posterior probabilities using Bayes’ theorem\np(Ck|x)= p(x|Ck)p(Ck)\np(x) . (4.2)\nWe shall discuss examples of all three approaches in this chapter.\nIn the linear regression models considered in Chapter 3, the model prediction\ny(x,w) was given by a linear function of the parameters w. In the simplest case,\nthe model is also linear in the input variables and therefore takes the form y(x)=\nwTx+w0, so that y is a real number. For classiﬁcation problems, however, we wish\nto predict discrete class labels, or more generally posterior probabilities that lie in\nthe range(0, 1). To achieve this, we consider a generalization of this model in which\nwe transform the linear function of w using a nonlinear function f( ·) so that\ny(x)= f\n(\nwTx + w0\n)\n. (4.3)\nIn the machine learning literature f( ·) is known as an activation function, whereas\nits inverse is called a link function in the statistics literature. The decision surfaces",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 199,
      "page_label": "180"
    }
  },
  {
    "page_content": "its inverse is called a link function in the statistics literature. The decision surfaces\ncorrespond to y(x) = constant, so that wTx + w0 = constant and hence the deci-\nsion surfaces are linear functions of x, even if the function f(·) is nonlinear. For this\nreason, the class of models described by (4.3) are called generalized linear models",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 199,
      "page_label": "180"
    }
  },
  {
    "page_content": "4.1. Discriminant Functions 181\n(McCullagh and Nelder, 1989). Note, however, that in contrast to the models used\nfor regression, they are no longer linear in the parameters due to the presence of the\nnonlinear functionf(·). This will lead to more complex analytical and computa-\ntional properties than for linear regression models. Nevertheless, these models are\nstill relatively simple compared to the more general nonlinear models that will be\nstudied in subsequent chapters.\nThe algorithms discussed in this chapter will be equally applicable if we ﬁrst\nmake a ﬁxed nonlinear transformation of the input variables using a vector of basis\nfunctionsφ(x) as we did for regression models in Chapter 3. We begin by consider-\ning classiﬁcation directly in the original input space x, while in Section 4.3 we shall\nﬁnd it convenient to switch to a notation involving basis functions for consistency\nwith later chapters.\n4.1. Discriminant Functions\nA discriminant is a function that takes an input vector x and assigns it to one of K\nclasses, denoted Ck. In this chapter, we shall restrict attention tolinear discriminants,\nnamely those for which the decision surfaces are hyperplanes. To simplify the dis-\ncussion, we consider ﬁrst the case of two classes and then investigate the extension\nto K> 2 classes.\n4.1.1 Two classes\nThe simplest representation of a linear discriminant function is obtained by tak-\ning a linear function of the input vector so that\ny(x)= wTx + w0 (4.4)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 200,
      "page_label": "181"
    }
  },
  {
    "page_content": "ing a linear function of the input vector so that\ny(x)= wTx + w0 (4.4)\nwhere w is called a weight vector, and w0 is a bias (not to be confused with bias in\nthe statistical sense). The negative of the bias is sometimes called a threshold.A n\ninput vector x is assigned to class C1 if y(x) ⩾ 0 and to class C2 otherwise. The cor-\nresponding decision boundary is therefore deﬁned by the relation y(x)=0 , which\ncorresponds to a (D − 1)-dimensional hyperplane within the D-dimensional input\nspace. Consider two points xA and xB both of which lie on the decision surface.\nBecause y(xA)= y(xB)=0 ,w eh a v ewT(xA −xB)=0 and hence the vector w is\northogonal to every vector lying within the decision surface, and sow determines the\norientation of the decision surface. Similarly, if x is a point on the decision surface,\nthen y(x)=0 , and so the normal distance from the origin to the decision surface is\ngiven by\nwTx\n∥w∥ = − w0\n∥w∥. (4.5)\nWe therefore see that the bias parameter w0 determines the location of the decision\nsurface. These properties are illustrated for the case of D =2 in Figure 4.1.\nFurthermore, we note that the value of y(x) gives a signed measure of the per-\npendicular distance r of the point x from the decision surface. To see this, consider",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 200,
      "page_label": "181"
    }
  },
  {
    "page_content": "182 4. LINEAR MODELS FOR CLASSIFICATION\nFigure 4.1 Illustration of the geometry of a\nlinear discriminant function in two dimensions.\nThe decision surface, shown in red, is perpen-\ndicular to w, and its displacement from the\norigin is controlled by the bias parameter w0.\nAlso, the signed orthogonal distance of a gen-\neral point x from the decision surface is given\nby y(x)/∥w∥.\nx2\nx1\nw\nx\ny(x)\n∥w∥\nx⊥\n−w0\n∥w∥\ny =0\ny< 0\ny> 0\nR2\nR1\nan arbitrary point x and let x⊥ be its orthogonal projection onto the decision surface,\nso that\nx = x⊥ + r w\n∥w∥. (4.6)\nMultiplying both sides of this result bywT and adding w0, and making use ofy(x)=\nwTx + w0 and y(x⊥)= wTx⊥ + w0 =0 ,w eh a v e\nr = y(x)\n∥w∥. (4.7)\nThis result is illustrated in Figure 4.1.\nAs with the linear regression models in Chapter 3, it is sometimes convenient\nto use a more compact notation in which we introduce an additional dummy ‘input’\nvalue x0 =1 and then deﬁne ˜w =( w0, w) and ˜x =( x0, x) so that\ny(x)= ˜wT˜x. (4.8)\nIn this case, the decision surfaces are D-dimensional hyperplanes passing through\nthe origin of the D +1 -dimensional expanded input space.\n4.1.2 Multiple classes\nNow consider the extension of linear discriminants to K> 2 classes. We might\nbe tempted be to build a K-class discriminant by combining a number of two-class\ndiscriminant functions. However, this leads to some serious difﬁculties (Duda and\nHart, 1973) as we now show.\nConsider the use ofK−1 classiﬁers each of which solves a two-class problem of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 201,
      "page_label": "182"
    }
  },
  {
    "page_content": "Hart, 1973) as we now show.\nConsider the use ofK−1 classiﬁers each of which solves a two-class problem of\nseparating points in a particular class Ck from points not in that class. This is known\nas a one-versus-the-rest classiﬁer. The left-hand example in Figure 4.2 shows an",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 201,
      "page_label": "182"
    }
  },
  {
    "page_content": "4.1. Discriminant Functions 183\nR1\nR2\nR3\n?\nC1\nnot C1\nC2\nnot C2\nR1\nR2\nR3\n?C1\nC2\nC1\nC3\nC2\nC3\nFigure 4.2 Attempting to construct a K class discriminant from a set of two class discriminants leads to am-\nbiguous regions, shown in green. On the left is an example involving the use of two discriminants designed to\ndistinguish points in class Ck from points not in class Ck. On the right is an example involving three discriminant\nfunctions each of which is used to separate a pair of classes Ck and Cj.\nexample involving three classes where this approach leads to regions of input space\nthat are ambiguously classiﬁed.\nAn alternative is to introduce K(K − 1)/2 binary discriminant functions, one\nfor every possible pair of classes. This is known as a one-versus-one classiﬁer. Each\npoint is then classiﬁed according to a majority vote amongst the discriminant func-\ntions. However, this too runs into the problem of ambiguous regions, as illustrated\nin the right-hand diagram of Figure 4.2.\nWe can avoid these difﬁculties by considering a single K-class discriminant\ncomprising K linear functions of the form\nyk(x)= wT\nk x + wk0 (4.9)\nand then assigning a point x to class Ck if yk(x) >y j(x) for all j ̸= k. The decision\nboundary between class Ck and class Cj is therefore given by yk(x)= yj(x) and\nhence corresponds to a (D − 1)-dimensional hyperplane deﬁned by\n(wk − wj)Tx +( wk0 − wj0)=0 . (4.10)\nThis has the same form as the decision boundary for the two-class case discussed in",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 202,
      "page_label": "183"
    }
  },
  {
    "page_content": "(wk − wj)Tx +( wk0 − wj0)=0 . (4.10)\nThis has the same form as the decision boundary for the two-class case discussed in\nSection 4.1.1, and so analogous geometrical properties apply.\nThe decision regions of such a discriminant are always singly connected and\nconvex. To see this, consider two pointsxA and xB both of which lie inside decision\nregion Rk, as illustrated in Figure 4.3. Any point ˆx that lies on the line connecting\nxA and xB can be expressed in the form\nˆx = λxA +( 1− λ)xB (4.11)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 202,
      "page_label": "183"
    }
  },
  {
    "page_content": "184 4. LINEAR MODELS FOR CLASSIFICATION\nFigure 4.3 Illustration of the decision regions for a mul-\nticlass linear discriminant, with the decision\nboundaries shown in red. If two points xA\nand xB both lie inside the same decision re-\ngion Rk, then any point bx that lies on the line\nconnecting these two points must also lie in\nRk, and hence the decision region must be\nsingly connected and convex.\nRi\nRj\nRk\nxA\nxB\nˆx\nwhere 0 ⩽ λ ⩽ 1. From the linearity of the discriminant functions, it follows that\nyk(ˆx)= λyk(xA)+( 1 − λ)yk(xB). (4.12)\nBecause both xA and xB lie inside Rk, it follows that yk(xA) >y j(xA), and\nyk(xB) >y j(xB), for all j ̸= k, and hence yk(ˆx) >y j(ˆx), and so ˆx also lies\ninside Rk. Thus Rk is singly connected and convex.\nNote that for two classes, we can either employ the formalism discussed here,\nbased on two discriminant functions y1(x) and y2(x), or else use the simpler but\nequivalent formulation described in Section 4.1.1 based on a single discriminant\nfunction y(x).\nWe now explore three approaches to learning the parameters of linear discrimi-\nnant functions, based on least squares, Fisher’s linear discriminant, and the percep-\ntron algorithm.\n4.1.3 Least squares for classiﬁcation\nIn Chapter 3, we considered models that were linear functions of the parame-\nters, and we saw that the minimization of a sum-of-squares error function led to a\nsimple closed-form solution for the parameter values. It is therefore tempting to see",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 203,
      "page_label": "184"
    }
  },
  {
    "page_content": "simple closed-form solution for the parameter values. It is therefore tempting to see\nif we can apply the same formalism to classiﬁcation problems. Consider a general\nclassiﬁcation problem with K classes, with a 1-of-K binary coding scheme for the\ntarget vector t. One justiﬁcation for using least squares in such a context is that it\napproximates the conditional expectation E[t|x] of the target values given the input\nvector. For the binary coding scheme, this conditional expectation is given by the\nvector of posterior class probabilities. Unfortunately, however, these probabilities\nare typically approximated rather poorly, indeed the approximations can have values\noutside the range (0, 1), due to the limited ﬂexibility of a linear model as we shall\nsee shortly.\nEach class Ck is described by its own linear model so that\nyk(x)= wT\nk x + wk0 (4.13)\nwhere k =1 ,...,K . We can conveniently group these together using vector nota-\ntion so that\ny(x)= ˜WT˜x (4.14)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 203,
      "page_label": "184"
    }
  },
  {
    "page_content": "4.1. Discriminant Functions 185\nwhere ˜W is a matrix whose kth column comprises the D +1 -dimensional vector\n˜wk =( wk0, wT\nk )T and ˜x is the corresponding augmented input vector(1,xT)T with\na dummy input x0 =1 . This representation was discussed in detail in Section 3.1. A\nnew input x is then assigned to the class for which the output yk = ˜wT\nk ˜x is largest.\nWe now determine the parameter matrix ˜W by minimizing a sum-of-squares\nerror function, as we did for regression in Chapter 3. Consider a training data set\n{xn, tn} where n =1 ,...,N , and deﬁne a matrixT whose nth row is the vectortT\nn,\ntogether with a matrix ˜X whose nth row is ˜xT\nn. The sum-of-squares error function\ncan then be written as\nED(˜W)= 1\n2Tr\n{\n(˜X˜W − T)T(˜X˜W − T)\n}\n. (4.15)\nSetting the derivative with respect to ˜W to zero, and rearranging, we then obtain the\nsolution for ˜W in the form\n˜W =( ˜XT ˜X)−1 ˜XTT = ˜X†T (4.16)\nwhere ˜X† is the pseudo-inverse of the matrix ˜X, as discussed in Section 3.1.1. We\nthen obtain the discriminant function in the form\ny(x)= ˜WT˜x = TT\n(\n˜X†\n)T\n˜x. (4.17)\nAn interesting property of least-squares solutions with multiple target variables\nis that if every target vector in the training set satisﬁes some linear constraint\naTtn + b =0 (4.18)\nfor some constants a and b, then the model prediction for any value of x will satisfy\nthe same constraint so thatExercise 4.2\naTy(x)+ b =0 . (4.19)\nThus if we use a 1-of- K coding scheme for K classes, then the predictions made",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 204,
      "page_label": "185"
    }
  },
  {
    "page_content": "the same constraint so thatExercise 4.2\naTy(x)+ b =0 . (4.19)\nThus if we use a 1-of- K coding scheme for K classes, then the predictions made\nby the model will have the property that the elements of y(x) will sum to 1 for any\nvalue of x. However, this summation constraint alone is not sufﬁcient to allow the\nmodel outputs to be interpreted as probabilities because they are not constrained to\nlie within the interval (0, 1).\nThe least-squares approach gives an exact closed-form solution for the discrimi-\nnant function parameters. However, even as a discriminant function (where we use it\nto make decisions directly and dispense with any probabilistic interpretation) it suf-\nfers from some severe problems. We have already seen that least-squares solutionsSection 2.3.7\nlack robustness to outliers, and this applies equally to the classiﬁcation application,\nas illustrated in Figure 4.4. Here we see that the additional data points in the right-\nhand ﬁgure produce a signiﬁcant change in the location of the decision boundary,\neven though these point would be correctly classiﬁed by the original decision bound-\nary in the left-hand ﬁgure. The sum-of-squares error function penalizes predictions\nthat are ‘too correct’ in that they lie a long way on the correct side of the decision",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 204,
      "page_label": "185"
    }
  },
  {
    "page_content": "186 4. LINEAR MODELS FOR CLASSIFICATION\n−4 −2 0 2 4 6 8\n−8\n−6\n−4\n−2\n0\n2\n4\n−4 −2 0 2 4 6 8\n−8\n−6\n−4\n−2\n0\n2\n4\nFigure 4.4 The left plot shows data from two classes, denoted by red crosses and blue circles, together with\nthe decision boundary found by least squares (magenta curve) and also by the logistic regression model (green\ncurve), which is discussed later in Section 4.3.2. The right-hand plot shows the corresponding results obtained\nwhen extra data points are added at the bottom left of the diagram, showing that least squares is highly sensitive\nto outliers, unlike logistic regression.\nboundary. In Section 7.1.2, we shall consider several alternative error functions for\nclassiﬁcation and we shall see that they do not suffer from this difﬁculty.\nHowever, problems with least squares can be more severe than simply lack of\nrobustness, as illustrated in Figure 4.5. This shows a synthetic data set drawn from\nthree classes in a two-dimensional input space (x1,x2), having the property that lin-\near decision boundaries can give excellent separation between the classes. Indeed,\nthe technique of logistic regression, described later in this chapter, gives a satisfac-\ntory solution as seen in the right-hand plot. However, the least-squares solution gives\npoor results, with only a small region of the input space assigned to the green class.\nThe failure of least squares should not surprise us when we recall that it cor-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 205,
      "page_label": "186"
    }
  },
  {
    "page_content": "The failure of least squares should not surprise us when we recall that it cor-\nresponds to maximum likelihood under the assumption of a Gaussian conditional\ndistribution, whereas binary target vectors clearly have a distribution that is far from\nGaussian. By adopting more appropriate probabilistic models, we shall obtain clas-\nsiﬁcation techniques with much better properties than least squares. For the moment,\nhowever, we continue to explore alternative nonprobabilistic methods for setting the\nparameters in the linear classiﬁcation models.\n4.1.4 Fisher’s linear discriminant\nOne way to view a linear classiﬁcation model is in terms of dimensionality\nreduction. Consider ﬁrst the case of two classes, and suppose we take the D-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 205,
      "page_label": "186"
    }
  },
  {
    "page_content": "4.1. Discriminant Functions 187\n−6 −4 −2 0 2 4 6\n−6\n−4\n−2\n0\n2\n4\n6\n−6 −4 −2 0 2 4 6\n−6\n−4\n−2\n0\n2\n4\n6\nFigure 4.5 Example of a synthetic data set comprising three classes, with training data points denoted in red\n(×), green (+), and blue (◦). Lines denote the decision boundaries, and the background colours denote the\nrespective classes of the decision regions. On the left is the result of using a least-squares discriminant. We see\nthat the region of input space assigned to the green class is too small and so most of the points from this class\nare misclassiﬁed. On the right is the result of using logistic regressions as described in Section 4.3.2 showing\ncorrect classiﬁcation of the training data.\ndimensional input vector x and project it down to one dimension using\ny = wTx. (4.20)\nIf we place a threshold on y and classify y ⩾ −w0 as class C1, and otherwise class\nC2, then we obtain our standard linear classiﬁer discussed in the previous section.\nIn general, the projection onto one dimension leads to a considerable loss of infor-\nmation, and classes that are well separated in the original D-dimensional space may\nbecome strongly overlapping in one dimension. However, by adjusting the com-\nponents of the weight vector w, we can select a projection that maximizes the class\nseparation. To begin with, consider a two-class problem in which there areN1 points\nof class C1 and N2 points of class C2, so that the mean vectors of the two classes are\ngiven by\nm1 = 1\nN1\n∑\nn ∈C 1\nxn, m2 = 1\nN2\n∑",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 206,
      "page_label": "187"
    }
  },
  {
    "page_content": "of class C1 and N2 points of class C2, so that the mean vectors of the two classes are\ngiven by\nm1 = 1\nN1\n∑\nn ∈C 1\nxn, m2 = 1\nN2\n∑\nn ∈C 2\nxn. (4.21)\nThe simplest measure of the separation of the classes, when projected onto w,i st h e\nseparation of the projected class means. This suggests that we might choose w so as\nto maximize\nm2 − m1 = wT(m2 − m1) (4.22)\nwhere\nmk = wTmk (4.23)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 206,
      "page_label": "187"
    }
  },
  {
    "page_content": "188 4. LINEAR MODELS FOR CLASSIFICATION\n−2 2 6\n−2\n0\n2\n4\n−2 2 6\n−2\n0\n2\n4\nFigure 4.6 The left plot shows samples from two classes (depicted in red and blue) along with the histograms\nresulting from projection onto the line joining the class means. Note that there is considerable class overlap in\nthe projected space. The right plot shows the corresponding projection based on the Fisher linear discriminant,\nshowing the greatly improved class separation.\nis the mean of the projected data from class Ck. However, this expression can be\nmade arbitrarily large simply by increasing the magnitude of w. To solve this\nproblem, we could constrain w to have unit length, so that ∑\ni w2\ni =1 . Using\na Lagrange multiplier to perform the constrained maximization, we then ﬁnd thatAppendix E\nw ∝ (m2 −m1). There is still a problem with this approach, however, as illustratedExercise 4.4\nin Figure 4.6. This shows two classes that are well separated in the original two-\ndimensional space (x1,x2) but that have considerable overlap when projected onto\nthe line joining their means. This difﬁculty arises from the strongly nondiagonal\ncovariances of the class distributions. The idea proposed by Fisher is to maximize\na function that will give a large separation between the projected class means while\nalso giving a small variance within each class, thereby minimizing the class overlap.\nThe projection formula (4.20) transforms the set of labelled data points in x",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 207,
      "page_label": "188"
    }
  },
  {
    "page_content": "The projection formula (4.20) transforms the set of labelled data points in x\ninto a labelled set in the one-dimensional space y. The within-class variance of the\ntransformed data from class Ck is therefore given by\ns2\nk =\n∑\nn∈Ck\n(yn − mk)2 (4.24)\nwhere yn = wTxn. We can deﬁne the total within-class variance for the whole\ndata set to be simply s2\n1 + s2\n2. The Fisher criterion is deﬁned to be the ratio of the\nbetween-class variance to the within-class variance and is given by\nJ(w)= (m2 − m1)2\ns2\n1 + s2\n2\n. (4.25)\nWe can make the dependence on w explicit by using (4.20), (4.23), and (4.24) to\nrewrite the Fisher criterion in the formExercise 4.5",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 207,
      "page_label": "188"
    }
  },
  {
    "page_content": "4.1. Discriminant Functions 189\nJ(w)= wTSBw\nwTSWw (4.26)\nwhere SB is the between-class covariance matrix and is given by\nSB =( m2 − m1)(m2 − m1)T (4.27)\nand SW is the total within-class covariance matrix, given by\nSW =\n∑\nn∈C1\n(xn − m1)(xn − m1)T +\n∑\nn∈C2\n(xn − m2)(xn − m2)T. (4.28)\nDifferentiating (4.26) with respect to w, we ﬁnd that J(w) is maximized when\n(wTSBw)SWw =( wTSWw)SBw. (4.29)\nFrom (4.27), we see thatSBw is always in the direction of(m2 −m1). Furthermore,\nwe do not care about the magnitude of w, only its direction, and so we can drop the\nscalar factors (wTSBw) and (wTSWw). Multiplying both sides of (4.29) by S−1\nW\nwe then obtain\nw ∝ S−1\nW (m2 − m1). (4.30)\nNote that if the within-class covariance is isotropic, so thatSW is proportional to the\nunit matrix, we ﬁnd that w is proportional to the difference of the class means, as\ndiscussed above.\nThe result (4.30) is known as Fisher’s linear discriminant, although strictly it\nis not a discriminant but rather a speciﬁc choice of direction for projection of the\ndata down to one dimension. However, the projected data can subsequently be used\nto construct a discriminant, by choosing a threshold y0 so that we classify a new\npoint as belonging to C1 if y(x) ⩾ y0 and classify it as belonging to C2 otherwise.\nFor example, we can model the class-conditional densities p(y|Ck) using Gaussian\ndistributions and then use the techniques of Section 1.2.4 to ﬁnd the parameters",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 208,
      "page_label": "189"
    }
  },
  {
    "page_content": "distributions and then use the techniques of Section 1.2.4 to ﬁnd the parameters\nof the Gaussian distributions by maximum likelihood. Having found Gaussian ap-\nproximations to the projected classes, the formalism of Section 1.5.1 then gives an\nexpression for the optimal threshold. Some justiﬁcation for the Gaussian assumption\ncomes from the central limit theorem by noting thaty = wTx is the sum of a set of\nrandom variables.\n4.1.5 Relation to least squares\nThe least-squares approach to the determination of a linear discriminant was\nbased on the goal of making the model predictions as close as possible to a set of\ntarget values. By contrast, the Fisher criterion was derived by requiring maximum\nclass separation in the output space. It is interesting to see the relationship between\nthese two approaches. In particular, we shall show that, for the two-class problem,\nthe Fisher criterion can be obtained as a special case of least squares.\nSo far we have considered 1-of-K coding for the target values. If, however, we\nadopt a slightly different target coding scheme, then the least-squares solution for",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 208,
      "page_label": "189"
    }
  },
  {
    "page_content": "190 4. LINEAR MODELS FOR CLASSIFICATION\nthe weights becomes equivalent to the Fisher solution (Duda and Hart, 1973). In\nparticular, we shall take the targets for classC1 to be N/N1, where N1 is the number\nof patterns in class C1, and N is the total number of patterns. This target value\napproximates the reciprocal of the prior probability for class C1. For class C2,w e\nshall take the targets to be −N/N2, where N2 is the number of patterns in class C2.\nThe sum-of-squares error function can be written\nE = 1\n2\nN∑\nn=1\n(\nwTxn + w0 − tn\n)2\n. (4.31)\nSetting the derivatives ofE with respect to w0 and w to zero, we obtain respectively\nN∑\nn=1\n(\nwTxn + w0 − tn\n)\n=0 (4.32)\nN∑\nn=1\n(\nwTxn + w0 − tn\n)\nxn =0 . (4.33)\nFrom (4.32), and making use of our choice of target coding scheme for the tn,w e\nobtain an expression for the bias in the form\nw0 = −wTm (4.34)\nwhere we have used\nN∑\nn=1\ntn = N1\nN\nN1\n− N2\nN\nN2\n=0 (4.35)\nand where m is the mean of the total data set and is given by\nm = 1\nN\nN∑\nn=1\nxn = 1\nN (N1m1 + N2m2). (4.36)\nAfter some straightforward algebra, and again making use of the choice of tn, the\nsecond equation (4.33) becomesExercise 4.6\n(\nSW + N1N2\nN SB\n)\nw = N(m1 − m2) (4.37)\nwhere SW is deﬁned by (4.28), SB is deﬁned by (4.27), and we have substituted for\nthe bias using (4.34). Using (4.27), we note that SBw is always in the direction of\n(m2 − m1). Thus we can write\nw ∝ S−1\nW (m2 − m1) (4.38)\nwhere we have ignored irrelevant scale factors. Thus the weight vector coincides",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 209,
      "page_label": "190"
    }
  },
  {
    "page_content": "(m2 − m1). Thus we can write\nw ∝ S−1\nW (m2 − m1) (4.38)\nwhere we have ignored irrelevant scale factors. Thus the weight vector coincides\nwith that found from the Fisher criterion. In addition, we have also found an expres-\nsion for the bias valuew0 given by (4.34). This tells us that a new vectorx should be\nclassiﬁed as belonging to classC1 if y(x)= wT(x−m) > 0 and class C2 otherwise.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 209,
      "page_label": "190"
    }
  },
  {
    "page_content": "4.1. Discriminant Functions 191\n4.1.6 Fisher’s discriminant for multiple classes\nWe now consider the generalization of the Fisher discriminant toK> 2 classes,\nand we shall assume that the dimensionality D of the input space is greater than the\nnumber K of classes. Next, we introduce D′> 1 linear ‘features’ yk = wT\nk x, where\nk =1 ,...,D ′. These feature values can conveniently be grouped together to form\na vector y. Similarly, the weight vectors {wk} can be considered to be the columns\nof a matrix W, so that\ny = WTx. (4.39)\nNote that again we are not including any bias parameters in the deﬁnition of y. The\ngeneralization of the within-class covariance matrix to the case ofK classes follows\nfrom (4.28) to give\nSW =\nK∑\nk=1\nSk (4.40)\nwhere\nSk =\n∑\nn∈Ck\n(xn − mk)(xn − mk)T (4.41)\nmk = 1\nNk\n∑\nn∈Ck\nxn (4.42)\nand Nk is the number of patterns in class Ck. In order to ﬁnd a generalization of the\nbetween-class covariance matrix, we follow Duda and Hart (1973) and consider ﬁrst\nthe total covariance matrix\nST =\nN∑\nn=1\n(xn − m)(xn − m)T (4.43)\nwhere m is the mean of the total data set\nm = 1\nN\nN∑\nn=1\nxn = 1\nN\nK∑\nk=1\nNkmk (4.44)\nand N = ∑\nk Nk is the total number of data points. The total covariance matrix can\nbe decomposed into the sum of the within-class covariance matrix, given by (4.40)\nand (4.41), plus an additional matrix SB, which we identify as a measure of the\nbetween-class covariance\nST = SW + SB (4.45)\nwhere\nSB =\nK∑\nk=1\nNk(mk − m)(mk − m)T. (4.46)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 210,
      "page_label": "191"
    }
  },
  {
    "page_content": "192 4. LINEAR MODELS FOR CLASSIFICATION\nThese covariance matrices have been deﬁned in the original x-space. We can now\ndeﬁne similar matrices in the projected D′-dimensional y-space\nsW =\nK∑\nk=1\n∑\nn∈Ck\n(yn − µk)(yn − µk)T (4.47)\nand\nsB =\nK∑\nk=1\nNk(µk − µ)(µk − µ)T (4.48)\nwhere\nµk = 1\nNk\n∑\nn∈Ck\nyn, µ = 1\nN\nK∑\nk=1\nNkµk. (4.49)\nAgain we wish to construct a scalar that is large when the between-class covariance\nis large and when the within-class covariance is small. There are now many possible\nchoices of criterion (Fukunaga, 1990). One example is given by\nJ(W)= Tr\n{\ns−1\nW sB\n}\n. (4.50)\nThis criterion can then be rewritten as an explicit function of the projection matrix\nW in the form\nJ(w)= Tr\n{\n(WSWWT)−1(WSBWT)\n}\n. (4.51)\nMaximization of such criteria is straightforward, though somewhat involved, and is\ndiscussed at length in Fukunaga (1990). The weight values are determined by those\neigenvectors ofS−1\nW SB that correspond to the D′largest eigenvalues.\nThere is one important result that is common to all such criteria, which is worth\nemphasizing. We ﬁrst note from (4.46) that SB is composed of the sum of K ma-\ntrices, each of which is an outer product of two vectors and therefore of rank 1. In\naddition, only (K −1) of these matrices are independent as a result of the constraint\n(4.44). Thus, SB has rank at most equal to (K −1) and so there are at most (K −1)\nnonzero eigenvalues. This shows that the projection onto the (K − 1)-dimensional",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 211,
      "page_label": "192"
    }
  },
  {
    "page_content": "nonzero eigenvalues. This shows that the projection onto the (K − 1)-dimensional\nsubspace spanned by the eigenvectors of SB does not alter the value of J(w), and\nso we are therefore unable to ﬁnd more than (K − 1) linear ‘features’ by this means\n(Fukunaga, 1990).\n4.1.7 The perceptron algorithm\nAnother example of a linear discriminant model is the perceptron of Rosenblatt\n(1962), which occupies an important place in the history of pattern recognition al-\ngorithms. It corresponds to a two-class model in which the input vector x is ﬁrst\ntransformed using a ﬁxed nonlinear transformation to give a feature vector φ(x),\nand this is then used to construct a generalized linear model of the form\ny(x)= f\n(\nwTφ(x)\n)\n(4.52)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 211,
      "page_label": "192"
    }
  },
  {
    "page_content": "4.1. Discriminant Functions 193\nwhere the nonlinear activation function f(·) is given by a step function of the form\nf(a)=\n{\n+1,a ⩾ 0\n−1,a < 0. (4.53)\nThe vector φ(x) will typically include a bias component φ0(x)=1 . In earlier\ndiscussions of two-class classiﬁcation problems, we have focussed on a target coding\nscheme in which t ∈{ 0, 1}, which is appropriate in the context of probabilistic\nmodels. For the perceptron, however, it is more convenient to use target values\nt =+ 1 for class C1 and t = −1 for class C2, which matches the choice of activation\nfunction.\nThe algorithm used to determine the parameters w of the perceptron can most\neasily be motivated by error function minimization. A natural choice of error func-\ntion would be the total number of misclassiﬁed patterns. However, this does not lead\nto a simple learning algorithm because the error is a piecewise constant function\nof w, with discontinuities wherever a change in w causes the decision boundary to\nmove across one of the data points. Methods based on changing w using the gradi-\nent of the error function cannot then be applied, because the gradient is zero almost\neverywhere.\nWe therefore consider an alternative error function known as theperceptron cri-\nterion. To derive this, we note that we are seeking a weight vector w such that\npatterns xn in class C1 will have wTφ(xn) > 0, whereas patterns xn in class C2\nhave wTφ(xn) < 0. Using the t ∈{ −1, +1} target coding scheme it follows that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 212,
      "page_label": "193"
    }
  },
  {
    "page_content": "have wTφ(xn) < 0. Using the t ∈{ −1, +1} target coding scheme it follows that\nwe would like all patterns to satisfy wTφ(xn)tn > 0. The perceptron criterion\nassociates zero error with any pattern that is correctly classiﬁed, whereas for a mis-\nclassiﬁed pattern xn it tries to minimize the quantity −wTφ(xn)tn. The perceptron\ncriterion is therefore given by\nEP(w)= −\n∑\nn∈M\nwTφntn (4.54)\nFrank Rosenblatt\n1928–1969\nRosenblatt’s perceptron played an\nimportant role in the history of ma-\nchine learning. Initially, Rosenblatt\nsimulated the perceptron on an IBM\n704 computer at Cornell in 1957,\nbut by the early 1960s he had built\nspecial-purpose hardware that provided a direct, par-\nallel implementation of perceptron learning. Many of\nhis ideas were encapsulated in “Principles of Neuro-\ndynamics: Perceptrons and the Theory of Brain Mech-\nanisms” published in 1962. Rosenblatt’s work was\ncriticized by Marvin Minksy, whose objections were\npublished in the book “Perceptrons”, co-authored with\nSeymour Papert. This book was widely misinter-\npreted at the time as showing that neural networks\nwere fatally ﬂawed and could only learn solutions for\nlinearly separable problems. In fact, it only proved\nsuch limitations in the case of single-layer networks\nsuch as the perceptron and merely conjectured (in-\ncorrectly) that they applied to more general network\nmodels. Unfortunately, however, this book contributed\nto the substantial decline in research funding for neu-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 212,
      "page_label": "193"
    }
  },
  {
    "page_content": "models. Unfortunately, however, this book contributed\nto the substantial decline in research funding for neu-\nral computing, a situation that was not reversed un-\ntil the mid-1980s. Today, there are many hundreds,\nif not thousands, of applications of neural networks\nin widespread use, with examples in areas such as\nhandwriting recognition and information retrieval be-\ning used routinely by millions of people.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 212,
      "page_label": "193"
    }
  },
  {
    "page_content": "194 4. LINEAR MODELS FOR CLASSIFICATION\nwhere M denotes the set of all misclassiﬁed patterns. The contribution to the error\nassociated with a particular misclassiﬁed pattern is a linear function of w in regions\nof w space where the pattern is misclassiﬁed and zero in regions where it is correctly\nclassiﬁed. The total error function is therefore piecewise linear.\nWe now apply the stochastic gradient descent algorithm to this error function.Section 3.1.3\nThe change in the weight vector w is then given by\nw(τ+1) = w(τ) − η∇EP(w)= w(τ) + ηφntn (4.55)\nwhere η is the learning rate parameter and τ is an integer that indexes the steps of\nthe algorithm. Because the perceptron function y(x,w) is unchanged if we multiply\nw by a constant, we can set the learning rate parameter η equal to 1 without of\ngenerality. Note that, as the weight vector evolves during training, the set of patterns\nthat are misclassiﬁed will change.\nThe perceptron learning algorithm has a simple interpretation, as follows. We\ncycle through the training patterns in turn, and for each pattern xn we evaluate the\nperceptron function (4.52). If the pattern is correctly classiﬁed, then the weight\nvector remains unchanged, whereas if it is incorrectly classiﬁed, then for class C1\nwe add the vector φ(xn) onto the current estimate of weight vector w while for\nclass C2 we subtract the vector φ(xn) from w. The perceptron learning algorithm is\nillustrated in Figure 4.7.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 213,
      "page_label": "194"
    }
  },
  {
    "page_content": "class C2 we subtract the vector φ(xn) from w. The perceptron learning algorithm is\nillustrated in Figure 4.7.\nIf we consider the effect of a single update in the perceptron learning algorithm,\nwe see that the contribution to the error from a misclassiﬁed pattern will be reduced\nbecause from (4.55) we have\n−w(τ+1)Tφntn = −w(τ)Tφntn − (φntn)Tφntn < −w(τ)Tφntn (4.56)\nwhere we have set η =1 , and made use of ∥φntn∥2 > 0. Of course, this does\nnot imply that the contribution to the error function from the other misclassiﬁed\npatterns will have been reduced. Furthermore, the change in weight vector may have\ncaused some previously correctly classiﬁed patterns to become misclassiﬁed. Thus\nthe perceptron learning rule is not guaranteed to reduce the total error function at\neach stage.\nHowever, the perceptron convergence theoremstates that if there exists an ex-\nact solution (in other words, if the training data set is linearly separable), then the\nperceptron learning algorithm is guaranteed to ﬁnd an exact solution in a ﬁnite num-\nber of steps. Proofs of this theorem can be found for example in Rosenblatt (1962),\nBlock (1962), Nilsson (1965), Minsky and Papert (1969), Hertz et al. (1991), and\nBishop (1995a). Note, however, that the number of steps required to achieve con-\nvergence could still be substantial, and in practice, until convergence is achieved,\nwe will not be able to distinguish between a nonseparable problem and one that is\nsimply slow to converge.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 213,
      "page_label": "194"
    }
  },
  {
    "page_content": "we will not be able to distinguish between a nonseparable problem and one that is\nsimply slow to converge.\nEven when the data set is linearly separable, there may be many solutions, and\nwhich one is found will depend on the initialization of the parameters and on the or-\nder of presentation of the data points. Furthermore, for data sets that are not linearly\nseparable, the perceptron learning algorithm will never converge.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 213,
      "page_label": "194"
    }
  },
  {
    "page_content": "4.1. Discriminant Functions 195\n−1 −0.5 0 0.5 1\n−1\n−0.5\n0\n0.5\n1\n−1 −0.5 0 0.5 1\n−1\n−0.5\n0\n0.5\n1\n−1 −0.5 0 0.5 1\n−1\n−0.5\n0\n0.5\n1\n−1 −0.5 0 0.5 1\n−1\n−0.5\n0\n0.5\n1\nFigure 4.7 Illustration of the convergence of the perceptron learning algorithm, showing data points from two\nclasses (red and blue) in a two-dimensional feature space (φ1,φ 2). The top left plot shows the initial parameter\nvector w shown as a black arrow together with the corresponding decision boundary (black line), in which the\narrow points towards the decision region which classiﬁed as belonging to the red class. The data point circled\nin green is misclassiﬁed and so its feature vector is added to the current weight vector, giving the new decision\nboundary shown in the top right plot. The bottom left plot shows the next misclassiﬁed point to be considered,\nindicated by the green circle, and its feature vector is again added to the weight vector giving the decision\nboundary shown in the bottom right plot for which all data points are correctly classiﬁed.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 214,
      "page_label": "195"
    }
  },
  {
    "page_content": "196 4. LINEAR MODELS FOR CLASSIFICATION\nFigure 4.8 Illustration of the Mark 1 perceptron hardware. The photograph on the left shows how the inputs\nwere obtained using a simple camera system in which an input scene, in this case a printed character, was\nilluminated by powerful lights, and an image focussed onto a20 × 20 array of cadmium sulphide photocells,\ngiving a primitive 400 pixel image. The perceptron also had a patch board, shown in the middle photograph,\nwhich allowed different conﬁgurations of input features to be tried. Often these were wired up at random to\ndemonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern\ndigital computer. The photograph on the right shows one of the racks of adaptive weights. Each weight was\nimplemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby\nallowing the value of the weight to be adjusted automatically by the learning algorithm.\nAside from difﬁculties with the learning algorithm, the perceptron does not pro-\nvide probabilistic outputs, nor does it generalize readily to K> 2 classes. The most\nimportant limitation, however, arises from the fact that (in common with all of the\nmodels discussed in this chapter and the previous one) it is based on linear com-\nbinations of ﬁxed basis functions. More detailed discussions of the limitations of\nperceptrons can be found in Minsky and Papert (1969) and Bishop (1995a).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 215,
      "page_label": "196"
    }
  },
  {
    "page_content": "perceptrons can be found in Minsky and Papert (1969) and Bishop (1995a).\nAnalogue hardware implementations of the perceptron were built by Rosenblatt,\nbased on motor-driven variable resistors to implement the adaptive parameters wj.\nThese are illustrated in Figure 4.8. The inputs were obtained from a simple camera\nsystem based on an array of photo-sensors, while the basis functions φ could be\nchosen in a variety of ways, for example based on simple ﬁxed functions of randomly\nchosen subsets of pixels from the input image. Typical applications involved learning\nto discriminate simple shapes or characters.\nAt the same time that the perceptron was being developed, a closely related\nsystem called the adaline, which is short for ‘adaptive linear element’, was being\nexplored by Widrow and co-workers. The functional form of the model was the same\nas for the perceptron, but a different approach to training was adopted (Widrow and\nHoff, 1960; Widrow and Lehr, 1990).\n4.2. Probabilistic Generative Models\nWe turn next to a probabilistic view of classiﬁcation and show how models with\nlinear decision boundaries arise from simple assumptions about the distribution of\nthe data. In Section 1.5.4, we discussed the distinction between the discriminative\nand the generative approaches to classiﬁcation. Here we shall adopt a generative",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 215,
      "page_label": "196"
    }
  },
  {
    "page_content": "4.2. Probabilistic Generative Models 197\nFigure 4.9 Plot of the logistic sigmoid function\nσ(a) deﬁned by (4.59), shown in\nred, together with the scaled pro-\nbit function Φ(λa),f o r λ2 = π/8,\nshown in dashed blue, where Φ(a)\nis deﬁned by (4.114). The scal-\ning factor π/8 is chosen so that the\nderivatives of the two curves are\nequal for a =0 .\n−5 0 5\n0\n0.5\n1\napproach in which we model the class-conditional densities p(x|Ck), as well as the\nclass priors p(Ck), and then use these to compute posterior probabilities p(Ck|x)\nthrough Bayes’ theorem.\nConsider ﬁrst of all the case of two classes. The posterior probability for class\nC1 can be written as\np(C1|x)= p(x|C1)p(C1)\np(x|C1)p(C1)+ p(x|C2)p(C2)\n= 1\n1+e x p (−a) = σ(a) (4.57)\nwhere we have deﬁned\na =l n p(x|C1)p(C1)\np(x|C2)p(C2) (4.58)\nand σ(a) is the logistic sigmoidfunction deﬁned by\nσ(a)= 1\n1+e x p (−a) (4.59)\nwhich is plotted in Figure 4.9. The term ‘sigmoid’ means S-shaped. This type of\nfunction is sometimes also called a ‘squashing function’ because it maps the whole\nreal axis into a ﬁnite interval. The logistic sigmoid has been encountered already\nin earlier chapters and plays an important role in many classiﬁcation algorithms. It\nsatisﬁes the following symmetry property\nσ(−a)=1 − σ(a) (4.60)\nas is easily veriﬁed. The inverse of the logistic sigmoid is given by\na =l n\n( σ\n1 − σ\n)\n(4.61)\nand is known as the logit function. It represents the log of the ratio of probabilities",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 216,
      "page_label": "197"
    }
  },
  {
    "page_content": "a =l n\n( σ\n1 − σ\n)\n(4.61)\nand is known as the logit function. It represents the log of the ratio of probabilities\nln [p(C1|x)/p(C2|x)] for the two classes, also known as the log odds.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 216,
      "page_label": "197"
    }
  },
  {
    "page_content": "198 4. LINEAR MODELS FOR CLASSIFICATION\nNote that in (4.57) we have simply rewritten the posterior probabilities in an\nequivalent form, and so the appearance of the logistic sigmoid may seem rather vac-\nuous. However, it will have signiﬁcance provideda(x) takes a simple functional\nform. We shall shortly consider situations in which a(x) is a linear function of x,i n\nwhich case the posterior probability is governed by a generalized linear model.\nFor the case of K> 2 classes, we have\np(Ck|x)= p(x|Ck)p(Ck)∑\nj p(x|Cj)p(Cj)\n= exp(ak)∑\nj exp(aj) (4.62)\nwhich is known as the normalized exponential and can be regarded as a multiclass\ngeneralization of the logistic sigmoid. Here the quantities ak are deﬁned by\nak =l np(x|Ck)p(Ck). (4.63)\nThe normalized exponential is also known as the softmax function, as it represents\na smoothed version of the ‘max’ function because, if ak ≫ aj for all j ̸=k, then\np(Ck|x) ≃ 1, and p(Cj|x) ≃ 0.\nWe now investigate the consequences of choosing speciﬁc forms for the class-\nconditional densities, looking ﬁrst at continuous input variables x and then dis-\ncussing brieﬂy the case of discrete inputs.\n4.2.1 Continuous inputs\nLet us assume that the class-conditional densities are Gaussian and then explore\nthe resulting form for the posterior probabilities. To start with, we shall assume that\nall classes share the same covariance matrix. Thus the density for class Ck is given\nby\np(x|Ck)= 1\n(2π)D/2\n1\n|Σ|1/2 exp\n{\n−1\n2(x − µk)TΣ−1(x − µk)\n}\n. (4.64)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 217,
      "page_label": "198"
    }
  },
  {
    "page_content": "by\np(x|Ck)= 1\n(2π)D/2\n1\n|Σ|1/2 exp\n{\n−1\n2(x − µk)TΣ−1(x − µk)\n}\n. (4.64)\nConsider ﬁrst the case of two classes. From (4.57) and (4.58), we have\np(C1|x)= σ(wTx + w0) (4.65)\nwhere we have deﬁned\nw = Σ−1(µ1 − µ2) (4.66)\nw0 = −1\n2µT\n1 Σ−1µ1 + 1\n2µT\n2 Σ−1µ2 +l n p(C1)\np(C2). (4.67)\nWe see that the quadratic terms in x from the exponents of the Gaussian densities\nhave cancelled (due to the assumption of common covariance matrices) leading to\na linear function ofx in the argument of the logistic sigmoid. This result is illus-\ntrated for the case of a two-dimensional input space x in Figure 4.10. The resulting",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 217,
      "page_label": "198"
    }
  },
  {
    "page_content": "4.2. Probabilistic Generative Models 199\nFigure 4.10 The left-hand plot shows the class-conditional densities for two classes, denoted red and blue.\nOn the right is the corresponding posterior probability p(C1|x), which is given by a logistic sigmoid of a linear\nfunction of x. The surface in the right-hand plot is coloured using a proportion of red ink given by p(C1|x) and a\nproportion of blue ink given by p(C2|x)=1 − p(C1|x).\ndecision boundaries correspond to surfaces along which the posterior probabilities\np(Ck|x) are constant and so will be given by linear functions of x, and therefore\nthe decision boundaries are linear in input space. The prior probabilities p(Ck) enter\nonly through the bias parameter w0 so that changes in the priors have the effect of\nmaking parallel shifts of the decision boundary and more generally of the parallel\ncontours of constant posterior probability.\nFor the general case of K classes we have, from (4.62) and (4.63),\nak(x)= wT\nk x + wk0 (4.68)\nwhere we have deﬁned\nwk = Σ−1µk (4.69)\nwk0 = −1\n2µT\nk Σ−1µk +l np(Ck). (4.70)\nWe see that theak(x) are again linear functions of x as a consequence of the cancel-\nlation of the quadratic terms due to the shared covariances. The resulting decision\nboundaries, corresponding to the minimum misclassiﬁcation rate, will occur when\ntwo of the posterior probabilities (the two largest) are equal, and so will be deﬁned\nby linear functions ofx, and so again we have a generalized linear model.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 218,
      "page_label": "199"
    }
  },
  {
    "page_content": "by linear functions ofx, and so again we have a generalized linear model.\nIf we relax the assumption of a shared covariance matrix and allow each class-\nconditional density p(x|Ck) to have its own covariance matrix Σk, then the earlier\ncancellations will no longer occur, and we will obtain quadratic functions of x,g i v -\ning rise to a quadratic discriminant. The linear and quadratic decision boundaries\nare illustrated in Figure 4.11.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 218,
      "page_label": "199"
    }
  },
  {
    "page_content": "200 4. LINEAR MODELS FOR CLASSIFICATION\n−2 −1 0 1 2\n−2.5\n−2\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n2\n2.5\nFigure 4.11 The left-hand plot shows the class-conditional densities for three classes each having a Gaussian\ndistribution, coloured red, green, and blue, in which the red and green classes have the same covariance matrix.\nThe right-hand plot shows the corresponding posterior probabilities, in which the RGB colour vector represents\nthe posterior probabilities for the respective three classes. The decision boundaries are also shown. Notice that\nthe boundary between the red and green classes, which have the same covariance matrix, is linear, whereas\nthose between the other pairs of classes are quadratic.\n4.2.2 Maximum likelihood solution\nOnce we have speciﬁed a parametric functional form for the class-conditional\ndensities p(x|Ck), we can then determine the values of the parameters, together with\nthe prior class probabilities p(Ck), using maximum likelihood. This requires a data\nset comprising observations of x along with their corresponding class labels.\nConsider ﬁrst the case of two classes, each having a Gaussian class-conditional\ndensity with a shared covariance matrix, and suppose we have a data set {xn,t n}\nwhere n =1 ,...,N . Here tn =1 denotes class C1 and tn =0 denotes class C2.W e\ndenote the prior class probability p(C1)= π, so that p(C2)=1 − π. For a data point\nxn from class C1,w eh a v etn =1 and hence\np(xn, C1)= p(C1)p(xn|C1)= πN(xn|µ1, Σ).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 219,
      "page_label": "200"
    }
  },
  {
    "page_content": "xn from class C1,w eh a v etn =1 and hence\np(xn, C1)= p(C1)p(xn|C1)= πN(xn|µ1, Σ).\nSimilarly for class C2,w eh a v etn =0 and hence\np(xn, C2)= p(C2)p(xn|C2)=( 1 − π)N(xn|µ2, Σ).\nThus the likelihood function is given by\np(t|π, µ1, µ2, Σ)=\nN∏\nn=1\n[πN(xn|µ1, Σ)]tn\n[(1 − π)N(xn|µ2, Σ)]1−tn\n(4.71)\nwhere t =( t1,...,t N )T. As usual, it is convenient to maximize the log of the\nlikelihood function. Consider ﬁrst the maximization with respect to π. The terms in",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 219,
      "page_label": "200"
    }
  },
  {
    "page_content": "4.2. Probabilistic Generative Models 201\nthe log likelihood function that depend on π are\nN∑\nn=1\n{tn lnπ +( 1− tn)l n ( 1− π)}. (4.72)\nSetting the derivative with respect to π equal to zero and rearranging, we obtain\nπ = 1\nN\nN∑\nn=1\ntn = N1\nN = N1\nN1 + N2\n(4.73)\nwhere N1 denotes the total number of data points in classC1, and N2 denotes the total\nnumber of data points in class C2. Thus the maximum likelihood estimate for π is\nsimply the fraction of points in classC1 as expected. This result is easily generalized\nto the multiclass case where again the maximum likelihood estimate of the prior\nprobability associated with class Ck is given by the fraction of the training set points\nassigned to that class.Exercise 4.9\nNow consider the maximization with respect to µ1. Again we can pick out of\nthe log likelihood function those terms that depend on µ1 giving\nN∑\nn=1\ntn ln N(xn|µ1, Σ)= −1\n2\nN∑\nn=1\ntn(xn − µ1)TΣ−1(xn − µ1)+c o n s t. (4.74)\nSetting the derivative with respect to µ1 to zero and rearranging, we obtain\nµ1 = 1\nN1\nN∑\nn=1\ntnxn (4.75)\nwhich is simply the mean of all the input vectors xn assigned to class C1.B y a\nsimilar argument, the corresponding result for µ2 is given by\nµ2 = 1\nN2\nN∑\nn=1\n(1 − tn)xn (4.76)\nwhich again is the mean of all the input vectors xn assigned to class C2.\nFinally, consider the maximum likelihood solution for the shared covariance\nmatrix Σ. Picking out the terms in the log likelihood function that depend on Σ,w e\nhave\n−1\n2\nN∑\nn=1\ntn ln |Σ|− 1\n2\nN∑\nn=1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 220,
      "page_label": "201"
    }
  },
  {
    "page_content": "matrix Σ. Picking out the terms in the log likelihood function that depend on Σ,w e\nhave\n−1\n2\nN∑\nn=1\ntn ln |Σ|− 1\n2\nN∑\nn=1\ntn(xn − µ1)TΣ−1(xn − µ1)\n−1\n2\nN∑\nn=1\n(1 − tn)l n|Σ|− 1\n2\nN∑\nn=1\n(1 − tn)(xn − µ2)TΣ−1(xn − µ2)\n= −N\n2 ln|Σ|− N\n2 Tr\n{\nΣ−1S\n}\n(4.77)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 220,
      "page_label": "201"
    }
  },
  {
    "page_content": "202 4. LINEAR MODELS FOR CLASSIFICATION\nwhere we have deﬁned\nS = N1\nN S1 + N2\nN S2 (4.78)\nS1 = 1\nN1\n∑\nn∈C1\n(xn − µ1)(xn − µ1)T (4.79)\nS2 = 1\nN2\n∑\nn∈C2\n(xn − µ2)(xn − µ2)T. (4.80)\nUsing the standard result for the maximum likelihood solution for a Gaussian distri-\nbution, we see that Σ = S, which represents a weighted average of the covariance\nmatrices associated with each of the two classes separately.\nThis result is easily extended to theK class problem to obtain the corresponding\nmaximum likelihood solutions for the parameters in which each class-conditional\ndensity is Gaussian with a shared covariance matrix. Note that the approach of ﬁttingExercise 4.10\nGaussian distributions to the classes is not robust to outliers, because the maximum\nlikelihood estimation of a Gaussian is not robust.Section 2.3.7\n4.2.3 Discrete features\nLet us now consider the case of discrete feature values xi. For simplicity, we\nbegin by looking at binary feature values xi ∈{ 0, 1} and discuss the extension to\nmore general discrete features shortly. If there are D inputs, then a general distribu-\ntion would correspond to a table of 2D numbers for each class, containing 2D − 1\nindependent variables (due to the summation constraint). Because this grows expo-\nnentially with the number of features, we might seek a more restricted representa-\ntion. Here we will make thenaive Bayesassumption in which the feature values areSection 8.2.2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 221,
      "page_label": "202"
    }
  },
  {
    "page_content": "tion. Here we will make thenaive Bayesassumption in which the feature values areSection 8.2.2\ntreated as independent, conditioned on the class Ck. Thus we have class-conditional\ndistributions of the form\np(x|Ck)=\nD∏\ni=1\nµxi\nki(1 − µki)1−xi (4.81)\nwhich contain D independent parameters for each class. Substituting into (4.63) then\ngives\nak(x)=\nD∑\ni=1\n{xi ln µki +( 1− xi)l n ( 1− µki)} +l np(Ck) (4.82)\nwhich again are linear functions of the input valuesxi. For the case ofK =2 classes,\nwe can alternatively consider the logistic sigmoid formulation given by (4.57). Anal-\nogous results are obtained for discrete variables each of which can take M> 2\nstates.Exercise 4.11\n4.2.4 Exponential family\nAs we have seen, for both Gaussian distributed and discrete inputs, the posterior\nclass probabilities are given by generalized linear models with logistic sigmoid (K =",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 221,
      "page_label": "202"
    }
  },
  {
    "page_content": "4.3. Probabilistic Discriminative Models 203\n2 classes) or softmax (K ⩾ 2 classes) activation functions. These are particular cases\nof a more general result obtained by assuming that the class-conditional densities\np(x|Ck) are members of the exponential family of distributions.\nUsing the form (2.194) for members of the exponential family, we see that the\ndistribution of x can be written in the form\np(x|λk)= h(x)g(λk)e x p\n{\nλT\nk u(x)\n}\n. (4.83)\nWe now restrict attention to the subclass of such distributions for which u(x)= x.\nThen we make use of (2.236) to introduce a scaling parameter s, so that we obtain\nthe restricted set of exponential family class-conditional densities of the form\np(x|λk,s )= 1\nsh\n( 1\nsx\n)\ng(λk)e x p\n{1\nsλT\nk x\n}\n. (4.84)\nNote that we are allowing each class to have its own parameter vectorλk but we are\nassuming that the classes share the same scale parameter s.\nFor the two-class problem, we substitute this expression for the class-conditional\ndensities into (4.58) and we see that the posterior class probability is again given by\na logistic sigmoid acting on a linear functiona(x) which is given by\na(x)=( λ1 − λ2)Tx +l ng(λ1) − ln g(λ2)+l n p(C1) − lnp(C2). (4.85)\nSimilarly, for the K-class problem, we substitute the class-conditional density ex-\npression into (4.63) to give\nak(x)= λT\nk x +l ng(λk)+l n p(Ck) (4.86)\nand so again is a linear function of x.\n4.3. Probabilistic Discriminative Models",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 222,
      "page_label": "203"
    }
  },
  {
    "page_content": "ak(x)= λT\nk x +l ng(λk)+l n p(Ck) (4.86)\nand so again is a linear function of x.\n4.3. Probabilistic Discriminative Models\nFor the two-class classiﬁcation problem, we have seen that the posterior probability\nof class C1 can be written as a logistic sigmoid acting on a linear function of x, for a\nwide choice of class-conditional distributions p(x|Ck). Similarly, for the multiclass\ncase, the posterior probability of class Ck is given by a softmax transformation of a\nlinear function of x. For speciﬁc choices of the class-conditional densities p(x|Ck),\nwe have used maximum likelihood to determine the parameters of the densities as\nwell as the class priorsp(Ck) and then used Bayes’ theorem to ﬁnd the posterior class\nprobabilities.\nHowever, an alternative approach is to use the functional form of the generalized\nlinear model explicitly and to determine its parameters directly by using maximum\nlikelihood. We shall see that there is an efﬁcient algorithm ﬁnding such solutions\nknown as iterative reweighted least squares,o r IRLS.\nThe indirect approach to ﬁnding the parameters of a generalized linear model,\nby ﬁtting class-conditional densities and class priors separately and then applying",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 222,
      "page_label": "203"
    }
  },
  {
    "page_content": "204 4. LINEAR MODELS FOR CLASSIFICATION\nx1\nx2\n−1 0 1\n−1\n0\n1\nφ1\nφ2\n0 0.5 1\n0\n0.5\n1\nFigure 4.12 Illustration of the role of nonlinear basis functions in linear classiﬁcation models. The left plot\nshows the original input space (x1,x 2) together with data points from two classes labelled red and blue. Two\n‘Gaussian’ basis functions φ1(x) and φ2(x) are deﬁned in this space with centres shown by the green crosses\nand with contours shown by the green circles. The right-hand plot shows the corresponding feature space\n(φ1,φ 2) together with the linear decision boundary obtained given by a logistic regression model of the form\ndiscussed in Section 4.3.2. This corresponds to a nonlinear decision boundary in the original input space,\nshown by the black curve in the left-hand plot.\nBayes’ theorem, represents an example of generative modelling, because we could\ntake such a model and generate synthetic data by drawing values of x from the\nmarginal distribution p(x). In the direct approach, we are maximizing a likelihood\nfunction deﬁned through the conditional distribution p(Ck|x), which represents a\nform of discriminative training. One advantage of the discriminative approach is\nthat there will typically be fewer adaptive parameters to be determined, as we shall\nsee shortly. It may also lead to improved predictive performance, particularly when\nthe class-conditional density assumptions give a poor approximation to the true dis-\ntributions.\n4.3.1 Fixed basis functions",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 223,
      "page_label": "204"
    }
  },
  {
    "page_content": "the class-conditional density assumptions give a poor approximation to the true dis-\ntributions.\n4.3.1 Fixed basis functions\nSo far in this chapter, we have considered classiﬁcation models that work di-\nrectly with the original input vector x. However, all of the algorithms are equally\napplicable if we ﬁrst make a ﬁxed nonlinear transformation of the inputs using a\nvector of basis functions φ(x). The resulting decision boundaries will be linear in\nthe feature space φ, and these correspond to nonlinear decision boundaries in the\noriginal x space, as illustrated in Figure 4.12. Classes that are linearly separable\nin the feature space φ(x) need not be linearly separable in the original observation\nspace x. Note that as in our discussion of linear models for regression, one of the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 223,
      "page_label": "204"
    }
  },
  {
    "page_content": "4.3. Probabilistic Discriminative Models 205\nbasis functions is typically set to a constant, say φ0(x)=1 , so that the correspond-\ning parameter w0 plays the role of a bias. For the remainder of this chapter, we shall\ninclude a ﬁxed basis function transformationφ(x), as this will highlight some useful\nsimilarities to the regression models discussed in Chapter 3.\nFor many problems of practical interest, there is signiﬁcant overlap between\nthe class-conditional densities p(x|Ck). This corresponds to posterior probabilities\np(Ck|x), which, for at least some values of x, are not 0 or 1. In such cases, the opti-\nmal solution is obtained by modelling the posterior probabilities accurately and then\napplying standard decision theory, as discussed in Chapter 1. Note that nonlinear\ntransformations φ(x) cannot remove such class overlap. Indeed, they can increase\nthe level of overlap, or create overlap where none existed in the original observation\nspace. However, suitable choices of nonlinearity can make the process of modelling\nthe posterior probabilities easier.\nSuch ﬁxed basis function models have important limitations, and these will beSection 3.6\nresolved in later chapters by allowing the basis functions themselves to adapt to the\ndata. Notwithstanding these limitations, models with ﬁxed nonlinear basis functions\nplay an important role in applications, and a discussion of such models will intro-\nduce many of the key concepts needed for an understanding of their more complex",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 224,
      "page_label": "205"
    }
  },
  {
    "page_content": "duce many of the key concepts needed for an understanding of their more complex\ncounterparts.\n4.3.2 Logistic regression\nWe begin our treatment of generalized linear models by considering the problem\nof two-class classiﬁcation. In our discussion of generative approaches in Section 4.2,\nwe saw that under rather general assumptions, the posterior probability of class C1\ncan be written as a logistic sigmoid acting on a linear function of the feature vector\nφ so that\np(C1|φ)= y(φ)= σ\n(\nwTφ\n)\n(4.87)\nwith p(C2|φ)=1 − p(C1|φ). Here σ(·) is the logistic sigmoid function deﬁned by\n(4.59). In the terminology of statistics, this model is known as logistic regression,\nalthough it should be emphasized that this is a model for classiﬁcation rather than\nregression.\nFor an M-dimensional feature spaceφ, this model hasM adjustable parameters.\nBy contrast, if we had ﬁtted Gaussian class conditional densities using maximum\nlikelihood, we would have used2M parameters for the means and M(M +1 )/2\nparameters for the (shared) covariance matrix. Together with the class prior p(C1),\nthis gives a total ofM(M +5)/2+1 parameters, which grows quadratically withM,\nin contrast to the linear dependence on M of the number of parameters in logistic\nregression. For large values of M, there is a clear advantage in working with the\nlogistic regression model directly.\nWe now use maximum likelihood to determine the parameters of the logistic",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 224,
      "page_label": "205"
    }
  },
  {
    "page_content": "logistic regression model directly.\nWe now use maximum likelihood to determine the parameters of the logistic\nregression model. To do this, we shall make use of the derivative of the logistic sig-\nmoid function, which can conveniently be expressed in terms of the sigmoid function\nitselfExercise 4.12\ndσ\nda = σ(1 − σ). (4.88)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 224,
      "page_label": "205"
    }
  },
  {
    "page_content": "206 4. LINEAR MODELS FOR CLASSIFICATION\nFor a data set {φn,t n}, where tn ∈{ 0, 1} and φn = φ(xn), with n =\n1,...,N , the likelihood function can be written\np(t|w)=\nN∏\nn=1\nytn\nn {1 − yn}1−tn\n(4.89)\nwhere t =( t1,...,t N )T and yn = p(C1|φn). As usual, we can deﬁne an error\nfunction by taking the negative logarithm of the likelihood, which gives the cross-\nentropy error function in the form\nE(w)= −lnp(t|w)= −\nN∑\nn=1\n{tn ln yn +( 1− tn)l n ( 1− yn)} (4.90)\nwhere yn = σ(an) and an = wTφn. Taking the gradient of the error function with\nrespect to w, we obtainExercise 4.13\n∇E(w)=\nN∑\nn=1\n(yn − tn)φn (4.91)\nwhere we have made use of (4.88). We see that the factor involving the derivative\nof the logistic sigmoid has cancelled, leading to a simpliﬁed form for the gradient\nof the log likelihood. In particular, the contribution to the gradient from data point\nn is given by the ‘error’ yn − tn between the target value and the prediction of the\nmodel, times the basis function vector φn. Furthermore, comparison with (3.13)\nshows that this takes precisely the same form as the gradient of the sum-of-squares\nerror function for the linear regression model.Section 3.1.1\nIf desired, we could make use of the result (4.91) to give a sequential algorithm\nin which patterns are presented one at a time, in which each of the weight vectors is\nupdated using (3.22) in which ∇En is the nth term in (4.91).\nIt is worth noting that maximum likelihood can exhibit severe over-ﬁtting for",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 225,
      "page_label": "206"
    }
  },
  {
    "page_content": "updated using (3.22) in which ∇En is the nth term in (4.91).\nIt is worth noting that maximum likelihood can exhibit severe over-ﬁtting for\ndata sets that are linearly separable. This arises because the maximum likelihood so-\nlution occurs when the hyperplane corresponding to σ =0 .5, equivalent to wTφ =\n0, separates the two classes and the magnitude of w goes to inﬁnity. In this case, the\nlogistic sigmoid function becomes inﬁnitely steep in feature space, corresponding to\na Heaviside step function, so that every training point from each class k is assigned\na posterior probability p(Ck|x)=1 . Furthermore, there is typically a continuumExercise 4.14\nof such solutions because any separating hyperplane will give rise to the same pos-\nterior probabilities at the training data points, as will be seen later in Figure 10.13.\nMaximum likelihood provides no way to favour one such solution over another, and\nwhich solution is found in practice will depend on the choice of optimization algo-\nrithm and on the parameter initialization. Note that the problem will arise even if\nthe number of data points is large compared with the number of parameters in the\nmodel, so long as the training data set is linearly separable. The singularity can be\navoided by inclusion of a prior and ﬁnding a MAP solution forw, or equivalently by\nadding a regularization term to the error function.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 225,
      "page_label": "206"
    }
  },
  {
    "page_content": "4.3. Probabilistic Discriminative Models 207\n4.3.3 Iterative reweighted least squares\nIn the case of the linear regression models discussed in Chapter 3, the maxi-\nmum likelihood solution, on the assumption of a Gaussian noise model, leads to a\nclosed-form solution. This was a consequence of the quadratic dependence of the\nlog likelihood function on the parameter vector w. For logistic regression, there\nis no longer a closed-form solution, due to the nonlinearity of the logistic sigmoid\nfunction. However, the departure from a quadratic form is not substantial. To be\nprecise, the error function is concave, as we shall see shortly, and hence has a unique\nminimum. Furthermore, the error function can be minimized by an efﬁcient iterative\ntechnique based on theNewton-Raphson iterative optimization scheme, which uses a\nlocal quadratic approximation to the log likelihood function. The Newton-Raphson\nupdate, for minimizing a function E(w), takes the form (Fletcher, 1987; Bishop and\nNabney, 2008)\nw(new) = w(old) − H−1∇E(w). (4.92)\nwhere H is the Hessian matrix whose elements comprise the second derivatives of\nE(w) with respect to the components of w.\nLet us ﬁrst of all apply the Newton-Raphson method to the linear regression\nmodel (3.3) with the sum-of-squares error function (3.12). The gradient and Hessian\nof this error function are given by\n∇E(w)=\nN∑\nn=1\n(wTφn − tn)φn = ΦTΦw − ΦTt (4.93)\nH = ∇∇E(w)=\nN∑\nn=1\nφnφT\nn = ΦTΦ (4.94)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 226,
      "page_label": "207"
    }
  },
  {
    "page_content": "of this error function are given by\n∇E(w)=\nN∑\nn=1\n(wTφn − tn)φn = ΦTΦw − ΦTt (4.93)\nH = ∇∇E(w)=\nN∑\nn=1\nφnφT\nn = ΦTΦ (4.94)\nwhere Φ is the N × M design matrix, whose nth row is given by φT\nn. The Newton-Section 3.1.1\nRaphson update then takes the form\nw(new) = w(old) − (ΦTΦ)−1 {\nΦTΦw(old) − ΦTt\n}\n=( ΦTΦ)−1ΦTt (4.95)\nwhich we recognize as the standard least-squares solution. Note that the error func-\ntion in this case is quadratic and hence the Newton-Raphson formula gives the exact\nsolution in one step.\nNow let us apply the Newton-Raphson update to the cross-entropy error function\n(4.90) for the logistic regression model. From (4.91) we see that the gradient and\nHessian of this error function are given by\n∇E(w)=\nN∑\nn=1\n(yn − tn)φn = ΦT(y − t) (4.96)\nH = ∇∇E(w)=\nN∑\nn=1\nyn(1 − yn)φnφT\nn = ΦTRΦ (4.97)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 226,
      "page_label": "207"
    }
  },
  {
    "page_content": "208 4. LINEAR MODELS FOR CLASSIFICATION\nwhere we have made use of (4.88). Also, we have introduced the N × N diagonal\nmatrix R with elements\nRnn = yn(1 − yn). (4.98)\nWe see that the Hessian is no longer constant but depends on w through the weight-\ning matrix R, corresponding to the fact that the error function is no longer quadratic.\nUsing the property 0 <y n < 1, which follows from the form of the logistic sigmoid\nfunction, we see that uTHu > 0 for an arbitrary vector u, and so the Hessian matrix\nH is positive deﬁnite. It follows that the error function is a concave function of w\nand hence has a unique minimum.Exercise 4.15\nThe Newton-Raphson update formula for the logistic regression model then be-\ncomes\nw(new) = w(old) − (ΦTRΦ)−1ΦT(y − t)\n=( ΦTRΦ)−1 {\nΦTRΦw(old) − ΦT(y − t)\n}\n=( ΦTRΦ)−1ΦTRz (4.99)\nwhere z is an N-dimensional vector with elements\nz = Φw(old) − R−1(y − t). (4.100)\nWe see that the update formula (4.99) takes the form of a set of normal equations for a\nweighted least-squares problem. Because the weighing matrix R is not constant but\ndepends on the parameter vector w, we must apply the normal equations iteratively,\neach time using the new weight vector w to compute a revised weighing matrix\nR. For this reason, the algorithm is known as iterative reweighted least squares,o r\nIRLS (Rubin, 1983). As in the weighted least-squares problem, the elements of the\ndiagonal weighting matrix R can be interpreted as variances because the mean and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 227,
      "page_label": "208"
    }
  },
  {
    "page_content": "diagonal weighting matrix R can be interpreted as variances because the mean and\nvariance of t in the logistic regression model are given by\nE[t]= σ(x)= y (4.101)\nvar[t]= E[t2] − E[t]2 = σ(x) − σ(x)2 = y(1 − y) (4.102)\nwhere we have used the propertyt2 = t for t ∈{ 0, 1}. In fact, we can interpret IRLS\nas the solution to a linearized problem in the space of the variable a = wTφ. The\nquantity zn, which corresponds to the nth element of z, can then be given a simple\ninterpretation as an effective target value in this space obtained by making a local\nlinear approximation to the logistic sigmoid function around the current operating\npointw(old)\nan(w) ≃ an(w(old))+ dan\ndyn\n⏐⏐⏐⏐\nw(old)\n(tn − yn)\n= φT\nnw(old) − (yn − tn)\nyn(1 − yn) = zn. (4.103)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 227,
      "page_label": "208"
    }
  },
  {
    "page_content": "4.3. Probabilistic Discriminative Models 209\n4.3.4 Multiclass logistic regression\nIn our discussion of generative models for multiclass classiﬁcation, we haveSection 4.2\nseen that for a large class of distributions, the posterior probabilities are given by a\nsoftmax transformation of linear functions of the feature variables, so that\np(Ck|φ)= yk(φ)= exp(ak)∑\nj exp(aj) (4.104)\nwhere the ‘activations’ ak are given by\nak = wT\nk φ. (4.105)\nThere we used maximum likelihood to determine separately the class-conditional\ndensities and the class priors and then found the corresponding posterior probabilities\nusing Bayes’ theorem, thereby implicitly determining the parameters{wk}. Here we\nconsider the use of maximum likelihood to determine the parameters {wk} of this\nmodel directly. To do this, we will require the derivatives ofyk with respect to all of\nthe activations aj. These are given byExercise 4.17\n∂yk\n∂aj\n= yk(Ikj − yj) (4.106)\nwhere Ikj are the elements of the identity matrix.\nNext we write down the likelihood function. This is most easily done using\nthe 1-of- K coding scheme in which the target vector tn for a feature vector φn\nbelonging to class Ck is a binary vector with all elements zero except for element k,\nwhich equals one. The likelihood function is then given by\np(T|w1,..., wK)=\nN∏\nn=1\nK∏\nk=1\np(Ck|φn)tnk =\nN∏\nn=1\nK∏\nk=1\nytnk\nnk (4.107)\nwhere ynk = yk(φn), and T is an N × K matrix of target variables with elements\ntnk. Taking the negative logarithm then gives",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 228,
      "page_label": "209"
    }
  },
  {
    "page_content": "K∏\nk=1\nytnk\nnk (4.107)\nwhere ynk = yk(φn), and T is an N × K matrix of target variables with elements\ntnk. Taking the negative logarithm then gives\nE(w1,..., wK)= −lnp(T|w1,..., wK)= −\nN∑\nn=1\nK∑\nk=1\ntnk lnynk (4.108)\nwhich is known as the cross-entropy error function for the multiclass classiﬁcation\nproblem.\nWe now take the gradient of the error function with respect to one of the param-\neter vectors wj. Making use of the result (4.106) for the derivatives of the softmax\nfunction, we obtainExercise 4.18\n∇wj E(w1,..., wK)=\nN∑\nn=1\n(ynj − tnj)φn (4.109)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 228,
      "page_label": "209"
    }
  },
  {
    "page_content": "210 4. LINEAR MODELS FOR CLASSIFICATION\nwhere we have made use of ∑\nk tnk =1 . Once again, we see the same form arising\nfor the gradient as was found for the sum-of-squares error function with the linear\nmodel and the cross-entropy error for the logistic regression model, namely the prod-\nuct of the error (ynj − tnj) times the basis function φn. Again, we could use this\nto formulate a sequential algorithm in which patterns are presented one at a time, in\nwhich each of the weight vectors is updated using (3.22).\nWe have seen that the derivative of the log likelihood function for a linear regres-\nsion model with respect to the parameter vector w for a data point n took the form\nof the ‘error’ yn − tn times the feature vector φn. Similarly, for the combination\nof logistic sigmoid activation function and cross-entropy error function (4.90), and\nfor the softmax activation function with the multiclass cross-entropy error function\n(4.108), we again obtain this same simple form. This is an example of a more general\nresult, as we shall see in Section 4.3.6.\nTo ﬁnd a batch algorithm, we again appeal to the Newton-Raphson update to\nobtain the corresponding IRLS algorithm for the multiclass problem. This requires\nevaluation of the Hessian matrix that comprises blocks of size M × M in which\nblock j, kis given by\n∇wk ∇wj E(w1,..., wK)= −\nN∑\nn=1\nynk(Ikj − ynj)φnφT\nn. (4.110)\nAs with the two-class problem, the Hessian matrix for the multiclass logistic regres-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 229,
      "page_label": "210"
    }
  },
  {
    "page_content": "∇wk ∇wj E(w1,..., wK)= −\nN∑\nn=1\nynk(Ikj − ynj)φnφT\nn. (4.110)\nAs with the two-class problem, the Hessian matrix for the multiclass logistic regres-\nsion model is positive deﬁnite and so the error function again has a unique minimum.Exercise 4.20\nPractical details of IRLS for the multiclass case can be found in Bishop and Nabney\n(2008).\n4.3.5 Probit regression\nWe have seen that, for a broad range of class-conditional distributions, described\nby the exponential family, the resulting posterior class probabilities are given by a\nlogistic (or softmax) transformation acting on a linear function of the feature vari-\nables. However, not all choices of class-conditional density give rise to such a simple\nform for the posterior probabilities (for instance, if the class-conditional densities are\nmodelled using Gaussian mixtures). This suggests that it might be worth exploring\nother types of discriminative probabilistic model. For the purposes of this chapter,\nhowever, we shall return to the two-class case, and again remain within the frame-\nwork of generalized linear models so that\np(t =1 |a)= f(a) (4.111)\nwhere a = wTφ, and f(·) is the activation function.\nOne way to motivate an alternative choice for the link function is to consider a\nnoisy threshold model, as follows. For each input φn, we evaluate an = wTφn and\nthen we set the target value according to\n{tn =1 if an ⩾ θ\ntn =0 otherwise. (4.112)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 229,
      "page_label": "210"
    }
  },
  {
    "page_content": "4.3. Probabilistic Discriminative Models 211\nFigure 4.13 Schematic example of a probability density p(θ)\nshown by the blue curve, given in this example by a mixture\nof two Gaussians, along with its cumulative distribution function\nf(a), shown by the red curve. Note that the value of the blue\ncurve at any point, such as that indicated by the vertical green\nline, corresponds to the slope of the red curve at the same point.\nConversely, the value of the red curve at this point corresponds\nto the area under the blue curve indicated by the shaded green\nregion. In the stochastic threshold model, the class label takes\nthe value t =1 if the value ofa = wTφ exceeds a threshold, oth-\nerwise it takes the valuet =0 . This is equivalent to an activation\nfunction given by the cumulative distribution function f(a).\n0 1 2 3 4\n0\n0.2\n0.4\n0.6\n0.8\n1\nIf the value of θ is drawn from a probability density p(θ), then the corresponding\nactivation function will be given by the cumulative distribution function\nf(a)=\n∫ a\n−∞\np(θ)d θ (4.113)\nas illustrated in Figure 4.13.\nAs a speciﬁc example, suppose that the density p(θ) is given by a zero mean,\nunit variance Gaussian. The corresponding cumulative distribution function is given\nby\nΦ(a)=\n∫ a\n−∞\nN(θ|0, 1) dθ (4.114)\nwhich is known as the probit function. It has a sigmoidal shape and is compared\nwith the logistic sigmoid function in Figure 4.9. Note that the use of a more gen-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 230,
      "page_label": "211"
    }
  },
  {
    "page_content": "with the logistic sigmoid function in Figure 4.9. Note that the use of a more gen-\neral Gaussian distribution does not change the model because this is equivalent to\na re-scaling of the linear coefﬁcients w. Many numerical packages provide for the\nevaluation of a closely related function deﬁned by\nerf(a)= 2√π\n∫ a\n0\nexp(−θ2/2) dθ (4.115)\nand known as the erf function or error function (not to be confused with the error\nfunction of a machine learning model). It is related to the probit function byExercise 4.21\nΦ(a)= 1\n2\n{\n1+ 1√\n2\nerf(a)\n}\n. (4.116)\nThe generalized linear model based on a probit activation function is known asprobit\nregression.\nWe can determine the parameters of this model using maximum likelihood, by a\nstraightforward extension of the ideas discussed earlier. In practice, the results found\nusing probit regression tend to be similar to those of logistic regression. We shall,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 230,
      "page_label": "211"
    }
  },
  {
    "page_content": "212 4. LINEAR MODELS FOR CLASSIFICATION\nhowever, ﬁnd another use for the probit model when we discuss Bayesian treatments\nof logistic regression in Section 4.5.\nOne issue that can occur in practical applications is that of outliers, which can\narise for instance through errors in measuring the input vector x or through misla-\nbelling of the target valuet. Because such points can lie a long way to the wrong side\nof the ideal decision boundary, they can seriously distort the classiﬁer. Note that the\nlogistic and probit regression models behave differently in this respect because the\ntails of the logistic sigmoid decay asymptotically likeexp(−x) for x →∞ , whereas\nfor the probit activation function they decay like exp(−x2), and so the probit model\ncan be signiﬁcantly more sensitive to outliers.\nHowever, both the logistic and the probit models assume the data is correctly\nlabelled. The effect of mislabelling is easily incorporated into a probabilistic model\nby introducing a probability ϵ that the target value t has been ﬂipped to the wrong\nvalue (Opper and Winther, 2000a), leading to a target value distribution for data point\nx of the form\np(t|x)=( 1 − ϵ)σ(x)+ ϵ(1 − σ(x))\n= ϵ +( 1− 2ϵ)σ(x) (4.117)\nwhere σ(x) is the activation function with input vector x. Here ϵ may be set in\nadvance, or it may be treated as a hyperparameter whose value is inferred from the\ndata.\n4.3.6 Canonical link functions\nFor the linear regression model with a Gaussian noise distribution, the error",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 231,
      "page_label": "212"
    }
  },
  {
    "page_content": "data.\n4.3.6 Canonical link functions\nFor the linear regression model with a Gaussian noise distribution, the error\nfunction, corresponding to the negative log likelihood, is given by (3.12). If we take\nthe derivative with respect to the parameter vectorw of the contribution to the error\nfunction from a data point n, this takes the form of the ‘error’ yn − tn times the\nfeature vector φn, where yn = wTφn. Similarly, for the combination of the logistic\nsigmoid activation function and the cross-entropy error function (4.90), and for the\nsoftmax activation function with the multiclass cross-entropy error function (4.108),\nwe again obtain this same simple form. We now show that this is a general result\nof assuming a conditional distribution for the target variable from the exponential\nfamily, along with a corresponding choice for the activation function known as the\ncanonical link function.\nWe again make use of the restricted form (4.84) of exponential family distribu-\ntions. Note that here we are applying the assumption of exponential family distribu-\ntion to the target variablet, in contrast to Section 4.2.4 where we applied it to the\ninput vector x. We therefore consider conditional distributions of the target variable\nof the form\np(t|η,s )= 1\nsh\n( t\ns\n)\ng(η)e x p\n{ηt\ns\n}\n. (4.118)\nUsing the same line of argument as led to the derivation of the result (2.226), we see\nthat the conditional mean of t, which we denote by y, is given by\ny ≡ E[t|η]= −s d\ndη ln g(η). (4.119)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 231,
      "page_label": "212"
    }
  },
  {
    "page_content": "4.4. The Laplace Approximation 213\nThus y and η must related, and we denote this relation through η = ψ(y).\nFollowing Nelder and Wedderburn (1972), we deﬁne ageneralized linear model\nto be one for which y is a nonlinear function of a linear combination of the input (or\nfeature) variables so that\ny = f(wTφ) (4.120)\nwhere f(·) is known as theactivation functionin the machine learning literature, and\nf−1(·) is known as the link functionin statistics.\nNow consider the log likelihood function for this model, which, as a function of\nη,i sg i v e nb y\nlnp(t|η,s )=\nN∑\nn=1\nln p(tn|η,s )=\nN∑\nn=1\n{\nlng(ηn)+ ηntn\ns\n}\n+c o n s t (4.121)\nwhere we are assuming that all observations share a common scale parameter (which\ncorresponds to the noise variance for a Gaussian distribution for instance) and so s\nis independent of n. The derivative of the log likelihood with respect to the model\nparameters w is then given by\n∇w ln p(t|η,s )=\nN∑\nn=1\n{ d\ndηn\nlng(ηn)+ tn\ns\n} dηn\ndyn\ndyn\ndan\n∇an\n=\nN∑\nn=1\n1\ns {tn − yn}ψ′(yn)f′(an)φn (4.122)\nwhere an = wTφn, and we have used yn = f(an) together with the result (4.119)\nfor E[t|η]. We now see that there is a considerable simpliﬁcation if we choose a\nparticular form for the link function f−1(y) given by\nf−1(y)= ψ(y) (4.123)\nwhich gives f(ψ(y)) = y and hence f′(ψ)ψ′(y)=1 . Also, because a = f−1(y),\nwe have a = ψ and hence f′(a)ψ′(y)=1 . In this case, the gradient of the error\nfunction reduces to\n∇lnE(w)= 1\ns\nN∑\nn=1\n{yn − tn}φn. (4.124)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 232,
      "page_label": "213"
    }
  },
  {
    "page_content": "we have a = ψ and hence f′(a)ψ′(y)=1 . In this case, the gradient of the error\nfunction reduces to\n∇lnE(w)= 1\ns\nN∑\nn=1\n{yn − tn}φn. (4.124)\nFor the Gaussian s = β−1, whereas for the logistic model s =1 .\n4.4. The Laplace Approximation\nIn Section 4.5 we shall discuss the Bayesian treatment of logistic regression. As\nwe shall see, this is more complex than the Bayesian treatment of linear regression\nmodels, discussed in Sections 3.3 and 3.5. In particular, we cannot integrate exactly",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 232,
      "page_label": "213"
    }
  },
  {
    "page_content": "214 4. LINEAR MODELS FOR CLASSIFICATION\nover the parameter vector w since the posterior distribution is no longer Gaussian.\nIt is therefore necessary to introduce some form of approximation. Later in the\nbook we shall consider a range of techniques based on analytical approximationsChapter 10\nand numerical sampling.Chapter 11\nHere we introduce a simple, but widely used, framework called the Laplace ap-\nproximation, that aims to ﬁnd a Gaussian approximation to a probability density\ndeﬁned over a set of continuous variables. Consider ﬁrst the case of a single contin-\nuous variable z, and suppose the distribution p(z) is deﬁned by\np(z)= 1\nZ f(z) (4.125)\nwhere Z =\n∫\nf(z)d z is the normalization coefﬁcient. We shall suppose that the\nvalue of Z is unknown. In the Laplace method the goal is to ﬁnd a Gaussian approx-\nimation q(z) which is centred on a mode of the distribution p(z). The ﬁrst step is to\nﬁnd a mode of p(z), in other words a point z0 such that p′(z0)=0 , or equivalently\ndf(z)\ndz\n⏐⏐⏐⏐\nz=z0\n=0 . (4.126)\nA Gaussian distribution has the property that its logarithm is a quadratic function\nof the variables. We therefore consider a Taylor expansion of lnf(z) centred on the\nmode z0 so that\nlnf(z) ≃ ln f(z0) − 1\n2A(z − z0)2 (4.127)\nwhere\nA = − d2\ndz2 lnf(z)\n⏐⏐⏐⏐\nz=z0\n. (4.128)\nNote that the ﬁrst-order term in the Taylor expansion does not appear since z0 is a\nlocal maximum of the distribution. Taking the exponential we obtain\nf(z) ≃ f(z0)e x p\n{\n−A\n2 (z − z0)2\n}\n. (4.129)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 233,
      "page_label": "214"
    }
  },
  {
    "page_content": "local maximum of the distribution. Taking the exponential we obtain\nf(z) ≃ f(z0)e x p\n{\n−A\n2 (z − z0)2\n}\n. (4.129)\nWe can then obtain a normalized distribution q(z) by making use of the standard\nresult for the normalization of a Gaussian, so that\nq(z)=\n( A\n2π\n)1/2\nexp\n{\n−A\n2 (z − z0)2\n}\n. (4.130)\nThe Laplace approximation is illustrated in Figure 4.14. Note that the Gaussian\napproximation will only be well deﬁned if its precision A> 0, in other words the\nstationary point z0 must be a local maximum, so that the second derivative of f(z)\nat the point z0 is negative.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 233,
      "page_label": "214"
    }
  },
  {
    "page_content": "4.4. The Laplace Approximation 215\n−2 −1 0 1 2 3 4\n0\n0.2\n0.4\n0.6\n0.8\n−2 −1 0 1 2 3 4\n0\n10\n20\n30\n40\nFigure 4.14 Illustration of the Laplace approximation applied to the distribution p(z) ∝ exp(−z2/2)σ(20z +4 )\nwhere σ(z) is the logistic sigmoid function deﬁned by σ(z)=( 1+ e−z)−1. The left plot shows the normalized\ndistribution p(z) in yellow, together with the Laplace approximation centred on the mode z0 of p(z) in red. The\nright plot shows the negative logarithms of the corresponding curves.\nWe can extend the Laplace method to approximate a distributionp(z)= f(z)/Z\ndeﬁned over an M-dimensional space z. At a stationary point z0 the gradient ∇f(z)\nwill vanish. Expanding around this stationary point we have\nln f(z) ≃ ln f(z0) − 1\n2(z − z0)TA(z − z0) (4.131)\nwhere the M × M Hessian matrix A is deﬁned by\nA = −∇ ∇ lnf(z)|z=z0\n(4.132)\nand ∇ is the gradient operator. Taking the exponential of both sides we obtain\nf(z) ≃ f(z0)e x p\n{\n−1\n2(z − z0)TA(z − z0)\n}\n. (4.133)\nThe distribution q(z) is proportional to f(z) and the appropriate normalization coef-\nﬁcient can be found by inspection, using the standard result (2.43) for a normalized\nmultivariate Gaussian, giving\nq(z)= |A|1/2\n(2π)M/2 exp\n{\n−1\n2(z − z0)TA(z − z0)\n}\n= N(z|z0, A−1) (4.134)\nwhere |A| denotes the determinant of A. This Gaussian distribution will be well\ndeﬁned provided its precision matrix, given byA, is positive deﬁnite, which implies\nthat the stationary point z0 must be a local maximum, not a minimum or a saddle",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 234,
      "page_label": "215"
    }
  },
  {
    "page_content": "that the stationary point z0 must be a local maximum, not a minimum or a saddle\npoint.\nIn order to apply the Laplace approximation we ﬁrst need to ﬁnd the mode z0,\nand then evaluate the Hessian matrix at that mode. In practice a mode will typi-\ncally be found by running some form of numerical optimization algorithm (Bishop",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 234,
      "page_label": "215"
    }
  },
  {
    "page_content": "216 4. LINEAR MODELS FOR CLASSIFICATION\nand Nabney, 2008). Many of the distributions encountered in practice will be mul-\ntimodal and so there will be different Laplace approximations according to which\nmode is being considered. Note that the normalization constant Z of the true distri-\nbution does not need to be known in order to apply the Laplace method. As a result\nof the central limit theorem, the posterior distribution for a model is expected to\nbecome increasingly better approximated by a Gaussian as the number of observed\ndata points is increased, and so we would expect the Laplace approximation to be\nmost useful in situations where the number of data points is relatively large.\nOne major weakness of the Laplace approximation is that, since it is based on a\nGaussian distribution, it is only directly applicable to real variables. In other cases\nit may be possible to apply the Laplace approximation to a transformation of the\nvariable. For instance if 0 ⩽ τ< ∞ then we can consider a Laplace approximation\nof lnτ. The most serious limitation of the Laplace framework, however, is that\nit is based purely on the aspects of the true distribution at a speciﬁc value of the\nvariable, and so can fail to capture important global properties. In Chapter 10 we\nshall consider alternative approaches which adopt a more global perspective.\n4.4.1 Model comparison and BIC\nAs well as approximating the distribution p(z) we can also obtain an approxi-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 235,
      "page_label": "216"
    }
  },
  {
    "page_content": "4.4.1 Model comparison and BIC\nAs well as approximating the distribution p(z) we can also obtain an approxi-\nmation to the normalization constant Z. Using the approximation (4.133) we have\nZ =\n∫\nf(z)d z\n≃ f(z0)\n∫\nexp\n{\n−1\n2(z − z0)TA(z − z0)\n}\ndz\n= f(z0)(2π)M/2\n|A|1/2 (4.135)\nwhere we have noted that the integrand is Gaussian and made use of the standard\nresult (2.43) for a normalized Gaussian distribution. We can use the result (4.135) to\nobtain an approximation to the model evidence which, as discussed in Section 3.4,\nplays a central role in Bayesian model comparison.\nConsider a data set D and a set of models {Mi} having parameters {θi}.F o r\neach model we deﬁne a likelihood function p(D|θi, Mi). If we introduce a prior\np(θi|Mi) over the parameters, then we are interested in computing the model evi-\ndence p(D|Mi) for the various models. From now on we omit the conditioning on\nMi to keep the notation uncluttered. From Bayes’ theorem the model evidence is\ngiven by\np(D)=\n∫\np(D|θ)p(θ)d θ. (4.136)\nIdentifying f(θ)= p(D|θ)p(θ) and Z = p(D), and applying the result (4.135), we\nobtainExercise 4.22\nlnp(D) ≃ lnp(D|θMAP)+ lnp(θMAP)+ M\n2 ln(2π) − 1\n2 ln|A|\n  \nOccam factor\n(4.137)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 235,
      "page_label": "216"
    }
  },
  {
    "page_content": "4.5. Bayesian Logistic Regression 217\nwhere θMAP is the value of θ at the mode of the posterior distribution, and A is the\nHessian matrix of second derivatives of the negative log posterior\nA = −∇∇lnp(D|θMAP)p(θMAP)= −∇∇lnp(θMAP|D). (4.138)\nThe ﬁrst term on the right hand side of (4.137) represents the log likelihood evalu-\nated using the optimized parameters, while the remaining three terms comprise the\n‘Occam factor’ which penalizes model complexity.\nIf we assume that the Gaussian prior distribution over parameters is broad, and\nthat the Hessian has full rank, then we can approximate (4.137) very roughly usingExercise 4.23\nln p(D) ≃ lnp(D|θMAP) − 1\n2M lnN (4.139)\nwhere N is the number of data points, M is the number of parameters in θ and\nwe have omitted additive constants. This is known as the Bayesian Information\nCriterion (BIC) or the Schwarz criterion (Schwarz, 1978). Note that, compared to\nAIC given by (1.73), this penalizes model complexity more heavily.\nComplexity measures such as AIC and BIC have the virtue of being easy to\nevaluate, but can also give misleading results. In particular, the assumption that the\nHessian matrix has full rank is often not valid since many of the parameters are not\n‘well-determined’. We can use the result (4.137) to obtain a more accurate estimateSection 3.5.3\nof the model evidence starting from the Laplace approximation, as we illustrate in\nthe context of neural networks in Section 5.7.\n4.5. Bayesian Logistic Regression",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 236,
      "page_label": "217"
    }
  },
  {
    "page_content": "the context of neural networks in Section 5.7.\n4.5. Bayesian Logistic Regression\nWe now turn to a Bayesian treatment of logistic regression. Exact Bayesian infer-\nence for logistic regression is intractable. In particular, evaluation of the posterior\ndistribution would require normalization of the product of a prior distribution and a\nlikelihood function that itself comprises a product of logistic sigmoid functions, one\nfor every data point. Evaluation of the predictive distribution is similarly intractable.\nHere we consider the application of the Laplace approximation to the problem of\nBayesian logistic regression (Spiegelhalter and Lauritzen, 1990; MacKay, 1992b).\n4.5.1 Laplace approximation\nRecall from Section 4.4 that the Laplace approximation is obtained by ﬁnding\nthe mode of the posterior distribution and then ﬁtting a Gaussian centred at that\nmode. This requires evaluation of the second derivatives of the log posterior, which\nis equivalent to ﬁnding the Hessian matrix.\nBecause we seek a Gaussian representation for the posterior distribution, it is\nnatural to begin with a Gaussian prior, which we write in the general form\np(w)= N(w|m0, S0) (4.140)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 236,
      "page_label": "217"
    }
  },
  {
    "page_content": "218 4. LINEAR MODELS FOR CLASSIFICATION\nwhere m0 and S0 are ﬁxed hyperparameters. The posterior distribution over w is\ngiven by\np(w|t) ∝ p(w)p(t|w) (4.141)\nwhere t =( t1,...,t N )T. Taking the log of both sides, and substituting for the prior\ndistribution using (4.140), and for the likelihood function using (4.89), we obtain\nln p(w|t)= −1\n2(w − m0)TS−1\n0 (w − m0)\n+\nN∑\nn=1\n{tn lnyn +( 1− tn)l n ( 1− yn)} +c o n s t(4.142)\nwhere yn = σ(wTφn). To obtain a Gaussian approximation to the posterior dis-\ntribution, we ﬁrst maximize the posterior distribution to give the MAP (maximum\nposterior) solutionwMAP, which deﬁnes the mean of the Gaussian. The covariance\nis then given by the inverse of the matrix of second derivatives of the negative log\nlikelihood, which takes the form\nSN = −∇∇lnp(w|t)= S−1\n0 +\nN∑\nn=1\nyn(1 − yn)φnφT\nn. (4.143)\nThe Gaussian approximation to the posterior distribution therefore takes the form\nq(w)= N(w|wMAP, SN ). (4.144)\nHaving obtained a Gaussian approximation to the posterior distribution, there\nremains the task of marginalizing with respect to this distribution in order to make\npredictions.\n4.5.2 Predictive distribution\nThe predictive distribution for class C1, given a new feature vector φ(x),i s\nobtained by marginalizing with respect to the posterior distribution p(w|t), which is\nitself approximated by a Gaussian distribution q(w) so that\np(C1|φ, t)=\n∫\np(C1|φ, w)p(w|t)d w ≃\n∫\nσ(wTφ)q(w)d w (4.145)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 237,
      "page_label": "218"
    }
  },
  {
    "page_content": "itself approximated by a Gaussian distribution q(w) so that\np(C1|φ, t)=\n∫\np(C1|φ, w)p(w|t)d w ≃\n∫\nσ(wTφ)q(w)d w (4.145)\nwith the corresponding probability for classC2 given by p(C2|φ, t)=1 −p(C1|φ, t).\nTo evaluate the predictive distribution, we ﬁrst note that the function σ(wTφ) de-\npends on w only through its projection onto φ. Denoting a = wTφ,w eh a v e\nσ(wTφ)=\n∫\nδ(a − wTφ)σ(a)d a (4.146)\nwhere δ(·) is the Dirac delta function. From this we obtain\n∫\nσ(wTφ)q(w)d w =\n∫\nσ(a)p(a)d a (4.147)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 237,
      "page_label": "218"
    }
  },
  {
    "page_content": "4.5. Bayesian Logistic Regression 219\nwhere\np(a)=\n∫\nδ(a − wTφ)q(w)d w. (4.148)\nWe can evaluate p(a) by noting that the delta function imposes a linear constraint\non w and so forms a marginal distribution from the joint distribution q(w) by inte-\ngrating out all directions orthogonal to φ. Because q(w) is Gaussian, we know from\nSection 2.3.2 that the marginal distribution will also be Gaussian. We can evaluate\nthe mean and covariance of this distribution by taking moments, and interchanging\nthe order of integration over a and w, so that\nµa = E[a]=\n∫\np(a)ada =\n∫\nq(w)wTφdw = wT\nMAPφ (4.149)\nwhere we have used the result (4.144) for the variational posterior distributionq(w).\nSimilarly\nσ2\na =v a r [a]=\n∫\np(a)\n{\na2 − E[a]2}\nda\n=\n∫\nq(w)\n{\n(wTφ)2 − (mT\nN φ)2}\ndw = φTSN φ. (4.150)\nNote that the distribution of a takes the same form as the predictive distribution\n(3.58) for the linear regression model, with the noise variance set to zero. Thus our\nvariational approximation to the predictive distribution becomes\np(C1|t)=\n∫\nσ(a)p(a)d a =\n∫\nσ(a)N(a|µa,σ 2\na)d a. (4.151)\nThis result can also be derived directly by making use of the results for the marginal\nof a Gaussian distribution given in Section 2.3.2.Exercise 4.24\nThe integral over a represents the convolution of a Gaussian with a logistic sig-\nmoid, and cannot be evaluated analytically. We can, however, obtain a good approx-\nimation (Spiegelhalter and Lauritzen, 1990; MacKay, 1992b; Barber and Bishop,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 238,
      "page_label": "219"
    }
  },
  {
    "page_content": "imation (Spiegelhalter and Lauritzen, 1990; MacKay, 1992b; Barber and Bishop,\n1998a) by making use of the close similarity between the logistic sigmoid function\nσ(a) deﬁned by (4.59) and the probit function Φ(a) deﬁned by (4.114). In order to\nobtain the best approximation to the logistic function we need to re-scale the hori-\nzontal axis, so that we approximate σ(a) by Φ(λa). We can ﬁnd a suitable value of\nλ by requiring that the two functions have the same slope at the origin, which gives\nλ2 = π/8. The similarity of the logistic sigmoid and the probit function, for thisExercise 4.25\nchoice of λ, is illustrated in Figure 4.9.\nThe advantage of using a probit function is that its convolution with a Gaussian\ncan be expressed analytically in terms of another probit function. Speciﬁcally we\ncan show thatExercise 4.26\n∫\nΦ(λa)N(a|µ, σ2)d a =Φ\n( µ\n(λ−2 + σ2)1/2\n)\n. (4.152)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 238,
      "page_label": "219"
    }
  },
  {
    "page_content": "220 4. LINEAR MODELS FOR CLASSIFICATION\nWe now apply the approximation σ(a) ≃ Φ(λa) to the probit functions appearing\non both sides of this equation, leading to the following approximation for the convo-\nlution of a logistic sigmoid with a Gaussian\n∫\nσ(a)N(a|µ, σ2)d a ≃ σ\n(\nκ(σ2)µ\n)\n(4.153)\nwhere we have deﬁned\nκ(σ2) = (1 +πσ2/8)−1/2. (4.154)\nApplying this result to (4.151) we obtain the approximate predictive distribution\nin the form\np(C1|φ, t)= σ\n(\nκ(σ2\na)µa\n)\n(4.155)\nwhere µa and σ2\na are deﬁned by (4.149) and (4.150), respectively, and κ(σ2\na) is de-\nﬁned by (4.154).\nNote that the decision boundary corresponding to p(C1|φ, t)=0 .5 is given by\nµa =0 , which is the same as the decision boundary obtained by using the MAP\nvalue for w. Thus if the decision criterion is based on minimizing misclassiﬁca-\ntion rate, with equal prior probabilities, then the marginalization over w has no ef-\nfect. However, for more complex decision criteria it will play an important role.\nMarginalization of the logistic sigmoid model under a Gaussian approximation to\nthe posterior distribution will be illustrated in the context of variational inference in\nFigure 10.13.\nExercises\n4.1 (⋆⋆ ) Given a set of data points {xn}, we can deﬁne the convex hullto be the set of\nall points x given by\nx =\n∑\nn\nαnxn (4.156)\nwhere αn ⩾ 0 and ∑\nn αn =1 . Consider a second set of points {yn} together with\ntheir corresponding convex hull. By deﬁnition, the two sets of points will be linearly",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 239,
      "page_label": "220"
    }
  },
  {
    "page_content": "n αn =1 . Consider a second set of points {yn} together with\ntheir corresponding convex hull. By deﬁnition, the two sets of points will be linearly\nseparable if there exists a vector ˆw and a scalar w0 such that ˆwTxn + w0 > 0 for all\nxn, and ˆwTyn +w0 < 0 for all yn. Show that if their convex hulls intersect, the two\nsets of points cannot be linearly separable, and conversely that if they are linearly\nseparable, their convex hulls do not intersect.\n4.2 (⋆⋆ ) www Consider the minimization of a sum-of-squares error function (4.15),\nand suppose that all of the target vectors in the training set satisfy a linear constraint\naTtn + b =0 (4.157)\nwhere tn corresponds to the nth row of the matrix T in (4.15). Show that as a\nconsequence of this constraint, the elements of the model prediction y(x) given by\nthe least-squares solution (4.17) also satisfy this constraint, so that\naTy(x)+ b =0 . (4.158)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 239,
      "page_label": "220"
    }
  },
  {
    "page_content": "Exercises 221\nTo do so, assume that one of the basis functionsφ0(x)=1 so that the corresponding\nparameter w0 plays the role of a bias.\n4.3 (⋆⋆ ) Extend the result of Exercise 4.2 to show that if multiple linear constraints\nare satisﬁed simultaneously by the target vectors, then the same constraints will also\nbe satisﬁed by the least-squares prediction of a linear model.\n4.4 (⋆) www Show that maximization of the class separation criterion given by (4.23)\nwith respect to w, using a Lagrange multiplier to enforce the constraint wTw =1 ,\nleads to the result that w ∝ (m2 − m1).\n4.5 (⋆) By making use of (4.20), (4.23), and (4.24), show that the Fisher criterion (4.25)\ncan be written in the form (4.26).\n4.6 (⋆) Using the deﬁnitions of the between-class and within-class covariance matrices\ngiven by (4.27) and (4.28), respectively, together with (4.34) and (4.36) and the\nchoice of target values described in Section 4.1.5, show that the expression (4.33)\nthat minimizes the sum-of-squares error function can be written in the form (4.37).\n4.7 (⋆) www Show that the logistic sigmoid function (4.59) satisﬁes the property\nσ(−a)=1 − σ(a) and that its inverse is given by σ−1(y)=l n {y/(1 − y)}.\n4.8 (⋆) Using (4.57) and (4.58), derive the result (4.65) for the posterior class probability\nin the two-class generative model with Gaussian densities, and verify the results\n(4.66) and (4.67) for the parametersw and w0.\n4.9 (⋆) www Consider a generative classiﬁcation model for K classes deﬁned by",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 240,
      "page_label": "221"
    }
  },
  {
    "page_content": "(4.66) and (4.67) for the parametersw and w0.\n4.9 (⋆) www Consider a generative classiﬁcation model for K classes deﬁned by\nprior class probabilities p(Ck)= πk and general class-conditional densities p(φ|Ck)\nwhere φ is the input feature vector. Suppose we are given a training data set{φn, tn}\nwhere n =1 ,...,N , and tn is a binary target vector of length K that uses the 1-of-\nK coding scheme, so that it has components tnj = Ijk if pattern n is from class Ck.\nAssuming that the data points are drawn independently from this model, show that\nthe maximum-likelihood solution for the prior probabilities is given by\nπk = Nk\nN (4.159)\nwhere Nk is the number of data points assigned to class Ck.\n4.10 (⋆⋆ ) Consider the classiﬁcation model of Exercise 4.9 and now suppose that the\nclass-conditional densities are given by Gaussian distributions with a shared covari-\nance matrix, so that\np(φ|Ck)= N(φ|µk, Σ). (4.160)\nShow that the maximum likelihood solution for the mean of the Gaussian distribution\nfor class Ck is given by\nµk = 1\nNk\nN∑\nn=1\ntnkφn (4.161)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 240,
      "page_label": "221"
    }
  },
  {
    "page_content": "222 4. LINEAR MODELS FOR CLASSIFICATION\nwhich represents the mean of those feature vectors assigned to class Ck. Similarly,\nshow that the maximum likelihood solution for the shared covariance matrix is given\nby\nΣ =\nK∑\nk=1\nNk\nN Sk (4.162)\nwhere\nSk = 1\nNk\nN∑\nn=1\ntnk(φn − µk)(φn − µk)T. (4.163)\nThus Σ is given by a weighted average of the covariances of the data associated with\neach class, in which the weighting coefﬁcients are given by the prior probabilities of\nthe classes.\n4.11 (⋆⋆ ) Consider a classiﬁcation problem with K classes for which the feature vector\nφ has M components each of which can take L discrete states. Let the values of the\ncomponents be represented by a 1-of-L binary coding scheme. Further suppose that,\nconditioned on the class Ck, the M components of φ are independent, so that the\nclass-conditional density factorizes with respect to the feature vector components.\nShow that the quantitiesak given by (4.63), which appear in the argument to the\nsoftmax function describing the posterior class probabilities, are linear functions of\nthe components of φ. Note that this represents an example of the naive Bayes model\nwhich is discussed in Section 8.2.2.\n4.12 (⋆) www Verify the relation (4.88) for the derivative of the logistic sigmoid func-\ntion deﬁned by (4.59).\n4.13 (⋆) www By making use of the result (4.88) for the derivative of the logistic sig-\nmoid, show that the derivative of the error function (4.90) for the logistic regression\nmodel is given by (4.91).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 241,
      "page_label": "222"
    }
  },
  {
    "page_content": "moid, show that the derivative of the error function (4.90) for the logistic regression\nmodel is given by (4.91).\n4.14 (⋆) Show that for a linearly separable data set, the maximum likelihood solution\nfor the logistic regression model is obtained by ﬁnding a vector w whose decision\nboundary wTφ(x)=0 separates the classes and then taking the magnitude of w to\ninﬁnity.\n4.15 (⋆⋆ ) Show that the Hessian matrix H for the logistic regression model, given by\n(4.97), is positive deﬁnite. Here R is a diagonal matrix with elements yn(1 − yn),\nand yn is the output of the logistic regression model for input vectorxn. Hence show\nthat the error function is a concave function of w and that it has a unique minimum.\n4.16 (⋆) Consider a binary classiﬁcation problem in which each observationxn is known\nto belong to one of two classes, corresponding to t =0 and t =1 , and suppose that\nthe procedure for collecting training data is imperfect, so that training points are\nsometimes mislabelled. For every data point xn, instead of having a value t for the\nclass label, we have instead a value πn representing the probability that tn =1 .\nGiven a probabilistic model p(t =1 |φ), write down the log likelihood function\nappropriate to such a data set.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 241,
      "page_label": "222"
    }
  },
  {
    "page_content": "Exercises 223\n4.17 (⋆) www Show that the derivatives of the softmax activation function (4.104),\nwhere the ak are deﬁned by (4.105), are given by (4.106).\n4.18 (⋆) Using the result (4.91) for the derivatives of the softmax activation function,\nshow that the gradients of the cross-entropy error (4.108) are given by (4.109).\n4.19 (⋆) www Write down expressions for the gradient of the log likelihood, as well\nas the corresponding Hessian matrix, for the probit regression model deﬁned in Sec-\ntion 4.3.5. These are the quantities that would be required to train such a model using\nIRLS.\n4.20 (⋆⋆ ) Show that the Hessian matrix for the multiclass logistic regression problem,\ndeﬁned by (4.110), is positive semideﬁnite. Note that the full Hessian matrix for\nthis problem is of size MK × MK , where M is the number of parameters and K\nis the number of classes. To prove the positive semideﬁnite property, consider the\nproduct uTHu where u is an arbitrary vector of lengthMK , and then apply Jensen’s\ninequality.\n4.21 (⋆) Show that the probit function (4.114) and the erf function (4.115) are related by\n(4.116).\n4.22 (⋆) Using the result (4.135), derive the expression (4.137) for the log model evi-\ndence under the Laplace approximation.\n4.23 (⋆⋆ ) www In this exercise, we derive the BIC result (4.139) starting from the\nLaplace approximation to the model evidence given by (4.137). Show that if the\nprior over parameters is Gaussian of the form p(θ)= N(θ|m, V0), the log model",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 242,
      "page_label": "223"
    }
  },
  {
    "page_content": "prior over parameters is Gaussian of the form p(θ)= N(θ|m, V0), the log model\nevidence under the Laplace approximation takes the form\nln p(D) ≃ lnp(D|θMAP) − 1\n2(θMAP − m)TV−1\n0 (θMAP − m) − 1\n2 ln|H| +c o n s t\nwhere H is the matrix of second derivatives of the log likelihoodlnp(D|θ) evaluated\nat θMAP. Now assume that the prior is broad so that V−1\n0 is small and the second\nterm on the right-hand side above can be neglected. Furthermore, consider the case\nof independent, identically distributed data so thatH is the sum of terms one for each\ndata point. Show that the log model evidence can then be written approximately in\nthe form of the BIC expression (4.139).\n4.24 (⋆⋆ ) Use the results from Section 2.3.2 to derive the result (4.151) for the marginal-\nization of the logistic regression model with respect to a Gaussian posterior distribu-\ntion over the parameters w.\n4.25 (⋆⋆ ) Suppose we wish to approximate the logistic sigmoid σ(a) deﬁned by (4.59)\nby a scaled probit function Φ(λa), where Φ(a) is deﬁned by (4.114). Show that if\nλ is chosen so that the derivatives of the two functions are equal at a =0 , then\nλ2 = π/8.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 242,
      "page_label": "223"
    }
  },
  {
    "page_content": "224 4. LINEAR MODELS FOR CLASSIFICATION\n4.26 (⋆⋆ ) In this exercise, we prove the relation (4.152) for the convolution of a probit\nfunction with a Gaussian distribution. To do this, show that the derivative of the left-\nhand side with respect toµ is equal to the derivative of the right-hand side, and then\nintegrate both sides with respect to µ and then show that the constant of integration\nvanishes. Note that before differentiating the left-hand side, it is convenient ﬁrst\nto introduce a change of variable given bya = µ + σz so that the integral over a\nis replaced by an integral over z. When we differentiate the left-hand side of the\nrelation (4.152), we will then obtain a Gaussian integral overz that can be evaluated\nanalytically.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 243,
      "page_label": "224"
    }
  },
  {
    "page_content": "5\nNeural\nNetworks\nIn Chapters 3 and 4 we considered models for regression and classiﬁcation that com-\nprised linear combinations of ﬁxed basis functions. We saw that such models have\nuseful analytical and computational properties but that their practical applicability\nwas limited by the curse of dimensionality. In order to apply such models to large-\nscale problems, it is necessary to adapt the basis functions to the data.\nSupport vector machines (SVMs), discussed in Chapter 7, address this by ﬁrst\ndeﬁning basis functions that are centred on the training data points and then selecting\na subset of these during training. One advantage of SVMs is that, although the\ntraining involves nonlinear optimization, the objective function is convex, and so the\nsolution of the optimization problem is relatively straightforward. The number of\nbasis functions in the resulting models is generally much smaller than the number of\ntraining points, although it is often still relatively large and typically increases with\nthe size of the training set. The relevance vector machine, discussed in Section 7.2,\nalso chooses a subset from a ﬁxed set of basis functions and typically results in much\n225",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 244,
      "page_label": "225"
    }
  },
  {
    "page_content": "226 5. NEURAL NETWORKS\nsparser models. Unlike the SVM it also produces probabilistic outputs, although this\nis at the expense of a nonconvex optimization during training.\nAn alternative approach is to ﬁx the number of basis functions in advance but\nallow them to be adaptive, in other words to use parametric forms for the basis func-\ntions in which the parameter values are adapted during training. The most successful\nmodel of this type in the context of pattern recognition is the feed-forward neural\nnetwork, also known as the multilayer perceptron, discussed in this chapter. In fact,\n‘multilayer perceptron’ is really a misnomer, because the model comprises multi-\nple layers of logistic regression models (with continuous nonlinearities) rather than\nmultiple perceptrons (with discontinuous nonlinearities). For many applications, the\nresulting model can be signiﬁcantly more compact, and hence faster to evaluate, than\na support vector machine having the same generalization performance. The price to\nbe paid for this compactness, as with the relevance vector machine, is that the like-\nlihood function, which forms the basis for network training, is no longer a convex\nfunction of the model parameters. In practice, however, it is often worth investing\nsubstantial computational resources during the training phase in order to obtain a\ncompact model that is fast at processing new data.\nThe term ‘neural network’ has its origins in attempts to ﬁnd mathematical rep-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 245,
      "page_label": "226"
    }
  },
  {
    "page_content": "compact model that is fast at processing new data.\nThe term ‘neural network’ has its origins in attempts to ﬁnd mathematical rep-\nresentations of information processing in biological systems (McCulloch and Pitts,\n1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhartet al., 1986). Indeed,\nit has been used very broadly to cover a wide range of different models, many of\nwhich have been the subject of exaggerated claims regarding their biological plau-\nsibility. From the perspective of practical applications of pattern recognition, how-\never, biological realism would impose entirely unnecessary constraints. Our focus in\nthis chapter is therefore on neural networks as efﬁcient models for statistical pattern\nrecognition. In particular, we shall restrict our attention to the speciﬁc class of neu-\nral networks that have proven to be of greatest practical value, namely the multilayer\nperceptron.\nWe begin by considering the functional form of the network model, including\nthe speciﬁc parameterization of the basis functions, and we then discuss the prob-\nlem of determining the network parameters within a maximum likelihood frame-\nwork, which involves the solution of a nonlinear optimization problem. This requires\nthe evaluation of derivatives of the log likelihood function with respect to the net-\nwork parameters, and we shall see how these can be obtained efﬁciently using the\ntechnique of error backpropagation. We shall also show how the backpropagation",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 245,
      "page_label": "226"
    }
  },
  {
    "page_content": "technique of error backpropagation. We shall also show how the backpropagation\nframework can be extended to allow other derivatives to be evaluated, such as the\nJacobian and Hessian matrices. Next we discuss various approaches to regulariza-\ntion of neural network training and the relationships between them. We also consider\nsome extensions to the neural network model, and in particular we describe a gen-\neral framework for modelling conditional probability distributions known asmixture\ndensity networks. Finally, we discuss the use of Bayesian treatments of neural net-\nworks. Additional background on neural network models can be found in Bishop\n(1995a).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 245,
      "page_label": "226"
    }
  },
  {
    "page_content": "5.1. Feed-forward Network Functions 227\n5.1. Feed-forward Network Functions\nThe linear models for regression and classiﬁcation discussed in Chapters 3 and 4, re-\nspectively, are based on linear combinations of ﬁxed nonlinear basis functionsφj(x)\nand take the form\ny(x,w)= f\n( M∑\nj=1\nwjφj(x)\n)\n(5.1)\nwhere f(·) is a nonlinear activation function in the case of classiﬁcation and is the\nidentity in the case of regression. Our goal is to extend this model by making the\nbasis functions φj(x) depend on parameters and then to allow these parameters to\nbe adjusted, along with the coefﬁcients {wj}, during training. There are, of course,\nmany ways to construct parametric nonlinear basis functions. Neural networks use\nbasis functions that follow the same form as (5.1), so that each basis function is itself\na nonlinear function of a linear combination of the inputs, where the coefﬁcients in\nthe linear combination are adaptive parameters.\nThis leads to the basic neural network model, which can be described a series\nof functional transformations. First we construct M linear combinations of the input\nvariables x1,...,x D in the form\naj =\nD∑\ni=1\nw(1)\nji xi + w(1)\nj0 (5.2)\nwhere j =1 ,...,M , and the superscript(1) indicates that the corresponding param-\neters are in the ﬁrst ‘layer’ of the network. We shall refer to the parameters w(1)\nji as\nweights and the parameters w(1)\nj0 as biases, following the nomenclature of Chapter 3.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 246,
      "page_label": "227"
    }
  },
  {
    "page_content": "ji as\nweights and the parameters w(1)\nj0 as biases, following the nomenclature of Chapter 3.\nThe quantities aj are known as activations. Each of them is then transformed using\na differentiable, nonlinear activation functionh(·) to give\nzj = h(aj). (5.3)\nThese quantities correspond to the outputs of the basis functions in (5.1) that, in the\ncontext of neural networks, are called hidden units. The nonlinear functions h(·) are\ngenerally chosen to be sigmoidal functions such as the logistic sigmoid or the ‘tanh’\nfunction. Following (5.1), these values are again linearly combined to give outputExercise 5.1\nunit activations\nak =\nM∑\nj=1\nw(2)\nkj zj + w(2)\nk0 (5.4)\nwhere k =1 ,...,K , and K is the total number of outputs. This transformation cor-\nresponds to the second layer of the network, and again the w(2)\nk0 are bias parameters.\nFinally, the output unit activations are transformed using an appropriate activation\nfunction to give a set of network outputsyk. The choice of activation function is\ndetermined by the nature of the data and the assumed distribution of target variables",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 246,
      "page_label": "227"
    }
  },
  {
    "page_content": "228 5. NEURAL NETWORKS\nFigure 5.1 Network diagram for the two-\nlayer neural network corre-\nsponding to (5.7). The input,\nhidden, and output variables\nare represented by nodes, and\nthe weight parameters are rep-\nresented by links between the\nnodes, in which the bias pa-\nrameters are denoted by links\ncoming from additional input\nand hidden variables x0 and\nz0. Arrows denote the direc-\ntion of information ﬂow through\nthe network during forward\npropagation. x0\nx1\nxD\nz0\nz1\nzM\ny1\nyK\nw(1)\nMD w(2)\nKM\nw(2)\n10\nhidden units\ninputs outputs\nand follows the same considerations as for linear models discussed in Chapters 3 and\n4. Thus for standard regression problems, the activation function is the identity so\nthat yk = ak. Similarly, for multiple binary classiﬁcation problems, each output unit\nactivation is transformed using a logistic sigmoid function so that\nyk = σ(ak) (5.5)\nwhere\nσ(a)= 1\n1+e x p (−a). (5.6)\nFinally, for multiclass problems, a softmax activation function of the form (4.62)\nis used. The choice of output unit activation function is discussed in detail in Sec-\ntion 5.2.\nWe can combine these various stages to give the overall network function that,\nfor sigmoidal output unit activation functions, takes the form\nyk(x,w)= σ\n( M∑\nj=1\nw(2)\nkj h\n( D∑\ni=1\nw(1)\nji xi + w(1)\nj0\n)\n+ w(2)\nk0\n)\n(5.7)\nwhere the set of all weight and bias parameters have been grouped together into a\nvector w. Thus the neural network model is simply a nonlinear function from a set",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 247,
      "page_label": "228"
    }
  },
  {
    "page_content": "vector w. Thus the neural network model is simply a nonlinear function from a set\nof input variables {xi} to a set of output variables {yk} controlled by a vector w of\nadjustable parameters.\nThis function can be represented in the form of a network diagram as shown\nin Figure 5.1. The process of evaluating (5.7) can then be interpreted as a forward\npropagation of information through the network. It should be emphasized that these\ndiagrams do not represent probabilistic graphical models of the kind to be consid-\nered in Chapter 8 because the internal nodes represent deterministic variables rather\nthan stochastic ones. For this reason, we have adopted a slightly different graphical",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 247,
      "page_label": "228"
    }
  },
  {
    "page_content": "5.1. Feed-forward Network Functions 229\nnotation for the two kinds of model. We shall see later how to give a probabilistic\ninterpretation to a neural network.\nAs discussed in Section 3.1, the bias parameters in (5.2) can be absorbed into\nthe set of weight parameters by deﬁning an additional input variablex0 whose value\nis clamped at x0 =1 , so that (5.2) takes the form\naj =\nD∑\ni=0\nw(1)\nji xi. (5.8)\nWe can similarly absorb the second-layer biases into the second-layer weights, so\nthat the overall network function becomes\nyk(x,w)= σ\n( M∑\nj=0\nw(2)\nkj h\n( D∑\ni=0\nw(1)\nji xi\n))\n. (5.9)\nAs can be seen from Figure 5.1, the neural network model comprises two stages\nof processing, each of which resembles the perceptron model of Section 4.1.7, and\nfor this reason the neural network is also known as the multilayer perceptron,o r\nMLP. A key difference compared to the perceptron, however, is that the neural net-\nwork uses continuous sigmoidal nonlinearities in the hidden units, whereas the per-\nceptron uses step-function nonlinearities. This means that the neural network func-\ntion is differentiable with respect to the network parameters, and this property will\nplay a central role in network training.\nIf the activation functions of all the hidden units in a network are taken to be\nlinear, then for any such network we can always ﬁnd an equivalent network without\nhidden units. This follows from the fact that the composition of successive linear",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 248,
      "page_label": "229"
    }
  },
  {
    "page_content": "hidden units. This follows from the fact that the composition of successive linear\ntransformations is itself a linear transformation. However, if the number of hidden\nunits is smaller than either the number of input or output units, then the transforma-\ntions that the network can generate are not the most general possible linear trans-\nformations from inputs to outputs because information is lost in the dimensionality\nreduction at the hidden units. In Section 12.4.2, we show that networks of linear\nunits give rise to principal component analysis. In general, however, there is little\ninterest in multilayer networks of linear units.\nThe network architecture shown in Figure 5.1 is the most commonly used one\nin practice. However, it is easily generalized, for instance by considering additional\nlayers of processing each consisting of a weighted linear combination of the form\n(5.4) followed by an element-wise transformation using a nonlinear activation func-\ntion. Note that there is some confusion in the literature regarding the terminology\nfor counting the number of layers in such networks. Thus the network in Figure 5.1\nmay be described as a 3-layer network (which counts the number of layers of units,\nand treats the inputs as units) or sometimes as a single-hidden-layer network (which\ncounts the number of layers of hidden units). We recommend a terminology in which\nFigure 5.1 is called a two-layer network, because it is the number of layers of adap-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 248,
      "page_label": "229"
    }
  },
  {
    "page_content": "Figure 5.1 is called a two-layer network, because it is the number of layers of adap-\ntive weights that is important for determining the network properties.\nAnother generalization of the network architecture is to include skip-layer con-\nnections, each of which is associated with a corresponding adaptive parameter. For",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 248,
      "page_label": "229"
    }
  },
  {
    "page_content": "230 5. NEURAL NETWORKS\nFigure 5.2 Example of a neural network having a\ngeneral feed-forward topology. Note that\neach hidden and output unit has an\nassociated bias parameter (omitted for\nclarity).\nx1\nx2\nz1\nz3\nz2\ny1\ny2\ninputs outputs\ninstance, in a two-layer network these would go directly from inputs to outputs. In\nprinciple, a network with sigmoidal hidden units can always mimic skip layer con-\nnections (for bounded input values) by using a sufﬁciently small ﬁrst-layer weight\nthat, over its operating range, the hidden unit is effectively linear, and then com-\npensating with a large weight value from the hidden unit to the output. In practice,\nhowever, it may be advantageous to include skip-layer connections explicitly.\nFurthermore, the network can be sparse, with not all possible connections within\na layer being present. We shall see an example of a sparse network architecture when\nwe consider convolutional neural networks in Section 5.5.6.\nBecause there is a direct correspondence between a network diagram and its\nmathematical function, we can develop more general network mappings by con-\nsidering more complex network diagrams. However, these must be restricted to a\nfeed-forward architecture, in other words to one having no closed directed cycles, to\nensure that the outputs are deterministic functions of the inputs. This is illustrated\nwith a simple example in Figure 5.2. Each (hidden or output) unit in such a network\ncomputes a function given by\nzk = h\n( ∑\nj\nwkjzj\n)\n(5.10)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 249,
      "page_label": "230"
    }
  },
  {
    "page_content": "with a simple example in Figure 5.2. Each (hidden or output) unit in such a network\ncomputes a function given by\nzk = h\n( ∑\nj\nwkjzj\n)\n(5.10)\nwhere the sum runs over all units that send connections to unit k (and a bias param-\neter is included in the summation). For a given set of values applied to the inputs of\nthe network, successive application of (5.10) allows the activations of all units in the\nnetwork to be evaluated including those of the output units.\nThe approximation properties of feed-forward networks have been widely stud-\nied (Funahashi, 1989; Cybenko, 1989; Horniket al., 1989; Stinchecombe and White,\n1989; Cotter, 1990; Ito, 1991; Hornik, 1991; Kreinovich, 1991; Ripley, 1996) and\nfound to be very general. Neural networks are therefore said to be universal ap-\nproximators. For example, a two-layer network with linear outputs can uniformly\napproximate any continuous function on a compact input domain to arbitrary accu-\nracy provided the network has a sufﬁciently large number of hidden units. This result\nholds for a wide range of hidden unit activation functions, but excluding polynomi-\nals. Although such theorems are reassuring, the key problem is how to ﬁnd suitable\nparameter values given a set of training data, and in later sections of this chapter we",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 249,
      "page_label": "230"
    }
  },
  {
    "page_content": "5.1. Feed-forward Network Functions 231\nFigure 5.3 Illustration of the ca-\npability of a multilayer perceptron\nto approximate four different func-\ntions comprising (a) f(x)= x2, (b)\nf(x)=s i n ( x),( c ) , f(x)= |x|,\nand (d) f(x)= H(x) where H(x)\nis the Heaviside step function. In\neach case,N =5 0 data points,\nshown as blue dots, have been sam-\npled uniformly inx over the interval\n(−1, 1) and the corresponding val-\nues of f(x) evaluated. These data\npoints are then used to train a two-\nlayer network having 3 hidden units\nwith ‘tanh’ activation functions and\nlinear output units. The resulting\nnetwork functions are shown by the\nred curves, and the outputs of the\nthree hidden units are shown by the\nthree dashed curves.\n(a) (b)\n(c) (d)\nwill show that there exist effective solutions to this problem based on both maximum\nlikelihood and Bayesian approaches.\nThe capability of a two-layer network to model a broad range of functions is\nillustrated in Figure 5.3. This ﬁgure also shows how individual hidden units work\ncollaboratively to approximate the ﬁnal function. The role of hidden units in a simple\nclassiﬁcation problem is illustrated in Figure 5.4 using the synthetic classiﬁcation\ndata set described in Appendix A.\n5.1.1 Weight-space symmetries\nOne property of feed-forward networks, which will play a role when we consider\nBayesian model comparison, is that multiple distinct choices for the weight vector",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 250,
      "page_label": "231"
    }
  },
  {
    "page_content": "Bayesian model comparison, is that multiple distinct choices for the weight vector\nw can all give rise to the same mapping function from inputs to outputs (Chenet al.,\n1993). Consider a two-layer network of the form shown in Figure 5.1 withM hidden\nunits having ‘tanh’ activation functions and full connectivity in both layers. If we\nchange the sign of all of the weights and the bias feeding into a particular hidden\nunit, then, for a given input pattern, the sign of the activation of the hidden unit will\nbe reversed, because ‘tanh’ is an odd function, so thattanh(−a)= −tanh(a). This\ntransformation can be exactly compensated by changing the sign of all of the weights\nleading out of that hidden unit. Thus, by changing the signs of a particular group of\nweights (and a bias), the input–output mapping function represented by the network\nis unchanged, and so we have found two different weight vectors that give rise to\nthe same mapping function. ForM hidden units, there will be M such ‘sign-ﬂip’",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 250,
      "page_label": "231"
    }
  },
  {
    "page_content": "232 5. NEURAL NETWORKS\nFigure 5.4 Example of the solution of a simple two-\nclass classiﬁcation problem involving\nsynthetic data using a neural network\nhaving two inputs, two hidden units with\n‘tanh’ activation functions, and a single\noutput having a logistic sigmoid activa-\ntion function. The dashed blue lines\nshow the z =0 .5 contours for each of\nthe hidden units, and the red line shows\nthe y =0 .5 decision surface for the net-\nwork. For comparison, the green line\ndenotes the optimal decision boundary\ncomputed from the distributions used to\ngenerate the data.\n−2 −1 0 1 2\n−2\n−1\n0\n1\n2\n3\nsymmetries, and thus any given weight vector will be one of a set 2M equivalent\nweight vectors .\nSimilarly, imagine that we interchange the values of all of the weights (and the\nbias) leading both into and out of a particular hidden unit with the corresponding\nvalues of the weights (and bias) associated with a different hidden unit. Again, this\nclearly leaves the network input–output mapping function unchanged, but it corre-\nsponds to a different choice of weight vector. For M hidden units, any given weight\nvector will belong to a set ofM! equivalent weight vectors associated with this inter-\nchange symmetry, corresponding to the M! different orderings of the hidden units.\nThe network will therefore have an overall weight-space symmetry factor ofM!2M .\nFor networks with more than two layers of weights, the total level of symmetry will",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 251,
      "page_label": "232"
    }
  },
  {
    "page_content": "For networks with more than two layers of weights, the total level of symmetry will\nbe given by the product of such factors, one for each layer of hidden units.\nIt turns out that these factors account for all of the symmetries in weight space\n(except for possible accidental symmetries due to speciﬁc choices for the weight val-\nues). Furthermore, the existence of these symmetries is not a particular property of\nthe ‘tanh’ function but applies to a wide range of activation functions (K˙urkov´a and\nKainen, 1994). In many cases, these symmetries in weight space are of little practi-\ncal consequence, although in Section 5.7 we shall encounter a situation in which we\nneed to take them into account.\n5.2. Network Training\nSo far, we have viewed neural networks as a general class of parametric nonlinear\nfunctions from a vector x of input variables to a vector y of output variables. A\nsimple approach to the problem of determining the network parameters is to make an\nanalogy with the discussion of polynomial curve ﬁtting in Section 1.1, and therefore\nto minimize a sum-of-squares error function. Given a training set comprising a set\nof input vectors {xn}, where n =1 ,...,N , together with a corresponding set of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 251,
      "page_label": "232"
    }
  },
  {
    "page_content": "5.2. Network Training 233\ntarget vectors {tn}, we minimize the error function\nE(w)= 1\n2\nN∑\nn=1\n∥y(xn, w) − tn∥2. (5.11)\nHowever, we can provide a much more general view of network training by ﬁrst\ngiving a probabilistic interpretation to the network outputs. We have already seen\nmany advantages of using probabilistic predictions in Section 1.5.4. Here it will also\nprovide us with a clearer motivation both for the choice of output unit nonlinearity\nand the choice of error function.\nWe start by discussing regression problems, and for the moment we consider\na single target variable t that can take any real value. Following the discussions\nin Section 1.2.5 and 3.1, we assume that t has a Gaussian distribution with an x-\ndependent mean, which is given by the output of the neural network, so that\np(t|x, w)= N\n(\nt|y(x, w),β −1)\n(5.12)\nwhere β is the precision (inverse variance) of the Gaussian noise. Of course this\nis a somewhat restrictive assumption, and in Section 5.6 we shall see how to extend\nthis approach to allow for more general conditional distributions. For the conditional\ndistribution given by (5.12), it is sufﬁcient to take the output unit activation function\nto be the identity, because such a network can approximate any continuous function\nfrom x to y. Given a data set of N independent, identically distributed observations\nX = {x1,..., xN }, along with corresponding target values t = {t1,...,t N },w e\ncan construct the corresponding likelihood function\np(t|X, w,β )=\nN∏",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 252,
      "page_label": "233"
    }
  },
  {
    "page_content": "X = {x1,..., xN }, along with corresponding target values t = {t1,...,t N },w e\ncan construct the corresponding likelihood function\np(t|X, w,β )=\nN∏\nn=1\np(tn|xn, w,β ).\nTaking the negative logarithm, we obtain the error function\nβ\n2\nN∑\nn=1\n{y(xn, w) − tn}2 − N\n2 lnβ + N\n2 ln(2π) (5.13)\nwhich can be used to learn the parameters w and β. In Section 5.7, we shall dis-\ncuss the Bayesian treatment of neural networks, while here we consider a maximum\nlikelihood approach. Note that in the neural networks literature, it is usual to con-\nsider the minimization of an error function rather than the maximization of the (log)\nlikelihood, and so here we shall follow this convention. Consider ﬁrst the determi-\nnation of w. Maximizing the likelihood function is equivalent to minimizing the\nsum-of-squares error function given by\nE(w)= 1\n2\nN∑\nn=1\n{y(xn, w) − tn}2 (5.14)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 252,
      "page_label": "233"
    }
  },
  {
    "page_content": "234 5. NEURAL NETWORKS\nwhere we have discarded additive and multiplicative constants. The value ofw found\nby minimizing E(w) will be denoted wML because it corresponds to the maximum\nlikelihood solution. In practice, the nonlinearity of the network function y(xn, w)\ncauses the error E(w) to be nonconvex, and so in practice local maxima of the\nlikelihood may be found, corresponding to local minima of the error function, as\ndiscussed in Section 5.2.1.\nHaving found wML, the value of β can be found by minimizing the negative log\nlikelihood to give\n1\nβML\n= 1\nN\nN∑\nn=1\n{y(xn, wML) − tn}2. (5.15)\nNote that this can be evaluated once the iterative optimization required to ﬁnd wML\nis completed. If we have multiple target variables, and we assume that they are inde-\npendent conditional on x and w with shared noise precision β, then the conditional\ndistribution of the target values is given by\np(t|x, w)= N\n(\nt|y(x, w),β −1I\n)\n. (5.16)\nFollowing the same argument as for a single target variable, we see that the maximum\nlikelihood weights are determined by minimizing the sum-of-squares error function\n(5.11). The noise precision is then given byExercise 5.2\n1\nβML\n= 1\nNK\nN∑\nn=1\n∥y(xn, wML) − tn∥2 (5.17)\nwhere K is the number of target variables. The assumption of independence can be\ndropped at the expense of a slightly more complex optimization problem.Exercise 5.3\nRecall from Section 4.3.6 that there is a natural pairing of the error function",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 253,
      "page_label": "234"
    }
  },
  {
    "page_content": "Recall from Section 4.3.6 that there is a natural pairing of the error function\n(given by the negative log likelihood) and the output unit activation function. In the\nregression case, we can view the network as having an output activation function that\nis the identity, so that yk = ak. The corresponding sum-of-squares error function\nhas the property\n∂E\n∂ak\n= yk − tk (5.18)\nwhich we shall make use of when discussing error backpropagation in Section 5.3.\nNow consider the case of binary classiﬁcation in which we have a single target\nvariable t such that t =1 denotes class C1 and t =0 denotes class C2. Following\nthe discussion of canonical link functions in Section 4.3.6, we consider a network\nhaving a single output whose activation function is a logistic sigmoid\ny = σ(a) ≡ 1\n1+e x p (−a) (5.19)\nso that 0 ⩽ y(x,w) ⩽ 1. We can interpret y(x,w) as the conditional probability\np(C1|x), with p(C2|x) given by 1 − y(x,w). The conditional distribution of targets\ngiven inputs is then a Bernoulli distribution of the form\np(t|x, w)= y(x,w)t {1 − y(x,w)}1−t . (5.20)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 253,
      "page_label": "234"
    }
  },
  {
    "page_content": "5.2. Network Training 235\nIf we consider a training set of independent observations, then the error function,\nwhich is given by the negative log likelihood, is then a cross-entropy error function\nof the form\nE(w)= −\nN∑\nn=1\n{tn lnyn +( 1− tn)l n ( 1− yn)} (5.21)\nwhere yn denotes y(xn, w). Note that there is no analogue of the noise precision β\nbecause the target values are assumed to be correctly labelled. However, the model\nis easily extended to allow for labelling errors. Simard et al. (2003) found that usingExercise 5.4\nthe cross-entropy error function instead of the sum-of-squares for a classiﬁcation\nproblem leads to faster training as well as improved generalization.\nIf we have K separate binary classiﬁcations to perform, then we can use a net-\nwork having K outputs each of which has a logistic sigmoid activation function.\nAssociated with each output is a binary class labeltk ∈{ 0, 1}, where k =1 ,...,K .\nIf we assume that the class labels are independent, given the input vector, then the\nconditional distribution of the targets is\np(t|x, w)=\nK∏\nk=1\nyk(x,w)tk [1 − yk(x, w)]1−tk . (5.22)\nTaking the negative logarithm of the corresponding likelihood function then gives\nthe following error functionExercise 5.5\nE(w)= −\nN∑\nn=1\nK∑\nk=1\n{tnk lnynk +( 1− tnk)l n ( 1− ynk)} (5.23)\nwhere ynk denotes yk(xn, w). Again, the derivative of the error function with re-\nspect to the activation for a particular output unit takes the form (5.18) just as in theExercise 5.6\nregression case.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 254,
      "page_label": "235"
    }
  },
  {
    "page_content": "spect to the activation for a particular output unit takes the form (5.18) just as in theExercise 5.6\nregression case.\nIt is interesting to contrast the neural network solution to this problem with the\ncorresponding approach based on a linear classiﬁcation model of the kind discussed\nin Chapter 4. Suppose that we are using a standard two-layer network of the kind\nshown in Figure 5.1. We see that the weight parameters in the ﬁrst layer of the\nnetwork are shared between the various outputs, whereas in the linear model each\nclassiﬁcation problem is solved independently. The ﬁrst layer of the network can\nbe viewed as performing a nonlinear feature extraction, and the sharing of features\nbetween the different outputs can save on computation and can also lead to improved\ngeneralization.\nFinally, we consider the standard multiclass classiﬁcation problem in which each\ninput is assigned to one of K mutually exclusive classes. The binary target variables\ntk ∈{ 0, 1} have a 1-of- K coding scheme indicating the class, and the network\noutputs are interpreted as yk(x,w)= p(tk =1 |x), leading to the following error\nfunction\nE(w)= −\nN∑\nn=1\nK∑\nk=1\ntkn lnyk(xn, w). (5.24)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 254,
      "page_label": "235"
    }
  },
  {
    "page_content": "236 5. NEURAL NETWORKS\nFigure 5.5 Geometrical view of the error function E(w) as\na surface sitting over weight space. Point wA is\na local minimum and wB is the global minimum.\nAt any point wC, the local gradient of the error\nsurface is given by the vector ∇E.\nw1\nw2\nE(w)\nwA wB wC\n∇E\nFollowing the discussion of Section 4.3.4, we see that the output unit activation\nfunction, which corresponds to the canonical link, is given by the softmax function\nyk(x,w)= exp(ak(x, w))∑\nj\nexp(aj(x,w))\n(5.25)\nwhich satisﬁes 0 ⩽ yk ⩽ 1 and ∑\nk yk =1 . Note that the yk(x, w) are unchanged\nif a constant is added to all of theak(x,w), causing the error function to be constant\nfor some directions in weight space. This degeneracy is removed if an appropriate\nregularization term (Section 5.5) is added to the error function.\nOnce again, the derivative of the error function with respect to the activation for\na particular output unit takes the familiar form (5.18).Exercise 5.7\nIn summary, there is a natural choice of both output unit activation function\nand matching error function, according to the type of problem being solved. For re-\ngression we use linear outputs and a sum-of-squares error, for (multiple independent)\nbinary classiﬁcations we use logistic sigmoid outputs and a cross-entropy error func-\ntion, and for multiclass classiﬁcation we use softmax outputs with the corresponding\nmulticlass cross-entropy error function. For classiﬁcation problems involving two",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 255,
      "page_label": "236"
    }
  },
  {
    "page_content": "multiclass cross-entropy error function. For classiﬁcation problems involving two\nclasses, we can use a single logistic sigmoid output, or alternatively we can use a\nnetwork with two outputs having a softmax output activation function.\n5.2.1 Parameter optimization\nWe turn next to the task of ﬁnding a weight vector w which minimizes the\nchosen function E(w). At this point, it is useful to have a geometrical picture of the\nerror function, which we can view as a surface sitting over weight space as shown in\nFigure 5.5. First note that if we make a small step in weight space fromw to w+δw\nthen the change in the error function isδE ≃ δwT∇E(w), where the vector∇E(w)\npoints in the direction of greatest rate of increase of the error function. Because the\nerror E(w) is a smooth continuous function of w, its smallest value will occur at a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 255,
      "page_label": "236"
    }
  },
  {
    "page_content": "5.2. Network Training 237\npoint in weight space such that the gradient of the error function vanishes, so that\n∇E(w)=0 (5.26)\nas otherwise we could make a small step in the direction of −∇E(w) and thereby\nfurther reduce the error. Points at which the gradient vanishes are called stationary\npoints, and may be further classiﬁed into minima, maxima, and saddle points.\nOur goal is to ﬁnd a vector w such that E(w) takes its smallest value. How-\never, the error function typically has a highly nonlinear dependence on the weights\nand bias parameters, and so there will be many points in weight space at which the\ngradient vanishes (or is numerically very small). Indeed, from the discussion in Sec-\ntion 5.1.1 we see that for any point w that is a local minimum, there will be other\npoints in weight space that are equivalent minima. For instance, in a two-layer net-\nwork of the kind shown in Figure 5.1, with M hidden units, each point in weight\nspace is a member of a family of M!2M equivalent points.Section 5.1.1\nFurthermore, there will typically be multiple inequivalent stationary points and\nin particular multiple inequivalent minima. A minimum that corresponds to the\nsmallest value of the error function for any weight vector is said to be aglobal\nminimum. Any other minima corresponding to higher values of the error function\nare said to be local minima. For a successful application of neural networks, it may\nnot be necessary to ﬁnd the global minimum (and in general it will not be known",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 256,
      "page_label": "237"
    }
  },
  {
    "page_content": "not be necessary to ﬁnd the global minimum (and in general it will not be known\nwhether the global minimum has been found) but it may be necessary to compare\nseveral local minima in order to ﬁnd a sufﬁciently good solution.\nBecause there is clearly no hope of ﬁnding an analytical solution to the equa-\ntion ∇E(w)=0 we resort to iterative numerical procedures. The optimization of\ncontinuous nonlinear functions is a widely studied problem and there exists an ex-\ntensive literature on how to solve it efﬁciently. Most techniques involve choosing\nsome initial value w(0) for the weight vector and then moving through weight space\nin a succession of steps of the form\nw(τ+1) = w(τ) +∆ w(τ) (5.27)\nwhere τ labels the iteration step. Different algorithms involve different choices for\nthe weight vector update∆w(τ). Many algorithms make use of gradient information\nand therefore require that, after each update, the value of ∇E(w) is evaluated at\nthe new weight vector w(τ+1). In order to understand the importance of gradient\ninformation, it is useful to consider a local approximation to the error function based\non a Taylor expansion.\n5.2.2 Local quadratic approximation\nInsight into the optimization problem, and into the various techniques for solv-\ning it, can be obtained by considering a local quadratic approximation to the error\nfunction.\nConsider the Taylor expansion of E(w) around some point ˆw in weight space\nE(w) ≃ E(ˆw)+( w − ˆw)Tb + 1\n2(w − ˆw)TH(w − ˆw) (5.28)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 256,
      "page_label": "237"
    }
  },
  {
    "page_content": "238 5. NEURAL NETWORKS\nwhere cubic and higher terms have been omitted. Hereb is deﬁned to be the gradient\nof E evaluated at ˆw\nb ≡∇ E|w=bw (5.29)\nand the Hessian matrix H = ∇∇E has elements\n(H)ij ≡ ∂E\n∂wi∂wj\n⏐⏐⏐⏐\nw=bw\n. (5.30)\nFrom (5.28), the corresponding local approximation to the gradient is given by\n∇E ≃ b + H(w − ˆw). (5.31)\nFor points w that are sufﬁciently close to ˆw, these expressions will give reasonable\napproximations for the error and its gradient.\nConsider the particular case of a local quadratic approximation around a point\nw⋆ that is a minimum of the error function. In this case there is no linear term,\nbecause ∇E =0 at w⋆ , and (5.28) becomes\nE(w)= E(w⋆ )+ 1\n2(w − w⋆ )TH(w − w⋆ ) (5.32)\nwhere the Hessian H is evaluated at w⋆ . In order to interpret this geometrically,\nconsider the eigenvalue equation for the Hessian matrix\nHui = λiui (5.33)\nwhere the eigenvectors ui form a complete orthonormal set (Appendix C) so that\nuT\ni uj = δij. (5.34)\nWe now expand (w − w⋆ ) as a linear combination of the eigenvectors in the form\nw − w⋆ =\n∑\ni\nαiui. (5.35)\nThis can be regarded as a transformation of the coordinate system in which the origin\nis translated to the point w⋆ , and the axes are rotated to align with the eigenvectors\n(through the orthogonal matrix whose columns are the ui), and is discussed in more\ndetail in Appendix C. Substituting (5.35) into (5.32), and using (5.33) and (5.34),\nallows the error function to be written in the form\nE(w)= E(w⋆ )+ 1\n2\n∑\ni\nλiα2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 257,
      "page_label": "238"
    }
  },
  {
    "page_content": "allows the error function to be written in the form\nE(w)= E(w⋆ )+ 1\n2\n∑\ni\nλiα2\ni . (5.36)\nA matrix H is said to be positive deﬁniteif, and only if,\nvTHv > 0 for all v. (5.37)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 257,
      "page_label": "238"
    }
  },
  {
    "page_content": "5.2. Network Training 239\nFigure 5.6 In the neighbourhood of a min-\nimum w⋆ , the error function\ncan be approximated by a\nquadratic. Contours of con-\nstant error are then ellipses\nwhose axes are aligned with\nthe eigenvectors ui of the Hes-\nsian matrix, with lengths that\nare inversely proportional to the\nsquare roots of the correspond-\ning eigenvectors λi.\nw1\nw2\nλ−1/2\n1\nλ−1/2\n2\nu1\nw⋆\nu2\nBecause the eigenvectors {ui} form a complete set, an arbitrary vector v can be\nwritten in the form\nv =\n∑\ni\nciui. (5.38)\nFrom (5.33) and (5.34), we then have\nvTHv =\n∑\ni\nc2\ni λi (5.39)\nand so H will be positive deﬁnite if, and only if, all of its eigenvalues are positive.Exercise 5.10\nIn the new coordinate system, whose basis vectors are given by the eigenvectors\n{ui}, the contours of constant E are ellipses centred on the origin, as illustratedExercise 5.11\nin Figure 5.6. For a one-dimensional weight space, a stationary point w⋆ will be a\nminimum if\n∂2E\n∂w2\n⏐⏐⏐⏐\nw⋆\n> 0. (5.40)\nThe corresponding result in D-dimensions is that the Hessian matrix, evaluated at\nw⋆ , should be positive deﬁnite.Exercise 5.12\n5.2.3 Use of gradient information\nAs we shall see in Section 5.3, it is possible to evaluate the gradient of an error\nfunction efﬁciently by means of the backpropagation procedure. The use of this\ngradient information can lead to signiﬁcant improvements in the speed with which\nthe minima of the error function can be located. We can see why this is so, as follows.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 258,
      "page_label": "239"
    }
  },
  {
    "page_content": "the minima of the error function can be located. We can see why this is so, as follows.\nIn the quadratic approximation to the error function, given in (5.28), the error\nsurface is speciﬁed by the quantities b and H, which contain a total of W(W +\n3)/2 independent elements (because the matrix H is symmetric), where W is theExercise 5.13\ndimensionality of w (i.e., the total number of adaptive parameters in the network).\nThe location of the minimum of this quadratic approximation therefore depends on\nO(W2) parameters, and we should not expect to be able to locate the minimum until\nwe have gathered O(W2) independent pieces of information. If we do not make\nuse of gradient information, we would expect to have to perform O(W2) function",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 258,
      "page_label": "239"
    }
  },
  {
    "page_content": "240 5. NEURAL NETWORKS\nevaluations, each of which would require O(W) steps. Thus, the computational\neffort needed to ﬁnd the minimum using such an approach would be O(W3).\nNow compare this with an algorithm that makes use of the gradient information.\nBecause each evaluation of ∇E brings W items of information, we might hope to\nﬁnd the minimum of the function in O(W) gradient evaluations. As we shall see,\nby using error backpropagation, each such evaluation takes only O(W) steps and so\nthe minimum can now be found in O(W2) steps. For this reason, the use of gradient\ninformation forms the basis of practical algorithms for training neural networks.\n5.2.4 Gradient descent optimization\nThe simplest approach to using gradient information is to choose the weight\nupdate in (5.27) to comprise a small step in the direction of the negative gradient, so\nthat\nw(τ+1) = w(τ) − η∇E(w(τ)) (5.41)\nwhere the parameterη> 0 is known as thelearning rate. After each such update, the\ngradient is re-evaluated for the new weight vector and the process repeated. Note that\nthe error function is deﬁned with respect to a training set, and so each step requires\nthat the entire training set be processed in order to evaluate∇E. Techniques that\nuse the whole data set at once are called batch methods. At each step the weight\nvector is moved in the direction of the greatest rate of decrease of the error function,\nand so this approach is known asgradient descent or steepest descent. Although",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 259,
      "page_label": "240"
    }
  },
  {
    "page_content": "and so this approach is known asgradient descent or steepest descent. Although\nsuch an approach might intuitively seem reasonable, in fact it turns out to be a poor\nalgorithm, for reasons discussed in Bishop and Nabney (2008).\nFor batch optimization, there are more efﬁcient methods, such asconjugate gra-\ndients and quasi-Newton methods, which are much more robust and much faster\nthan simple gradient descent (Gill et al., 1981; Fletcher, 1987; Nocedal and Wright,\n1999). Unlike gradient descent, these algorithms have the property that the error\nfunction always decreases at each iteration unless the weight vector has arrived at a\nlocal or global minimum.\nIn order to ﬁnd a sufﬁciently good minimum, it may be necessary to run a\ngradient-based algorithm multiple times, each time using a different randomly cho-\nsen starting point, and comparing the resulting performance on an independent vali-\ndation set.\nThere is, however, an on-line version of gradient descent that has proved useful\nin practice for training neural networks on large data sets (Le Cun et al., 1989).\nError functions based on maximum likelihood for a set of independent observations\ncomprise a sum of terms, one for each data point\nE(w)=\nN∑\nn=1\nEn(w). (5.42)\nOn-line gradient descent, also known as sequential gradient descent or stochastic\ngradient descent, makes an update to the weight vector based on one data point at a\ntime, so that\nw(τ+1) = w(τ) − η∇En(w(τ)). (5.43)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 259,
      "page_label": "240"
    }
  },
  {
    "page_content": "5.3. Error Backpropagation 241\nThis update is repeated by cycling through the data either in sequence or by selecting\npoints at random with replacement. There are of course intermediate scenarios in\nwhich the updates are based on batches of data points.\nOne advantage of on-line methods compared to batch methods is that the former\nhandle redundancy in the data much more efﬁciently. To see, this consider an ex-\ntreme example in which we take a data set and double its size by duplicating every\ndata point. Note that this simply multiplies the error function by a factor of 2 and so\nis equivalent to using the original error function. Batch methods will require double\nthe computational effort to evaluate the batch error function gradient, whereas on-\nline methods will be unaffected. Another property of on-line gradient descent is the\npossibility of escaping from local minima, since a stationary point with respect to\nthe error function for the whole data set will generally not be a stationary point for\neach data point individually.\nNonlinear optimization algorithms, and their practical application to neural net-\nwork training, are discussed in detail in Bishop and Nabney (2008).\n5.3. Error Backpropagation\nOur goal in this section is to ﬁnd an efﬁcient technique for evaluating the gradient\nof an error function E(w) for a feed-forward neural network. We shall see that\nthis can be achieved using a local message passing scheme in which information is",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 260,
      "page_label": "241"
    }
  },
  {
    "page_content": "this can be achieved using a local message passing scheme in which information is\nsent alternately forwards and backwards through the network and is known as error\nbackpropagation, or sometimes simply as backprop.\nIt should be noted that the term backpropagation is used in the neural com-\nputing literature to mean a variety of different things. For instance, the multilayer\nperceptron architecture is sometimes called a backpropagation network. The term\nbackpropagation is also used to describe the training of a multilayer perceptron us-\ning gradient descent applied to a sum-of-squares error function. In order to clarify\nthe terminology, it is useful to consider the nature of the training process more care-\nfully. Most training algorithms involve an iterative procedure for minimization of an\nerror function, with adjustments to the weights being made in a sequence of steps. At\neach such step, we can distinguish between two distinct stages. In the ﬁrst stage, the\nderivatives of the error function with respect to the weights must be evaluated. As\nwe shall see, the important contribution of the backpropagation technique is in pro-\nviding a computationally efﬁcient method for evaluating such derivatives. Because\nit is at this stage that errors are propagated backwards through the network, we shall\nuse the term backpropagation speciﬁcally to describe the evaluation of derivatives.\nIn the second stage, the derivatives are then used to compute the adjustments to be",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 260,
      "page_label": "241"
    }
  },
  {
    "page_content": "In the second stage, the derivatives are then used to compute the adjustments to be\nmade to the weights. The simplest such technique, and the one originally considered\nby Rumelhartet al. (1986), involves gradient descent. It is important to recognize\nthat the two stages are distinct. Thus, the ﬁrst stage, namely the propagation of er-\nrors backwards through the network in order to evaluate derivatives, can be applied\nto many other kinds of network and not just the multilayer perceptron. It can also be\napplied to error functions other that just the simple sum-of-squares, and to the eval-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 260,
      "page_label": "241"
    }
  },
  {
    "page_content": "242 5. NEURAL NETWORKS\nuation of other derivatives such as the Jacobian and Hessian matrices, as we shall\nsee later in this chapter. Similarly, the second stage of weight adjustment using the\ncalculated derivatives can be tackled using a variety of optimization schemes, many\nof which are substantially more powerful than simple gradient descent.\n5.3.1 Evaluation of error-function derivatives\nWe now derive the backpropagation algorithm for a general network having ar-\nbitrary feed-forward topology, arbitrary differentiable nonlinear activation functions,\nand a broad class of error function. The resulting formulae will then be illustrated\nusing a simple layered network structure having a single layer of sigmoidal hidden\nunits together with a sum-of-squares error.\nMany error functions of practical interest, for instance those deﬁned by maxi-\nmum likelihood for a set of i.i.d. data, comprise a sum of terms, one for each data\npoint in the training set, so that\nE(w)=\nN∑\nn=1\nEn(w). (5.44)\nHere we shall consider the problem of evaluating ∇En(w) for one such term in the\nerror function. This may be used directly for sequential optimization, or the results\ncan be accumulated over the training set in the case of batch methods.\nConsider ﬁrst a simple linear model in which the outputs yk are linear combina-\ntions of the input variables xi so that\nyk =\n∑\ni\nwkixi (5.45)\ntogether with an error function that, for a particular input pattern n, takes the form\nEn = 1\n2\n∑\nk\n(ynk − tnk)2 (5.46)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 261,
      "page_label": "242"
    }
  },
  {
    "page_content": "yk =\n∑\ni\nwkixi (5.45)\ntogether with an error function that, for a particular input pattern n, takes the form\nEn = 1\n2\n∑\nk\n(ynk − tnk)2 (5.46)\nwhere ynk = yk(xn, w). The gradient of this error function with respect to a weight\nwji is given by\n∂En\n∂wji\n=( ynj − tnj)xni (5.47)\nwhich can be interpreted as a ‘local’ computation involving the product of an ‘error\nsignal’ ynj − tnj associated with the output end of the link wji and the variable xni\nassociated with the input end of the link. In Section 4.3.2, we saw how a similar\nformula arises with the logistic sigmoid activation function together with the cross\nentropy error function, and similarly for the softmax activation function together\nwith its matching cross-entropy error function. We shall now see how this simple\nresult extends to the more complex setting of multilayer feed-forward networks.\nIn a general feed-forward network, each unit computes a weighted sum of its\ninputs of the form\naj =\n∑\ni\nwjizi (5.48)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 261,
      "page_label": "242"
    }
  },
  {
    "page_content": "5.3. Error Backpropagation 243\nwhere zi is the activation of a unit, or input, that sends a connection to unitj, and wji\nis the weight associated with that connection. In Section 5.1, we saw that biases can\nbe included in this sum by introducing an extra unit, or input, with activation ﬁxed\nat +1. We therefore do not need to deal with biases explicitly. The sum in (5.48) is\ntransformed by a nonlinear activation functionh(·) to give the activationzj of unit j\nin the form\nzj = h(aj). (5.49)\nNote that one or more of the variables zi in the sum in (5.48) could be an input, and\nsimilarly, the unit j in (5.49) could be an output.\nFor each pattern in the training set, we shall suppose that we have supplied the\ncorresponding input vector to the network and calculated the activations of all of\nthe hidden and output units in the network by successive application of (5.48) and\n(5.49). This process is often called forward propagationbecause it can be regarded\nas a forward ﬂow of information through the network.\nNow consider the evaluation of the derivative of En with respect to a weight\nwji. The outputs of the various units will depend on the particular input pattern n.\nHowever, in order to keep the notation uncluttered, we shall omit the subscript n\nfrom the network variables. First we note that En depends on the weight wji only\nvia the summed input aj to unit j. We can therefore apply the chain rule for partial\nderivatives to give\n∂En\n∂wji\n= ∂En\n∂aj\n∂aj\n∂wji\n. (5.50)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 262,
      "page_label": "243"
    }
  },
  {
    "page_content": "via the summed input aj to unit j. We can therefore apply the chain rule for partial\nderivatives to give\n∂En\n∂wji\n= ∂En\n∂aj\n∂aj\n∂wji\n. (5.50)\nWe now introduce a useful notation\nδj ≡ ∂En\n∂aj\n(5.51)\nwhere the δ’s are often referred to as errors for reasons we shall see shortly. Using\n(5.48), we can write\n∂aj\n∂wji\n= zi. (5.52)\nSubstituting (5.51) and (5.52) into (5.50), we then obtain\n∂En\n∂wji\n= δjzi. (5.53)\nEquation (5.53) tells us that the required derivative is obtained simply by multiplying\nthe value of δ for the unit at the output end of the weight by the value ofz for the unit\nat the input end of the weight (wherez =1 in the case of a bias). Note that this takes\nthe same form as for the simple linear model considered at the start of this section.\nThus, in order to evaluate the derivatives, we need only to calculate the value ofδj\nfor each hidden and output unit in the network, and then apply (5.53).\nAs we have seen already, for the output units, we have\nδk = yk − tk (5.54)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 262,
      "page_label": "243"
    }
  },
  {
    "page_content": "244 5. NEURAL NETWORKS\nFigure 5.7 Illustration of the calculation of δj for hidden unit j by\nbackpropagation of the δ’s from those units k to which\nunit j sends connections. The blue arrow denotes the\ndirection of information ﬂow during forward propagation,\nand the red arrows indicate the backward propagation\nof error information.\nzi\nzj\nδj\nδk\nδ1\nwji wkj\nprovided we are using the canonical link as the output-unit activation function. To\nevaluate the δ’s for hidden units, we again make use of the chain rule for partial\nderivatives,\nδj ≡ ∂En\n∂aj\n=\n∑\nk\n∂En\n∂ak\n∂ak\n∂aj\n(5.55)\nwhere the sum runs over all units k to which unit j sends connections. The arrange-\nment of units and weights is illustrated in Figure 5.7. Note that the units labelled k\ncould include other hidden units and/or output units. In writing down (5.55), we are\nmaking use of the fact that variations in aj give rise to variations in the error func-\ntion only through variations in the variables ak. If we now substitute the deﬁnition\nof δ given by (5.51) into (5.55), and make use of (5.48) and (5.49), we obtain the\nfollowing backpropagation formula\nδj = h′(aj)\n∑\nk\nwkjδk (5.56)\nwhich tells us that the value of δ for a particular hidden unit can be obtained by\npropagating the δ’s backwards from units higher up in the network, as illustrated\nin Figure 5.7. Note that the summation in (5.56) is taken over the ﬁrst index on\nwkj (corresponding to backward propagation of information through the network),",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 263,
      "page_label": "244"
    }
  },
  {
    "page_content": "wkj (corresponding to backward propagation of information through the network),\nwhereas in the forward propagation equation (5.10) it is taken over the second index.\nBecause we already know the values of the δ’s for the output units, it follows that\nby recursively applying (5.56) we can evaluate theδ’s for all of the hidden units in a\nfeed-forward network, regardless of its topology.\nThe backpropagation procedure can therefore be summarized as follows.\nError Backpropagation\n1. Apply an input vector xn to the network and forward propagate through\nthe network using (5.48) and (5.49) to ﬁnd the activations of all the hidden\nand output units.\n2. Evaluate the δk for all the output units using (5.54).\n3. Backpropagate the δ’s using (5.56) to obtain δj for each hidden unit in the\nnetwork.\n4. Use (5.53) to evaluate the required derivatives.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 263,
      "page_label": "244"
    }
  },
  {
    "page_content": "5.3. Error Backpropagation 245\nFor batch methods, the derivative of the total error E can then be obtained by\nrepeating the above steps for each pattern in the training set and then summing over\nall patterns:\n∂E\n∂wji\n=\n∑\nn\n∂En\n∂wji\n. (5.57)\nIn the above derivation we have implicitly assumed that each hidden or output unit in\nthe network has the same activation function h(·). The derivation is easily general-\nized, however, to allow different units to have individual activation functions, simply\nby keeping track of which form of h(·) goes with which unit.\n5.3.2 A simple example\nThe above derivation of the backpropagation procedure allowed for general\nforms for the error function, the activation functions, and the network topology. In\norder to illustrate the application of this algorithm, we shall consider a particular\nexample. This is chosen both for its simplicity and for its practical importance, be-\ncause many applications of neural networks reported in the literature make use of\nthis type of network. Speciﬁcally, we shall consider a two-layer network of the form\nillustrated in Figure 5.1, together with a sum-of-squares error, in which the output\nunits have linear activation functions, so that yk = ak, while the hidden units have\nlogistic sigmoid activation functions given by\nh(a) ≡ tanh(a) (5.58)\nwhere\ntanh(a)= ea − e−a\nea + e−a . (5.59)\nA useful feature of this function is that its derivative can be expressed in a par-\nticularly simple form:\nh′(a)=1 − h(a)2. (5.60)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 264,
      "page_label": "245"
    }
  },
  {
    "page_content": "ea + e−a . (5.59)\nA useful feature of this function is that its derivative can be expressed in a par-\nticularly simple form:\nh′(a)=1 − h(a)2. (5.60)\nWe also consider a standard sum-of-squares error function, so that for pattern n the\nerror is given by\nEn = 1\n2\nK∑\nk=1\n(yk − tk)2 (5.61)\nwhere yk is the activation of output unit k, and tk is the corresponding target, for a\nparticular input pattern xn.\nFor each pattern in the training set in turn, we ﬁrst perform a forward propagation\nusing\naj =\nD∑\ni=0\nw(1)\nji xi (5.62)\nzj =t a n h (aj) (5.63)\nyk =\nM∑\nj=0\nw(2)\nkj zj. (5.64)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 264,
      "page_label": "245"
    }
  },
  {
    "page_content": "246 5. NEURAL NETWORKS\nNext we compute the δ’s for each output unit using\nδk = yk − tk. (5.65)\nThen we backpropagate these to obtain δs for the hidden units using\nδj =( 1− z2\nj )\nK∑\nk=1\nwkjδk. (5.66)\nFinally, the derivatives with respect to the ﬁrst-layer and second-layer weights are\ngiven by\n∂En\n∂w(1)\nji\n= δjxi, ∂En\n∂w(2)\nkj\n= δkzj. (5.67)\n5.3.3 Efﬁciency of backpropagation\nOne of the most important aspects of backpropagation is its computational efﬁ-\nciency. To understand this, let us examine how the number of computer operations\nrequired to evaluate the derivatives of the error function scales with the total number\nW of weights and biases in the network. A single evaluation of the error function\n(for a given input pattern) would require O(W) operations, for sufﬁciently large W.\nThis follows from the fact that, except for a network with very sparse connections,\nthe number of weights is typically much greater than the number of units, and so the\nbulk of the computational effort in forward propagation is concerned with evaluat-\ning the sums in (5.48), with the evaluation of the activation functions representing a\nsmall overhead. Each term in the sum in (5.48) requires one multiplication and one\naddition, leading to an overall computational cost that is O(W).\nAn alternative approach to backpropagation for computing the derivatives of the\nerror function is to use ﬁnite differences. This can be done by perturbing each weight",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 265,
      "page_label": "246"
    }
  },
  {
    "page_content": "error function is to use ﬁnite differences. This can be done by perturbing each weight\nin turn, and approximating the derivatives by the expression\n∂En\n∂wji\n= En(wji + ϵ) − En(wji)\nϵ + O(ϵ) (5.68)\nwhere ϵ ≪ 1. In a software simulation, the accuracy of the approximation to the\nderivatives can be improved by makingϵ smaller, until numerical roundoff problems\narise. The accuracy of the ﬁnite differences method can be improved signiﬁcantly\nby using symmetrical central differencesof the form\n∂En\n∂wji\n= En(wji + ϵ) − En(wji − ϵ)\n2ϵ + O(ϵ2). (5.69)\nIn this case, the O(ϵ) corrections cancel, as can be veriﬁed by Taylor expansion onExercise 5.14\nthe right-hand side of (5.69), and so the residual corrections are O(ϵ2). The number\nof computational steps is, however, roughly doubled compared with (5.68).\nThe main problem with numerical differentiation is that the highly desirable\nO(W) scaling has been lost. Each forward propagation requires O(W) steps, and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 265,
      "page_label": "246"
    }
  },
  {
    "page_content": "5.3. Error Backpropagation 247\nFigure 5.8 Illustration of a modular pattern\nrecognition system in which the\nJacobian matrix can be used\nto backpropagate error signals\nfrom the outputs through to ear-\nlier modules in the system.\nx\nu\nw\ny\nz\nv\nthere are W weights in the network each of which must be perturbed individually, so\nthat the overall scaling is O(W2).\nHowever, numerical differentiation plays an important role in practice, because a\ncomparison of the derivatives calculated by backpropagation with those obtained us-\ning central differences provides a powerful check on the correctness of any software\nimplementation of the backpropagation algorithm. When training networks in prac-\ntice, derivatives should be evaluated using backpropagation, because this gives the\ngreatest accuracy and numerical efﬁciency. However, the results should be compared\nwith numerical differentiation using (5.69) for some test cases in order to check the\ncorrectness of the implementation.\n5.3.4 The Jacobian matrix\nWe have seen how the derivatives of an error function with respect to the weights\ncan be obtained by the propagation of errors backwards through the network. The\ntechnique of backpropagation can also be applied to the calculation of other deriva-\ntives. Here we consider the evaluation of the Jacobian matrix, whose elements are\ngiven by the derivatives of the network outputs with respect to the inputs\nJki ≡ ∂yk\n∂xi\n(5.70)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 266,
      "page_label": "247"
    }
  },
  {
    "page_content": "given by the derivatives of the network outputs with respect to the inputs\nJki ≡ ∂yk\n∂xi\n(5.70)\nwhere each such derivative is evaluated with all other inputs held ﬁxed. Jacobian\nmatrices play a useful role in systems built from a number of distinct modules, as\nillustrated in Figure 5.8. Each module can comprise a ﬁxed or adaptive function,\nwhich can be linear or nonlinear, so long as it is differentiable. Suppose we wish\nto minimize an error function E with respect to the parameter w in Figure 5.8. The\nderivative of the error function is given by\n∂E\n∂w =\n∑\nk,j\n∂E\n∂yk\n∂yk\n∂zj\n∂zj\n∂w (5.71)\nin which the Jacobian matrix for the red module in Figure 5.8 appears in the middle\nterm.\nBecause the Jacobian matrix provides a measure of the local sensitivity of the\noutputs to changes in each of the input variables, it also allows any known errors∆xi",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 266,
      "page_label": "247"
    }
  },
  {
    "page_content": "248 5. NEURAL NETWORKS\nassociated with the inputs to be propagated through the trained network in order to\nestimate their contribution ∆yk to the errors at the outputs, through the relation\n∆yk ≃\n∑\ni\n∂yk\n∂xi\n∆xi (5.72)\nwhich is valid provided the |∆xi| are small. In general, the network mapping rep-\nresented by a trained neural network will be nonlinear, and so the elements of the\nJacobian matrix will not be constants but will depend on the particular input vector\nused. Thus (5.72) is valid only for small perturbations of the inputs, and the Jacobian\nitself must be re-evaluated for each new input vector.\nThe Jacobian matrix can be evaluated using a backpropagation procedure that is\nsimilar to the one derived earlier for evaluating the derivatives of an error function\nwith respect to the weights. We start by writing the element Jki in the form\nJki = ∂yk\n∂xi\n=\n∑\nj\n∂yk\n∂aj\n∂aj\n∂xi\n=\n∑\nj\nwji\n∂yk\n∂aj\n(5.73)\nwhere we have made use of (5.48). The sum in (5.73) runs over all units j to which\nthe input unit i sends connections (for example, over all units in the ﬁrst hidden\nlayer in the layered topology considered earlier). We now write down a recursive\nbackpropagation formula to determine the derivatives∂yk/∂aj\n∂yk\n∂aj\n=\n∑\nl\n∂yk\n∂al\n∂al\n∂aj\n= h′(aj)\n∑\nl\nwlj\n∂yk\n∂al\n(5.74)\nwhere the sum runs over all unitsl to which unit j sends connections (corresponding\nto the ﬁrst index of wlj). Again, we have made use of (5.48) and (5.49). This",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 267,
      "page_label": "248"
    }
  },
  {
    "page_content": "to the ﬁrst index of wlj). Again, we have made use of (5.48) and (5.49). This\nbackpropagation starts at the output units for which the required derivatives can be\nfound directly from the functional form of the output-unit activation function. For\ninstance, if we have individual sigmoidal activation functions at each output unit,\nthen\n∂yk\n∂aj\n= δkjσ′(aj) (5.75)\nwhereas for softmax outputs we have\n∂yk\n∂aj\n= δkjyk − ykyj. (5.76)\nWe can summarize the procedure for evaluating the Jacobian matrix as follows.\nApply the input vector corresponding to the point in input space at which the Ja-\ncobian matrix is to be found, and forward propagate in the usual way to obtain the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 267,
      "page_label": "248"
    }
  },
  {
    "page_content": "5.4. The Hessian Matrix 249\nactivations of all of the hidden and output units in the network. Next, for each row\nk of the Jacobian matrix, corresponding to the output unit k, backpropagate using\nthe recursive relation (5.74), starting with (5.75) or (5.76), for all of the hidden units\nin the network. Finally, use (5.73) to do the backpropagation to the inputs. The\nJacobian can also be evaluated using an alternative forward propagation formalism,\nwhich can be derived in an analogous way to the backpropagation approach given\nhere.Exercise 5.15\nAgain, the implementation of such algorithms can be checked by using numeri-\ncal differentiation in the form\n∂yk\n∂xi\n= yk(xi + ϵ) − yk(xi − ϵ)\n2ϵ + O(ϵ2) (5.77)\nwhich involves 2D forward propagations for a network having D inputs.\n5.4. The Hessian Matrix\nWe have shown how the technique of backpropagation can be used to obtain the ﬁrst\nderivatives of an error function with respect to the weights in the network. Back-\npropagation can also be used to evaluate the second derivatives of the error, given\nby\n∂2E\n∂wji∂wlk\n. (5.78)\nNote that it is sometimes convenient to consider all of the weight and bias parameters\nas elements wi of a single vector, denoted w, in which case the second derivatives\nform the elements Hij of the Hessian matrix H, where i, j ∈{ 1,...,W } and W is\nthe total number of weights and biases. The Hessian plays an important role in many\naspects of neural computing, including the following:",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 268,
      "page_label": "249"
    }
  },
  {
    "page_content": "the total number of weights and biases. The Hessian plays an important role in many\naspects of neural computing, including the following:\n1. Several nonlinear optimization algorithms used for training neural networks\nare based on considerations of the second-order properties of the error surface,\nwhich are controlled by the Hessian matrix (Bishop and Nabney, 2008).\n2. The Hessian forms the basis of a fast procedure for re-training a feed-forward\nnetwork following a small change in the training data (Bishop, 1991).\n3. The inverse of the Hessian has been used to identify the least signiﬁcant weights\nin a network as part of network ‘pruning’ algorithms (Le Cun et al., 1990).\n4. The Hessian plays a central role in the Laplace approximation for a Bayesian\nneural network (see Section 5.7). Its inverse is used to determine the predic-\ntive distribution for a trained network, its eigenvalues determine the values of\nhyperparameters, and its determinant is used to evaluate the model evidence.\nVarious approximation schemes have been used to evaluate the Hessian matrix\nfor a neural network. However, the Hessian can also be calculated exactly using an\nextension of the backpropagation technique.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 268,
      "page_label": "249"
    }
  },
  {
    "page_content": "250 5. NEURAL NETWORKS\nAn important consideration for many applications of the Hessian is the efﬁciency\nwith which it can be evaluated. If there areW parameters (weights and biases) in the\nnetwork, then the Hessian matrix has dimensions W × W and so the computational\neffort needed to evaluate the Hessian will scale like O(W2) for each pattern in the\ndata set. As we shall see, there are efﬁcient methods for evaluating the Hessian\nwhose scaling is indeedO(W2).\n5.4.1 Diagonal approximation\nSome of the applications for the Hessian matrix discussed above require the\ninverse of the Hessian, rather than the Hessian itself. For this reason, there has\nbeen some interest in using a diagonal approximation to the Hessian, in other words\none that simply replaces the off-diagonal elements with zeros, because its inverse is\ntrivial to evaluate. Again, we shall consider an error function that consists of a sum\nof terms, one for each pattern in the data set, so that E = ∑\nn En. The Hessian can\nthen be obtained by considering one pattern at a time, and then summing the results\nover all patterns. From (5.48), the diagonal elements of the Hessian, for pattern n,\ncan be written\n∂2En\n∂w2\nji\n= ∂2En\n∂a2\nj\nz2\ni . (5.79)\nUsing (5.48) and (5.49), the second derivatives on the right-hand side of (5.79) can\nbe found recursively using the chain rule of differential calculus to give a backprop-\nagation equation of the form\n∂2En\n∂a2\nj\n= h′(aj)2\n∑\nk\n∑\nk′\nwkjwk′j\n∂2En\n∂ak∂ak′\n+ h′′(aj)\n∑\nk\nwkj\n∂En\n∂ak",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 269,
      "page_label": "250"
    }
  },
  {
    "page_content": "agation equation of the form\n∂2En\n∂a2\nj\n= h′(aj)2\n∑\nk\n∑\nk′\nwkjwk′j\n∂2En\n∂ak∂ak′\n+ h′′(aj)\n∑\nk\nwkj\n∂En\n∂ak\n. (5.80)\nIf we now neglect off-diagonal elements in the second-derivative terms, we obtain\n(Becker and Le Cun, 1989; Le Cun et al., 1990)\n∂2En\n∂a2\nj\n= h′(aj)2\n∑\nk\nw2\nkj\n∂2En\n∂a2\nk\n+ h′′(aj)\n∑\nk\nwkj\n∂En\n∂ak\n. (5.81)\nNote that the number of computational steps required to evaluate this approximation\nis O(W), where W is the total number of weight and bias parameters in the network,\ncompared with O(W2) for the full Hessian.\nRicotti et al. (1988) also used the diagonal approximation to the Hessian, but\nthey retained all terms in the evaluation of ∂2En/∂a2\nj and so obtained exact expres-\nsions for the diagonal terms. Note that this no longer has O(W) scaling. The major\nproblem with diagonal approximations, however, is that in practice the Hessian is\ntypically found to be strongly nondiagonal, and so these approximations, which are\ndriven mainly be computational convenience, must be treated with care.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 269,
      "page_label": "250"
    }
  },
  {
    "page_content": "5.4. The Hessian Matrix 251\n5.4.2 Outer product approximation\nWhen neural networks are applied to regression problems, it is common to use\na sum-of-squares error function of the form\nE = 1\n2\nN∑\nn=1\n(yn − tn)2 (5.82)\nwhere we have considered the case of a single output in order to keep the notation\nsimple (the extension to several outputs is straightforward). We can then write theExercise 5.16\nHessian matrix in the form\nH = ∇∇E =\nN∑\nn=1\n∇yn∇yn +\nN∑\nn=1\n(yn − tn)∇∇yn. (5.83)\nIf the network has been trained on the data set, and its outputs yn happen to be very\nclose to the target values tn, then the second term in (5.83) will be small and can\nbe neglected. More generally, however, it may be appropriate to neglect this term\nby the following argument. Recall from Section 1.5.5 that the optimal function that\nminimizes a sum-of-squares loss is the conditional average of the target data. The\nquantity (yn − tn) is then a random variable with zero mean. If we assume that its\nvalue is uncorrelated with the value of the second derivative term on the right-hand\nside of (5.83), then the whole term will average to zero in the summation overn.Exercise 5.17\nBy neglecting the second term in (5.83), we arrive at the Levenberg–Marquardt\napproximation or outer product approximation (because the Hessian matrix is built\nup from a sum of outer products of vectors), given by\nH ≃\nN∑\nn=1\nbnbT\nn (5.84)\nwhere bn = ∇yn = ∇an because the activation function for the output units is",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 270,
      "page_label": "251"
    }
  },
  {
    "page_content": "H ≃\nN∑\nn=1\nbnbT\nn (5.84)\nwhere bn = ∇yn = ∇an because the activation function for the output units is\nsimply the identity. Evaluation of the outer product approximation for the Hessian\nis straightforward as it only involves ﬁrst derivatives of the error function, which\ncan be evaluated efﬁciently inO(W) steps using standard backpropagation. The\nelements of the matrix can then be found in O(W2) steps by simple multiplication.\nIt is important to emphasize that this approximation is only likely to be valid for a\nnetwork that has been trained appropriately, and that for a general network mapping\nthe second derivative terms on the right-hand side of (5.83) will typically not be\nnegligible.\nIn the case of the cross-entropy error function for a network with logistic sigmoid\noutput-unit activation functions, the corresponding approximation is given byExercise 5.19\nH ≃\nN∑\nn=1\nyn(1 − yn)bnbT\nn. (5.85)\nAn analogous result can be obtained for multiclass networks having softmax output-\nunit activation functions.Exercise 5.20",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 270,
      "page_label": "251"
    }
  },
  {
    "page_content": "252 5. NEURAL NETWORKS\n5.4.3 Inverse Hessian\nWe can use the outer-product approximation to develop a computationally ef-\nﬁcient procedure for approximating the inverse of the Hessian (Hassibi and Stork,\n1993). First we write the outer-product approximation in matrix notation as\nHN =\nN∑\nn=1\nbnbT\nn (5.86)\nwhere bn ≡∇ wan is the contribution to the gradient of the output unit activation\narising from data point n. We now derive a sequential procedure for building up the\nHessian by including data points one at a time. Suppose we have already obtained\nthe inverse Hessian using the ﬁrstL data points. By separating off the contribution\nfrom data point L +1 , we obtain\nHL+1 = HL + bL+1bT\nL+1. (5.87)\nIn order to evaluate the inverse of the Hessian, we now consider the matrix identity\n(\nM + vvT)−1\n= M−1 − (M−1v)\n(\nvTM−1)\n1+ vTM−1v (5.88)\nwhere I is the unit matrix, which is simply a special case of the Woodbury identity\n(C.7). If we now identify HL with M and bL+1 with v, we obtain\nH−1\nL+1 = H−1\nL − H−1\nL bL+1bT\nL+1H−1\nL\n1+ bT\nL+1H−1\nL bL+1\n. (5.89)\nIn this way, data points are sequentially absorbed untilL+1 = N and the whole data\nset has been processed. This result therefore represents a procedure for evaluating\nthe inverse of the Hessian using a single pass through the data set. The initial matrix\nH0 is chosen to be αI, where α is a small quantity, so that the algorithm actually\nﬁnds the inverse of H + αI. The results are not particularly sensitive to the precise",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 271,
      "page_label": "252"
    }
  },
  {
    "page_content": "ﬁnds the inverse of H + αI. The results are not particularly sensitive to the precise\nvalue of α. Extension of this algorithm to networks having more than one output is\nstraightforward.Exercise 5.21\nWe note here that the Hessian matrix can sometimes be calculated indirectly as\npart of the network training algorithm. In particular, quasi-Newton nonlinear opti-\nmization algorithms gradually build up an approximation to the inverse of the Hes-\nsian during training. Such algorithms are discussed in detail in Bishop and Nabney\n(2008).\n5.4.4 Finite differences\nAs in the case of the ﬁrst derivatives of the error function, we can ﬁnd the second\nderivatives by using ﬁnite differences, with accuracy limited by numerical precision.\nIf we perturb each possible pair of weights in turn, we obtain\n∂2E\n∂wji∂wlk\n= 1\n4ϵ2 {E(wji + ϵ, wlk + ϵ) − E(wji + ϵ, wlk − ϵ)\n−E(wji − ϵ, wlk + ϵ)+ E(wji − ϵ, wlk − ϵ)} + O(ϵ2). (5.90)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 271,
      "page_label": "252"
    }
  },
  {
    "page_content": "5.4. The Hessian Matrix 253\nAgain, by using a symmetrical central differences formulation, we ensure that the\nresidual errors are O(ϵ2) rather than O(ϵ). Because there are W2 elements in the\nHessian matrix, and because the evaluation of each element requires four forward\npropagations each needing O(W) operations (per pattern), we see that this approach\nwill require O(W3) operations to evaluate the complete Hessian. It therefore has\npoor scaling properties, although in practice it is very useful as a check on the soft-\nware implementation of backpropagation methods.\nA more efﬁcient version of numerical differentiation can be found by applying\ncentral differences to the ﬁrst derivatives of the error function, which are themselves\ncalculated using backpropagation. This gives\n∂2E\n∂wji∂wlk\n= 1\n2ϵ\n{ ∂E\n∂wji\n(wlk + ϵ) − ∂E\n∂wji\n(wlk − ϵ)\n}\n+ O(ϵ2). (5.91)\nBecause there are now only W weights to be perturbed, and because the gradients\ncan be evaluated inO(W) steps, we see that this method gives the Hessian inO(W2)\noperations.\n5.4.5 Exact evaluation of the Hessian\nSo far, we have considered various approximation schemes for evaluating the\nHessian matrix or its inverse. The Hessian can also be evaluated exactly, for a net-\nwork of arbitrary feed-forward topology, using extension of the technique of back-\npropagation used to evaluate ﬁrst derivatives, which shares many of its desirable\nfeatures including computational efﬁciency (Bishop, 1991; Bishop, 1992). It can be",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 272,
      "page_label": "253"
    }
  },
  {
    "page_content": "features including computational efﬁciency (Bishop, 1991; Bishop, 1992). It can be\napplied to any differentiable error function that can be expressed as a function of\nthe network outputs and to networks having arbitrary differentiable activation func-\ntions. The number of computational steps needed to evaluate the Hessian scales\nlikeO(W2). Similar algorithms have also been considered by Buntine and Weigend\n(1993).\nHere we consider the speciﬁc case of a network having two layers of weights,\nfor which the required equations are easily derived. We shall use indices i and i′Exercise 5.22\nto denote inputs, indices j and j′to denoted hidden units, and indices k and k′to\ndenote outputs. We ﬁrst deﬁne\nδk = ∂En\n∂ak\n,M kk′ ≡ ∂2En\n∂ak∂ak′\n(5.92)\nwhere En is the contribution to the error from data point n. The Hessian matrix for\nthis network can then be considered in three separate blocks as follows.\n1. Both weights in the second layer:\n∂2En\n∂w(2)\nkj ∂w(2)\nk′j′\n= zjzj′Mkk′. (5.93)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 272,
      "page_label": "253"
    }
  },
  {
    "page_content": "254 5. NEURAL NETWORKS\n2. Both weights in the ﬁrst layer:\n∂2En\n∂w(1)\nji ∂w(1)\nj′i′\n= xixi′h′′(aj′)Ijj′\n∑\nk\nw(2)\nkj′δk\n+xixi′h′(aj′)h′(aj)\n∑\nk\n∑\nk′\nw(2)\nk′j′w(2)\nkj Mkk′. (5.94)\n3. One weight in each layer:\n∂2En\n∂w(1)\nji ∂w(2)\nkj′\n= xih′(aj′)\n{\nδkIjj′ + zj\n∑\nk′\nw(2)\nk′j′Hkk′\n}\n. (5.95)\nHere Ijj′ is the j, j′element of the identity matrix. If one or both of the weights is\na bias term, then the corresponding expressions are obtained simply by setting the\nappropriate activation(s) to1. Inclusion of skip-layer connections is straightforward.Exercise 5.23\n5.4.6 Fast multiplication by the Hessian\nFor many applications of the Hessian, the quantity of interest is not the Hessian\nmatrix H itself but the product of H with some vector v. We have seen that the\nevaluation of the Hessian takesO(W2) operations, and it also requires storage that is\nO(W2). The vector vTH that we wish to calculate, however, has only W elements,\nso instead of computing the Hessian as an intermediate step, we can instead try to\nﬁnd an efﬁcient approach to evaluating vTH directly in a way that requires only\nO(W) operations.\nTo do this, we ﬁrst note that\nvTH = vT∇(∇E) (5.96)\nwhere ∇ denotes the gradient operator in weight space. We can then write down\nthe standard forward-propagation and backpropagation equations for the evaluation\nof ∇E and apply (5.96) to these equations to give a set of forward-propagation and\nbackpropagation equations for the evaluation of vTH (Møller, 1993; Pearlmutter,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 273,
      "page_label": "254"
    }
  },
  {
    "page_content": "backpropagation equations for the evaluation of vTH (Møller, 1993; Pearlmutter,\n1994). This corresponds to acting on the original forward-propagation and back-\npropagation equations with a differential operatorvT∇. Pearlmutter (1994) used the\nnotation R{·} to denote the operator vT∇, and we shall follow this convention. The\nanalysis is straightforward and makes use of the usual rules of differential calculus,\ntogether with the result\nR{w} = v. (5.97)\nThe technique is best illustrated with a simple example, and again we choose a\ntwo-layer network of the form shown in Figure 5.1, with linear output units and a\nsum-of-squares error function. As before, we consider the contribution to the error\nfunction from one pattern in the data set. The required vector is then obtained as",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 273,
      "page_label": "254"
    }
  },
  {
    "page_content": "5.4. The Hessian Matrix 255\nusual by summing over the contributions from each of the patterns separately. For\nthe two-layer network, the forward-propagation equations are given by\naj =\n∑\ni\nwjixi (5.98)\nzj = h(aj) (5.99)\nyk =\n∑\nj\nwkjzj. (5.100)\nWe now act on these equations using the R{·} operator to obtain a set of forward\npropagation equations in the form\nR{aj} =\n∑\ni\nvjixi (5.101)\nR{zj} = h′(aj)R{aj} (5.102)\nR{yk} =\n∑\nj\nwkjR{zj} +\n∑\nj\nvkjzj (5.103)\nwhere vji is the element of the vector v that corresponds to the weight wji. Quan-\ntities of the form R{zj}, R{aj} and R{yk} are to be regarded as new variables\nwhose values are found using the above equations.\nBecause we are considering a sum-of-squares error function, we have the fol-\nlowing standard backpropagation expressions:\nδk = yk − tk (5.104)\nδj = h′(aj)\n∑\nk\nwkjδk. (5.105)\nAgain, we act on these equations with theR{·} operator to obtain a set of backprop-\nagation equations in the form\nR{δk} = R{yk} (5.106)\nR{δj} = h′′(aj)R{aj}\n∑\nk\nwkjδk\n+ h′(aj)\n∑\nk\nvkjδk + h′(aj)\n∑\nk\nwkjR{δk}. (5.107)\nFinally, we have the usual equations for the ﬁrst derivatives of the error\n∂E\n∂wkj\n= δkzj (5.108)\n∂E\n∂wji\n= δjxi (5.109)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 274,
      "page_label": "255"
    }
  },
  {
    "page_content": "256 5. NEURAL NETWORKS\nand acting on these with the R{·} operator, we obtain expressions for the elements\nof the vector vTH\nR\n{ ∂E\n∂wkj\n}\n= R{δk}zj + δkR{zj} (5.110)\nR\n{ ∂E\n∂wji\n}\n= xiR{δj}. (5.111)\nThe implementation of this algorithm involves the introduction of additional\nvariables R{aj}, R{zj} and R{δj} for the hidden units and R{δk} and R{yk}\nfor the output units. For each input pattern, the values of these quantities can be\nfound using the above results, and the elements ofvTH are then given by (5.110)\nand (5.111). An elegant aspect of this technique is that the equations for evaluating\nvTH mirror closely those for standard forward and backward propagation, and so the\nextension of existing software to compute this product is typically straightforward.\nIf desired, the technique can be used to evaluate the full Hessian matrix by\nchoosing the vector v to be given successively by a series of unit vectors of the\nform (0, 0,..., 1,..., 0) each of which picks out one column of the Hessian. This\nleads to a formalism that is analytically equivalent to the backpropagation procedure\nof Bishop (1992), as described in Section 5.4.5, though with some loss of efﬁciency\ndue to redundant calculations.\n5.5. Regularization in Neural Networks\nThe number of input and outputs units in a neural network is generally determined\nby the dimensionality of the data set, whereas the numberM of hidden units is a free\nparameter that can be adjusted to give the best predictive performance. Note that M",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 275,
      "page_label": "256"
    }
  },
  {
    "page_content": "parameter that can be adjusted to give the best predictive performance. Note that M\ncontrols the number of parameters (weights and biases) in the network, and so we\nmight expect that in a maximum likelihood setting there will be an optimum value\nofM that gives the best generalization performance, corresponding to the optimum\nbalance between under-ﬁtting and over-ﬁtting. Figure 5.9 shows an example of the\neffect of different values of M for the sinusoidal regression problem.\nThe generalization error, however, is not a simple function of M due to the\npresence of local minima in the error function, as illustrated in Figure 5.10. Here\nwe see the effect of choosing multiple random initializations for the weight vector\nfor a range of values of M. The overall best validation set performance in this\ncase occurred for a particular solution having M =8 . In practice, one approach to\nchoosing M is in fact to plot a graph of the kind shown in Figure 5.10 and then to\nchoose the speciﬁc solution having the smallest validation set error.\nThere are, however, other ways to control the complexity of a neural network\nmodel in order to avoid over-ﬁtting. From our discussion of polynomial curve ﬁtting\nin Chapter 1, we see that an alternative approach is to choose a relatively large value\nfor M and then to control complexity by the addition of a regularization term to the\nerror function. The simplest regularizer is the quadratic, giving a regularized error",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 275,
      "page_label": "256"
    }
  },
  {
    "page_content": "5.5. Regularization in Neural Networks 257\nM =1\n0 1\n−1\n0\n1 M =3\n0 1\n−1\n0\n1 M =1 0\n0 1\n−1\n0\n1\nFigure 5.9 Examples of two-layer networks trained on 10 data points drawn from the sinusoidal data set. The\ngraphs show the result of ﬁtting networks having M =1 , 3 and 10 hidden units, respectively, by minimizing a\nsum-of-squares error function using a scaled conjugate-gradient algorithm.\nof the form\n˜E(w)= E(w)+ λ\n2wTw. (5.112)\nThis regularizer is also known as weight decay and has been discussed at length\nin Chapter 3. The effective model complexity is then determined by the choice of\nthe regularization coefﬁcient λ. As we have seen previously, this regularizer can be\ninterpreted as the negative logarithm of a zero-mean Gaussian prior distribution over\nthe weight vector w.\n5.5.1 Consistent Gaussian priors\nOne of the limitations of simple weight decay in the form (5.112) is that is\ninconsistent with certain scaling properties of network mappings. To illustrate this,\nconsider a multilayer perceptron network having two layers of weights and linear\noutput units, which performs a mapping from a set of input variables {xi} to a set\nof output variables {yk}. The activations of the hidden units in the ﬁrst hidden layer\nFigure 5.10 Plot of the sum-of-squares test-set\nerror for the polynomial data set ver-\nsus the number of hidden units in the\nnetwork, with 30 random starts for\neach network size, showing the ef-\nfect of local minima. For each new\nstart, the weight vector was initial-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 276,
      "page_label": "257"
    }
  },
  {
    "page_content": "network, with 30 random starts for\neach network size, showing the ef-\nfect of local minima. For each new\nstart, the weight vector was initial-\nized by sampling from an isotropic\nGaussian distribution having a mean\nof zero and a variance of 10.\n0 2 4 6 8 10\n60\n80\n100\n120\n140\n160",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 276,
      "page_label": "257"
    }
  },
  {
    "page_content": "258 5. NEURAL NETWORKS\ntake the form\nzj = h\n( ∑\ni\nwjixi + wj0\n)\n(5.113)\nwhile the activations of the output units are given by\nyk =\n∑\nj\nwkjzj + wk0. (5.114)\nSuppose we perform a linear transformation of the input data of the form\nxi → ˜xi = axi + b. (5.115)\nThen we can arrange for the mapping performed by the network to be unchanged\nby making a corresponding linear transformation of the weights and biases from the\ninputs to the units in the hidden layer of the formExercise 5.24\nwji → ˜wji = 1\nawji (5.116)\nwj0 → ˜wj0 = wj0 − b\na\n∑\ni\nwji. (5.117)\nSimilarly, a linear transformation of the output variables of the network of the form\nyk → ˜yk = cyk + d (5.118)\ncan be achieved by making a transformation of the second-layer weights and biases\nusing\nwkj → ˜wkj = cwkj (5.119)\nwk0 → ˜wk0 = cwk0 + d. (5.120)\nIf we train one network using the original data and one network using data for which\nthe input and/or target variables are transformed by one of the above linear transfor-\nmations, then consistency requires that we should obtain equivalent networks that\ndiffer only by the linear transformation of the weights as given. Any regularizer\nshould be consistent with this property, otherwise it arbitrarily favours one solution\nover another, equivalent one. Clearly, simple weight decay (5.112), that treats all\nweights and biases on an equal footing, does not satisfy this property.\nWe therefore look for a regularizer which is invariant under the linear trans-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 277,
      "page_label": "258"
    }
  },
  {
    "page_content": "We therefore look for a regularizer which is invariant under the linear trans-\nformations (5.116), (5.117), (5.119) and (5.120). These require that the regularizer\nshould be invariant to re-scaling of the weights and to shifts of the biases. Such a\nregularizer is given by\nλ1\n2\n∑\nw∈W1\nw2 + λ2\n2\n∑\nw∈W2\nw2 (5.121)\nwhere W1 denotes the set of weights in the ﬁrst layer, W2 denotes the set of weights\nin the second layer, and biases are excluded from the summations. This regularizer",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 277,
      "page_label": "258"
    }
  },
  {
    "page_content": "5.5. Regularization in Neural Networks 259\nwill remain unchanged under the weight transformations provided the regularization\nparameters are re-scaled using λ1 → a1/2λ1 and λ2 → c−1/2λ2.\nThe regularizer (5.121) corresponds to a prior of the form\np(w|α1,α 2) ∝ exp\n(\n−α1\n2\n∑\nw∈W1\nw2 − α2\n2\n∑\nw∈W2\nw2\n)\n. (5.122)\nNote that priors of this form are improper (they cannot be normalized) because the\nbias parameters are unconstrained. The use of improper priors can lead to difﬁculties\nin selecting regularization coefﬁcients and in model comparison within the Bayesian\nframework, because the corresponding evidence is zero. It is therefore common to\ninclude separate priors for the biases (which then break shift invariance) having their\nown hyperparameters. We can illustrate the effect of the resulting four hyperpa-\nrameters by drawing samples from the prior and plotting the corresponding network\nfunctions, as shown in Figure 5.11.\nMore generally, we can consider priors in which the weights are divided into\nany number of groups Wk so that\np(w) ∝ exp\n(\n−1\n2\n∑\nk\nαk∥w∥2\nk\n)\n(5.123)\nwhere\n∥w∥2\nk =\n∑\nj∈Wk\nw2\nj . (5.124)\nAs a special case of this prior, if we choose the groups to correspond to the sets\nof weights associated with each of the input units, and we optimize the marginal\nlikelihood with respect to the corresponding parametersαk, we obtain automatic\nrelevance determinationas discussed in Section 7.2.2.\n5.5.2 Early stopping",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 278,
      "page_label": "259"
    }
  },
  {
    "page_content": "relevance determinationas discussed in Section 7.2.2.\n5.5.2 Early stopping\nAn alternative to regularization as a way of controlling the effective complexity\nof a network is the procedure of early stopping. The training of nonlinear network\nmodels corresponds to an iterative reduction of the error function deﬁned with re-\nspect to a set of training data. For many of the optimization algorithms used for\nnetwork training, such as conjugate gradients, the error is a nonincreasing function\nof the iteration index. However, the error measured with respect to independent data,\ngenerally called a validation set, often shows a decrease at ﬁrst, followed by an in-\ncrease as the network starts to over-ﬁt. Training can therefore be stopped at the point\nof smallest error with respect to the validation data set, as indicated in Figure 5.12,\nin order to obtain a network having good generalization performance.\nThe behaviour of the network in this case is sometimes explained qualitatively\nin terms of the effective number of degrees of freedom in the network, in which this\nnumber starts out small and then to grows during the training process, corresponding\nto a steady increase in the effective complexity of the model. Halting training before",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 278,
      "page_label": "259"
    }
  },
  {
    "page_content": "260 5. NEURAL NETWORKS\nαw\n1 =1 , αb\n1 =1 , αw\n2 =1 , αb\n2 =1\n−1 −0.5 0 0.5 1\n−6\n−4\n−2\n0\n2\n4\nαw\n1 =1 , αb\n1 =1 , αw\n2 = 10, αb\n2 =1\n−1 −0.5 0 0.5 1\n−60\n−40\n−20\n0\n20\n40\nαw\n1 = 1000, αb\n1 = 100, αw\n2 =1 , αb\n2 =1\n−1 −0.5 0 0.5 1\n−10\n−5\n0\n5\nαw\n1 = 1000, αb\n1 = 1000, αw\n2 =1 , αb\n2 =1\n−1 −0.5 0 0.5 1\n−10\n−5\n0\n5\nFigure 5.11 Illustration of the effect of the hyperparameters governing the prior distribution over weights and\nbiases in a two-layer network having a single input, a single linear output, and 12 hidden units having ‘ tanh’\nactivation functions. The priors are governed by four hyperparameters αb\n1 , αw\n1 , αb\n2 , and αw\n2 , which represent\nthe precisions of the Gaussian distributions of the ﬁrst-layer biases, ﬁrst-layer weights, second-layer biases, and\nsecond-layer weights, respectively. We see that the parameter αw\n2 governs the vertical scale of functions (note\nthe different vertical axis ranges on the top two diagrams), αw\n1 governs the horizontal scale of variations in the\nfunction values, and αb\n1 governs the horizontal range over which variations occur. The parameter αb\n2 , whose\neffect is not illustrated here, governs the range of vertical offsets of the functions.\na minimum of the training error has been reached then represents a way of limiting\nthe effective network complexity.\nIn the case of a quadratic error function, we can verify this insight, and show\nthat early stopping should exhibit similar behaviour to regularization using a sim-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 279,
      "page_label": "260"
    }
  },
  {
    "page_content": "that early stopping should exhibit similar behaviour to regularization using a sim-\nple weight-decay term. This can be understood from Figure 5.13, in which the axes\nin weight space have been rotated to be parallel to the eigenvectors of the Hessian\nmatrix. If, in the absence of weight decay, the weight vector starts at the origin and\nproceeds during training along a path that follows the local negative gradient vec-\ntor, then the weight vector will move initially parallel to the w2 axis through a point\ncorresponding roughly to ˜w and then move towards the minimum of the error func-\ntion wML. This follows from the shape of the error surface and the widely differing\neigenvalues of the Hessian. Stopping at a point near ˜w is therefore similar to weight\ndecay. The relationship between early stopping and weight decay can be made quan-\ntitative, thereby showing that the quantity τη (where τ is the iteration index, and ηExercise 5.25\nis the learning rate parameter) plays the role of the reciprocal of the regularization",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 279,
      "page_label": "260"
    }
  },
  {
    "page_content": "5.5. Regularization in Neural Networks 261\n0 10 20 30 40 50\n0.15\n0.2\n0.25\n0 10 20 30 40 50\n0.35\n0.4\n0.45\nFigure 5.12 An illustration of the behaviour of training set error (left) and validation set error (right) during a\ntypical training session, as a function of the iteration step, for the sinusoidal data set. The goal of achieving\nthe best generalization performance suggests that training should be stopped at the point shown by the vertical\ndashed lines, corresponding to the minimum of the validation set error.\nparameter λ. The effective number of parameters in the network therefore grows\nduring the course of training.\n5.5.3 Invariances\nIn many applications of pattern recognition, it is known that predictions should\nbe unchanged, or invariant, under one or more transformations of the input vari-\nables. For example, in the classiﬁcation of objects in two-dimensional images, such\nas handwritten digits, a particular object should be assigned the same classiﬁcation\nirrespective of its position within the image ( translation invariance) or of its size\n(scale invariance). Such transformations produce signiﬁcant changes in the raw\ndata, expressed in terms of the intensities at each of the pixels in the image, and\nyet should give rise to the same output from the classiﬁcation system. Similarly\nin speech recognition, small levels of nonlinear warping along the time axis, which\npreserve temporal ordering, should not change the interpretation of the signal.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 280,
      "page_label": "261"
    }
  },
  {
    "page_content": "preserve temporal ordering, should not change the interpretation of the signal.\nIf sufﬁciently large numbers of training patterns are available, then an adaptive\nmodel such as a neural network can learn the invariance, at least approximately. This\ninvolves including within the training set a sufﬁciently large number of examples of\nthe effects of the various transformations. Thus, for translation invariance in an im-\nage, the training set should include examples of objects at many different positions.\nThis approach may be impractical, however, if the number of training examples\nis limited, or if there are several invariants (because the number of combinations of\ntransformations grows exponentially with the number of such transformations). We\ntherefore seek alternative approaches for encouraging an adaptive model to exhibit\nthe required invariances. These can broadly be divided into four categories:\n1. The training set is augmented using replicas of the training patterns, trans-\nformed according to the desired invariances. For instance, in our digit recog-\nnition example, we could make multiple copies of each example in which the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 280,
      "page_label": "261"
    }
  },
  {
    "page_content": "262 5. NEURAL NETWORKS\nFigure 5.13 A schematic illustration of why\nearly stopping can give similar\nresults to weight decay in the\ncase of a quadratic error func-\ntion. The ellipse shows a con-\ntour of constant error, and wML\ndenotes the minimum of the er-\nror function. If the weight vector\nstarts at the origin and moves ac-\ncording to the local negative gra-\ndient direction, then it will follow\nthe path shown by the curve. By\nstopping training early, a weight\nvector ew is found that is qual-\nitatively similar to that obtained\nwith a simple weight-decay reg-\nularizer and training to the mini-\nmum of the regularized error, as\ncan be seen by comparing with\nFigure 3.15.\nw1\nw2\n˜w\nwML\ndigit is shifted to a different position in each image.\n2. A regularization term is added to the error function that penalizes changes in\nthe model output when the input is transformed. This leads to the technique of\ntangent propagation, discussed in Section 5.5.4.\n3. Invariance is built into the pre-processing by extracting features that are invari-\nant under the required transformations. Any subsequent regression or classi-\nﬁcation system that uses such features as inputs will necessarily also respect\nthese invariances.\n4. The ﬁnal option is to build the invariance properties into the structure of a neu-\nral network (or into the deﬁnition of a kernel function in the case of techniques\nsuch as the relevance vector machine). One way to achieve this is through the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 281,
      "page_label": "262"
    }
  },
  {
    "page_content": "such as the relevance vector machine). One way to achieve this is through the\nuse of local receptive ﬁelds and shared weights, as discussed in the context of\nconvolutional neural networks in Section 5.5.6.\nApproach 1 is often relatively easy to implement and can be used to encourage com-\nplex invariances such as those illustrated in Figure 5.14. For sequential training\nalgorithms, this can be done by transforming each input pattern before it is presented\nto the model so that, if the patterns are being recycled, a different transformation\n(drawn from an appropriate distribution) is added each time. For batch methods, a\nsimilar effect can be achieved by replicating each data point a number of times and\ntransforming each copy independently. The use of such augmented data can lead to\nsigniﬁcant improvements in generalization (Simardet al., 2003), although it can also\nbe computationally costly.\nApproach 2 leaves the data set unchanged but modiﬁes the error function through\nthe addition of a regularizer. In Section 5.5.5, we shall show that this approach is\nclosely related to approach 2.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 281,
      "page_label": "262"
    }
  },
  {
    "page_content": "5.5. Regularization in Neural Networks 263\nFigure 5.14 Illustration of the synthetic warping of a handwritten digit. The original image is shown on the\nleft. On the right, the top row shows three examples of warped digits, with the corresponding displacement\nﬁelds shown on the bottom row. These displacement ﬁelds are generated by sampling random displacements\n∆x, ∆y ∈ (0, 1) at each pixel and then smoothing by convolution with Gaussians of width 0.01, 30 and 60\nrespectively.\nOne advantage of approach 3 is that it can correctly extrapolate well beyond the\nrange of transformations included in the training set. However, it can be difﬁcult\nto ﬁnd hand-crafted features with the required invariances that do not also discard\ninformation that can be useful for discrimination.\n5.5.4 Tangent propagation\nWe can use regularization to encourage models to be invariant to transformations\nof the input through the technique of tangent propagation (Simard et al., 1992).\nConsider the effect of a transformation on a particular input vector xn. Provided the\ntransformation is continuous (such as translation or rotation, but not mirror reﬂection\nfor instance), then the transformed pattern will sweep out a manifold M within the\nD-dimensional input space. This is illustrated in Figure 5.15, for the case of D =\n2 for simplicity. Suppose the transformation is governed by a single parameter ξ\n(which might be rotation angle for instance). Then the subspace M swept out by xn",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 282,
      "page_label": "263"
    }
  },
  {
    "page_content": "(which might be rotation angle for instance). Then the subspace M swept out by xn\nFigure 5.15 Illustration of a two-dimensional input space\nshowing the effect of a continuous transforma-\ntion on a particular input vector xn. A one-\ndimensional transformation, parameterized by\nthe continuous variableξ, applied toxn causes\nit to sweep out a one-dimensional manifoldM.\nLocally, the effect of the transformation can be\napproximated by the tangent vector τn.\nx1\nx2\nxn\nτn\nξ\nM",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 282,
      "page_label": "263"
    }
  },
  {
    "page_content": "264 5. NEURAL NETWORKS\nwill be one-dimensional, and will be parameterized by ξ. Let the vector that results\nfrom acting on xn by this transformation be denoted by s(xn,ξ ), which is deﬁned\nso that s(x,0) = x. Then the tangent to the curve M is given by the directional\nderivative τ = ∂s/∂ξ, and the tangent vector at the point xn is given by\nτn = ∂s(xn,ξ )\n∂ξ\n⏐⏐⏐⏐\nξ=0\n. (5.125)\nUnder a transformation of the input vector, the network output vector will, in general,\nchange. The derivative of output k with respect to ξ is given by\n∂yk\n∂ξ\n⏐⏐⏐⏐\nξ=0\n=\nD∑\ni=1\n∂yk\n∂xi\n∂xi\n∂ξ\n⏐⏐⏐⏐⏐\nξ=0\n=\nD∑\ni=1\nJkiτi (5.126)\nwhere Jki is the (k,i ) element of the Jacobian matrixJ, as discussed in Section 5.3.4.\nThe result (5.126) can be used to modify the standard error function, so as to encour-\nage local invariance in the neighbourhood of the data points, by the addition to the\noriginal error function E of a regularization function Ω to give a total error function\nof the form\n˜E = E + λΩ (5.127)\nwhere λ is a regularization coefﬁcient and\nΩ= 1\n2\n∑\nn\n∑\nk\n(\n∂ynk\n∂ξ\n⏐⏐⏐⏐\nξ=0\n) 2\n= 1\n2\n∑\nn\n∑\nk\n( D∑\ni=1\nJnkiτni\n) 2\n. (5.128)\nThe regularization function will be zero when the network mapping function is in-\nvariant under the transformation in the neighbourhood of each pattern vector, and\nthe value of the parameter λ determines the balance between ﬁtting the training data\nand learning the invariance property.\nIn a practical implementation, the tangent vector τn can be approximated us-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 283,
      "page_label": "264"
    }
  },
  {
    "page_content": "and learning the invariance property.\nIn a practical implementation, the tangent vector τn can be approximated us-\ning ﬁnite differences, by subtracting the original vector xn from the corresponding\nvector after transformation using a small value of ξ, and then dividing by ξ. This is\nillustrated in Figure 5.16.\nThe regularization function depends on the network weights through the Jaco-\nbian J. A backpropagation formalism for computing the derivatives of the regu-\nlarizer with respect to the network weights is easily obtained by extension of theExercise 5.26\ntechniques introduced in Section 5.3.\nIf the transformation is governed by L parameters (e.g., L =3 for the case of\ntranslations combined with in-plane rotations in a two-dimensional image), then the\nmanifold M will have dimensionality L, and the corresponding regularizer is given\nby the sum of terms of the form (5.128), one for each transformation. If several\ntransformations are considered at the same time, and the network mapping is made\ninvariant to each separately, then it will be (locally) invariant to combinations of the\ntransformations (Simard et al., 1992).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 283,
      "page_label": "264"
    }
  },
  {
    "page_content": "5.5. Regularization in Neural Networks 265\nFigure 5.16 Illustration showing\n(a) the original image x of a hand-\nwritten digit, (b) the tangent vector\nτ corresponding to an inﬁnitesimal\nclockwise rotation, (c) the result of\nadding a small contribution from the\ntangent vector to the original image\ngiving x + ϵτ with ϵ =1 5 degrees,\nand (d) the true image rotated for\ncomparison.\n(a)\n (b)\n(c)\n (d)\nA related technique, called tangent distance, can be used to build invariance\nproperties into distance-based methods such as nearest-neighbour classiﬁers (Simard\net al., 1993).\n5.5.5 Training with transformed data\nWe have seen that one way to encourage invariance of a model to a set of trans-\nformations is to expand the training set using transformed versions of the original\ninput patterns. Here we show that this approach is closely related to the technique of\ntangent propagation (Bishop, 1995b; Leen, 1995).\nAs in Section 5.5.4, we shall consider a transformation governed by a single\nparameter ξ and described by the function s(x,ξ ), with s(x, 0) = x. We shall\nalso consider a sum-of-squares error function. The error function for untransformed\ninputs can be written (in the inﬁnite data set limit) in the form\nE = 1\n2\n∫∫\n{y(x) − t}2p(t|x)p(x)dxdt (5.129)\nas discussed in Section 1.5.5. Here we have considered a network having a single\noutput, in order to keep the notation uncluttered. If we now consider an inﬁnite",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 284,
      "page_label": "265"
    }
  },
  {
    "page_content": "output, in order to keep the notation uncluttered. If we now consider an inﬁnite\nnumber of copies of each data point, each of which is perturbed by the transformation",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 284,
      "page_label": "265"
    }
  },
  {
    "page_content": "266 5. NEURAL NETWORKS\nin which the parameter ξ is drawn from a distribution p(ξ), then the error function\ndeﬁned over this expanded data set can be written as\n˜E = 1\n2\n∫∫∫\n{y(s(x,ξ )) − t}2p(t|x)p(x)p(ξ)d xdtdξ. (5.130)\nWe now assume that the distributionp(ξ) has zero mean with small variance, so that\nwe are only considering small transformations of the original input vectors. We can\nthen expand the transformation function as a Taylor series in powers of ξ to give\ns(x,ξ )= s(x,0) +ξ ∂\n∂ξ s(x,ξ )\n⏐⏐⏐⏐\nξ=0\n+ ξ2\n2\n∂2\n∂ξ2 s(x,ξ )\n⏐⏐⏐⏐\nξ=0\n+ O(ξ3)\n= x + ξτ + 1\n2ξ2τ′+ O(ξ3)\nwhere τ′denotes the second derivative ofs(x,ξ ) with respect toξ evaluated atξ =0 .\nThis allows us to expand the model function to give\ny(s(x,ξ )) = y(x)+ ξτT∇y(x)+ ξ2\n2\n[\n(τ′)\nT\n∇y(x)+ τT∇∇y(x)τ\n]\n+ O(ξ3).\nSubstituting into the mean error function (5.130) and expanding, we then have\n˜E = 1\n2\n∫∫\n{y(x) − t}2p(t|x)p(x)d xdt\n+ E[ξ]\n∫∫\n{y(x) − t}τT∇y(x)p(t|x)p(x)dxdt\n+ E[ξ2]\n∫∫ [\n{y(x) − t}1\n2\n{\n(τ′)\nT\n∇y(x)+ τT∇∇y(x)τ\n}\n+\n(\nτT∇y(x)\n)2]\np(t|x)p(x)dxdt + O(ξ3).\nBecause the distribution of transformations has zero mean we have E[ξ]=0 . Also,\nwe shall denote E[ξ2] by λ. Omitting terms of O(ξ3), the average error function then\nbecomes\n˜E = E + λΩ (5.131)\nwhere E is the original sum-of-squares error, and the regularization termΩ takes the\nform\nΩ=\n∫ [\n{y(x) − E[t|x]}1\n2\n{\n(τ′)\nT\n∇y(x)+ τT∇∇y(x)τ\n}\n+\n(\nτT ∇y(x)\n)2\n]\np(x)dx (5.132)\nin which we have performed the integration over t.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 285,
      "page_label": "266"
    }
  },
  {
    "page_content": "5.5. Regularization in Neural Networks 267\nWe can further simplify this regularization term as follows. In Section 1.5.5 we\nsaw that the function that minimizes the sum-of-squares error is given by the condi-\ntional averageE[t|x] of the target values t. From (5.131) we see that the regularized\nerror will equal the unregularized sum-of-squares plus terms which areO(ξ), and so\nthe network function that minimizes the total error will have the form\ny(x)= E[t|x]+ O(ξ). (5.133)\nThus, to leading order in ξ, the ﬁrst term in the regularizer vanishes and we are left\nwith\nΩ= 1\n2\n∫ (\nτT ∇y(x)\n)2\np(x)dx (5.134)\nwhich is equivalent to the tangent propagation regularizer (5.128).\nIf we consider the special case in which the transformation of the inputs simply\nconsists of the addition of random noise, so that x → x + ξ, then the regularizer\ntakes the formExercise 5.27\nΩ= 1\n2\n∫\n∥∇y(x)∥2 p(x)dx (5.135)\nwhich is known as Tikhonov regularization (Tikhonov and Arsenin, 1977; Bishop,\n1995b). Derivatives of this regularizer with respect to the network weights can be\nfound using an extended backpropagation algorithm (Bishop, 1993). We see that, for\nsmall noise amplitudes, Tikhonov regularization is related to the addition of random\nnoise to the inputs, which has been shown to improve generalization in appropriate\ncircumstances (Sietsma and Dow, 1991).\n5.5.6 Convolutional networks\nAnother approach to creating models that are invariant to certain transformation",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 286,
      "page_label": "267"
    }
  },
  {
    "page_content": "circumstances (Sietsma and Dow, 1991).\n5.5.6 Convolutional networks\nAnother approach to creating models that are invariant to certain transformation\nof the inputs is to build the invariance properties into the structure of a neural net-\nwork. This is the basis for the convolutional neural network(Le Cun et al., 1989;\nLeCun et al., 1998), which has been widely applied to image data.\nConsider the speciﬁc task of recognizing handwritten digits. Each input image\ncomprises a set of pixel intensity values, and the desired output is a posterior proba-\nbility distribution over the ten digit classes. We know that the identity of the digit is\ninvariant under translations and scaling as well as (small) rotations. Furthermore, the\nnetwork must also exhibit invariance to more subtle transformations such as elastic\ndeformations of the kind illustrated in Figure 5.14. One simple approach would be to\ntreat the image as the input to a fully connected network, such as the kind shown in\nFigure 5.1. Given a sufﬁciently large training set, such a network could in principle\nyield a good solution to this problem and would learn the appropriate invariances by\nexample.\nHowever, this approach ignores a key property of images, which is that nearby\npixels are more strongly correlated than more distant pixels. Many of the modern\napproaches to computer vision exploit this property by extracting local features that\ndepend only on small subregions of the image. Information from such features can",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 286,
      "page_label": "267"
    }
  },
  {
    "page_content": "depend only on small subregions of the image. Information from such features can\nthen be merged in later stages of processing in order to detect higher-order features",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 286,
      "page_label": "267"
    }
  },
  {
    "page_content": "268 5. NEURAL NETWORKS\nInput image Convolutional layer Sub-sampling\nlayer\nFigure 5.17 Diagram illustrating part of a convolutional neural network, showing a layer of convolu-\ntional units followed by a layer of subsampling units. Several successive pairs of such\nlayers may be used.\nand ultimately to yield information about the image as whole. Also, local features\nthat are useful in one region of the image are likely to be useful in other regions of\nthe image, for instance if the object of interest is translated.\nThese notions are incorporated into convolutional neural networks through three\nmechanisms: (i) local receptive ﬁelds, (ii) weight sharing, and (iii) subsampling. The\nstructure of a convolutional network is illustrated in Figure 5.17. In the convolutional\nlayer the units are organized into planes, each of which is called afeature map. Units\nin a feature map each take inputs only from a small subregion of the image, and all\nof the units in a feature map are constrained to share the same weight values. For\ninstance, a feature map might consist of 100 units arranged in a 10 × 10 grid, with\neach unit taking inputs from a5×5 pixel patch of the image. The whole feature map\ntherefore has 25 adjustable weight parameters plus one adjustable bias parameter.\nInput values from a patch are linearly combined using the weights and the bias, and\nthe result transformed by a sigmoidal nonlinearity using (5.1). If we think of the units",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 287,
      "page_label": "268"
    }
  },
  {
    "page_content": "the result transformed by a sigmoidal nonlinearity using (5.1). If we think of the units\nas feature detectors, then all of the units in a feature map detect the same pattern but\nat different locations in the input image. Due to the weight sharing, the evaluation\nof the activations of these units is equivalent to a convolution of the image pixel\nintensities with a ‘kernel’ comprising the weight parameters. If the input image is\nshifted, the activations of the feature map will be shifted by the same amount but will\notherwise be unchanged. This provides the basis for the (approximate) invariance of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 287,
      "page_label": "268"
    }
  },
  {
    "page_content": "5.5. Regularization in Neural Networks 269\nthe network outputs to translations and distortions of the input image. Because we\nwill typically need to detect multiple features in order to build an effective model,\nthere will generally be multiple feature maps in the convolutional layer, each having\nits own set of weight and bias parameters.\nThe outputs of the convolutional units form the inputs to the subsampling layer\nof the network. For each feature map in the convolutional layer, there is a plane of\nunits in the subsampling layer and each unit takes inputs from a small receptive ﬁeld\nin the corresponding feature map of the convolutional layer. These units perform\nsubsampling. For instance, each subsampling unit might take inputs from a2 × 2\nunit region in the corresponding feature map and would compute the average of\nthose inputs, multiplied by an adaptive weight with the addition of an adaptive bias\nparameter, and then transformed using a sigmoidal nonlinear activation function.\nThe receptive ﬁelds are chosen to be contiguous and nonoverlapping so that there\nare half the number of rows and columns in the subsampling layer compared with\nthe convolutional layer. In this way, the response of a unit in the subsampling layer\nwill be relatively insensitive to small shifts of the image in the corresponding regions\nof the input space.\nIn a practical architecture, there may be several pairs of convolutional and sub-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 288,
      "page_label": "269"
    }
  },
  {
    "page_content": "of the input space.\nIn a practical architecture, there may be several pairs of convolutional and sub-\nsampling layers. At each stage there is a larger degree of invariance to input trans-\nformations compared to the previous layer. There may be several feature maps in a\ngiven convolutional layer for each plane of units in the previous subsampling layer,\nso that the gradual reduction in spatial resolution is then compensated by an increas-\ning number of features. The ﬁnal layer of the network would typically be a fully\nconnected, fully adaptive layer, with a softmax output nonlinearity in the case of\nmulticlass classiﬁcation.\nThe whole network can be trained by error minimization using backpropagation\nto evaluate the gradient of the error function. This involves a slight modiﬁcation\nof the usual backpropagation algorithm to ensure that the shared-weight constraints\nare satisﬁed. Due to the use of local receptive ﬁelds, the number of weights inExercise 5.28\nthe network is smaller than if the network were fully connected. Furthermore, the\nnumber of independent parameters to be learned from the data is much smaller still,\ndue to the substantial numbers of constraints on the weights.\n5.5.7 Soft weight sharing\nOne way to reduce the effective complexity of a network with a large number\nof weights is to constrain weights within certain groups to be equal. This is the\ntechnique of weight sharing that was discussed in Section 5.5.6 as a way of building",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 288,
      "page_label": "269"
    }
  },
  {
    "page_content": "technique of weight sharing that was discussed in Section 5.5.6 as a way of building\ntranslation invariance into networks used for image interpretation. It is only appli-\ncable, however, to particular problems in which the form of the constraints can be\nspeciﬁed in advance. Here we consider a form of soft weight sharing(Nowlan and\nHinton, 1992) in which the hard constraint of equal weights is replaced by a form\nof regularization in which groups of weights are encouraged to have similar values.\nFurthermore, the division of weights into groups, the mean weight value for each\ngroup, and the spread of values within the groups are all determined as part of the\nlearning process.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 288,
      "page_label": "269"
    }
  },
  {
    "page_content": "270 5. NEURAL NETWORKS\nRecall that the simple weight decay regularizer, given in (5.112), can be viewed\nas the negative log of a Gaussian prior distribution over the weights. We can encour-\nage the weight values to form several groups, rather than just one group, by consid-\nering instead a probability distribution that is a mixture of Gaussians. The centresSection 2.3.9\nand variances of the Gaussian components, as well as the mixing coefﬁcients, will be\nconsidered as adjustable parameters to be determined as part of the learning process.\nThus, we have a probability density of the form\np(w)=\n∏\ni\np(wi) (5.136)\nwhere\np(wi)=\nM∑\nj=1\nπjN(wi|µj,σ 2\nj ) (5.137)\nand πj are the mixing coefﬁcients. Taking the negative logarithm then leads to a\nregularization function of the form\nΩ(w)= −\n∑\ni\nln\n( M∑\nj=1\nπjN(wi|µj,σ 2\nj )\n)\n. (5.138)\nThe total error function is then given by\n˜E(w)= E(w)+ λΩ(w) (5.139)\nwhere λ is the regularization coefﬁcient. This error is minimized both with respect\nto the weights wi and with respect to the parameters {πj,µj,σ j} of the mixture\nmodel. If the weights were constant, then the parameters of the mixture model could\nbe determined by using the EM algorithm discussed in Chapter 9. However, the dis-\ntribution of weights is itself evolving during the learning process, and so to avoid nu-\nmerical instability, a joint optimization is performed simultaneously over the weights\nand the mixture-model parameters. This can be done using a standard optimization",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 289,
      "page_label": "270"
    }
  },
  {
    "page_content": "and the mixture-model parameters. This can be done using a standard optimization\nalgorithm such as conjugate gradients or quasi-Newton methods.\nIn order to minimize the total error function, it is necessary to be able to evaluate\nits derivatives with respect to the various adjustable parameters. To do this it is con-\nvenient to regard the{πj} as prior probabilities and to introduce the corresponding\nposterior probabilities which, following (2.192), are given by Bayes’ theorem in the\nform\nγj(w)= πjN(w|µj,σ 2\nj )∑\nk πkN(w|µk,σ 2\nk). (5.140)\nThe derivatives of the total error function with respect to the weights are then given\nbyExercise 5.29\n∂˜E\n∂wi\n= ∂E\n∂wi\n+ λ\n∑\nj\nγj(wi)(wi − µj)\nσ2\nj\n. (5.141)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 289,
      "page_label": "270"
    }
  },
  {
    "page_content": "5.5. Regularization in Neural Networks 271\nThe effect of the regularization term is therefore to pull each weight towards the\ncentre of the jth Gaussian, with a force proportional to the posterior probability of\nthat Gaussian for the given weight. This is precisely the kind of effect that we are\nseeking.\nDerivatives of the error with respect to the centres of the Gaussians are also\neasily computed to giveExercise 5.30\n∂˜E\n∂µj\n= λ\n∑\ni\nγj(wi)(µi − wj)\nσ2\nj\n(5.142)\nwhich has a simple intuitive interpretation, because it pushes µj towards an aver-\nage of the weight values, weighted by the posterior probabilities that the respective\nweight parameters were generated by component j. Similarly, the derivatives with\nrespect to the variances are given byExercise 5.31\n∂˜E\n∂σj\n= λ\n∑\ni\nγj(wi)\n( 1\nσj\n− (wi − µj)2\nσ3\nj\n)\n(5.143)\nwhich drivesσj towards the weighted average of the squared deviations of the weights\naround the corresponding centreµj, where the weighting coefﬁcients are again given\nby the posterior probability that each weight is generated by component j. Note that\nin a practical implementation, new variables ηj deﬁned by\nσ2\nj = exp(ηj) (5.144)\nare introduced, and the minimization is performed with respect to the ηj. This en-\nsures that the parameters σj remain positive. It also has the effect of discouraging\npathological solutions in which one or more of the σj goes to zero, corresponding\nto a Gaussian component collapsing onto one of the weight parameter values. Such",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 290,
      "page_label": "271"
    }
  },
  {
    "page_content": "to a Gaussian component collapsing onto one of the weight parameter values. Such\nsolutions are discussed in more detail in the context of Gaussian mixture models in\nSection 9.2.1.\nFor the derivatives with respect to the mixing coefﬁcients πj, we need to take\naccount of the constraints\n∑\nj\nπj =1 , 0 ⩽ πi ⩽ 1 (5.145)\nwhich follow from the interpretation of the πj as prior probabilities. This can be\ndone by expressing the mixing coefﬁcients in terms of a set of auxiliary variables\n{ηj} using the softmax function given by\nπj = exp(ηj)\n∑ M\nk=1 exp(ηk)\n. (5.146)\nThe derivatives of the regularized error function with respect to the {ηj} then take\nthe formExercise 5.32",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 290,
      "page_label": "271"
    }
  },
  {
    "page_content": "272 5. NEURAL NETWORKS\nFigure 5.18 The left ﬁgure shows a two-link robot arm,\nin which the Cartesian coordinates (x1,x 2) of the end ef-\nfector are determined uniquely by the two joint angles θ1\nand θ2 and the (ﬁxed) lengths L1 and L2 of the arms. This\nis know as the forward kinematics of the arm. In prac-\ntice, we have to ﬁnd the joint angles that will give rise to a\ndesired end effector position and, as shown in the right ﬁg-\nure, this inverse kinematics has two solutions correspond-\ning to ‘elbow up’ and ‘elbow down’.\nL1\nL2\nθ1\nθ2\n(x1,x2) (x1,x2)\nelbow\ndown\nelbow\nup\n∂˜E\n∂ηj\n=\n∑\ni\n{πj − γj(wi)}. (5.147)\nWe see that πj is therefore driven towards the average posterior probability for com-\nponent j.\n5.6. Mixture Density Networks\nThe goal of supervised learning is to model a conditional distribution p(t|x), which\nfor many simple regression problems is chosen to be Gaussian. However, practical\nmachine learning problems can often have signiﬁcantly non-Gaussian distributions.\nThese can arise, for example, with inverse problemsin which the distribution can be\nmultimodal, in which case the Gaussian assumption can lead to very poor predic-\ntions.\nAs a simple example of an inverse problem, consider the kinematics of a robot\narm, as illustrated in Figure 5.18. The forward probleminvolves ﬁnding the end ef-Exercise 5.33\nfector position given the joint angles and has a unique solution. However, in practice",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 291,
      "page_label": "272"
    }
  },
  {
    "page_content": "fector position given the joint angles and has a unique solution. However, in practice\nwe wish to move the end effector of the robot to a speciﬁc position, and to do this we\nmust set appropriate joint angles. We therefore need to solve the inverse problem,\nwhich has two solutions as seen in Figure 5.18.\nForward problems often corresponds to causality in a physical system and gen-\nerally have a unique solution. For instance, a speciﬁc pattern of symptoms in the\nhuman body may be caused by the presence of a particular disease. In pattern recog-\nnition, however, we typically have to solve an inverse problem, such as trying to\npredict the presence of a disease given a set of symptoms. If the forward problem\ninvolves a many-to-one mapping, then the inverse problem will have multiple solu-\ntions. For instance, several different diseases may result in the same symptoms.\nIn the robotics example, the kinematics is deﬁned by geometrical equations, and\nthe multimodality is readily apparent. However, in many machine learning problems\nthe presence of multimodality, particularly in problems involving spaces of high di-\nmensionality, can be less obvious. For tutorial purposes, however, we shall consider\na simple toy problem for which we can easily visualize the multimodality. Data for\nthis problem is generated by sampling a variablex uniformly over the interval(0, 1),\nto give a set of values {xn}, and the corresponding target values tn are obtained",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 291,
      "page_label": "272"
    }
  },
  {
    "page_content": "5.6. Mixture Density Networks 273\nFigure 5.19 On the left is the data\nset for a simple ‘forward problem’ in\nwhich the red curve shows the result\nof ﬁtting a two-layer neural network\nby minimizing the sum-of-squares\nerror function. The corresponding\ninverse problem, shown on the right,\nis obtained by exchanging the roles\nof x and t. Here the same net-\nwork trained again by minimizing the\nsum-of-squares error function gives\na very poor ﬁt to the data due to the\nmultimodality of the data set. 0 1\n0\n1\n0 1\n0\n1\nby computing the function xn +0 .3 sin(2πxn) and then adding uniform noise over\nthe interval (−0.1,0.1). The inverse problem is then obtained by keeping the same\ndata points but exchanging the roles of x and t. Figure 5.19 shows the data sets for\nthe forward and inverse problems, along with the results of ﬁtting two-layer neural\nnetworks having 6 hidden units and a single linear output unit by minimizing a sum-\nof-squares error function. Least squares corresponds to maximum likelihood under\na Gaussian assumption. We see that this leads to a very poor model for the highly\nnon-Gaussian inverse problem.\nWe therefore seek a general framework for modelling conditional probability\ndistributions. This can be achieved by using a mixture model for p(t|x) in which\nboth the mixing coefﬁcients as well as the component densities are ﬂexible functions\nof the input vector x, giving rise to the mixture density network. For any given value",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 292,
      "page_label": "273"
    }
  },
  {
    "page_content": "of the input vector x, giving rise to the mixture density network. For any given value\nof x, the mixture model provides a general formalism for modelling an arbitrary\nconditional density function p(t|x). Provided we consider a sufﬁciently ﬂexible\nnetwork, we then have a framework for approximating arbitrary conditional distri-\nbutions.\nHere we shall develop the model explicitly for Gaussian components, so that\np(t|x)=\nK∑\nk=1\nπk(x)N\n(\nt|µk(x),σ 2\nk(x)\n)\n. (5.148)\nThis is an example of a heteroscedastic model since the noise variance on the data\nis a function of the input vector x. Instead of Gaussians, we can use other distribu-\ntions for the components, such as Bernoulli distributions if the target variables are\nbinary rather than continuous. We have also specialized to the case of isotropic co-\nvariances for the components, although the mixture density network can readily be\nextended to allow for general covariance matrices by representing the covariances\nusing a Cholesky factorization (Williams, 1996). Even with isotropic components,\nthe conditional distribution p(t|x) does not assume factorization with respect to the\ncomponents of t (in contrast to the standard sum-of-squares regression model) as a\nconsequence of the mixture distribution.\nWe now take the various parameters of the mixture model, namely the mixing\ncoefﬁcients πk(x), the means µk(x), and the variances σ2\nk(x), to be governed by",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 292,
      "page_label": "273"
    }
  },
  {
    "page_content": "274 5. NEURAL NETWORKS\nx1\nxD\nθ1\nθM\nθ\nt\np(t|x)\nFigure 5.20 The mixture density network can represent general conditional probability densitiesp(t|x)\nby considering a parametric mixture model for the distribution of t whose parameters are\ndetermined by the outputs of a neural network that takes x as its input vector.\nthe outputs of a conventional neural network that takes x as its input. The structure\nof this mixture density network is illustrated in Figure 5.20. The mixture density\nnetwork is closely related to the mixture of experts discussed in Section 14.5.3. The\nprinciple difference is that in the mixture density network the same function is used\nto predict the parameters of all of the component densities as well as the mixing co-\nefﬁcients, and so the nonlinear hidden units are shared amongst the input-dependent\nfunctions.\nThe neural network in Figure 5.20 can, for example, be a two-layer network\nhaving sigmoidal (‘ tanh’) hidden units. If there are L components in the mixture\nmodel (5.148), and if t has K components, then the network will have L output unit\nactivations denoted by aπ\nk that determine the mixing coefﬁcients πk(x), K outputs\ndenoted by aσ\nk that determine the kernel widths σk(x), and L × K outputs denoted\nby aµ\nkj that determine the components µkj(x) of the kernel centres µk(x). The total\nnumber of network outputs is given by (K +2 )L, as compared with the usual K\noutputs for a network, which simply predicts the conditional means of the target\nvariables.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 293,
      "page_label": "274"
    }
  },
  {
    "page_content": "outputs for a network, which simply predicts the conditional means of the target\nvariables.\nThe mixing coefﬁcients must satisfy the constraints\nK∑\nk=1\nπk(x)=1 , 0 ⩽ πk(x) ⩽ 1 (5.149)\nwhich can be achieved using a set of softmax outputs\nπk(x)= exp(aπ\nk)\n∑ K\nl=1 exp(aπ\nl )\n. (5.150)\nSimilarly, the variances must satisfy σ2\nk(x) ⩾ 0 and so can be represented in terms\nof the exponentials of the corresponding network activations using\nσk(x)=e x p (aσ\nk). (5.151)\nFinally, because the means µk(x) have real components, they can be represented",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 293,
      "page_label": "274"
    }
  },
  {
    "page_content": "5.6. Mixture Density Networks 275\ndirectly by the network output activations\nµkj(x)= aµ\nkj. (5.152)\nThe adaptive parameters of the mixture density network comprise the vector w\nof weights and biases in the neural network, that can be set by maximum likelihood,\nor equivalently by minimizing an error function deﬁned to be the negative logarithm\nof the likelihood. For independent data, this error function takes the form\nE(w)= −\nN∑\nn=1\nln\n{ k∑\nk=1\nπk(xn, w)N\n(\ntn|µk(xn, w),σ 2\nk(xn, w)\n)\n}\n(5.153)\nwhere we have made the dependencies on w explicit.\nIn order to minimize the error function, we need to calculate the derivatives of\nthe error E(w) with respect to the components of w. These can be evaluated by\nusing the standard backpropagation procedure, provided we obtain suitable expres-\nsions for the derivatives of the error with respect to the output-unit activations. These\nrepresent error signals δ for each pattern and for each output unit, and can be back-\npropagated to the hidden units and the error function derivatives evaluated in the\nusual way. Because the error function (5.153) is composed of a sum of terms, one\nfor each training data point, we can consider the derivatives for a particular pattern\nnand then ﬁnd the derivatives of E by summing over all patterns.\nBecause we are dealing with mixture distributions, it is convenient to view the\nmixing coefﬁcients πk(x) as x-dependent prior probabilities and to introduce the\ncorresponding posterior probabilities given by",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 294,
      "page_label": "275"
    }
  },
  {
    "page_content": "mixing coefﬁcients πk(x) as x-dependent prior probabilities and to introduce the\ncorresponding posterior probabilities given by\nγk(t|x)= πkNnk\n∑ K\nl=1 πlNnl\n(5.154)\nwhere Nnk denotes N (tn|µk(xn),σ 2\nk(xn)).\nThe derivatives with respect to the network output activations governing the mix-\ning coefﬁcients are given byExercise 5.34\n∂En\n∂aπ\nk\n= πk − γk. (5.155)\nSimilarly, the derivatives with respect to the output activations controlling the com-\nponent means are given byExercise 5.35\n∂En\n∂aµ\nkl\n= γk\n{µkl − tl\nσ2\nk\n}\n. (5.156)\nFinally, the derivatives with respect to the output activations controlling the compo-\nnent variances are given byExercise 5.36\n∂En\n∂aσ\nk\n= −γk\n{∥t − µk∥2\nσ3\nk\n− 1\nσk\n}\n. (5.157)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 294,
      "page_label": "275"
    }
  },
  {
    "page_content": "276 5. NEURAL NETWORKS\nFigure 5.21 (a) Plot of the mixing\ncoefﬁcients πk(x) as a function of\nx for the three kernel functions in a\nmixture density network trained on\nthe data shown in Figure 5.19. The\nmodel has three Gaussian compo-\nnents, and uses a two-layer multi-\nlayer perceptron with ﬁve ‘tanh’ sig-\nmoidal units in the hidden layer, and\nnine outputs (corresponding to the 3\nmeans and 3 variances of the Gaus-\nsian components and the 3 mixing\ncoefﬁcients). At both small and large\nvalues of x, where the conditional\nprobability density of the target data\nis unimodal, only one of the ker-\nnels has a high value for its prior\nprobability, while at intermediate val-\nues of x, where the conditional den-\nsity is trimodal, the three mixing co-\nefﬁcients have comparable values.\n(b) Plots of the means µk(x) using\nthe same colour coding as for the\nmixing coefﬁcients. (c) Plot of the\ncontours of the corresponding con-\nditional probability density of the tar-\nget data for the same mixture den-\nsity network. (d) Plot of the ap-\nproximate conditional mode, shown\nby the red points, of the conditional\ndensity.\n0 1\n0\n1\n(a)\n0 1\n0\n1\n(b)\n(c)\n0 1\n0\n1\n0 1\n0\n1\n(d)\nWe illustrate the use of a mixture density network by returning to the toy ex-\nample of an inverse problem shown in Figure 5.19. Plots of the mixing coefﬁ-\ncients πk(x), the means µk(x), and the conditional density contours corresponding\nto p(t|x), are shown in Figure 5.21. The outputs of the neural network, and hence the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 295,
      "page_label": "276"
    }
  },
  {
    "page_content": "to p(t|x), are shown in Figure 5.21. The outputs of the neural network, and hence the\nparameters in the mixture model, are necessarily continuous single-valued functions\nof the input variables. However, we see from Figure 5.21(c) that the model is able to\nproduce a conditional density that is unimodal for some values ofx and trimodal for\nother values by modulating the amplitudes of the mixing components πk(x).\nOnce a mixture density network has been trained, it can predict the conditional\ndensity function of the target data for any given value of the input vector. This\nconditional density represents a complete description of the generator of the data, so\nfar as the problem of predicting the value of the output vector is concerned. From\nthis density function we can calculate more speciﬁc quantities that may be of interest\nin different applications. One of the simplest of these is the mean, corresponding to\nthe conditional average of the target data, and is given by\nE [t|x]=\n∫\ntp(t|x)d t =\nK∑\nk=1\nπk(x)µk(x) (5.158)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 295,
      "page_label": "276"
    }
  },
  {
    "page_content": "5.7. Bayesian Neural Networks 277\nwhere we have used (5.148). Because a standard network trained by least squares\nis approximating the conditional mean, we see that a mixture density network can\nreproduce the conventional least-squares result as a special case. Of course, as we\nhave already noted, for a multimodal distribution the conditional mean is of limited\nvalue.\nWe can similarly evaluate the variance of the density function about the condi-\ntional average, to giveExercise 5.37\ns2(x)= E\n[\n∥t − E[t|x]∥2 |x\n]\n(5.159)\n=\nK∑\nk=1\nπk(x)\n⎧\n⎨\n⎩σ2\nk(x)+\nµk(x) −\nK∑\nl=1\nπl(x)µl(x)\n\n2⎫\n⎬\n⎭ (5.160)\nwhere we have used (5.148) and (5.158). This is more general than the corresponding\nleast-squares result because the variance is a function of x.\nWe have seen that for multimodal distributions, the conditional mean can give\na poor representation of the data. For instance, in controlling the simple robot arm\nshown in Figure 5.18, we need to pick one of the two possible joint angle settings\nin order to achieve the desired end-effector location, whereas the average of the two\nsolutions is not itself a solution. In such cases, the conditional mode may be of\nmore value. Because the conditional mode for the mixture density network does not\nhave a simple analytical solution, this would require numerical iteration. A simple\nalternative is to take the mean of the most probable component (i.e., the one with the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 296,
      "page_label": "277"
    }
  },
  {
    "page_content": "alternative is to take the mean of the most probable component (i.e., the one with the\nlargest mixing coefﬁcient) at each value of x. This is shown for the toy data set in\nFigure 5.21(d).\n5.7. Bayesian Neural Networks\nSo far, our discussion of neural networks has focussed on the use of maximum like-\nlihood to determine the network parameters (weights and biases). Regularized max-\nimum likelihood can be interpreted as a MAP (maximum posterior) approach in\nwhich the regularizer can be viewed as the logarithm of a prior parameter distribu-\ntion. However, in a Bayesian treatment we need to marginalize over the distribution\nof parameters in order to make predictions.\nIn Section 3.3, we developed a Bayesian solution for a simple linear regression\nmodel under the assumption of Gaussian noise. We saw that the posterior distribu-\ntion, which is Gaussian, could be evaluated exactly and that the predictive distribu-\ntion could also be found in closed form. In the case of a multilayered network, the\nhighly nonlinear dependence of the network function on the parameter values means\nthat an exact Bayesian treatment can no longer be found. In fact, the log of the pos-\nterior distribution will be nonconvex, corresponding to the multiple local minima in\nthe error function.\nThe technique of variational inference, to be discussed in Chapter 10, has been\napplied to Bayesian neural networks using a factorized Gaussian approximation",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 296,
      "page_label": "277"
    }
  },
  {
    "page_content": "278 5. NEURAL NETWORKS\nto the posterior distribution (Hinton and van Camp, 1993) and also using a full-\ncovariance Gaussian (Barber and Bishop, 1998a; Barber and Bishop, 1998b). The\nmost complete treatment, however, has been based on the Laplace approximation\n(MacKay, 1992c; MacKay, 1992b) and forms the basis for the discussion given here.\nWe will approximate the posterior distribution by a Gaussian, centred at a mode of\nthe true posterior. Furthermore, we shall assume that the covariance of this Gaus-\nsian is small so that the network function is approximately linear with respect to the\nparameters over the region of parameter space for which the posterior probability is\nsigniﬁcantly nonzero. With these two approximations, we will obtain models that\nare analogous to the linear regression and classiﬁcation models discussed in earlier\nchapters and so we can exploit the results obtained there. We can then make use of\nthe evidence framework to provide point estimates for the hyperparameters and to\ncompare alternative models (for example, networks having different numbers of hid-\nden units). To start with, we shall discuss the regression case and then later consider\nthe modiﬁcations needed for solving classiﬁcation tasks.\n5.7.1 Posterior parameter distribution\nConsider the problem of predicting a single continuous target variable t from\na vector x of inputs (the extension to multiple targets is straightforward). We shall",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 297,
      "page_label": "278"
    }
  },
  {
    "page_content": "a vector x of inputs (the extension to multiple targets is straightforward). We shall\nsuppose that the conditional distribution p(t|x) is Gaussian, with an x-dependent\nmean given by the output of a neural network model y(x,w), and with precision\n(inverse variance) β\np(t|x, w,β )= N(t|y(x,w),β −1). (5.161)\nSimilarly, we shall choose a prior distribution over the weightsw that is Gaussian of\nthe form\np(w|α)= N(w|0,α −1I). (5.162)\nFor an i.i.d. data set ofN observations x1,..., xN , with a corresponding set of target\nvalues D = {t1,...,t N }, the likelihood function is given by\np(D|w,β )=\nN∏\nn=1\nN(tn|y(xn, w),β −1) (5.163)\nand so the resulting posterior distribution is then\np(w|D,α ,β) ∝ p(w|α)p(D|w,β ). (5.164)\nwhich, as a consequence of the nonlinear dependence of y(x,w) on w, will be non-\nGaussian.\nWe can ﬁnd a Gaussian approximation to the posterior distribution by using the\nLaplace approximation. To do this, we must ﬁrst ﬁnd a (local) maximum of the\nposterior, and this must be done using iterative numerical optimization. As usual, it\nis convenient to maximize the logarithm of the posterior, which can be written in the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 297,
      "page_label": "278"
    }
  },
  {
    "page_content": "5.7. Bayesian Neural Networks 279\nform\nln p(w|D)= −α\n2 wTw − β\n2\nN∑\nn=1\n{y(xn, w) − tn}2 +c o n s t (5.165)\nwhich corresponds to a regularized sum-of-squares error function. Assuming for\nthe moment that α and β are ﬁxed, we can ﬁnd a maximum of the posterior, which\nwe denote wMAP, by standard nonlinear optimization algorithms such as conjugate\ngradients, using error backpropagation to evaluate the required derivatives.\nHaving found a mode wMAP, we can then build a local Gaussian approximation\nby evaluating the matrix of second derivatives of the negative log posterior distribu-\ntion. From (5.165), this is given by\nA = −∇∇lnp(w|D,α ,β)= αI + βH (5.166)\nwhere H is the Hessian matrix comprising the second derivatives of the sum-of-\nsquares error function with respect to the components ofw. Algorithms for comput-\ning and approximating the Hessian were discussed in Section 5.4. The corresponding\nGaussian approximation to the posterior is then given from (4.134) by\nq(w|D)= N(w|wMAP, A−1). (5.167)\nSimilarly, the predictive distribution is obtained by marginalizing with respect\nto this posterior distribution\np(t|x, D)=\n∫\np(t|x, w)q(w|D)d w. (5.168)\nHowever, even with the Gaussian approximation to the posterior, this integration is\nstill analytically intractable due to the nonlinearity of the network function y(x, w)\nas a function of w. To make progress, we now assume that the posterior distribution\nhas small variance compared with the characteristic scales of w over which y(x, w)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 298,
      "page_label": "279"
    }
  },
  {
    "page_content": "has small variance compared with the characteristic scales of w over which y(x, w)\nis varying. This allows us to make a Taylor series expansion of the network function\naround wMAP and retain only the linear terms\ny(x,w) ≃ y(x,wMAP)+ gT(w − wMAP) (5.169)\nwhere we have deﬁned\ng = ∇wy(x, w)|w=wMAP\n. (5.170)\nWith this approximation, we now have a linear-Gaussian model with a Gaussian\ndistribution for p(w) and a Gaussian for p(t|w) whose mean is a linear function of\nw of the form\np(t|x, w,β ) ≃N\n(\nt|y(x,wMAP)+ gT(w − wMAP),β −1)\n. (5.171)\nWe can therefore make use of the general result (2.115) for the marginalp(t) to giveExercise 5.38\np(t|x, D,α ,β)= N\n(\nt|y(x,wMAP),σ 2(x)\n)\n(5.172)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 298,
      "page_label": "279"
    }
  },
  {
    "page_content": "280 5. NEURAL NETWORKS\nwhere the input-dependent variance is given by\nσ2(x)= β−1 + gTA−1g. (5.173)\nWe see that the predictive distribution p(t|x, D) is a Gaussian whose mean is given\nby the network functiony(x,wMAP) with the parameter set to their MAP value. The\nvariance has two terms, the ﬁrst of which arises from the intrinsic noise on the target\nvariable, whereas the second is an x-dependent term that expresses the uncertainty\nin the interpolant due to the uncertainty in the model parameters w. This should\nbe compared with the corresponding predictive distribution for the linear regression\nmodel, given by (3.58) and (3.59).\n5.7.2 Hyperparameter optimization\nSo far, we have assumed that the hyperparametersα and β are ﬁxed and known.\nWe can make use of the evidence framework, discussed in Section 3.5, together with\nthe Gaussian approximation to the posterior obtained using the Laplace approxima-\ntion, to obtain a practical procedure for choosing the values of such hyperparameters.\nThe marginal likelihood, or evidence, for the hyperparameters is obtained by\nintegrating over the network weights\np(D|α, β)=\n∫\np(D|w,β )p(w|α)d w. (5.174)\nThis is easily evaluated by making use of the Laplace approximation result (4.135).Exercise 5.39\nTaking logarithms then gives\nlnp(D|α, β) ≃− E(wMAP) − 1\n2 ln|A| + W\n2 lnα + N\n2 lnβ − N\n2 ln(2π) (5.175)\nwhere W is the total number of parameters in w, and the regularized error function\nis deﬁned by\nE(wMAP)= β\n2\nN∑\nn=1\n{y(xn, wMAP) − tn}2 + α\n2 wT",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 299,
      "page_label": "280"
    }
  },
  {
    "page_content": "where W is the total number of parameters in w, and the regularized error function\nis deﬁned by\nE(wMAP)= β\n2\nN∑\nn=1\n{y(xn, wMAP) − tn}2 + α\n2 wT\nMAPwMAP. (5.176)\nWe see that this takes the same form as the corresponding result (3.86) for the linear\nregression model.\nIn the evidence framework, we make point estimates forα and β by maximizing\nlnp(D|α, β). Consider ﬁrst the maximization with respect to α, which can be done\nby analogy with the linear regression case discussed in Section 3.5.2. We ﬁrst deﬁne\nthe eigenvalue equation\nβHui = λiui (5.177)\nwhere H is the Hessian matrix comprising the second derivatives of the sum-of-\nsquares error function, evaluated at w = wMAP. By analogy with (3.92), we obtain\nα = γ\nwT\nMAPwMAP\n(5.178)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 299,
      "page_label": "280"
    }
  },
  {
    "page_content": "5.7. Bayesian Neural Networks 281\nwhere γ represents the effective number of parameters and is deﬁned bySection 3.5.3\nγ =\nW∑\ni=1\nλi\nα + λi\n. (5.179)\nNote that this result was exact for the linear regression case. For the nonlinear neural\nnetwork, however, it ignores the fact that changes in α will cause changes in the\nHessian H, which in turn will change the eigenvalues. We have therefore implicitly\nignored terms involving the derivatives ofλi with respect to α.\nSimilarly, from (3.95) we see that maximizing the evidence with respect to β\ngives the re-estimation formula\n1\nβ = 1\nN − γ\nN∑\nn=1\n{y(xn, wMAP) − tn}2. (5.180)\nAs with the linear model, we need to alternate between re-estimation of the hyper-\nparameters α and β and updating of the posterior distribution. The situation with\na neural network model is more complex, however, due to the multimodality of the\nposterior distribution. As a consequence, the solution for wMAP found by maximiz-\ning the log posterior will depend on the initialization ofw. Solutions that differ only\nas a consequence of the interchange and sign reversal symmetries in the hidden unitsSection 5.1.1\nare identical so far as predictions are concerned, and it is irrelevant which of the\nequivalent solutions is found. However, there may be inequivalent solutions as well,\nand these will generally yield different values for the optimized hyperparameters.\nIn order to compare different models, for example neural networks having differ-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 300,
      "page_label": "281"
    }
  },
  {
    "page_content": "In order to compare different models, for example neural networks having differ-\nent numbers of hidden units, we need to evaluate the model evidencep(D). This can\nbe approximated by taking (5.175) and substituting the values of α and β obtained\nfrom the iterative optimization of these hyperparameters. A more careful evaluation\nis obtained by marginalizing overα and β, again by making a Gaussian approxima-\ntion (MacKay, 1992c; Bishop, 1995a). In either case, it is necessary to evaluate the\ndeterminant |A| of the Hessian matrix. This can be problematic in practice because\nthe determinant, unlike the trace, is sensitive to the small eigenvalues that are often\ndifﬁcult to determine accurately.\nThe Laplace approximation is based on a local quadratic expansion around a\nmode of the posterior distribution over weights. We have seen in Section 5.1.1 that\nany given mode in a two-layer network is a member of a set of M!2M equivalent\nmodes that differ by interchange and sign-change symmetries, where M is the num-\nber of hidden units. When comparing networks having different numbers of hid-\nden units, this can be taken into account by multiplying the evidence by a factor of\nM!2M .\n5.7.3 Bayesian neural networks for classiﬁcation\nSo far, we have used the Laplace approximation to develop a Bayesian treat-\nment of neural network regression models. We now discuss the modiﬁcations to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 300,
      "page_label": "281"
    }
  },
  {
    "page_content": "282 5. NEURAL NETWORKS\nthis framework that arise when it is applied to classiﬁcation. Here we shall con-\nsider a network having a single logistic sigmoid output corresponding to a two-class\nclassiﬁcation problem. The extension to networks with multiclass softmax outputs\nis straightforward. We shall build extensively on the analogous results for linearExercise 5.40\nclassiﬁcation models discussed in Section 4.5, and so we encourage the reader to\nfamiliarize themselves with that material before studying this section.\nThe log likelihood function for this model is given by\nlnp(D|w)=\n∑\nn\n=1 N {tn lnyn +( 1− tn)l n ( 1− yn)} (5.181)\nwhere tn ∈{ 0, 1} are the target values, and yn ≡ y(xn, w). Note that there is no\nhyperparameter β, because the data points are assumed to be correctly labelled. As\nbefore, the prior is taken to be an isotropic Gaussian of the form (5.162).\nThe ﬁrst stage in applying the Laplace framework to this model is to initialize\nthe hyperparameter α, and then to determine the parameter vector w by maximizing\nthe log posterior distribution. This is equivalent to minimizing the regularized error\nfunction\nE(w)= −lnp(D|w)+ α\n2 wTw (5.182)\nand can be achieved using error backpropagation combined with standard optimiza-\ntion algorithms, as discussed in Section 5.3.\nHaving found a solution wMAP for the weight vector, the next step is to eval-\nuate the Hessian matrix H comprising the second derivatives of the negative log",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 301,
      "page_label": "282"
    }
  },
  {
    "page_content": "uate the Hessian matrix H comprising the second derivatives of the negative log\nlikelihood function. This can be done, for instance, using the exact method of Sec-\ntion 5.4.5, or using the outer product approximation given by (5.85). The second\nderivatives of the negative log posterior can again be written in the form (5.166), and\nthe Gaussian approximation to the posterior is then given by (5.167).\nTo optimize the hyperparameter α, we again maximize the marginal likelihood,\nwhich is easily shown to take the formExercise 5.41\nlnp(D|α) ≃− E(wMAP) − 1\n2 ln|A| + W\n2 lnα +c o n s t (5.183)\nwhere the regularized error function is deﬁned by\nE(wMAP)= −\nN∑\nn=1\n{tn lnyn +( 1− tn)l n ( 1− yn)} + α\n2 wT\nMAPwMAP (5.184)\nin which yn ≡ y(xn, wMAP). Maximizing this evidence function with respect to α\nagain leads to the re-estimation equation given by (5.178).\nThe use of the evidence procedure to determine α is illustrated in Figure 5.22\nfor the synthetic two-dimensional data discussed in Appendix A.\nFinally, we need the predictive distribution, which is deﬁned by (5.168). Again,\nthis integration is intractable due to the nonlinearity of the network function. The",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 301,
      "page_label": "282"
    }
  },
  {
    "page_content": "5.7. Bayesian Neural Networks 283\nFigure 5.22 Illustration of the evidence framework\napplied to a synthetic two-class data set.\nThe green curve shows the optimal de-\ncision boundary, the black curve shows\nthe result of ﬁtting a two-layer network\nwith 8 hidden units by maximum likeli-\nhood, and the red curve shows the re-\nsult of including a regularizer in which\nα is optimized using the evidence pro-\ncedure, starting from the initial value\nα =0 . Note that the evidence proce-\ndure greatly reduces the over-ﬁtting of\nthe network.\n−2 −1 0 1 2\n−2\n−1\n0\n1\n2\n3\nsimplest approximation is to assume that the posterior distribution is very narrow\nand hence make the approximation\np(t|x, D) ≃ p(t|x, wMAP). (5.185)\nWe can improve on this, however, by taking account of the variance of the posterior\ndistribution. In this case, a linear approximation for the network outputs, as was used\nin the case of regression, would be inappropriate due to the logistic sigmoid output-\nunit activation function that constrains the output to lie in the range (0, 1). Instead,\nwe make a linear approximation for the output unit activation in the form\na(x,w) ≃ aMAP(x)+ bT(w − wMAP) (5.186)\nwhere aMAP(x)= a(x,wMAP), and the vector b ≡∇ a(x,wMAP) can be found by\nbackpropagation.\nBecause we now have a Gaussian approximation for the posterior distribution\nover w, and a model for a that is a linear function of w, we can now appeal to the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 302,
      "page_label": "283"
    }
  },
  {
    "page_content": "over w, and a model for a that is a linear function of w, we can now appeal to the\nresults of Section 4.5.2. The distribution of output unit activation values, induced by\nthe distribution over network weights, is given by\np(a|x,D)=\n∫\nδ\n(\na − aMAP(x) − bT(x)(w − wMAP)\n)\nq(w|D)d w (5.187)\nwhere q(w|D) is the Gaussian approximation to the posterior distribution given by\n(5.167). From Section 4.5.2, we see that this distribution is Gaussian with mean\naMAP ≡ a(x,wMAP), and variance\nσ2\na(x)= bT(x)A−1b(x). (5.188)\nFinally, to obtain the predictive distribution, we must marginalize overa using\np(t =1 |x,D)=\n∫\nσ(a)p(a|x,D)d a. (5.189)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 302,
      "page_label": "283"
    }
  },
  {
    "page_content": "284 5. NEURAL NETWORKS\n−2 −1 0 1 2\n−2\n−1\n0\n1\n2\n3\n−2 −1 0 1 2\n−2\n−1\n0\n1\n2\n3\nFigure 5.23 An illustration of the Laplace approximation for a Bayesian neural network having 8 hidden units\nwith ‘tanh’ activation functions and a single logistic-sigmoid output unit. The weight parameters were found using\nscaled conjugate gradients, and the hyperparameter α was optimized using the evidence framework. On the left\nis the result of using the simple approximation (5.185) based on a point estimate wMAP of the parameters,\nin which the green curve shows the y =0 .5 decision boundary, and the other contours correspond to output\nprobabilities of y =0 .1, 0.3, 0.7, and 0.9. On the right is the corresponding result obtained using (5.190). Note\nthat the effect of marginalization is to spread out the contours and to make the predictions less conﬁdent, so\nthat at each input point x, the posterior probabilities are shifted towards 0.5, while the y =0 .5 contour itself is\nunaffected.\nThe convolution of a Gaussian with a logistic sigmoid is intractable. We therefore\napply the approximation (4.153) to (5.189) giving\np(t =1 |x,D)= σ\n(\nκ(σ2\na)bTwMAP\n)\n(5.190)\nwhere κ(·) is deﬁned by (4.154). Recall that both σ2\na and b are functions of x.\nFigure 5.23 shows an example of this framework applied to the synthetic classi-\nﬁcation data set described in Appendix A.\nExercises\n5.1 (⋆⋆ ) Consider a two-layer network function of the form (5.7) in which the hidden-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 303,
      "page_label": "284"
    }
  },
  {
    "page_content": "ﬁcation data set described in Appendix A.\nExercises\n5.1 (⋆⋆ ) Consider a two-layer network function of the form (5.7) in which the hidden-\nunit nonlinear activation functionsg(·) are given by logistic sigmoid functions of the\nform\nσ(a)= {1+e x p (−a)}−1 . (5.191)\nShow that there exists an equivalent network, which computes exactly the same func-\ntion, but with hidden unit activation functions given bytanh(a) where the tanh func-\ntion is deﬁned by (5.59). Hint: ﬁrst ﬁnd the relation between σ(a) and tanh(a), and\nthen show that the parameters of the two networks differ by linear transformations.\n5.2 (⋆) www Show that maximizing the likelihood function under the conditional\ndistribution (5.16) for a multioutput neural network is equivalent to minimizing the\nsum-of-squares error function (5.11).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 303,
      "page_label": "284"
    }
  },
  {
    "page_content": "Exercises 285\n5.3 (⋆⋆ ) Consider a regression problem involving multiple target variables in which it\nis assumed that the distribution of the targets, conditioned on the input vector x,i sa\nGaussian of the form\np(t|x, w)= N(t|y(x, w), Σ) (5.192)\nwhere y(x,w) is the output of a neural network with input vector x and weight\nvector w, and Σ is the covariance of the assumed Gaussian noise on the targets.\nGiven a set of independent observations of x and t, write down the error function\nthat must be minimized in order to ﬁnd the maximum likelihood solution for w,i f\nwe assume that Σ is ﬁxed and known. Now assume that Σ is also to be determined\nfrom the data, and write down an expression for the maximum likelihood solution\nforΣ. Note that the optimizations of w and Σ are now coupled, in contrast to the\ncase of independent target variables discussed in Section 5.2.\n5.4 (⋆⋆ ) Consider a binary classiﬁcation problem in which the target values are t ∈\n{0, 1}, with a network output y(x,w) that represents p(t =1 |x), and suppose that\nthere is a probabilityϵthat the class label on a training data point has been incorrectly\nset. Assuming independent and identically distributed data, write down the error\nfunction corresponding to the negative log likelihood. Verify that the error function\n(5.21) is obtained when ϵ =0 . Note that this error function makes the model robust\nto incorrectly labelled data, in contrast to the usual error function.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 304,
      "page_label": "285"
    }
  },
  {
    "page_content": "to incorrectly labelled data, in contrast to the usual error function.\n5.5 (⋆) www Show that maximizing likelihood for a multiclass neural network model\nin which the network outputs have the interpretation yk(x,w)= p(tk =1 |x) is\nequivalent to the minimization of the cross-entropy error function (5.24).\n5.6 (⋆) www Show the derivative of the error function (5.21) with respect to the\nactivation ak for an output unit having a logistic sigmoid activation function satisﬁes\n(5.18).\n5.7 (⋆) Show the derivative of the error function (5.24) with respect to the activationak\nfor output units having a softmax activation function satisﬁes (5.18).\n5.8 (⋆) We saw in (4.88) that the derivative of the logistic sigmoid activation function\ncan be expressed in terms of the function value itself. Derive the corresponding result\nfor the ‘tanh’ activation function deﬁned by (5.59).\n5.9 (⋆) www The error function (5.21) for binary classiﬁcation problems was de-\nrived for a network having a logistic-sigmoid output activation function, so that\n0 ⩽ y(x,w) ⩽ 1, and data having target values t ∈{ 0, 1}. Derive the correspond-\ning error function if we consider a network having an output −1 ⩽ y(x,w) ⩽ 1\nand target values t =1 for class C1 and t = −1 for class C2. What would be the\nappropriate choice of output unit activation function?\n5.10 (⋆) www Consider a Hessian matrix H with eigenvector equation (5.33). By\nsetting the vector v in (5.39) equal to each of the eigenvectors ui in turn, show that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 304,
      "page_label": "285"
    }
  },
  {
    "page_content": "setting the vector v in (5.39) equal to each of the eigenvectors ui in turn, show that\nH is positive deﬁnite if, and only if, all of its eigenvalues are positive.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 304,
      "page_label": "285"
    }
  },
  {
    "page_content": "286 5. NEURAL NETWORKS\n5.11 (⋆⋆ ) www Consider a quadratic error function deﬁned by (5.32), in which the\nHessian matrix H has an eigenvalue equation given by (5.33). Show that the con-\ntours of constant error are ellipses whose axes are aligned with the eigenvectors ui,\nwith lengths that are inversely proportional to the square root of the corresponding\neigenvalues λi.\n5.12 (⋆⋆ ) www By considering the local Taylor expansion (5.32) of an error function\nabout a stationary point w⋆ , show that the necessary and sufﬁcient condition for the\nstationary point to be a local minimum of the error function is that the Hessian matrix\nH, deﬁned by (5.30) with ˆw = w⋆ , be positive deﬁnite.\n5.13 (⋆) Show that as a consequence of the symmetry of the Hessian matrix H, the\nnumber of independent elements in the quadratic error function (5.28) is given by\nW(W +3 )/2.\n5.14 (⋆) By making a Taylor expansion, verify that the terms that areO(ϵ) cancel on the\nright-hand side of (5.69).\n5.15 (⋆⋆ ) In Section 5.3.4, we derived a procedure for evaluating the Jacobian matrix of a\nneural network using a backpropagation procedure. Derive an alternative formalism\nfor ﬁnding the Jacobian based on forward propagationequations.\n5.16 (⋆) The outer product approximation to the Hessian matrix for a neural network\nusing a sum-of-squares error function is given by (5.84). Extend this result to the\ncase of multiple outputs.\n5.17 (⋆) Consider a squared loss function of the form\nE = 1\n2\n∫∫",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 305,
      "page_label": "286"
    }
  },
  {
    "page_content": "case of multiple outputs.\n5.17 (⋆) Consider a squared loss function of the form\nE = 1\n2\n∫∫\n{y(x,w) − t}2 p(x,t )d xdt (5.193)\nwhere y(x,w) is a parametric function such as a neural network. The result (1.89)\nshows that the function y(x,w) that minimizes this error is given by the conditional\nexpectation of t given x. Use this result to show that the second derivative ofE with\nrespect to two elements wr and ws of the vector w, is given by\n∂2E\n∂wr∂ws\n=\n∫ ∂y\n∂wr\n∂y\n∂ws\np(x)dx. (5.194)\nNote that, for a ﬁnite sample from p(x), we obtain (5.84).\n5.18 (⋆) Consider a two-layer network of the form shown in Figure 5.1 with the addition\nof extra parameters corresponding to skip-layer connections that go directly from\nthe inputs to the outputs. By extending the discussion of Section 5.3.2, write down\nthe equations for the derivatives of the error function with respect to these additional\nparameters.\n5.19 (⋆) www Derive the expression (5.85) for the outer product approximation to\nthe Hessian matrix for a network having a single output with a logistic sigmoid\noutput-unit activation function and a cross-entropy error function, corresponding to\nthe result (5.84) for the sum-of-squares error function.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 305,
      "page_label": "286"
    }
  },
  {
    "page_content": "Exercises 287\n5.20 (⋆) Derive an expression for the outer product approximation to the Hessian matrix\nfor a network having K outputs with a softmax output-unit activation function and\na cross-entropy error function, corresponding to the result (5.84) for the sum-of-\nsquares error function.\n5.21 (⋆⋆⋆ ) Extend the expression (5.86) for the outer product approximation of the Hes-\nsian matrix to the case of K> 1 output units. Hence, derive a recursive expression\nanalogous to (5.87) for incrementing the number N of patterns and a similar expres-\nsion for incrementing the number K of outputs. Use these results, together with the\nidentity (5.88), to ﬁnd sequential update expressions analogous to (5.89) for ﬁnding\nthe inverse of the Hessian by incrementally including both extra patterns and extra\noutputs.\n5.22 (⋆⋆ ) Derive the results (5.93), (5.94), and (5.95) for the elements of the Hessian\nmatrix of a two-layer feed-forward network by application of the chain rule of cal-\nculus.\n5.23 (⋆⋆ ) Extend the results of Section 5.4.5 for the exact Hessian of a two-layer network\nto include skip-layer connections that go directly from inputs to outputs.\n5.24 (⋆) Verify that the network function deﬁned by (5.113) and (5.114) is invariant un-\nder the transformation (5.115) applied to the inputs, provided the weights and biases\nare simultaneously transformed using (5.116) and (5.117). Similarly, show that the\nnetwork outputs can be transformed according (5.118) by applying the transforma-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 306,
      "page_label": "287"
    }
  },
  {
    "page_content": "network outputs can be transformed according (5.118) by applying the transforma-\ntion (5.119) and (5.120) to the second-layer weights and biases.\n5.25 (⋆⋆⋆ ) www Consider a quadratic error function of the form\nE = E0 + 1\n2(w − w⋆ )TH(w − w⋆ ) (5.195)\nwhere w⋆ represents the minimum, and the Hessian matrixH is positive deﬁnite and\nconstant. Suppose the initial weight vector w(0) is chosen to be at the origin and is\nupdated using simple gradient descent\nw(τ) = w(τ−1) − ρ∇E (5.196)\nwhere τ denotes the step number, and ρ is the learning rate (which is assumed to be\nsmall). Show that, after τ steps, the components of the weight vector parallel to the\neigenvectors of H can be written\nw(τ)\nj = {1 − (1 − ρηj)τ }w⋆\nj (5.197)\nwhere wj = wTuj, and uj and ηj are the eigenvectors and eigenvalues, respectively,\nof H so that\nHuj = ηjuj. (5.198)\nShow that as τ →∞ , this gives w(τ) → w⋆ as expected, provided |1 − ρηj| < 1.\nNow suppose that training is halted after a ﬁnite number τ of steps. Show that the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 306,
      "page_label": "287"
    }
  },
  {
    "page_content": "288 5. NEURAL NETWORKS\ncomponents of the weight vector parallel to the eigenvectors of the Hessian satisfy\nw(τ)\nj ≃ w⋆\nj when ηj ≫ (ρτ)−1 (5.199)\n|w(τ)\nj |≪| w⋆\nj | when ηj ≪ (ρτ)−1. (5.200)\nCompare this result with the discussion in Section 3.5.3 of regularization with simple\nweight decay, and hence show that (ρτ)−1 is analogous to the regularization param-\neter λ. The above results also show that the effective number of parameters in the\nnetwork, as deﬁned by (3.91), grows as the training progresses.\n5.26 (⋆⋆ ) Consider a multilayer perceptron with arbitrary feed-forward topology, which\nis to be trained by minimizing the tangent propagation error function (5.127) in\nwhich the regularizing function is given by (5.128). Show that the regularization\nterm Ω can be written as a sum over patterns of terms of the form\nΩn = 1\n2\n∑\nk\n(Gyk)2 (5.201)\nwhere G is a differential operator deﬁned by\nG≡\n∑\ni\nτi\n∂\n∂xi\n. (5.202)\nBy acting on the forward propagation equations\nzj = h(aj),a j =\n∑\ni\nwjizi (5.203)\nwith the operator G, show that Ωn can be evaluated by forward propagation using\nthe following equations:\nαj = h′(aj)βj,β j =\n∑\ni\nwjiαi. (5.204)\nwhere we have deﬁned the new variables\nαj ≡G zj,β j ≡G aj. (5.205)\nNow show that the derivatives ofΩn with respect to a weight wrs in the network can\nbe written in the form\n∂Ωn\n∂wrs\n=\n∑\nk\nαk {φkrzs + δkrαs} (5.206)\nwhere we have deﬁned\nδkr ≡ ∂yk\n∂ar\n,φ kr ≡G δkr. (5.207)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 307,
      "page_label": "288"
    }
  },
  {
    "page_content": "be written in the form\n∂Ωn\n∂wrs\n=\n∑\nk\nαk {φkrzs + δkrαs} (5.206)\nwhere we have deﬁned\nδkr ≡ ∂yk\n∂ar\n,φ kr ≡G δkr. (5.207)\nWrite down the backpropagation equations for δkr, and hence derive a set of back-\npropagation equations for the evaluation of the φkr.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 307,
      "page_label": "288"
    }
  },
  {
    "page_content": "Exercises 289\n5.27 (⋆⋆ ) www Consider the framework for training with transformed data in the\nspecial case in which the transformation consists simply of the addition of random\nnoisex → x + ξ where ξ has a Gaussian distribution with zero mean and unit\ncovariance. By following an argument analogous to that of Section 5.5.5, show that\nthe resulting regularizer reduces to the Tikhonov form (5.135).\n5.28 (⋆) www Consider a neural network, such as the convolutional network discussed\nin Section 5.5.6, in which multiple weights are constrained to have the same value.\nDiscuss how the standard backpropagation algorithm must be modiﬁed in order to\nensure that such constraints are satisﬁed when evaluating the derivatives of an error\nfunction with respect to the adjustable parameters in the network.\n5.29 (⋆) www Verify the result (5.141).\n5.30 (⋆) Verify the result (5.142).\n5.31 (⋆) Verify the result (5.143).\n5.32 (⋆⋆ ) Show that the derivatives of the mixing coefﬁcients {πk}, deﬁned by (5.146),\nwith respect to the auxiliary parameters {ηj} are given by\n∂πk\n∂ηj\n= δjkπj − πjπk. (5.208)\nHence, by making use of the constraint ∑\nk πk =1 , derive the result (5.147).\n5.33 (⋆) Write down a pair of equations that express the Cartesian coordinates (x1,x2)\nfor the robot arm shown in Figure 5.18 in terms of the joint angles θ1 and θ2 and\nthe lengths L1 and L2 of the links. Assume the origin of the coordinate system is",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 308,
      "page_label": "289"
    }
  },
  {
    "page_content": "the lengths L1 and L2 of the links. Assume the origin of the coordinate system is\ngiven by the attachment point of the lower arm. These equations deﬁne the ‘forward\nkinematics’ of the robot arm.\n5.34 (⋆) www Derive the result (5.155) for the derivative of the error function with\nrespect to the network output activations controlling the mixing coefﬁcients in the\nmixture density network.\n5.35 (⋆) Derive the result (5.156) for the derivative of the error function with respect\nto the network output activations controlling the component means in the mixture\ndensity network.\n5.36 (⋆) Derive the result (5.157) for the derivative of the error function with respect to\nthe network output activations controlling the component variances in the mixture\ndensity network.\n5.37 (⋆) Verify the results (5.158) and (5.160) for the conditional mean and variance of\nthe mixture density network model.\n5.38 (⋆) Using the general result (2.115), derive the predictive distribution (5.172) for\nthe Laplace approximation to the Bayesian neural network model.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 308,
      "page_label": "289"
    }
  },
  {
    "page_content": "290 5. NEURAL NETWORKS\n5.39 (⋆) www Make use of the Laplace approximation result (4.135) to show that the\nevidence function for the hyperparameters α and β in the Bayesian neural network\nmodel can be approximated by (5.175).\n5.40 (⋆) www Outline the modiﬁcations needed to the framework for Bayesian neural\nnetworks, discussed in Section 5.7.3, to handle multiclass problems using networks\nhaving softmax output-unit activation functions.\n5.41 (⋆⋆ ) By following analogous steps to those given in Section 5.7.1 for regression\nnetworks, derive the result (5.183) for the marginal likelihood in the case of a net-\nwork having a cross-entropy error function and logistic-sigmoid output-unit activa-\ntion function.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 309,
      "page_label": "290"
    }
  },
  {
    "page_content": "6\nKernel\nMethods\nIn Chapters 3 and 4, we considered linear parametric models for regression and\nclassiﬁcation in which the form of the mapping y(x,w) from input x to output y\nis governed by a vector w of adaptive parameters. During the learning phase, a\nset of training data is used either to obtain a point estimate of the parameter vector\nor to determine a posterior distribution over this vector. The training data is then\ndiscarded, and predictions for new inputs are based purely on the learned parameter\nvector w. This approach is also used in nonlinear parametric models such as neural\nnetworks.Chapter 5\nHowever, there is a class of pattern recognition techniques, in which the training\ndata points, or a subset of them, are kept and used also during the prediction phase.\nFor instance, the Parzen probability density model comprised a linear combinationSection 2.5.1\nof ‘kernel’ functions each one centred on one of the training data points. Similarly,\nin Section 2.5.2 we introduced a simple technique for classiﬁcation called nearest\nneighbours, which involved assigning to each new test vector the same label as the\n291",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 310,
      "page_label": "291"
    }
  },
  {
    "page_content": "292 6. KERNEL METHODS\nclosest example from the training set. These are examples ofmemory-based methods\nthat involve storing the entire training set in order to make predictions for future data\npoints. They typically require a metric to be deﬁned that measures the similarity of\nany two vectors in input space, and are generally fast to ‘train’ but slow at making\npredictions for test data points.\nMany linear parametric models can be re-cast into an equivalent ‘dual represen-\ntation’ in which the predictions are also based on linear combinations of a kernel\nfunction evaluated at the training data points. As we shall see, for models which are\nbased on a ﬁxed nonlinear feature spacemapping φ(x), the kernel function is given\nby the relation\nk(x,x′)= φ(x)Tφ(x′). (6.1)\nFrom this deﬁnition, we see that the kernel is a symmetric function of its arguments\nso that k(x,x′)= k(x′, x). The kernel concept was introduced into the ﬁeld of pat-\ntern recognition by Aizerman et al. (1964) in the context of the method of potential\nfunctions, so-called because of an analogy with electrostatics. Although neglected\nfor many years, it was re-introduced into machine learning in the context of large-\nmargin classiﬁers by Boseret al. (1992) giving rise to the technique of support\nvector machines. Since then, there has been considerable interest in this topic, bothChapter 7\nin terms of theory and applications. One of the most signiﬁcant developments has",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 311,
      "page_label": "292"
    }
  },
  {
    "page_content": "in terms of theory and applications. One of the most signiﬁcant developments has\nbeen the extension of kernels to handle symbolic objects, thereby greatly expanding\nthe range of problems that can be addressed.\nThe simplest example of a kernel function is obtained by considering the identity\nmapping for the feature space in (6.1) so that φ(x)= x, in which case k(x,x′)=\nxTx′. We shall refer to this as the linear kernel.\nThe concept of a kernel formulated as an inner product in a feature space allows\nus to build interesting extensions of many well-known algorithms by making use of\nthe kernel trick, also known askernel substitution. The general idea is that, if we have\nan algorithm formulated in such a way that the input vectorx enters only in the form\nof scalar products, then we can replace that scalar product with some other choice of\nkernel. For instance, the technique of kernel substitution can be applied to principal\ncomponent analysis in order to develop a nonlinear variant of PCA (Sch¨olkopf et al.,Section 12.3\n1998). Other examples of kernel substitution include nearest-neighbour classiﬁers\nand the kernel Fisher discriminant (Mika et al., 1999; Roth and Steinhage, 2000;\nBaudat and Anouar, 2000).\nThere are numerous forms of kernel functions in common use, and we shall en-\ncounter several examples in this chapter. Many have the property of being a function\nonly of the difference between the arguments, so that k(x,x′)= k(x − x′), which",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 311,
      "page_label": "292"
    }
  },
  {
    "page_content": "only of the difference between the arguments, so that k(x,x′)= k(x − x′), which\nare known as stationary kernels because they are invariant to translations in input\nspace. A further specialization involves homogeneous kernels, also known as ra-\ndial basis functions, which depend only on the magnitude of the distance (typicallySection 6.3\nEuclidean) between the arguments so that k(x,x′)= k(∥x − x′∥).\nFor recent textbooks on kernel methods, see Sch ¨olkopf and Smola (2002), Her-\nbrich (2002), and Shawe-Taylor and Cristianini (2004).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 311,
      "page_label": "292"
    }
  },
  {
    "page_content": "6.1. Dual Representations 293\n6.1. Dual Representations\nMany linear models for regression and classiﬁcation can be reformulated in terms of\na dual representation in which the kernel function arises naturally. This concept will\nplay an important role when we consider support vector machines in the next chapter.\nHere we consider a linear regression model whose parameters are determined by\nminimizing a regularized sum-of-squares error function given by\nJ(w)= 1\n2\nN∑\nn=1\n{\nwTφ(xn) − tn\n}2\n+ λ\n2wTw (6.2)\nwhere λ ⩾ 0. If we set the gradient of J(w) with respect to w equal to zero, we see\nthat the solution for w takes the form of a linear combination of the vectors φ(xn),\nwith coefﬁcients that are functions of w, of the form\nw = −1\nλ\nN∑\nn=1\n{\nwTφ(xn) − tn\n}\nφ(xn)=\nN∑\nn=1\nanφ(xn)= ΦTa (6.3)\nwhere Φ is the design matrix, whose nth row is given by φ(xn)T. Here the vector\na =( a1,...,a N )T, and we have deﬁned\nan = −1\nλ\n{\nwTφ(xn) − tn\n}\n. (6.4)\nInstead of working with the parameter vector w, we can now reformulate the least-\nsquares algorithm in terms of the parameter vector a, giving rise to a dual represen-\ntation. If we substitute w = ΦTa into J(w), we obtain\nJ(a)= 1\n2aTΦΦTΦΦTa − aTΦΦTt + 1\n2tTt + λ\n2aTΦΦTa (6.5)\nwhere t =( t1,...,t N )T. We now deﬁne the Gram matrix K = ΦΦT, which is an\nN × N symmetric matrix with elements\nKnm = φ(xn)Tφ(xm)= k(xn, xm) (6.6)\nwhere we have introduced the kernel function k(x,x′) deﬁned by (6.1). In terms of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 312,
      "page_label": "293"
    }
  },
  {
    "page_content": "Knm = φ(xn)Tφ(xm)= k(xn, xm) (6.6)\nwhere we have introduced the kernel function k(x,x′) deﬁned by (6.1). In terms of\nthe Gram matrix, the sum-of-squares error function can be written as\nJ(a)= 1\n2aTKKa − aTKt + 1\n2tTt + λ\n2aTKa. (6.7)\nSetting the gradient of J(a) with respect to a to zero, we obtain the following solu-\ntion\na =( K + λIN )−1 t. (6.8)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 312,
      "page_label": "293"
    }
  },
  {
    "page_content": "294 6. KERNEL METHODS\nIf we substitute this back into the linear regression model, we obtain the following\nprediction for a new input x\ny(x)= wTφ(x)= aTΦφ(x)= k(x)T (K + λIN )−1 t (6.9)\nwhere we have deﬁned the vector k(x) with elements kn(x)= k(xn, x). Thus we\nsee that the dual formulation allows the solution to the least-squares problem to be\nexpressed entirely in terms of the kernel functionk(x, x′). This is known as a dual\nformulation because, by noting that the solution for a can be expressed as a linear\ncombination of the elements ofφ(x), we recover the original formulation in terms of\nthe parameter vectorw. Note that the prediction atx is given by a linear combinationExercise 6.1\nof the target values from the training set. In fact, we have already obtained this result,\nusing a slightly different notation, in Section 3.3.3.\nIn the dual formulation, we determine the parameter vector a by inverting an\nN ×N matrix, whereas in the original parameter space formulation we had to invert\nan M × M matrix in order to determine w. Because N is typically much larger\nthan M, the dual formulation does not seem to be particularly useful. However, the\nadvantage of the dual formulation, as we shall see, is that it is expressed entirely in\nterms of the kernel functionk(x,x′). We can therefore work directly in terms of\nkernels and avoid the explicit introduction of the feature vector φ(x), which allows\nus implicitly to use feature spaces of high, even inﬁnite, dimensionality.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 313,
      "page_label": "294"
    }
  },
  {
    "page_content": "us implicitly to use feature spaces of high, even inﬁnite, dimensionality.\nThe existence of a dual representation based on the Gram matrix is a property of\nmany linear models, including the perceptron. In Section 6.4, we will develop a dual-Exercise 6.2\nity between probabilistic linear models for regression and the technique of Gaussian\nprocesses. Duality will also play an important role when we discuss support vector\nmachines in Chapter 7.\n6.2. Constructing Kernels\nIn order to exploit kernel substitution, we need to be able to construct valid kernel\nfunctions. One approach is to choose a feature space mappingφ(x) and then use\nthis to ﬁnd the corresponding kernel, as is illustrated in Figure 6.1. Here the kernel\nfunction is deﬁned for a one-dimensional input space by\nk(x, x′)= φ(x)Tφ(x′)=\nM∑\ni=1\nφi(x)φi(x′) (6.10)\nwhere φi(x) are the basis functions.\nAn alternative approach is to construct kernel functions directly. In this case,\nwe must ensure that the function we choose is a valid kernel, in other words that it\ncorresponds to a scalar product in some (perhaps inﬁnite dimensional) feature space.\nAs a simple example, consider a kernel function given by\nk(x,z)=\n(\nxTz\n)2\n. (6.11)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 313,
      "page_label": "294"
    }
  },
  {
    "page_content": "6.2. Constructing Kernels 295\n−1 0 1\n−1\n−0.5\n0\n0.5\n1\n−1 0 1\n0\n0.25\n0.5\n0.75\n1\n−1 0 1\n0\n0.25\n0.5\n0.75\n1\n−1 0 1\n0\n0.02\n0.04\n−1 0 1\n0\n0.02\n0.04\n−1 0 1\n0\n0.02\n0.04\nFigure 6.1 Illustration of the construction of kernel functions starting from a corresponding set of basis func-\ntions. In each column the lower plot shows the kernel function k(x, x′) deﬁned by (6.10) plotted as a function of\nx for x′ =0 , while the upper plot shows the corresponding basis functions given by polynomials (left column),\n‘Gaussians’ (centre column), and logistic sigmoids (right column).\nIf we take the particular case of a two-dimensional input space x =( x1,x2) we\ncan expand out the terms and thereby identify the corresponding nonlinear feature\nmapping\nk(x,z)=\n(\nxTz\n)2\n=( x1z1 + x2z2)2\n= x2\n1z2\n1 +2 x1z1x2z2 + x2\n2z2\n2\n=( x2\n1,\n√\n2x1x2,x2\n2)(z2\n1,\n√\n2z1z2,z 2\n2)T\n= φ(x)Tφ(z). (6.12)\nWe see that the feature mapping takes the form φ(x)=( x2\n1,\n√\n2x1x2,x2\n2)T and\ntherefore comprises all possible second order terms, with a speciﬁc weighting be-\ntween them.\nMore generally, however, we need a simple way to test whether a function con-\nstitutes a valid kernel without having to construct the function φ(x) explicitly. A\nnecessary and sufﬁcient condition for a functionk(x,x′) to be a valid kernel (Shawe-\nTaylor and Cristianini, 2004) is that the Gram matrixK, whose elements are given by\nk(xn, xm), should be positive semideﬁnite for all possible choices of the set {xn}.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 314,
      "page_label": "295"
    }
  },
  {
    "page_content": "k(xn, xm), should be positive semideﬁnite for all possible choices of the set {xn}.\nNote that a positive semideﬁnite matrix is not the same thing as a matrix whose\nelements are nonnegative.Appendix C\nOne powerful technique for constructing new kernels is to build them out of\nsimpler kernels as building blocks. This can be done using the following properties:",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 314,
      "page_label": "295"
    }
  },
  {
    "page_content": "296 6. KERNEL METHODS\nTechniques for Constructing New Kernels.\nGiven valid kernels k1(x,x′) and k2(x,x′), the following new kernels will also\nbe valid:\nk(x,x′)= ck1(x,x′) (6.13)\nk(x,x′)= f(x)k1(x,x′)f(x′) (6.14)\nk(x,x′)= q (k1(x,x′)) (6.15)\nk(x,x′)=e x p ( k1(x,x′)) (6.16)\nk(x,x′)= k1(x, x′)+ k2(x,x′) (6.17)\nk(x,x′)= k1(x, x′)k2(x,x′) (6.18)\nk(x,x′)= k3 (φ(x),φ(x′)) (6.19)\nk(x,x′)= xTAx′ (6.20)\nk(x,x′)= ka(xa, x′\na)+ kb(xb, x′\nb) (6.21)\nk(x,x′)= ka(xa, x′\na)kb(xb, x′\nb) (6.22)\nwhere c> 0 is a constant,f(·) is any function,q(·) is a polynomial with nonneg-\native coefﬁcients, φ(x) is a function from x to RM , k3(·, ·) is a valid kernel in\nRM , A is a symmetric positive semideﬁnite matrix,xa and xb are variables (not\nnecessarily disjoint) with x =( xa, xb), and ka and kb are valid kernel functions\nover their respective spaces.\nEquipped with these properties, we can now embark on the construction of more\ncomplex kernels appropriate to speciﬁc applications. We require that the kernel\nk(x,x′) be symmetric and positive semideﬁnite and that it expresses the appropriate\nform of similarity between x and x′according to the intended application. Here we\nconsider a few common examples of kernel functions. For a more extensive discus-\nsion of ‘kernel engineering’, see Shawe-Taylor and Cristianini (2004).\nWe saw that the simple polynomial kernel k(x, x′)=\n(\nxTx′)2\ncontains only\nterms of degree two. If we consider the slightly generalized kernel k(x,x′)=(\nxTx′+ c\n)2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 315,
      "page_label": "296"
    }
  },
  {
    "page_content": "(\nxTx′)2\ncontains only\nterms of degree two. If we consider the slightly generalized kernel k(x,x′)=(\nxTx′+ c\n)2\nwith c> 0, then the corresponding feature mappingφ(x) contains con-\nstant and linear terms as well as terms of order two. Similarly, k(x,x′)=\n(\nxTx′)M\ncontains all monomials of order M. For instance, if x and x′are two images, then\nthe kernel represents a particular weighted sum of all possible products of M pixels\nin the ﬁrst image with M pixels in the second image. This can similarly be gener-\nalized to include all terms up to degree M by considering k(x,x′)=\n(\nxTx′+ c\n)M\nwith c> 0. Using the results (6.17) and (6.18) for combining kernels we see that\nthese will all be valid kernel functions.\nAnother commonly used kernel takes the form\nk(x,x′) = exp\n(\n−∥x − x′∥2/2σ2)\n(6.23)\nand is often called a ‘Gaussian’ kernel. Note, however, that in this context it is\nnot interpreted as a probability density, and hence the normalization coefﬁcient is",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 315,
      "page_label": "296"
    }
  },
  {
    "page_content": "6.2. Constructing Kernels 297\nomitted. We can see that this is a valid kernel by expanding the square\n∥x − x′∥2 = xTx +( x′)Tx′− 2xTx′ (6.24)\nto give\nk(x,x′)=e x p\n(\n−xTx/2σ2)\nexp\n(\nxTx′/σ2)\nexp\n(\n−(x′)Tx′/2σ2)\n(6.25)\nand then making use of (6.14) and (6.16), together with the validity of the linear\nkernel k(x,x′)= xTx′. Note that the feature vector that corresponds to the Gaussian\nkernel has inﬁnite dimensionality.Exercise 6.11\nThe Gaussian kernel is not restricted to the use of Euclidean distance. If we use\nkernel substitution in (6.24) to replace xTx′with a nonlinear kernel κ(x, x′),w e\nobtain\nk(x,x′) = exp\n{\n− 1\n2σ2 (κ(x, x)+ κ(x′, x′) − 2κ(x, x′))\n}\n. (6.26)\nAn important contribution to arise from the kernel viewpoint has been the exten-\nsion to inputs that are symbolic, rather than simply vectors of real numbers. Kernel\nfunctions can be deﬁned over objects as diverse as graphs, sets, strings, and text doc-\numents. Consider, for instance, a ﬁxed set and deﬁne a nonvectorial space consisting\nof all possible subsets of this set. If A1 and A2 are two such subsets then one simple\nchoice of kernel would be\nk(A1,A 2)=2 |A1∩A2| (6.27)\nwhere A1 ∩ A2 denotes the intersection of sets A1 and A2, and |A| denotes the\nnumber of subsets in A. This is a valid kernel function because it can be shown to\ncorrespond to an inner product in a feature space.Exercise 6.12\nOne powerful approach to the construction of kernels starts from a probabilistic",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 316,
      "page_label": "297"
    }
  },
  {
    "page_content": "correspond to an inner product in a feature space.Exercise 6.12\nOne powerful approach to the construction of kernels starts from a probabilistic\ngenerative model (Haussler, 1999), which allows us to apply generative models in a\ndiscriminative setting. Generative models can deal naturally with missing data and\nin the case of hidden Markov models can handle sequences of varying length. By\ncontrast, discriminative models generally give better performance on discriminative\ntasks than generative models. It is therefore of some interest to combine these two\napproaches (Lasserreet al., 2006). One way to combine them is to use a generative\nmodel to deﬁne a kernel, and then use this kernel in a discriminative approach.\nGiven a generative model p(x) we can deﬁne a kernel by\nk(x,x′)= p(x)p(x′). (6.28)\nThis is clearly a valid kernel function because we can interpret it as an inner product\nin the one-dimensional feature space deﬁned by the mapping p(x). It says that two\ninputs x and x′are similar if they both have high probabilities. We can use (6.13) and\n(6.17) to extend this class of kernels by considering sums over products of different\nprobability distributions, with positive weighting coefﬁcients p(i), of the form\nk(x,x′)=\n∑\ni\np(x|i)p(x′|i)p(i). (6.29)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 316,
      "page_label": "297"
    }
  },
  {
    "page_content": "298 6. KERNEL METHODS\nThis is equivalent, up to an overall multiplicative constant, to a mixture distribution\nin which the components factorize, with the index i playing the role of a ‘latent’\nvariable. Two inputs x and x′will give a large value for the kernel function, andSection 9.2\nhence appear similar, if they have signiﬁcant probability under a range of different\ncomponents. Taking the limit of an inﬁnite sum, we can also consider kernels of the\nform\nk(x,x′)=\n∫\np(x|z)p(x′|z)p(z)d z (6.30)\nwhere z is a continuous latent variable.\nNow suppose that our data consists of ordered sequences of length L so that\nan observation is given by X = {x1,..., xL}. A popular generative model for\nsequences is the hidden Markov model, which expresses the distribution p(X) as aSection 13.2\nmarginalization over a corresponding sequence of hidden states Z = {z1,..., zL}.\nWe can use this approach to deﬁne a kernel function measuring the similarity of two\nsequencesX and X′by extending the mixture representation (6.29) to give\nk(X, X′)=\n∑\nZ\np(X|Z)p(X′|Z)p(Z) (6.31)\nso that both observed sequences are generated by the same hidden sequence Z. This\nmodel can easily be extended to allow sequences of differing length to be compared.\nAn alternative technique for using generative models to deﬁne kernel functions\nis known as the Fisher kernel(Jaakkola and Haussler, 1999). Consider a parametric\ngenerative model p(x|θ) where θ denotes the vector of parameters. The goal is to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 317,
      "page_label": "298"
    }
  },
  {
    "page_content": "generative model p(x|θ) where θ denotes the vector of parameters. The goal is to\nﬁnd a kernel that measures the similarity of two input vectorsx and x′induced by the\ngenerative model. Jaakkola and Haussler (1999) consider the gradient with respect\nto θ, which deﬁnes a vector in a ‘feature’ space having the same dimensionality as\nθ. In particular, they consider the Fisher score\ng(θ, x)= ∇θ lnp(x|θ) (6.32)\nfrom which the Fisher kernel is deﬁned by\nk(x,x′)= g(θ, x)TF−1g(θ, x′). (6.33)\nHere F is the Fisher information matrix, given by\nF = Ex\n[\ng(θ, x)g(θ, x)T]\n(6.34)\nwhere the expectation is with respect to x under the distribution p(x|θ). This can\nbe motivated from the perspective of information geometry (Amari, 1998), which\nconsiders the differential geometry of the space of model parameters. Here we sim-\nply note that the presence of the Fisher information matrix causes this kernel to be\ninvariant under a nonlinear re-parameterization of the density modelθ → ψ(θ).Exercise 6.13\nIn practice, it is often infeasible to evaluate the Fisher information matrix. One\napproach is simply to replace the expectation in the deﬁnition of the Fisher informa-\ntion with the sample average, giving\nF ≃ 1\nN\nN∑\nn=1\ng(θ, xn)g(θ, xn)T. (6.35)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 317,
      "page_label": "298"
    }
  },
  {
    "page_content": "6.3. Radial Basis Function Networks 299\nThis is the covariance matrix of the Fisher scores, and so the Fisher kernel corre-\nsponds to a whitening of these scores. More simply, we can just omit the FisherSection 12.1.3\ninformation matrix altogether and use the noninvariant kernel\nk(x,x′)= g(θ, x)Tg(θ, x′). (6.36)\nAn application of Fisher kernels to document retrieval is given by Hofmann (2000).\nA ﬁnal example of a kernel function is the sigmoidal kernel given by\nk(x,x′) = tanh\n(\naxTx′+ b\n)\n(6.37)\nwhose Gram matrix in general is not positive semideﬁnite. This form of kernel\nhas, however, been used in practice (Vapnik, 1995), possibly because it gives kernel\nexpansions such as the support vector machine a superﬁcial resemblance to neural\nnetwork models. As we shall see, in the limit of an inﬁnite number of basis functions,\na Bayesian neural network with an appropriate prior reduces to a Gaussian process,\nthereby providing a deeper link between neural networks and kernel methods.Section 6.4.7\n6.3. Radial Basis Function Networks\nIn Chapter 3, we discussed regression models based on linear combinations of ﬁxed\nbasis functions, although we did not discuss in detail what form those basis functions\nmight take. One choice that has been widely used is that of radial basis functions,\nwhich have the property that each basis function depends only on the radial distance\n(typically Euclidean) from a centreµj, so that φj(x)= h(∥x − µj∥).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 318,
      "page_label": "299"
    }
  },
  {
    "page_content": "(typically Euclidean) from a centreµj, so that φj(x)= h(∥x − µj∥).\nHistorically, radial basis functions were introduced for the purpose of exact func-\ntion interpolation (Powell, 1987). Given a set of input vectors {x1,..., xN } along\nwith corresponding target values {t1,...,t N }, the goal is to ﬁnd a smooth function\nf(x) that ﬁts every target value exactly, so that f(xn)= tn for n =1 ,...,N . This\nis achieved by expressing f(x) as a linear combination of radial basis functions, one\ncentred on every data point\nf(x)=\nN∑\nn=1\nwnh(∥x − xn∥). (6.38)\nThe values of the coefﬁcients {wn} are found by least squares, and because there\nare the same number of coefﬁcients as there are constraints, the result is a function\nthat ﬁts every target value exactly. In pattern recognition applications, however, the\ntarget values are generally noisy, and exact interpolation is undesirable because this\ncorresponds to an over-ﬁtted solution.\nExpansions in radial basis functions also arise from regularization theory (Pog-\ngio and Girosi, 1990; Bishop, 1995a). For a sum-of-squares error function with a\nregularizer deﬁned in terms of a differential operator, the optimal solution is given\nby an expansion in theGreen’s functionsof the operator (which are analogous to the\neigenvectors of a discrete matrix), again with one basis function centred on each data",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 318,
      "page_label": "299"
    }
  },
  {
    "page_content": "300 6. KERNEL METHODS\npoint. If the differential operator is isotropic then the Green’s functions depend only\non the radial distance from the corresponding data point. Due to the presence of the\nregularizer, the solution no longer interpolates the training data exactly.\nAnother motivation for radial basis functions comes from a consideration of\nthe interpolation problem when the input (rather than the target) variables are noisy\n(Webb, 1994; Bishop, 1995a). If the noise on the input variablex is described\nby a variable ξ having a distribution ν(ξ), then the sum-of-squares error function\nbecomes\nE = 1\n2\nN∑\nn=1\n∫\n{y(xn + ξ) − tn}2 ν(ξ)d ξ. (6.39)\nUsing the calculus of variations, we can optimize with respect to the function f(x)Appendix D\nto giveExercise 6.17\ny(xn)=\nN∑\nn=1\ntnh(x − xn) (6.40)\nwhere the basis functions are given by\nh(x − xn)= ν(x − xn)\nN∑\nn=1\nν(x − xn)\n. (6.41)\nWe see that there is one basis function centred on every data point. This is known as\nthe Nadaraya-Watson model and will be derived again from a different perspective\nin Section 6.3.1. If the noise distribution ν(ξ) is isotropic, so that it is a function\nonly of ∥ξ∥, then the basis functions will be radial.\nNote that the basis functions (6.41) are normalized, so that ∑\nn h(x − xn)=1\nfor any value ofx. The effect of such normalization is shown in Figure 6.2. Normal-\nization is sometimes used in practice as it avoids having regions of input space where",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 319,
      "page_label": "300"
    }
  },
  {
    "page_content": "ization is sometimes used in practice as it avoids having regions of input space where\nall of the basis functions take small values, which would necessarily lead to predic-\ntions in such regions that are either small or controlled purely by the bias parameter.\nAnother situation in which expansions in normalized radial basis functions arise\nis in the application of kernel density estimation to the problem of regression, as we\nshall discuss in Section 6.3.1.\nBecause there is one basis function associated with every data point, the corre-\nsponding model can be computationally costly to evaluate when making predictions\nfor new data points. Models have therefore been proposed (Broomhead and Lowe,\n1988; Moody and Darken, 1989; Poggio and Girosi, 1990), which retain the expan-\nsion in radial basis functions but where the numberM of basis functions is smaller\nthan the number N of data points. Typically, the number of basis functions, and the\nlocations µi of their centres, are determined based on the input data{xn} alone. The\nbasis functions are then kept ﬁxed and the coefﬁcients {wi} are determined by least\nsquares by solving the usual set of linear equations, as discussed in Section 3.1.1.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 319,
      "page_label": "300"
    }
  },
  {
    "page_content": "6.3. Radial Basis Function Networks 301\n−1 −0.5 0 0.5 1\n0\n0.2\n0.4\n0.6\n0.8\n1\n−1 −0.5 0 0.5 1\n0\n0.2\n0.4\n0.6\n0.8\n1\nFigure 6.2 Plot of a set of Gaussian basis functions on the left, together with the corresponding normalized\nbasis functions on the right.\nOne of the simplest ways of choosing basis function centres is to use a randomly\nchosen subset of the data points. A more systematic approach is called orthogonal\nleast squares (Chen et al., 1991). This is a sequential selection process in which at\neach step the next data point to be chosen as a basis function centre corresponds to\nthe one that gives the greatest reduction in the sum-of-squares error. Values for the\nexpansion coefﬁcients are determined as part of the algorithm. Clustering algorithms\nsuch as K-means have also been used, which give a set of basis function centres thatSection 9.1\nno longer coincide with training data points.\n6.3.1 Nadaraya-Watson model\nIn Section 3.3.3, we saw that the prediction of a linear regression model for a\nnew input x takes the form of a linear combination of the training set target values\nwith coefﬁcients given by the ‘equivalent kernel’ (3.62) where the equivalent kernel\nsatisﬁes the summation constraint (3.64).\nWe can motivate the kernel regression model (3.61) from a different perspective,\nstarting with kernel density estimation. Suppose we have a training set {xn,t n} and\nwe use a Parzen density estimator to model the joint distribution p(x,t ), so thatSection 2.5.1\np(x,t )= 1\nN\nN∑\nn=1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 320,
      "page_label": "301"
    }
  },
  {
    "page_content": "we use a Parzen density estimator to model the joint distribution p(x,t ), so thatSection 2.5.1\np(x,t )= 1\nN\nN∑\nn=1\nf(x − xn,t − tn) (6.42)\nwhere f(x,t ) is the component density function, and there is one such component\ncentred on each data point. We now ﬁnd an expression for the regression function\ny(x), corresponding to the conditional average of the target variable conditioned on",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 320,
      "page_label": "301"
    }
  },
  {
    "page_content": "302 6. KERNEL METHODS\nthe input variable, which is given by\ny(x)= E[t|x]=\n∫ ∞\n−∞\ntp(t|x)d t\n=\n∫\ntp(x,t )d t\n∫\np(x,t )d t\n=\n∑\nn\n∫\ntf(x − xn,t − tn)d t\n∑\nm\n∫\nf(x − xm,t − tm)d t\n. (6.43)\nWe now assume for simplicity that the component density functions have zero mean\nso that ∫ ∞\n−∞\nf(x,t )t dt =0 (6.44)\nfor all values of x. Using a simple change of variable, we then obtain\ny(x)=\n∑\nn\ng(x − xn)tn\n∑\nm\ng(x − xm)\n=\n∑\nn\nk(x,xn)tn (6.45)\nwhere n, m=1 ,...,N and the kernel function k(x,xn) is given by\nk(x,xn)= g(x − xn)∑\nm\ng(x − xm)\n(6.46)\nand we have deﬁned\ng(x)=\n∫ ∞\n−∞\nf(x,t )d t. (6.47)\nThe result (6.45) is known as the Nadaraya-Watson model, or kernel regression\n(Nadaraya, 1964; Watson, 1964). For a localized kernel function, it has the prop-\nerty of giving more weight to the data points xn that are close to x. Note that the\nkernel (6.46) satisﬁes the summation constraint\nN∑\nn=1\nk(x,xn)=1 .",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 321,
      "page_label": "302"
    }
  },
  {
    "page_content": "6.4. Gaussian Processes 303\nFigure 6.3 Illustration of the Nadaraya-Watson kernel\nregression model using isotropic Gaussian kernels, for the\nsinusoidal data set. The original sine function is shown\nby the green curve, the data points are shown in blue,\nand each is the centre of an isotropic Gaussian kernel.\nThe resulting regression function, given by the condi-\ntional mean, is shown by the red line, along with the two-\nstandard-deviation region for the conditional distribution\np(t|x) shown by the red shading. The blue ellipse around\neach data point shows one standard deviation contour for\nthe corresponding kernel. These appear noncircular due\nto the different scales on the horizontal and vertical axes.\n0 0.2 0.4 0.6 0.8 1\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\nIn fact, this model deﬁnes not only a conditional expectation but also a full\nconditional distribution given by\np(t|x)= p(t, x)∫\np(t, x)dt\n=\n∑\nn\nf(x − xn,t − tn)\n∑\nm\n∫\nf(x − xm,t − tm)d t\n(6.48)\nfrom which other expectations can be evaluated.\nAs an illustration we consider the case of a single input variable x in which\nf(x, t) is given by a zero-mean isotropic Gaussian over the variable z =( x, t) with\nvariance σ2. The corresponding conditional distribution (6.48) is given by a Gaus-\nsian mixture, and is shown, together with the conditional mean, for the sinusoidalExercise 6.18\nsynthetic data set in Figure 6.3.\nAn obvious extension of this model is to allow for more ﬂexible forms of Gaus-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 322,
      "page_label": "303"
    }
  },
  {
    "page_content": "synthetic data set in Figure 6.3.\nAn obvious extension of this model is to allow for more ﬂexible forms of Gaus-\nsian components, for instance having different variance parameters for the input and\ntarget variables. More generally, we could model the joint distribution p(t, x) using\na Gaussian mixture model, trained using techniques discussed in Chapter 9 (Ghahra-\nmani and Jordan, 1994), and then ﬁnd the corresponding conditional distribution\np(t|x). In this latter case we no longer have a representation in terms of kernel func-\ntions evaluated at the training set data points. However, the number of components\nin the mixture model can be smaller than the number of training set points, resulting\nin a model that is faster to evaluate for test data points. We have thereby accepted an\nincreased computational cost during the training phase in order to have a model that\nis faster at making predictions.\n6.4. Gaussian Processes\nIn Section 6.1, we introduced kernels by applying the concept of duality to a non-\nprobabilistic model for regression. Here we extend the role of kernels to probabilis-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 322,
      "page_label": "303"
    }
  },
  {
    "page_content": "304 6. KERNEL METHODS\ntic discriminative models, leading to the framework of Gaussian processes. We shall\nthereby see how kernels arise naturally in a Bayesian setting.\nIn Chapter 3, we considered linear regression models of the form y(x, w)=\nwTφ(x) in which w is a vector of parameters andφ(x) is a vector of ﬁxed nonlinear\nbasis functions that depend on the input vectorx. We showed that a prior distribution\nover w induced a corresponding prior distribution over functions y(x,w). Given a\ntraining data set, we then evaluated the posterior distribution over w and thereby\nobtained the corresponding posterior distribution over regression functions, which\nin turn (with the addition of noise) implies a predictive distributionp(t|x) for new\ninput vectors x.\nIn the Gaussian process viewpoint, we dispense with the parametric model and\ninstead deﬁne a prior probability distribution over functions directly. At ﬁrst sight, it\nmight seem difﬁcult to work with a distribution over the uncountably inﬁnite space of\nfunctions. However, as we shall see, for a ﬁnite training set we only need to consider\nthe values of the function at the discrete set of input values xn corresponding to the\ntraining set and test set data points, and so in practice we can work in a ﬁnite space.\nModels equivalent to Gaussian processes have been widely studied in many dif-\nferent ﬁelds. For instance, in the geostatistics literature Gaussian process regression",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 323,
      "page_label": "304"
    }
  },
  {
    "page_content": "ferent ﬁelds. For instance, in the geostatistics literature Gaussian process regression\nis known as kriging (Cressie, 1993). Similarly, ARMA (autoregressive moving aver-\nage) models, Kalman ﬁlters, and radial basis function networks can all be viewed as\nforms of Gaussian process models. Reviews of Gaussian processes from a machine\nlearning perspective can be found in MacKay (1998), Williams (1999), and MacKay\n(2003), and a comparison of Gaussian process models with alternative approaches is\ngiven in Rasmussen (1996). See also Rasmussen and Williams (2006) for a recent\ntextbook on Gaussian processes.\n6.4.1 Linear regression revisited\nIn order to motivate the Gaussian process viewpoint, let us return to the linear\nregression example and re-derive the predictive distribution by working in terms\nof distributions over functions y(x,w). This will provide a speciﬁc example of a\nGaussian process.\nConsider a model deﬁned in terms of a linear combination of M ﬁxed basis\nfunctions given by the elements of the vector φ(x) so that\ny(x)= wTφ(x) (6.49)\nwhere x is the input vector andw is the M-dimensional weight vector. Now consider\na prior distribution over w given by an isotropic Gaussian of the form\np(w)= N(w|0,α −1I) (6.50)\ngoverned by the hyperparameterα, which represents the precision (inverse variance)\nof the distribution. For any given value of w, the deﬁnition (6.49) deﬁnes a partic-\nular function of x. The probability distribution over w deﬁned by (6.50) therefore",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 323,
      "page_label": "304"
    }
  },
  {
    "page_content": "ular function of x. The probability distribution over w deﬁned by (6.50) therefore\ninduces a probability distribution over functions y(x). In practice, we wish to eval-\nuate this function at speciﬁc values of x, for example at the training data points",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 323,
      "page_label": "304"
    }
  },
  {
    "page_content": "6.4. Gaussian Processes 305\nx1,..., xN . We are therefore interested in the joint distribution of the function val-\nues y(x1),...,y (xN ), which we denote by the vector y with elements yn = y(xn)\nfor n =1 ,...,N . From (6.49), this vector is given by\ny = Φw (6.51)\nwhere Φ is the design matrix with elements Φnk = φk(xn). We can ﬁnd the proba-\nbility distribution ofy as follows. First of all we note thaty is a linear combination of\nGaussian distributed variables given by the elements of w and hence is itself Gaus-\nsian. We therefore need only to ﬁnd its mean and covariance, which are given fromExercise 2.31\n(6.50) by\nE[y]= ΦE[w]= 0 (6.52)\ncov[y]= E\n[\nyyT]\n= ΦE\n[\nwwT]\nΦT = 1\nαΦΦT = K (6.53)\nwhere K is the Gram matrix with elements\nKnm = k(xn, xm)= 1\nαφ(xn)Tφ(xm) (6.54)\nand k(x,x′) is the kernel function.\nThis model provides us with a particular example of a Gaussian process. In gen-\neral, a Gaussian process is deﬁned as a probability distribution over functions y(x)\nsuch that the set of values of y(x) evaluated at an arbitrary set of points x1,..., xN\njointly have a Gaussian distribution. In cases where the input vector x is two di-\nmensional, this may also be known as a Gaussian random ﬁeld. More generally, a\nstochastic process y(x) is speciﬁed by giving the joint probability distribution for\nany ﬁnite set of values y(x1),...,y (xN ) in a consistent manner.\nA key point about Gaussian stochastic processes is that the joint distribution",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 324,
      "page_label": "305"
    }
  },
  {
    "page_content": "any ﬁnite set of values y(x1),...,y (xN ) in a consistent manner.\nA key point about Gaussian stochastic processes is that the joint distribution\nover N variables y1,...,y N is speciﬁed completely by the second-order statistics,\nnamely the mean and the covariance. In most applications, we will not have any\nprior knowledge about the mean ofy(x) and so by symmetry we take it to be zero.\nThis is equivalent to choosing the mean of the prior over weight values p(w|α) to\nbe zero in the basis function viewpoint. The speciﬁcation of the Gaussian process is\nthen completed by giving the covariance of y(x) evaluated at any two values of x,\nwhich is given by the kernel function\nE [y(xn)y(xm)] = k(xn, xm). (6.55)\nFor the speciﬁc case of a Gaussian process deﬁned by the linear regression model\n(6.49) with a weight prior (6.50), the kernel function is given by (6.54).\nWe can also deﬁne the kernel function directly, rather than indirectly through a\nchoice of basis function. Figure 6.4 shows samples of functions drawn from Gaus-\nsian processes for two different choices of kernel function. The ﬁrst of these is a\n‘Gaussian’ kernel of the form (6.23), and the second is the exponential kernel given\nby\nk(x, x′)=e x p(−θ |x − x′|) (6.56)\nwhich corresponds to the Ornstein-Uhlenbeck processoriginally introduced by Uh-\nlenbeck and Ornstein (1930) to describe Brownian motion.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 324,
      "page_label": "305"
    }
  },
  {
    "page_content": "306 6. KERNEL METHODS\nFigure 6.4 Samples from Gaus-\nsian processes for a ‘Gaussian’ ker-\nnel (left) and an exponential kernel\n(right).\n−1 −0.5 0 0.5 1\n−3\n−1.5\n0\n1.5\n3\n−1 −0.5 0 0.5 1\n−3\n−1.5\n0\n1.5\n3\n6.4.2 Gaussian processes for regression\nIn order to apply Gaussian process models to the problem of regression, we need\nto take account of the noise on the observed target values, which are given by\ntn = yn + ϵn (6.57)\nwhere yn = y(xn), and ϵn is a random noise variable whose value is chosen inde-\npendently for each observation n. Here we shall consider noise processes that have\na Gaussian distribution, so that\np(tn|yn)= N(tn|yn,β −1) (6.58)\nwhere β is a hyperparameter representing the precision of the noise. Because the\nnoise is independent for each data point, the joint distribution of the target values\nt =( t1,...,t N )T conditioned on the values of y =( y1,...,y N )T is given by an\nisotropic Gaussian of the form\np(t|y)= N(t|y,β −1IN ) (6.59)\nwhere IN denotes the N ×N unit matrix. From the deﬁnition of a Gaussian process,\nthe marginal distribution p(y) is given by a Gaussian whose mean is zero and whose\ncovariance is deﬁned by a Gram matrix K so that\np(y)= N(y|0,K). (6.60)\nThe kernel function that determines K is typically chosen to express the property\nthat, for points xn and xm that are similar, the corresponding values y(xn) and\ny(xm) will be more strongly correlated than for dissimilar points. Here the notion\nof similarity will depend on the application.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 325,
      "page_label": "306"
    }
  },
  {
    "page_content": "y(xm) will be more strongly correlated than for dissimilar points. Here the notion\nof similarity will depend on the application.\nIn order to ﬁnd the marginal distribution p(t), conditioned on the input values\nx1,..., xN , we need to integrate over y. This can be done by making use of the\nresults from Section 2.3.3 for the linear-Gaussian model. Using (2.115), we see that\nthe marginal distribution of t is given by\np(t)=\n∫\np(t|y)p(y)dy = N(t|0,C) (6.61)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 325,
      "page_label": "306"
    }
  },
  {
    "page_content": "6.4. Gaussian Processes 307\nwhere the covariance matrix C has elements\nC(xn, xm)= k(xn, xm)+ β−1δnm. (6.62)\nThis result reﬂects the fact that the two Gaussian sources of randomness, namely\nthat associated with y(x) and that associated with ϵ, are independent and so their\ncovariances simply add.\nOne widely used kernel function for Gaussian process regression is given by the\nexponential of a quadratic form, with the addition of constant and linear terms to\ngive\nk(xn, xm)= θ0 exp\n{\n−θ1\n2 ∥xn − xm∥2\n}\n+ θ2 + θ3xT\nnxm. (6.63)\nNote that the term involving θ3 corresponds to a parametric model that is a linear\nfunction of the input variables. Samples from this prior are plotted for various values\nof the parameters θ0,...,θ 3 in Figure 6.5, and Figure 6.6 shows a set of points sam-\npled from the joint distribution (6.60) along with the corresponding values deﬁned\nby (6.61).\nSo far, we have used the Gaussian process viewpoint to build a model of the\njoint distribution over sets of data points. Our goal in regression, however, is to\nmake predictions of the target variables for new inputs, given a set of training data.\nLet us suppose that tN =( t1,...,t N )T, corresponding to input values x1,..., xN ,\ncomprise the observed training set, and our goal is to predict the target variabletN+1\nfor a new input vector xN+1. This requires that we evaluate the predictive distri-\nbution p(tN+1|tN ). Note that this distribution is conditioned also on the variables",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 326,
      "page_label": "307"
    }
  },
  {
    "page_content": "bution p(tN+1|tN ). Note that this distribution is conditioned also on the variables\nx1,..., xN and xN+1. However, to keep the notation simple we will not show these\nconditioning variables explicitly.\nTo ﬁnd the conditional distribution p(tN+1|t), we begin by writing down the\njoint distribution p(tN+1), where tN+1 denotes the vector (t1,...,t N ,t N+1)T.W e\nthen apply the results from Section 2.3.1 to obtain the required conditional distribu-\ntion, as illustrated in Figure 6.7.\nFrom (6.61), the joint distribution over t1,...,t N+1 will be given by\np(tN+1)= N(tN+1|0,CN+1) (6.64)\nwhere CN+1 is an (N +1 ) × (N +1 ) covariance matrix with elements given by\n(6.62). Because this joint distribution is Gaussian, we can apply the results from\nSection 2.3.1 to ﬁnd the conditional Gaussian distribution. To do this, we partition\nthe covariance matrix as follows\nCN+1 =\n(\nCN k\nkT c\n)\n(6.65)\nwhere CN is the N ×N covariance matrix with elements given by (6.62) forn, m=\n1,...,N , the vector k has elements k(xn, xN+1) for n =1 ,...,N , and the scalar",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 326,
      "page_label": "307"
    }
  },
  {
    "page_content": "308 6. KERNEL METHODS\n(1.00, 4.00, 0.00, 0.00)\n−1 −0.5 0 0.5 1\n−3\n−1.5\n0\n1.5\n3\n(9.00, 4.00, 0.00, 0.00)\n−1 −0.5 0 0.5 1\n−9\n−4.5\n0\n4.5\n9\n(1.00, 64.00, 0.00, 0.00)\n−1 −0.5 0 0.5 1\n−3\n−1.5\n0\n1.5\n3\n(1.00, 0.25, 0.00, 0.00)\n−1 −0.5 0 0.5 1\n−3\n−1.5\n0\n1.5\n3\n(1.00, 4.00, 10.00, 0.00)\n−1 −0.5 0 0.5 1\n−9\n−4.5\n0\n4.5\n9\n(1.00, 4.00, 0.00, 5.00)\n−1 −0.5 0 0.5 1\n−4\n−2\n0\n2\n4\nFigure 6.5 Samples from a Gaussian process prior deﬁned by the covariance function (6.63). The title above\neach plot denotes (θ0,θ 1,θ 2,θ 3).\nc = k(xN+1, xN+1)+ β−1. Using the results (2.81) and (2.82), we see that the con-\nditional distribution p(tN+1|t) is a Gaussian distribution with mean and covariance\ngiven by\nm(xN+1)= kTC−1\nN t (6.66)\nσ2(xN+1)= c − kTC−1\nN k. (6.67)\nThese are the key results that deﬁne Gaussian process regression. Because the vector\nk is a function of the test point input valuexN+1, we see that the predictive distribu-\ntion is a Gaussian whose mean and variance both depend on xN+1. An example of\nGaussian process regression is shown in Figure 6.8.\nThe only restriction on the kernel function is that the covariance matrix given by\n(6.62) must be positive deﬁnite. If λi is an eigenvalue of K, then the corresponding\neigenvalue of C will be λi + β−1. It is therefore sufﬁcient that the kernel matrix\nk(xn, xm) be positive semideﬁnite for any pair of pointsxn and xm, so that λi ⩾ 0,\nbecause any eigenvalue λi that is zero will still give rise to a positive eigenvalue",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 327,
      "page_label": "308"
    }
  },
  {
    "page_content": "because any eigenvalue λi that is zero will still give rise to a positive eigenvalue\nfor C because β> 0. This is the same restriction on the kernel function discussed\nearlier, and so we can again exploit all of the techniques in Section 6.2 to construct",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 327,
      "page_label": "308"
    }
  },
  {
    "page_content": "6.4. Gaussian Processes 309\nFigure 6.6 Illustration of the sampling of data\npoints {tn} from a Gaussian process.\nThe blue curve shows a sample func-\ntion from the Gaussian process prior\nover functions, and the red points\nshow the values of yn obtained by\nevaluating the function at a set of in-\nput values {xn}. The correspond-\ning values of {tn}, shown in green,\nare obtained by adding independent\nGaussian noise to each of the {yn}.\nx\nt\n−1 0 1\n−3\n0\n3\nsuitable kernels.\nNote that the mean (6.66) of the predictive distribution can be written, as a func-\ntion of xN+1, in the form\nm(xN+1)=\nN∑\nn=1\nank(xn, xN+1) (6.68)\nwhere an is the nth component of C−1\nN t. Thus, if the kernel function k(xn, xm)\ndepends only on the distance ∥xn − xm∥, then we obtain an expansion in radial\nbasis functions.\nThe results (6.66) and (6.67) deﬁne the predictive distribution for Gaussian pro-\ncess regression with an arbitrary kernel functionk(xn, xm). In the particular case in\nwhich the kernel functionk(x,x′) is deﬁned in terms of a ﬁnite set of basis functions,\nwe can derive the results obtained previously in Section 3.3.2 for linear regression\nstarting from the Gaussian process viewpoint.Exercise 6.21\nFor such models, we can therefore obtain the predictive distribution either by\ntaking a parameter space viewpoint and using the linear regression result or by taking\na function space viewpoint and using the Gaussian process result.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 328,
      "page_label": "309"
    }
  },
  {
    "page_content": "a function space viewpoint and using the Gaussian process result.\nThe central computational operation in using Gaussian processes will involve\nthe inversion of a matrix of sizeN ×N, for which standard methods require O(N3)\ncomputations. By contrast, in the basis function model we have to invert a matrix\nSN of size M × M, which has O(M3) computational complexity. Note that for\nboth viewpoints, the matrix inversion must be performed once for the given training\nset. For each new test point, both methods require a vector-matrix multiply, which\nhas cost O(N2) in the Gaussian process case and O(M2) for the linear basis func-\ntion model. If the number M of basis functions is smaller than the number N of\ndata points, it will be computationally more efﬁcient to work in the basis function",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 328,
      "page_label": "309"
    }
  },
  {
    "page_content": "310 6. KERNEL METHODS\nFigure 6.7 Illustration of the mechanism of\nGaussian process regression for\nthe case of one training point and\none test point, in which the red el-\nlipses show contours of the joint dis-\ntribution p(t1,t 2). Here t1 is the\ntraining data point, and condition-\ni n go nt h ev a l u eo ft1, correspond-\ning to the vertical blue line, we ob-\ntain p(t2|t1) shown as a function of\nt2 by the green curve. t1\nt2\nm(x2)\n−1 0 1\n−1\n0\n1\nframework. However, an advantage of a Gaussian processes viewpoint is that we\ncan consider covariance functions that can only be expressed in terms of an inﬁnite\nnumber of basis functions.\nFor large training data sets, however, the direct application of Gaussian process\nmethods can become infeasible, and so a range of approximation schemes have been\ndeveloped that have better scaling with training set size than the exact approach\n(Gibbs, 1997; Tresp, 2001; Smola and Bartlett, 2001; Williams and Seeger, 2001;\nCsat´o and Opper, 2002; Seeger et al., 2003). Practical issues in the application of\nGaussian processes are discussed in Bishop and Nabney (2008).\nWe have introduced Gaussian process regression for the case of a single tar-\nget variable. The extension of this formalism to multiple target variables, known\nas co-kriging (Cressie, 1993), is straightforward. Various other extensions of Gaus-Exercise 6.23\nFigure 6.8 Illustration of Gaussian process re-\ngression applied to the sinusoidal\ndata set in Figure A.6 in which the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 329,
      "page_label": "310"
    }
  },
  {
    "page_content": "Figure 6.8 Illustration of Gaussian process re-\ngression applied to the sinusoidal\ndata set in Figure A.6 in which the\nthree right-most data points have\nbeen omitted. The green curve\nshows the sinusoidal function from\nwhich the data points, shown in\nblue, are obtained by sampling and\naddition of Gaussian noise. The\nred line shows the mean of the\nGaussian process predictive distri-\nbution, and the shaded region cor-\nresponds to plus and minus two\nstandard deviations. Notice how\nthe uncertainty increases in the re-\ngion to the right of the data points.\n0 0.2 0.4 0.6 0.8 1\n−1\n−0.5\n0\n0.5\n1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 329,
      "page_label": "310"
    }
  },
  {
    "page_content": "6.4. Gaussian Processes 311\nsian process regression have also been considered, for purposes such as modelling\nthe distribution over low-dimensional manifolds for unsupervised learning (Bishop\net al., 1998a) and the solution of stochastic differential equations (Graepel, 2003).\n6.4.3 Learning the hyperparameters\nThe predictions of a Gaussian process model will depend, in part, on the choice\nof covariance function. In practice, rather than ﬁxing the covariance function, we\nmay prefer to use a parametric family of functions and then infer the parameter\nvalues from the data. These parameters govern such things as the length scale of the\ncorrelations and the precision of the noise and correspond to the hyperparameters in\na standard parametric model.\nTechniques for learning the hyperparameters are based on the evaluation of the\nlikelihood function p(t|θ) where θ denotes the hyperparameters of the Gaussian pro-\ncess model. The simplest approach is to make a point estimate of θ by maximizing\nthe log likelihood function. Because θ represents a set of hyperparameters for the\nregression problem, this can be viewed as analogous to the type 2 maximum like-\nlihood procedure for linear regression models. Maximization of the log likelihoodSection 3.5\ncan be done using efﬁcient gradient-based optimization algorithms such as conjugate\ngradients (Fletcher, 1987; Nocedal and Wright, 1999; Bishop and Nabney, 2008).\nThe log likelihood function for a Gaussian process regression model is easily",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 330,
      "page_label": "311"
    }
  },
  {
    "page_content": "The log likelihood function for a Gaussian process regression model is easily\nevaluated using the standard form for a multivariate Gaussian distribution, giving\nlnp(t|θ)= −1\n2 ln|CN |− 1\n2tTC−1\nN t − N\n2 ln(2π). (6.69)\nFor nonlinear optimization, we also need the gradient of the log likelihood func-\ntion with respect to the parameter vector θ. We shall assume that evaluation of the\nderivatives of CN is straightforward, as would be the case for the covariance func-\ntions considered in this chapter. Making use of the result (C.21) for the derivative of\nC−1\nN , together with the result (C.22) for the derivative of ln |CN |, we obtain\n∂\n∂θi\nlnp(t|θ)= −1\n2Tr\n(\nC−1\nN\n∂CN\n∂θi\n)\n+ 1\n2tTC−1\nN\n∂CN\n∂θi\nC−1\nN t. (6.70)\nBecause lnp(t|θ) will in general be a nonconvex function, it can have multiple max-\nima.\nIt is straightforward to introduce a prior over θ and to maximize the log poste-\nrior using gradient-based methods. In a fully Bayesian treatment, we need to evaluate\nmarginals overθ weighted by the product of the prior p(θ) and the likelihood func-\ntion p(t|θ). In general, however, exact marginalization will be intractable, and we\nmust resort to approximations.\nThe Gaussian process regression model gives a predictive distribution whose\nmean and variance are functions of the input vector x. However, we have assumed\nthat the contribution to the predictive variance arising from the additive noise, gov-\nerned by the parameter β, is a constant. For some problems, known asheteroscedas-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 330,
      "page_label": "311"
    }
  },
  {
    "page_content": "erned by the parameter β, is a constant. For some problems, known asheteroscedas-\ntic, the noise variance itself will also depend on x. To model this, we can extend the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 330,
      "page_label": "311"
    }
  },
  {
    "page_content": "312 6. KERNEL METHODS\nFigure 6.9 Samples from the ARD\nprior for Gaussian processes, in\nwhich the kernel function is given by\n(6.71). The left plot corresponds to\nη1 = η2 =1 , and the right plot cor-\nresponds to η1 =1 , η2 =0 .01.\nGaussian process framework by introducing a second Gaussian process to represent\nthe dependence of β on the input x (Goldberg et al., 1998). Because β is a variance,\nand hence nonnegative, we use the Gaussian process to model ln β(x).\n6.4.4 Automatic relevance determination\nIn the previous section, we saw how maximum likelihood could be used to de-\ntermine a value for the correlation length-scale parameter in a Gaussian process.\nThis technique can usefully be extended by incorporating a separate parameter for\neach input variable (Rasmussen and Williams, 2006). The result, as we shall see, is\nthat the optimization of these parameters by maximum likelihood allows the relative\nimportance of different inputs to be inferred from the data. This represents an exam-\nple in the Gaussian process context of automatic relevance determination,o r ARD,\nwhich was originally formulated in the framework of neural networks (MacKay,\n1994; Neal, 1996). The mechanism by which appropriate inputs are preferred is\ndiscussed in Section 7.2.2.\nConsider a Gaussian process with a two-dimensional input space x =( x1,x2),\nhaving a kernel function of the form\nk(x,x′)= θ0 exp\n{\n−1\n2\n2∑\ni=1\nηi(xi − x′\ni)2\n}\n. (6.71)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 331,
      "page_label": "312"
    }
  },
  {
    "page_content": "having a kernel function of the form\nk(x,x′)= θ0 exp\n{\n−1\n2\n2∑\ni=1\nηi(xi − x′\ni)2\n}\n. (6.71)\nSamples from the resulting prior over functions y(x) are shown for two different\nsettings of the precision parameters ηi in Figure 6.9. We see that, as a particu-\nlar parameter ηi becomes small, the function becomes relatively insensitive to the\ncorresponding input variable xi. By adapting these parameters to a data set using\nmaximum likelihood, it becomes possible to detect input variables that have little\neffect on the predictive distribution, because the corresponding values of ηi will be\nsmall. This can be useful in practice because it allows such inputs to be discarded.\nARD is illustrated using a simple synthetic data set having three inputsx1, x2 and x3\n(Nabney, 2002) in Figure 6.10. The target variable t, is generated by sampling 100\nvalues of x1 from a Gaussian, evaluating the function sin(2πx1), and then adding",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 331,
      "page_label": "312"
    }
  },
  {
    "page_content": "6.4. Gaussian Processes 313\nFigure 6.10 Illustration of automatic rele-\nvance determination in a Gaus-\nsian process for a synthetic prob-\nlem having three inputs x1, x2,\nand x3, for which the curves\nshow the corresponding values of\nthe hyperparameters η1 (red), η2\n(green), and η3 (blue) as a func-\ntion of the number of iterations\nwhen optimizing the marginal\nlikelihood. Details are given in\nthe text. Note the logarithmic\nscale on the vertical axis.\n0 20 40 60 80 100\n10\n−4\n10\n−2\n10\n0\n10\n2\nGaussian noise. Values of x2 are given by copying the corresponding values of x1\nand adding noise, and values of x3 are sampled from an independent Gaussian dis-\ntribution. Thus x1 is a good predictor of t, x2 is a more noisy predictor of t, and x3\nhas only chance correlations with t. The marginal likelihood for a Gaussian process\nwith ARD parameters η1,η2,η3 is optimized using the scaled conjugate gradients\nalgorithm. We see from Figure 6.10 that η1 converges to a relatively large value, η2\nconverges to a much smaller value, and η3 becomes very small indicating that x3 is\nirrelevant for predicting t.\nThe ARD framework is easily incorporated into the exponential-quadratic kernel\n(6.63) to give the following form of kernel function, which has been found useful for\napplications of Gaussian processes to a range of regression problems\nk(xn, xm)= θ0 exp\n{\n−1\n2\nD∑\ni=1\nηi(xni − xmi)2\n}\n+ θ2 + θ3\nD∑\ni=1\nxnixmi (6.72)\nwhere D is the dimensionality of the input space.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 332,
      "page_label": "313"
    }
  },
  {
    "page_content": "k(xn, xm)= θ0 exp\n{\n−1\n2\nD∑\ni=1\nηi(xni − xmi)2\n}\n+ θ2 + θ3\nD∑\ni=1\nxnixmi (6.72)\nwhere D is the dimensionality of the input space.\n6.4.5 Gaussian processes for classiﬁcation\nIn a probabilistic approach to classiﬁcation, our goal is to model the posterior\nprobabilities of the target variable for a new input vector, given a set of training\ndata. These probabilities must lie in the interval (0, 1), whereas a Gaussian process\nmodel makes predictions that lie on the entire real axis. However, we can easily\nadapt Gaussian processes to classiﬁcation problems by transforming the output of\nthe Gaussian process using an appropriate nonlinear activation function.\nConsider ﬁrst the two-class problem with a target variable t ∈{ 0, 1}. If we de-\nﬁne a Gaussian process over a function a(x) and then transform the function using\na logistic sigmoid y = σ(a), given by (4.59), then we will obtain a non-Gaussian\nstochastic process over functions y(x) where y ∈ (0, 1). This is illustrated for the\ncase of a one-dimensional input space in Figure 6.11 in which the probability distri-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 332,
      "page_label": "313"
    }
  },
  {
    "page_content": "314 6. KERNEL METHODS\n−1 −0.5 0 0.5 1\n−10\n−5\n0\n5\n10\n−1 −0.5 0 0.5 1\n0\n0.25\n0.5\n0.75\n1\nFigure 6.11 The left plot shows a sample from a Gaussian process prior over functionsa(x), and the right plot\nshows the result of transforming this sample using a logistic sigmoid function.\nbution over the target variable t is then given by the Bernoulli distribution\np(t|a)= σ(a)t(1 − σ(a))1−t. (6.73)\nAs usual, we denote the training set inputs by x1,..., xN with corresponding\nobserved target variables t =( t1,...,t N )T. We also consider a single test point\nxN+1 with target value tN+1. Our goal is to determine the predictive distribution\np(tN+1|t), where we have left the conditioning on the input variables implicit. To do\nthis we introduce a Gaussian process prior over the vector aN+1, which has compo-\nnents a(x1),...,a (xN+1). This in turn deﬁnes a non-Gaussian process over tN+1,\nand by conditioning on the training data tN we obtain the required predictive distri-\nbution. The Gaussian process prior for aN+1 takes the form\np(aN+1)= N(aN+1|0,CN+1). (6.74)\nUnlike the regression case, the covariance matrix no longer includes a noise term\nbecause we assume that all of the training data points are correctly labelled. How-\never, for numerical reasons it is convenient to introduce a noise-like term governed\nby a parameter ν that ensures that the covariance matrix is positive deﬁnite. Thus\nthe covariance matrix CN+1 has elements given by\nC(xn, xm)= k(xn, xm)+ νδnm (6.75)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 333,
      "page_label": "314"
    }
  },
  {
    "page_content": "the covariance matrix CN+1 has elements given by\nC(xn, xm)= k(xn, xm)+ νδnm (6.75)\nwhere k(xn, xm) is any positive semideﬁnite kernel function of the kind considered\nin Section 6.2, and the value of ν is typically ﬁxed in advance. We shall assume that\nthe kernel function k(x,x′) is governed by a vector θ of parameters, and we shall\nlater discuss how θ may be learned from the training data.\nFor two-class problems, it is sufﬁcient to predict p(tN+1 =1 |tN ) because the\nvalue of p(tN+1 =0 |tN ) is then given by 1 − p(tN+1 =1 |tN ). The required",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 333,
      "page_label": "314"
    }
  },
  {
    "page_content": "6.4. Gaussian Processes 315\npredictive distribution is given by\np(tN+1 =1 |tN )=\n∫\np(tN+1 =1 |aN+1)p(aN+1|tN )d aN+1 (6.76)\nwhere p(tN+1 =1 |aN+1)= σ(aN+1).\nThis integral is analytically intractable, and so may be approximated using sam-\npling methods (Neal, 1997). Alternatively, we can consider techniques based on\nan analytical approximation. In Section 4.5.2, we derived the approximate formula\n(4.153) for the convolution of a logistic sigmoid with a Gaussian distribution. We\ncan use this result to evaluate the integral in (6.76) provided we have a Gaussian\napproximation to the posterior distributionp(aN+1|tN ). The usual justiﬁcation for a\nGaussian approximation to a posterior distribution is that the true posterior will tend\nto a Gaussian as the number of data points increases as a consequence of the central\nlimit theorem. In the case of Gaussian processes, the number of variables grows withSection 2.3\nthe number of data points, and so this argument does not apply directly. However, if\nwe consider increasing the number of data points falling in a ﬁxed region ofx space,\nthen the corresponding uncertainty in the function a(x) will decrease, again leading\nasymptotically to a Gaussian (Williams and Barber, 1998).\nThree different approaches to obtaining a Gaussian approximation have been\nconsidered. One technique is based on variational inference (Gibbs and MacKay,Section 10.1\n2000) and makes use of the local variational bound (10.144) on the logistic sigmoid.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 334,
      "page_label": "315"
    }
  },
  {
    "page_content": "2000) and makes use of the local variational bound (10.144) on the logistic sigmoid.\nThis allows the product of sigmoid functions to be approximated by a product of\nGaussians thereby allowing the marginalization over aN to be performed analyti-\ncally. The approach also yields a lower bound on the likelihood function p(tN |θ).\nThe variational framework for Gaussian process classiﬁcation can also be extended\nto multiclass (K> 2) problems by using a Gaussian approximation to the softmax\nfunction (Gibbs, 1997).\nA second approach uses expectation propagation (Opper and Winther, 2000b;Section 10.7\nMinka, 2001b; Seeger, 2003). Because the true posterior distribution is unimodal, as\nwe shall see shortly, the expectation propagation approach can give good results.\n6.4.6 Laplace approximation\nThe third approach to Gaussian process classiﬁcation is based on the Laplace\napproximation, which we now consider in detail. In order to evaluate the predictiveSection 4.4\ndistribution (6.76), we seek a Gaussian approximation to the posterior distribution\nover aN+1, which, using Bayes’ theorem, is given by\np(aN+1|tN )=\n∫\np(aN+1, aN |tN )d aN\n= 1\np(tN )\n∫\np(aN+1, aN )p(tN |aN+1, aN )d aN\n= 1\np(tN )\n∫\np(aN+1|aN )p(aN )p(tN |aN )d aN\n=\n∫\np(aN+1|aN )p(aN |tN )d aN (6.77)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 334,
      "page_label": "315"
    }
  },
  {
    "page_content": "316 6. KERNEL METHODS\nwhere we have used p(tN |aN+1, aN )= p(tN |aN ). The conditional distribution\np(aN+1|aN ) is obtained by invoking the results (6.66) and (6.67) for Gaussian pro-\ncess regression, to give\np(aN+1|aN )= N(aN+1|kTC−1\nN aN ,c − kTC−1\nN k). (6.78)\nWe can therefore evaluate the integral in (6.77) by ﬁnding a Laplace approximation\nfor the posterior distribution p(aN |tN ), and then using the standard result for the\nconvolution of two Gaussian distributions.\nThe prior p(aN ) is given by a zero-mean Gaussian process with covariance ma-\ntrix CN , and the data term (assuming independence of the data points) is given by\np(tN |aN )=\nN∏\nn=1\nσ(an)tn (1 − σ(an))1−tn =\nN∏\nn=1\neantn σ(−an). (6.79)\nWe then obtain the Laplace approximation by Taylor expanding the logarithm of\np(aN |tN ), which up to an additive normalization constant is given by the quantity\nΨ(aN )=l n p(aN )+l n p(tN |aN )\n= −1\n2aT\nN C−1\nN aN − N\n2 ln(2π) − 1\n2 ln|CN | + tT\nN aN\n−\nN∑\nn=1\nln(1 +ean ) + const. (6.80)\nFirst we need to ﬁnd the mode of the posterior distribution, and this requires that we\nevaluate the gradient of Ψ(aN ), which is given by\n∇Ψ(aN )= tN − σN − C−1\nN aN (6.81)\nwhere σN is a vector with elements σ(an). We cannot simply ﬁnd the mode by\nsetting this gradient to zero, because σN depends nonlinearly on aN , and so we\nresort to an iterative scheme based on the Newton-Raphson method, which gives rise",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 335,
      "page_label": "316"
    }
  },
  {
    "page_content": "resort to an iterative scheme based on the Newton-Raphson method, which gives rise\nto an iterative reweighted least squares (IRLS) algorithm. This requires the secondSection 4.3.3\nderivatives of Ψ(aN ), which we also require for the Laplace approximation anyway,\nand which are given by\n∇∇Ψ(aN )= −WN − C−1\nN (6.82)\nwhere WN is a diagonal matrix with elements σ(an)(1−σ(an)), and we have used\nthe result (4.88) for the derivative of the logistic sigmoid function. Note that these\ndiagonal elements lie in the range (0,1/4), and hence WN is a positive deﬁnite\nmatrix. Because CN (and hence its inverse) is positive deﬁnite by construction, and\nbecause the sum of two positive deﬁnite matrices is also positive deﬁnite, we seeExercise 6.24\nthat the Hessian matrix A = −∇∇Ψ(aN ) is positive deﬁnite and so the posterior\ndistribution p(aN |tN ) is log convex and therefore has a single mode that is the global",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 335,
      "page_label": "316"
    }
  },
  {
    "page_content": "6.4. Gaussian Processes 317\nmaximum. The posterior distribution is not Gaussian, however, because the Hessian\nis a function of aN .\nUsing the Newton-Raphson formula (4.92), the iterative update equation foraN\nis given byExercise 6.25\nanew\nN = CN (I + WN CN )−1 {tN − σN + WN aN }. (6.83)\nThese equations are iterated until they converge to the mode which we denote by\na⋆\nN . At the mode, the gradient ∇Ψ(aN ) will vanish, and hence a⋆\nN will satisfy\na⋆\nN = CN (tN − σN ). (6.84)\nOnce we have found the mode a⋆\nN of the posterior, we can evaluate the Hessian\nmatrix given by\nH = −∇∇Ψ(aN )= WN + C−1\nN (6.85)\nwhere the elements of WN are evaluated using a⋆\nN . This deﬁnes our Gaussian ap-\nproximation to the posterior distribution p(aN |tN ) given by\nq(aN )= N(aN |a⋆\nN , H−1). (6.86)\nWe can now combine this with (6.78) and hence evaluate the integral (6.77). Because\nthis corresponds to a linear-Gaussian model, we can use the general result (2.115) to\ngiveExercise 6.26\nE[aN+1|tN ]= kT(tN − σN ) (6.87)\nvar[aN+1|tN ]= c − kT(W−1\nN + CN )−1k. (6.88)\nNow that we have a Gaussian distribution for p(aN+1|tN ), we can approximate\nthe integral (6.76) using the result (4.153). As with the Bayesian logistic regression\nmodel of Section 4.5, if we are only interested in the decision boundary correspond-\ning top(tN+1|tN )=0 .5, then we need only consider the mean and we can ignore\nthe effect of the variance.\nWe also need to determine the parameters θ of the covariance function. One",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 336,
      "page_label": "317"
    }
  },
  {
    "page_content": "the effect of the variance.\nWe also need to determine the parameters θ of the covariance function. One\napproach is to maximize the likelihood function given byp(tN |θ) for which we need\nexpressions for the log likelihood and its gradient. If desired, suitable regularization\nterms can also be added, leading to a penalized maximum likelihood solution. The\nlikelihood function is deﬁned by\np(tN |θ)=\n∫\np(tN |aN )p(aN |θ)d aN . (6.89)\nThis integral is analytically intractable, so again we make use of the Laplace approx-\nimation. Using the result (4.135), we obtain the following approximation for the log\nof the likelihood function\nlnp(tN |θ)=Ψ ( a⋆\nN ) − 1\n2 ln |WN + C−1\nN | + N\n2 ln(2π) (6.90)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 336,
      "page_label": "317"
    }
  },
  {
    "page_content": "318 6. KERNEL METHODS\nwhere Ψ(a⋆\nN )=l n p(a⋆\nN |θ)+l n p(tN |a⋆\nN ). We also need to evaluate the gradient\nof lnp(tN |θ) with respect to the parameter vector θ. Note that changes in θ will\ncause changes in a⋆\nN , leading to additional terms in the gradient. Thus, when we\ndifferentiate (6.90) with respect to θ, we obtain two sets of terms, the ﬁrst arising\nfrom the dependence of the covariance matrix CN on θ, and the rest arising from\ndependence of a⋆\nN on θ.\nThe terms arising from the explicit dependence on θ can be found by using\n(6.80) together with the results (C.21) and (C.22), and are given by\n∂ ln p(tN |θ)\n∂θj\n= 1\n2a⋆T\nN C−1\nN\n∂CN\n∂θj\nC−1\nN a⋆\nN\n−1\n2Tr\n[\n(I + CN WN )−1WN\n∂CN\n∂θj\n]\n. (6.91)\nTo compute the terms arising from the dependence of a⋆\nN on θ, we note that\nthe Laplace approximation has been constructed such that Ψ(aN ) has zero gradient\nat aN = a⋆\nN , and so Ψ(a⋆\nN ) gives no contribution to the gradient as a result of its\ndependence on a⋆\nN . This leaves the following contribution to the derivative with\nrespect to a component θj of θ\n−1\n2\nN∑\nn=1\n∂ ln|WN + C−1\nN |\n∂a⋆n\n∂a⋆\nn\n∂θj\n= −1\n2\nN∑\nn=1\n[\n(I + CN WN )−1CN\n]\nnn σ⋆\nn(1 − σ⋆\nn)(1 − 2σ⋆\nn)∂a⋆\nn\n∂θj\n(6.92)\nwhere σ⋆\nn = σ(a⋆\nn), and again we have used the result (C.22) together with the\ndeﬁnition of WN . We can evaluate the derivative ofa⋆\nN with respect to θj by differ-\nentiating the relation (6.84) with respect to θj to give\n∂a⋆\nn\n∂θj\n= ∂CN\n∂θj\n(tN − σN ) − CN WN\n∂a⋆\nn\n∂θj\n. (6.93)\nRearranging then gives\n∂a⋆",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 337,
      "page_label": "318"
    }
  },
  {
    "page_content": "entiating the relation (6.84) with respect to θj to give\n∂a⋆\nn\n∂θj\n= ∂CN\n∂θj\n(tN − σN ) − CN WN\n∂a⋆\nn\n∂θj\n. (6.93)\nRearranging then gives\n∂a⋆\nn\n∂θj\n=( I + WN CN )−1 ∂CN\n∂θj\n(tN − σN ). (6.94)\nCombining (6.91), (6.92), and (6.94), we can evaluate the gradient of the log\nlikelihood function, which can be used with standard nonlinear optimization algo-\nrithms in order to determine a value for θ.\nWe can illustrate the application of the Laplace approximation for Gaussian pro-\ncesses using the synthetic two-class data set shown in Figure 6.12. Extension of theAppendix A\nLaplace approximation to Gaussian processes involving K> 2 classes, using the\nsoftmax activation function, is straightforward (Williams and Barber, 1998).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 337,
      "page_label": "318"
    }
  },
  {
    "page_content": "6.4. Gaussian Processes 319\n−2 0 2\n−2\n0\n2\nFigure 6.12 Illustration of the use of a Gaussian process for classiﬁcation, showing the data on the left together\nwith the optimal decision boundary from the true distribution in green, and the decision boundary from the\nGaussian process classiﬁer in black. On the right is the predicted posterior probability for the blue and red\nclasses together with the Gaussian process decision boundary.\n6.4.7 Connection to neural networks\nWe have seen that the range of functions which can be represented by a neural\nnetwork is governed by the number M of hidden units, and that, for sufﬁciently\nlarge M, a two-layer network can approximate any given function with arbitrary\naccuracy. In the framework of maximum likelihood, the number of hidden units\nneeds to be limited (to a level dependent on the size of the training set) in order\nto avoid over-ﬁtting. However, from a Bayesian perspective it makes little sense to\nlimit the number of parameters in the network according to the size of the training\nset.\nIn a Bayesian neural network, the prior distribution over the parameter vector\nw, in conjunction with the network function f(x,w), produces a prior distribution\nover functions from y(x) where y is the vector of network outputs. Neal (1996)\nhas shown that, for a broad class of prior distributions over w, the distribution of\nfunctions generated by a neural network will tend to a Gaussian process in the limit",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 338,
      "page_label": "319"
    }
  },
  {
    "page_content": "functions generated by a neural network will tend to a Gaussian process in the limit\nM →∞ . It should be noted, however, that in this limit the output variables of the\nneural network become independent. One of the great merits of neural networks is\nthat the outputs share the hidden units and so they can ‘borrow statistical strength’\nfrom each other, that is, the weights associated with each hidden unit are inﬂuenced\nby all of the output variables not just by one of them. This property is therefore lost\nin the Gaussian process limit.\nWe have seen that a Gaussian process is determined by its covariance (kernel)\nfunction. Williams (1998) has given explicit forms for the covariance in the case of\ntwo speciﬁc choices for the hidden unit activation function (probit and Gaussian).\nThese kernel functions k(x,x′) are nonstationary, i.e. they cannot be expressed as\na function of the difference x − x′, as a consequence of the Gaussian weight prior\nbeing centred on zero which breaks translation invariance in weight space.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 338,
      "page_label": "319"
    }
  },
  {
    "page_content": "320 6. KERNEL METHODS\nBy working directly with the covariance function we have implicitly marginal-\nized over the distribution of weights. If the weight prior is governed by hyperpa-\nrameters, then their values will determine the length scales of the distribution over\nfunctions, as can be understood by studying the examples in Figure 5.11 for the case\nof a ﬁnite number of hidden units. Note that we cannot marginalize out the hyperpa-\nrameters analytically, and must instead resort to techniques of the kind discussed in\nSection 6.4.\nExercises\n6.1 (⋆⋆ ) www Consider the dual formulation of the least squares linear regression\nproblem given in Section 6.1. Show that the solution for the components an of\nthe vector a can be expressed as a linear combination of the elements of the vector\nφ(xn). Denoting these coefﬁcients by the vector w, show that the dual of the dual\nformulation is given by the original representation in terms of the parameter vector\nw.\n6.2 (⋆⋆ ) In this exercise, we develop a dual formulation of the perceptron learning\nalgorithm. Using the perceptron learning rule (4.55), show that the learned weight\nvectorw can be written as a linear combination of the vectors tnφ(xn) where tn ∈\n{−1, +1}. Denote the coefﬁcients of this linear combination by αn and derive a\nformulation of the perceptron learning algorithm, and the predictive function for the\nperceptron, in terms of theαn. Show that the feature vector φ(x) enters only in the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 339,
      "page_label": "320"
    }
  },
  {
    "page_content": "perceptron, in terms of theαn. Show that the feature vector φ(x) enters only in the\nform of the kernel function k(x,x′)= φ(x)Tφ(x′).\n6.3 (⋆) The nearest-neighbour classiﬁer (Section 2.5.2) assigns a new input vector x\nto the same class as that of the nearest input vector xn from the training set, where\nin the simplest case, the distance is deﬁned by the Euclidean metric ∥x − xn∥2.B y\nexpressing this rule in terms of scalar products and then making use of kernel sub-\nstitution, formulate the nearest-neighbour classiﬁer for a general nonlinear kernel.\n6.4 (⋆) In Appendix C, we give an example of a matrix that has positive elements but\nthat has a negative eigenvalue and hence that is not positive deﬁnite. Find an example\nof the converse property, namely a2 × 2 matrix with positive eigenvalues yet that\nhas at least one negative element.\n6.5 (⋆) www Verify the results (6.13) and (6.14) for constructing valid kernels.\n6.6 (⋆) Verify the results (6.15) and (6.16) for constructing valid kernels.\n6.7 (⋆) www Verify the results (6.17) and (6.18) for constructing valid kernels.\n6.8 (⋆) Verify the results (6.19) and (6.20) for constructing valid kernels.\n6.9 (⋆) Verify the results (6.21) and (6.22) for constructing valid kernels.\n6.10 (⋆) Show that an excellent choice of kernel for learning a function f(x) is given\nby k(x,x′)= f(x)f(x′) by showing that a linear learning machine based on this\nkernel will always ﬁnd a solution proportional to f(x).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 339,
      "page_label": "320"
    }
  },
  {
    "page_content": "Exercises 321\n6.11 (⋆) By making use of the expansion (6.25), and then expanding the middle factor\nas a power series, show that the Gaussian kernel (6.23) can be expressed as the inner\nproduct of an inﬁnite-dimensional feature vector.\n6.12 (⋆⋆ ) www Consider the space of all possible subsets A of a given ﬁxed set D.\nShow that the kernel function (6.27) corresponds to an inner product in a feature\nspace of dimensionality 2|D| deﬁned by the mapping φ(A) where A is a subset of D\nand the element φU (A), indexed by the subset U, is given by\nφU (A)=\n{\n1, if U ⊆ A;\n0, otherwise. (6.95)\nHere U ⊆ A denotes that U is either a subset of A or is equal to A.\n6.13 (⋆) Show that the Fisher kernel, deﬁned by (6.33), remains invariant if we make\na nonlinear transformation of the parameter vector θ → ψ(θ), where the function\nψ(·) is invertible and differentiable.\n6.14 (⋆) www Write down the form of the Fisher kernel, deﬁned by (6.33), for the\ncase of a distribution p(x|µ)= N(x|µ,S) that is Gaussian with mean µ and ﬁxed\ncovariance S.\n6.15 (⋆) By considering the determinant of a 2 × 2 Gram matrix, show that a positive-\ndeﬁnite kernel function k(x, x′) satisﬁes the Cauchy-Schwartz inequality\nk(x1,x2)2 ⩽ k(x1,x1)k(x2,x2). (6.96)\n6.16 (⋆⋆ ) Consider a parametric model governed by the parameter vector w together\nwith a data set of input values x1,..., xN and a nonlinear feature mapping φ(x).\nSuppose that the dependence of the error function on w takes the form",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 340,
      "page_label": "321"
    }
  },
  {
    "page_content": "Suppose that the dependence of the error function on w takes the form\nJ(w)= f(wTφ(x1),..., wTφ(xN )) +g(wTw) (6.97)\nwhere g(·) is a monotonically increasing function. By writing w in the form\nw =\nN∑\nn=1\nαnφ(xn)+ w⊥ (6.98)\nshow that the value ofw that minimizes J(w) takes the form of a linear combination\nof the basis functions φ(xn) for n =1 ,...,N .\n6.17 (⋆⋆ ) www Consider the sum-of-squares error function (6.39) for data having\nnoisy inputs, where ν(ξ) is the distribution of the noise. Use the calculus of vari-\nations to minimize this error function with respect to the function y(x), and hence\nshow that the optimal solution is given by an expansion of the form (6.40) in which\nthe basis functions are given by (6.41).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 340,
      "page_label": "321"
    }
  },
  {
    "page_content": "322 6. KERNEL METHODS\n6.18 (⋆) Consider a Nadaraya-Watson model with one input variable x and one target\nvariable t having Gaussian components with isotropic covariances, so that the co-\nvariance matrix is given by σ2I where I is the unit matrix. Write down expressions\nfor the conditional density p(t|x) and for the conditional mean E[t|x] and variance\nvar[t|x], in terms of the kernel function k(x, xn).\n6.19 (⋆⋆ ) Another viewpoint on kernel regression comes from a consideration of re-\ngression problems in which the input variables as well as the target variables are\ncorrupted with additive noise. Suppose each target value tn is generated as usual\nby taking a function y(zn) evaluated at a point zn, and adding Gaussian noise. The\nvalue of zn is not directly observed, however, but only a noise corrupted version\nxn = zn + ξn where the random variable ξ is governed by some distribution g(ξ).\nConsider a set of observations {xn,t n}, where n =1 ,...,N , together with a cor-\nresponding sum-of-squares error function deﬁned by averaging over the distribution\nof input noise to give\nE = 1\n2\nN∑\nn=1\n∫\n{y(xn − ξn) − tn}2 g(ξn)d ξn. (6.99)\nBy minimizing E with respect to the function y(z) using the calculus of variations\n(Appendix D), show that optimal solution for y(x) is given by a Nadaraya-Watson\nkernel regression solution of the form (6.45) with a kernel of the form (6.46).\n6.20 (⋆⋆ ) www Verify the results (6.66) and (6.67).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 341,
      "page_label": "322"
    }
  },
  {
    "page_content": "kernel regression solution of the form (6.45) with a kernel of the form (6.46).\n6.20 (⋆⋆ ) www Verify the results (6.66) and (6.67).\n6.21 (⋆⋆ ) www Consider a Gaussian process regression model in which the kernel\nfunction is deﬁned in terms of a ﬁxed set of nonlinear basis functions. Show that the\npredictive distribution is identical to the result (3.58) obtained in Section 3.3.2 for the\nBayesian linear regression model. To do this, note that both models have Gaussian\npredictive distributions, and so it is only necessary to show that the conditional mean\nand variance are the same. For the mean, make use of the matrix identity (C.6), and\nfor the variance, make use of the matrix identity (C.7).\n6.22 (⋆⋆ ) Consider a regression problem with N training set input vectors x1,..., xN\nand L test set input vectors xN+1,..., xN+L, and suppose we deﬁne a Gaussian\nprocess prior over functions t(x). Derive an expression for the joint predictive dis-\ntribution for t(xN+1),...,t (xN+L), given the values oft(x1),...,t (xN ). Show the\nmarginal of this distribution for one of the test observations tj where N +1 ⩽ j ⩽\nN + L is given by the usual Gaussian process regression result (6.66) and (6.67).\n6.23 (⋆⋆ ) www Consider a Gaussian process regression model in which the target\nvariable t has dimensionality D. Write down the conditional distribution of tN+1\nfor a test input vector xN+1, given a training set of input vectors x1,..., xN+1 and\ncorresponding target observations t1,..., tN .",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 341,
      "page_label": "322"
    }
  },
  {
    "page_content": "for a test input vector xN+1, given a training set of input vectors x1,..., xN+1 and\ncorresponding target observations t1,..., tN .\n6.24 (⋆) Show that a diagonal matrixW whose elements satisfy 0 <W ii < 1 is positive\ndeﬁnite. Show that the sum of two positive deﬁnite matrices is itself positive deﬁnite.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 341,
      "page_label": "322"
    }
  },
  {
    "page_content": "Exercises 323\n6.25 (⋆) www Using the Newton-Raphson formula (4.92), derive the iterative update\nformula (6.83) for ﬁnding the mode a⋆\nN of the posterior distribution in the Gaussian\nprocess classiﬁcation model.\n6.26 (⋆) Using the result (2.115), derive the expressions (6.87) and (6.88) for the mean\nand variance of the posterior distribution p(aN+1|tN ) in the Gaussian process clas-\nsiﬁcation model.\n6.27 (⋆⋆⋆ ) Derive the result (6.90) for the log likelihood function in the Laplace approx-\nimation framework for Gaussian process classiﬁcation. Similarly, derive the results\n(6.91), (6.92), and (6.94) for the terms in the gradient of the log likelihood.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 342,
      "page_label": "323"
    }
  },
  {
    "page_content": "7\nSparse Kernel\nMachines\nIn the previous chapter, we explored a variety of learning algorithms based on non-\nlinear kernels. One of the signiﬁcant limitations of many such algorithms is that\nthe kernel functionk(xn, xm) must be evaluated for all possible pairs xn and xm\nof training points, which can be computationally infeasible during training and can\nlead to excessive computation times when making predictions for new data points.\nIn this chapter we shall look at kernel-based algorithms that havesparse solutions,\nso that predictions for new inputs depend only on the kernel function evaluated at a\nsubset of the training data points.\nWe begin by looking in some detail at thesupport vector machine(SVM), which\nbecame popular in some years ago for solving problems in classiﬁcation, regression,\nand novelty detection. An important property of support vector machines is that the\ndetermination of the model parameters corresponds to a convex optimization prob-\nlem, and so any local solution is also a global optimum. Because the discussion of\nsupport vector machines makes extensive use of Lagrange multipliers, the reader is\n325",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 344,
      "page_label": "325"
    }
  },
  {
    "page_content": "326 7. SPARSE KERNEL MACHINES\nencouraged to review the key concepts covered in Appendix E. Additional infor-\nmation on support vector machines can be found in Vapnik (1995), Burges (1998),\nCristianini and Shawe-Taylor (2000), M¨uller et al. (2001), Sch ¨olkopf and Smola\n(2002), and Herbrich (2002).\nThe SVM is a decision machine and so does not provide posterior probabilities.\nWe have already discussed some of the beneﬁts of determining probabilities in Sec-\ntion 1.5.4. An alternative sparse kernel technique, known as the relevance vector\nmachine (RVM), is based on a Bayesian formulation and provides posterior proba-Section 7.2\nbilistic outputs, as well as having typically much sparser solutions than the SVM.\n7.1. Maximum Margin Classiﬁers\nWe begin our discussion of support vector machines by returning to the two-class\nclassiﬁcation problem using linear models of the form\ny(x)= wTφ(x)+ b (7.1)\nwhere φ(x) denotes a ﬁxed feature-space transformation, and we have made the\nbias parameter b explicit. Note that we shall shortly introduce a dual representation\nexpressed in terms of kernel functions, which avoids having to work explicitly in\nfeature space. The training data set comprisesN input vectors x1,..., xN , with\ncorresponding target values t1,...,t N where tn ∈{ −1, 1}, and new data points x\nare classiﬁed according to the sign of y(x).\nWe shall assume for the moment that the training data set is linearly separable in",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 345,
      "page_label": "326"
    }
  },
  {
    "page_content": "are classiﬁed according to the sign of y(x).\nWe shall assume for the moment that the training data set is linearly separable in\nfeature space, so that by deﬁnition there exists at least one choice of the parameters\nw and b such that a function of the form (7.1) satisﬁes y(xn) > 0 for points having\ntn =+ 1 and y(xn) < 0 for points having tn = −1, so that tny(xn) > 0 for all\ntraining data points.\nThere may of course exist many such solutions that separate the classes exactly.\nIn Section 4.1.7, we described the perceptron algorithm that is guaranteed to ﬁnd\na solution in a ﬁnite number of steps. The solution that it ﬁnds, however, will be\ndependent on the (arbitrary) initial values chosen for w and b as well as on the\norder in which the data points are presented. If there are multiple solutions all of\nwhich classify the training data set exactly, then we should try to ﬁnd the one that\nwill give the smallest generalization error. The support vector machine approaches\nthis problem through the concept of themargin, which is deﬁned to be the smallest\ndistance between the decision boundary and any of the samples, as illustrated in\nFigure 7.1.\nIn support vector machines the decision boundary is chosen to be the one for\nwhich the margin is maximized. The maximum margin solution can be motivated us-\ningcomputational learning theory, also known as statistical learning theory.H o w -Section 7.1.5\never, a simple insight into the origins of maximum margin has been given by Tong",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 345,
      "page_label": "326"
    }
  },
  {
    "page_content": "ever, a simple insight into the origins of maximum margin has been given by Tong\nand Koller (2000) who consider a framework for classiﬁcation based on a hybrid of\ngenerative and discriminative approaches. They ﬁrst model the distribution over in-\nput vectors x for each class using a Parzen density estimator with Gaussian kernels",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 345,
      "page_label": "326"
    }
  },
  {
    "page_content": "7.1. Maximum Margin Classiﬁers 327\ny =1\ny =0\ny = −1\nmargin\ny =1\ny =0\ny = −1\nFigure 7.1 The margin is deﬁned as the perpendicular distance between the decision boundary and the closest\nof the data points, as shown on the left ﬁgure. Maximizing the margin leads to a particular choice of decision\nboundary, as shown on the right. The location of this boundary is determined by a subset of the data points,\nknown as support vectors, which are indicated by the circles.\nhaving a common parameter σ2. Together with the class priors, this deﬁnes an opti-\nmal misclassiﬁcation-rate decision boundary. However, instead of using this optimal\nboundary, they determine the best hyperplane by minimizing the probability of error\nrelative to the learned density model. In the limit σ2 → 0, the optimal hyperplane\nis shown to be the one having maximum margin. The intuition behind this result is\nthat as σ2 is reduced, the hyperplane is increasingly dominated by nearby data points\nrelative to more distant ones. In the limit, the hyperplane becomes independent of\ndata points that are not support vectors.\nWe shall see in Figure 10.13 that marginalization with respect to the prior distri-\nbution of the parameters in a Bayesian approach for a simple linearly separable data\nset leads to a decision boundary that lies in the middle of the region separating the\ndata points. The large margin solution has similar behaviour.\nRecall from Figure 4.1 that the perpendicular distance of a pointx from a hyper-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 346,
      "page_label": "327"
    }
  },
  {
    "page_content": "data points. The large margin solution has similar behaviour.\nRecall from Figure 4.1 that the perpendicular distance of a pointx from a hyper-\nplane deﬁned by y(x)=0 where y(x) takes the form (7.1) is given by |y(x)|/∥w∥.\nFurthermore, we are only interested in solutions for which all data points are cor-\nrectly classiﬁed, so that tny(xn) > 0 for all n. Thus the distance of a point xn to the\ndecision surface is given by\ntny(xn)\n∥w∥ = tn(wTφ(xn)+ b)\n∥w∥ . (7.2)\nThe margin is given by the perpendicular distance to the closest point xn from the\ndata set, and we wish to optimize the parameters w and b in order to maximize this\ndistance. Thus the maximum margin solution is found by solving\narg max\nw,b\n{ 1\n∥w∥min\nn\n[\ntn\n(\nwTφ(xn)+ b\n)]}\n(7.3)\nwhere we have taken the factor 1/∥w∥outside the optimization over n because w",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 346,
      "page_label": "327"
    }
  },
  {
    "page_content": "328 7. SPARSE KERNEL MACHINES\ndoes not depend on n. Direct solution of this optimization problem would be very\ncomplex, and so we shall convert it into an equivalent problem that is much easier\nto solve. To do this we note that if we make the rescalingw → κw and b → κb,\nthen the distance from any point xn to the decision surface, given by tny(xn)/∥w∥,\nis unchanged. We can use this freedom to set\ntn\n(\nwTφ(xn)+ b\n)\n=1 (7.4)\nfor the point that is closest to the surface. In this case, all data points will satisfy the\nconstraints\ntn\n(\nwTφ(xn)+ b\n)\n⩾ 1,n =1 ,...,N. (7.5)\nThis is known as the canonical representation of the decision hyperplane. In the\ncase of data points for which the equality holds, the constraints are said to be active,\nwhereas for the remainder they are said to be inactive. By deﬁnition, there will\nalways be at least one active constraint, because there will always be a closest point,\nand once the margin has been maximized there will be at least two active constraints.\nThe optimization problem then simply requires that we maximize ∥w∥−1, which is\nequivalent to minimizing ∥w∥2, and so we have to solve the optimization problem\narg min\nw,b\n1\n2∥w∥2 (7.6)\nsubject to the constraints given by (7.5). The factor of 1/2 in (7.6) is included for\nlater convenience. This is an example of aquadratic programmingproblem in which\nwe are trying to minimize a quadratic function subject to a set of linear inequality",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 347,
      "page_label": "328"
    }
  },
  {
    "page_content": "we are trying to minimize a quadratic function subject to a set of linear inequality\nconstraints. It appears that the bias parameterb has disappeared from the optimiza-\ntion. However, it is determined implicitly via the constraints, because these require\nthat changes to ∥w∥be compensated by changes to b. We shall see how this works\nshortly.\nIn order to solve this constrained optimization problem, we introduce Lagrange\nmultipliers an ⩾ 0, with one multiplier an for each of the constraints in (7.5), givingAppendix E\nthe Lagrangian function\nL(w,b ,a)= 1\n2∥w∥2 −\nN∑\nn=1\nan\n{\ntn(wTφ(xn)+ b) − 1\n}\n(7.7)\nwhere a =( a1,...,a N )T. Note the minus sign in front of the Lagrange multiplier\nterm, because we are minimizing with respect to w and b, and maximizing with\nrespect to a. Setting the derivatives of L(w,b ,a) with respect to w and b equal to\nzero, we obtain the following two conditions\nw =\nN∑\nn=1\nantnφ(xn) (7.8)\n0=\nN∑\nn=1\nantn. (7.9)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 347,
      "page_label": "328"
    }
  },
  {
    "page_content": "7.1. Maximum Margin Classiﬁers 329\nEliminating w and b from L(w,b ,a) using these conditions then gives the dual\nrepresentation of the maximum margin problem in which we maximize\n˜L(a)=\nN∑\nn=1\nan − 1\n2\nN∑\nn=1\nN∑\nm=1\nanamtntmk(xn, xm) (7.10)\nwith respect to a subject to the constraints\nan ⩾ 0,n =1 ,...,N, (7.11)\nN∑\nn=1\nantn =0 . (7.12)\nHere the kernel function is deﬁned byk(x,x′)= φ(x)Tφ(x′). Again, this takes the\nform of a quadratic programming problem in which we optimize a quadratic function\nof a subject to a set of inequality constraints. We shall discuss techniques for solving\nsuch quadratic programming problems in Section 7.1.1.\nThe solution to a quadratic programming problem in M variables in general has\ncomputational complexity that is O(M3). In going to the dual formulation we have\nturned the original optimization problem, which involved minimizing (7.6) over M\nvariables, into the dual problem (7.10), which has N variables. For a ﬁxed set of\nbasis functions whose number M is smaller than the number N of data points, the\nmove to the dual problem appears disadvantageous. However, it allows the model to\nbe reformulated using kernels, and so the maximum margin classiﬁer can be applied\nefﬁciently to feature spaces whose dimensionality exceeds the number of data points,\nincluding inﬁnite feature spaces. The kernel formulation also makes clear the role\nof the constraint that the kernel functionk(x,x′) be positive deﬁnite, because this",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 348,
      "page_label": "329"
    }
  },
  {
    "page_content": "of the constraint that the kernel functionk(x,x′) be positive deﬁnite, because this\nensures that the Lagrangian function ˜L(a) is bounded below, giving rise to a well-\ndeﬁned optimization problem.\nIn order to classify new data points using the trained model, we evaluate the sign\nof y(x) deﬁned by (7.1). This can be expressed in terms of the parameters {an} and\nthe kernel function by substituting for w using (7.8) to give\ny(x)=\nN∑\nn=1\nantnk(x, xn)+ b. (7.13)\nJoseph-Louis Lagrange\n1736–1813\nAlthough widely considered to be\na French mathematician, Lagrange\nwas born in Turin in Italy. By the age\nof nineteen, he had already made\nimportant contributions mathemat-\nics and had been appointed as Pro-\nfessor at the Royal Artillery School in Turin. For many\nyears, Euler worked hard to persuade Lagrange to\nmove to Berlin, which he eventually did in 1766 where\nhe succeeded Euler as Director of Mathematics at\nthe Berlin Academy. Later he moved to Paris, nar-\nrowly escaping with his life during the French revo-\nlution thanks to the personal intervention of Lavoisier\n(the French chemist who discovered oxygen) who him-\nself was later executed at the guillotine. Lagrange\nmade key contributions to the calculus of variations\nand the foundations of dynamics.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 348,
      "page_label": "329"
    }
  },
  {
    "page_content": "330 7. SPARSE KERNEL MACHINES\nIn Appendix E, we show that a constrained optimization of this form satisﬁes the\nKarush-Kuhn-Tucker (KKT) conditions, which in this case require that the following\nthree properties hold\nan ⩾ 0 (7.14)\ntny(xn) − 1 ⩾ 0 (7.15)\nan {tny(xn) − 1} =0 . (7.16)\nThus for every data point, either an =0 or tny(xn)=1 . Any data point for\nwhich an =0 will not appear in the sum in (7.13) and hence plays no role in making\npredictions for new data points. The remaining data points are calledsupport vectors,\nand because they satisfy tny(xn)=1 , they correspond to points that lie on the\nmaximum margin hyperplanes in feature space, as illustrated in Figure 7.1. This\nproperty is central to the practical applicability of support vector machines. Once\nthe model is trained, a signiﬁcant proportion of the data points can be discarded and\nonly the support vectors retained.\nHaving solved the quadratic programming problem and found a value for a,w e\ncan then determine the value of the threshold parameter b by noting that any support\nvector xn satisﬁes tny(xn)=1 . Using (7.13) this gives\ntn\n( ∑\nm∈S\namtmk(xn, xm)+ b\n)\n=1 (7.17)\nwhere S denotes the set of indices of the support vectors. Although we can solve\nthis equation for b using an arbitrarily chosen support vector xn, a numerically more\nstable solution is obtained by ﬁrst multiplying through by tn, making use of t2\nn =1 ,\nand then averaging these equations over all support vectors and solving for b to give\nb = 1\nNS\n∑\nn∈S",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 349,
      "page_label": "330"
    }
  },
  {
    "page_content": "n =1 ,\nand then averaging these equations over all support vectors and solving for b to give\nb = 1\nNS\n∑\nn∈S\n(\ntn −\n∑\nm∈S\namtmk(xn, xm)\n)\n(7.18)\nwhere NS is the total number of support vectors.\nFor later comparison with alternative models, we can express the maximum-\nmargin classiﬁer in terms of the minimization of an error function, with a simple\nquadratic regularizer, in the form\nN∑\nn=1\nE∞(y(xn)tn − 1) +λ∥w∥2 (7.19)\nwhere E∞(z) is a function that is zero if z ⩾ 0 and ∞ otherwise and ensures that\nthe constraints (7.5) are satisﬁed. Note that as long as the regularization parameter\nsatisﬁes λ> 0, its precise value plays no role.\nFigure 7.2 shows an example of the classiﬁcation resulting from training a sup-\nport vector machine on a simple synthetic data set using a Gaussian kernel of the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 349,
      "page_label": "330"
    }
  },
  {
    "page_content": "7.1. Maximum Margin Classiﬁers 331\nFigure 7.2 Example of synthetic data from\ntwo classes in two dimensions\nshowing contours of constant\ny(x) obtained from a support\nvector machine having a Gaus-\nsian kernel function. Also shown\nare the decision boundary, the\nmargin boundaries, and the sup-\nport vectors.\nform (6.23). Although the data set is not linearly separable in the two-dimensional\ndata space x, it is linearly separable in the nonlinear feature space deﬁned implicitly\nby the nonlinear kernel function. Thus the training data points are perfectly separated\nin the original data space.\nThis example also provides a geometrical insight into the origin of sparsity in\nthe SVM. The maximum margin hyperplane is deﬁned by the location of the support\nvectors. Other data points can be moved around freely (so long as they remain out-\nside the margin region) without changing the decision boundary, and so the solution\nwill be independent of such data points.\n7.1.1 Overlapping class distributions\nSo far, we have assumed that the training data points are linearly separable in the\nfeature space φ(x). The resulting support vector machine will give exact separation\nof the training data in the original input spacex, although the corresponding decision\nboundary will be nonlinear. In practice, however, the class-conditional distributions\nmay overlap, in which case exact separation of the training data can lead to poor\ngeneralization.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 350,
      "page_label": "331"
    }
  },
  {
    "page_content": "may overlap, in which case exact separation of the training data can lead to poor\ngeneralization.\nWe therefore need a way to modify the support vector machine so as to allow\nsome of the training points to be misclassiﬁed. From (7.19) we see that in the case\nof separable classes, we implicitly used an error function that gave inﬁnite error\nif a data point was misclassiﬁed and zero error if it was classiﬁed correctly, and\nthen optimized the model parameters to maximize the margin. We now modify this\napproach so that data points are allowed to be on the ‘wrong side’ of the margin\nboundary, but with a penalty that increases with the distance from that boundary. For\nthe subsequent optimization problem, it is convenient to make this penalty a linear\nfunction of this distance. To do this, we introduce slack variables, ξn ⩾ 0 where\nn =1 ,...,N , with one slack variable for each training data point (Bennett, 1992;\nCortes and Vapnik, 1995). These are deﬁned byξn =0 for data points that are on or\ninside the correct margin boundary and ξn = |tn − y(xn)| for other points. Thus a\ndata point that is on the decision boundary y(xn)=0 will have ξn =1 , and points",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 350,
      "page_label": "331"
    }
  },
  {
    "page_content": "332 7. SPARSE KERNEL MACHINES\nFigure 7.3 Illustration of the slack variables ξn ⩾ 0.\nData points with circles around them are\nsupport vectors.\ny =1\ny =0\ny = −1\nξ> 1\nξ< 1\nξ =0\nξ =0\nwith ξn > 1 will be misclassiﬁed. The exact classiﬁcation constraints (7.5) are then\nreplaced with\ntny(xn) ⩾ 1 − ξn,n =1 ,...,N (7.20)\nin which the slack variables are constrained to satisfy ξn ⩾ 0. Data points for which\nξn =0 are correctly classiﬁed and are either on the margin or on the correct side\nof the margin. Points for which 0 <ξ n ⩽ 1 lie inside the margin, but on the cor-\nrect side of the decision boundary, and those data points for which ξn > 1 lie on\nthe wrong side of the decision boundary and are misclassiﬁed, as illustrated in Fig-\nure 7.3. This is sometimes described as relaxing the hard margin constraint to give a\nsoft marginand allows some of the training set data points to be misclassiﬁed. Note\nthat while slack variables allow for overlapping class distributions, this framework is\nstill sensitive to outliers because the penalty for misclassiﬁcation increases linearly\nwith ξ.\nOur goal is now to maximize the margin while softly penalizing points that lie\non the wrong side of the margin boundary. We therefore minimize\nC\nN∑\nn=1\nξn + 1\n2∥w∥2 (7.21)\nwhere the parameter C> 0 controls the trade-off between the slack variable penalty\nand the margin. Because any point that is misclassiﬁed has ξn > 1, it follows that∑",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 351,
      "page_label": "332"
    }
  },
  {
    "page_content": "and the margin. Because any point that is misclassiﬁed has ξn > 1, it follows that∑\nn ξn is an upper bound on the number of misclassiﬁed points. The parameter C is\ntherefore analogous to (the inverse of) a regularization coefﬁcient because it controls\nthe trade-off between minimizing training errors and controlling model complexity.\nIn the limit C →∞ , we will recover the earlier support vector machine for separable\ndata.\nWe now wish to minimize (7.21) subject to the constraints (7.20) together with\nξn ⩾ 0. The corresponding Lagrangian is given by\nL(w,b ,a)= 1\n2∥w∥2 +C\nN∑\nn=1\nξn −\nN∑\nn=1\nan {tny(xn) − 1+ ξn}−\nN∑\nn=1\nµnξn (7.22)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 351,
      "page_label": "332"
    }
  },
  {
    "page_content": "7.1. Maximum Margin Classiﬁers 333\nwhere {an ⩾ 0} and {µn ⩾ 0} are Lagrange multipliers. The corresponding set of\nKKT conditions are given byAppendix E\nan ⩾ 0 (7.23)\ntny(xn) − 1+ ξn ⩾ 0 (7.24)\nan (tny(xn) − 1+ ξn)=0 (7.25)\nµn ⩾ 0 (7.26)\nξn ⩾ 0 (7.27)\nµnξn =0 (7.28)\nwhere n =1 ,...,N .\nWe now optimize out w, b, and {ξn} making use of the deﬁnition (7.1) of y(x)\nto give\n∂L\n∂w =0 ⇒ w =\nN∑\nn=1\nantnφ(xn) (7.29)\n∂L\n∂b =0 ⇒\nN∑\nn=1\nantn =0 (7.30)\n∂L\n∂ξn\n=0 ⇒ an = C − µn. (7.31)\nUsing these results to eliminate w, b, and {ξn} from the Lagrangian, we obtain the\ndual Lagrangian in the form\n˜L(a)=\nN∑\nn=1\nan − 1\n2\nN∑\nn=1\nN∑\nm=1\nanamtntmk(xn, xm) (7.32)\nwhich is identical to the separable case, except that the constraints are somewhat\ndifferent. To see what these constraints are, we note that an ⩾ 0 is required because\nthese are Lagrange multipliers. Furthermore, (7.31) together with µn ⩾ 0 implies\nan ⩽ C. We therefore have to minimize (7.32) with respect to the dual variables\n{an} subject to\n0 ⩽ an ⩽ C (7.33)\nN∑\nn=1\nantn =0 (7.34)\nfor n =1 ,...,N , where (7.33) are known as box constraints. This again represents\na quadratic programming problem. If we substitute (7.29) into (7.1), we see that\npredictions for new data points are again made by using (7.13).\nWe can now interpret the resulting solution. As before, a subset of the data\npoints may have an =0 , in which case they do not contribute to the predictive",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 352,
      "page_label": "333"
    }
  },
  {
    "page_content": "334 7. SPARSE KERNEL MACHINES\nmodel (7.13). The remaining data points constitute the support vectors. These have\nan > 0 and hence from (7.25) must satisfy\ntny(xn)=1 − ξn. (7.35)\nIf an <C , then (7.31) implies that µn > 0, which from (7.28) requires ξn =0 and\nhence such points lie on the margin. Points with an = C can lie inside the margin\nand can either be correctly classiﬁed if ξn ⩽ 1 or misclassiﬁed if ξn > 1.\nTo determine the parameter b in (7.1), we note that those support vectors for\nwhich 0 <a n <C have ξn =0 so that tny(xn)=1 and hence will satisfy\ntn\n( ∑\nm∈S\namtmk(xn, xm)+ b\n)\n=1 . (7.36)\nAgain, a numerically stable solution is obtained by averaging to give\nb = 1\nNM\n∑\nn∈M\n(\ntn −\n∑\nm∈S\namtmk(xn, xm)\n)\n(7.37)\nwhere M denotes the set of indices of data points having 0 <a n <C .\nAn alternative, equivalent formulation of the support vector machine, known as\nthe ν-SVM, has been proposed by Sch¨olkopf et al.(2000). This involves maximizing\n˜L(a)= −1\n2\nN∑\nn=1\nN∑\nm=1\nanamtntmk(xn, xm) (7.38)\nsubject to the constraints\n0 ⩽ an ⩽ 1/N (7.39)\nN∑\nn=1\nantn =0 (7.40)\nN∑\nn=1\nan ⩾ ν. (7.41)\nThis approach has the advantage that the parameter ν, which replaces C, can be\ninterpreted as both an upper bound on the fraction ofmargin errors(points for which\nξn > 0 and hence which lie on the wrong side of the margin boundary and which may\nor may not be misclassiﬁed) and a lower bound on the fraction of support vectors. An",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 353,
      "page_label": "334"
    }
  },
  {
    "page_content": "or may not be misclassiﬁed) and a lower bound on the fraction of support vectors. An\nexample of theν-SVM applied to a synthetic data set is shown in Figure 7.4. Here\nGaussian kernels of the form exp (−γ∥x − x′∥2) have been used, with γ =0 .45.\nAlthough predictions for new inputs are made using only the support vectors,\nthe training phase (i.e., the determination of the parameters a and b) makes use of\nthe whole data set, and so it is important to have efﬁcient algorithms for solving",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 353,
      "page_label": "334"
    }
  },
  {
    "page_content": "7.1. Maximum Margin Classiﬁers 335\nFigure 7.4 Illustration of the ν-SVM applied\nto a nonseparable data set in two\ndimensions. The support vectors\nare indicated by circles.\n−2 0 2\n−2\n0\n2\nthe quadratic programming problem. We ﬁrst note that the objective function ˜L(a)\ngiven by (7.10) or (7.32) is quadratic and so any local optimum will also be a global\noptimum provided the constraints deﬁne a convex region (which they do as a conse-\nquence of being linear). Direct solution of the quadratic programming problem us-\ning traditional techniques is often infeasible due to the demanding computation and\nmemory requirements, and so more practical approaches need to be found. The tech-\nnique of chunking (Vapnik, 1982) exploits the fact that the value of the Lagrangian\nis unchanged if we remove the rows and columns of the kernel matrix corresponding\nto Lagrange multipliers that have value zero. This allows the full quadratic pro-\ngramming problem to be broken down into a series of smaller ones, whose goal is\neventually to identify all of the nonzero Lagrange multipliers and discard the others.\nChunking can be implemented using protected conjugate gradients(Burges, 1998).\nAlthough chunking reduces the size of the matrix in the quadratic function from the\nnumber of data points squared to approximately the number of nonzero Lagrange\nmultipliers squared, even this may be too big to ﬁt in memory for large-scale appli-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 354,
      "page_label": "335"
    }
  },
  {
    "page_content": "multipliers squared, even this may be too big to ﬁt in memory for large-scale appli-\ncations. Decomposition methods (Osuna et al., 1996) also solve a series of smaller\nquadratic programming problems but are designed so that each of these is of a ﬁxed\nsize, and so the technique can be applied to arbitrarily large data sets. However, it\nstill involves numerical solution of quadratic programming subproblems and these\ncan be problematic and expensive. One of the most popular approaches to training\nsupport vector machines is called sequential minimal optimization,o r SMO (Platt,\n1999). It takes the concept of chunking to the extreme limit and considers just two\nLagrange multipliers at a time. In this case, the subproblem can be solved analyti-\ncally, thereby avoiding numerical quadratic programming altogether. Heuristics are\ngiven for choosing the pair of Lagrange multipliers to be considered at each step.\nIn practice, SMO is found to have a scaling with the number of data points that is\nsomewhere between linear and quadratic depending on the particular application.\nWe have seen that kernel functions correspond to inner products in feature spaces\nthat can have high, or even inﬁnite, dimensionality. By working directly in terms of\nthe kernel function, without introducing the feature space explicitly, it might there-\nfore seem that support vector machines somehow manage to avoid the curse of di-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 354,
      "page_label": "335"
    }
  },
  {
    "page_content": "336 7. SPARSE KERNEL MACHINES\nmensionality. This is not the case, however, because there are constraints amongstSection 1.4\nthe feature values that restrict the effective dimensionality of feature space. To see\nthis consider a simple second-order polynomial kernel that we can expand in terms\nof its components\nk(x,z)=\n(\n1+ xTz\n)2\n=( 1+ x1z1 + x2z2)2\n=1 + 2 x1z1 +2 x2z2 + x2\n1z2\n1 +2 x1z1x2z2 + x2\n2z2\n2\n=( 1 ,\n√\n2x1,\n√\n2x2,x2\n1,\n√\n2x1x2,x2\n2)(1,\n√\n2z1,\n√\n2z2,z 2\n1,\n√\n2z1z2,z 2\n2)T\n= φ(x)Tφ(z). (7.42)\nThis kernel function therefore represents an inner product in a feature space having\nsix dimensions, in which the mapping from input space to feature space is described\nby the vector functionφ(x). However, the coefﬁcients weighting these different\nfeatures are constrained to have speciﬁc forms. Thus any set of points in the original\ntwo-dimensional space x would be constrained to lie exactly on a two-dimensional\nnonlinear manifold embedded in the six-dimensional feature space.\nWe have already highlighted the fact that the support vector machine does not\nprovide probabilistic outputs but instead makes classiﬁcation decisions for new in-\nput vectors. Veropoulos et al. (1999) discuss modiﬁcations to the SVM to allow\nthe trade-off between false positive and false negative errors to be controlled. How-\never, if we wish to use the SVM as a module in a larger probabilistic system, then\nprobabilistic predictions of the class label t for new inputs x are required.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 355,
      "page_label": "336"
    }
  },
  {
    "page_content": "probabilistic predictions of the class label t for new inputs x are required.\nTo address this issue, Platt (2000) has proposed ﬁtting a logistic sigmoid to the\noutputs of a previously trained support vector machine. Speciﬁcally, the required\nconditional probability is assumed to be of the form\np(t =1 |x)= σ (Ay(x)+ B) (7.43)\nwhere y(x) is deﬁned by (7.1). Values for the parameters A and B are found by\nminimizing the cross-entropy error function deﬁned by a training set consisting of\npairs of valuesy(xn) and tn. The data used to ﬁt the sigmoid needs to be independent\nof that used to train the original SVM in order to avoid severe over-ﬁtting. This two-\nstage approach is equivalent to assuming that the output y(x) of the support vector\nmachine represents the log-odds of x belonging to class t =1 . Because the SVM\ntraining procedure is not speciﬁcally intended to encourage this, the SVM can give\na poor approximation to the posterior probabilities (Tipping, 2001).\n7.1.2 Relation to logistic regression\nAs with the separable case, we can re-cast the SVM for nonseparable distri-\nbutions in terms of the minimization of a regularized error function. This will also\nallow us to highlight similarities, and differences, compared to the logistic regression\nmodel.Section 4.3.2\nWe have seen that for data points that are on the correct side of the margin\nboundary, and which therefore satisfy yntn ⩾ 1,w eh a v e ξn =0 , and for the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 355,
      "page_label": "336"
    }
  },
  {
    "page_content": "7.1. Maximum Margin Classiﬁers 337\nFigure 7.5 Plot of the ‘hinge’ error function used\nin support vector machines, shown\nin blue, along with the error function\nfor logistic regression, rescaled by a\nfactor of 1/ ln(2) so that it passes\nthrough the point (0, 1), shown in red.\nAlso shown are the misclassiﬁcation\nerror in black and the squared error\nin green.\n−2 −1 012 z\nE(z)\nremaining points we have ξn =1 − yntn. Thus the objective function (7.21) can be\nwritten (up to an overall multiplicative constant) in the form\nN∑\nn=1\nESV(yntn)+ λ∥w∥2 (7.44)\nwhere λ =( 2C)−1, and ESV(·) is the hinge error function deﬁned by\nESV(yntn)=[ 1 − yntn]+ (7.45)\nwhere [ ·]+ denotes the positive part. The hinge error function, so-called because\nof its shape, is plotted in Figure 7.5. It can be viewed as an approximation to the\nmisclassiﬁcation error, i.e., the error function that ideally we would like to minimize,\nwhich is also shown in Figure 7.5.\nWhen we considered the logistic regression model in Section 4.3.2, we found it\nconvenient to work with target variable t ∈{ 0, 1}. For comparison with the support\nvector machine, we ﬁrst reformulate maximum likelihood logistic regression using\nthe target variable t ∈{ −1, 1}. To do this, we note that p(t =1 |y)= σ(y) where\ny(x) is given by (7.1), and σ(y) is the logistic sigmoid function deﬁned by (4.59). It\nfollows that p(t = −1|y)=1 − σ(y)= σ(−y), where we have used the properties\nof the logistic sigmoid function, and so we can write",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 356,
      "page_label": "337"
    }
  },
  {
    "page_content": "follows that p(t = −1|y)=1 − σ(y)= σ(−y), where we have used the properties\nof the logistic sigmoid function, and so we can write\np(t|y)= σ(yt). (7.46)\nFrom this we can construct an error function by taking the negative logarithm of the\nlikelihood function that, with a quadratic regularizer, takes the formExercise 7.6\nN∑\nn=1\nELR(yntn)+ λ∥w∥2. (7.47)\nwhere\nELR(yt)=l n( 1+e x p (−yt)). (7.48)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 356,
      "page_label": "337"
    }
  },
  {
    "page_content": "338 7. SPARSE KERNEL MACHINES\nFor comparison with other error functions, we can divide by ln(2) so that the error\nfunction passes through the point (0, 1). This rescaled error function is also plotted\nin Figure 7.5 and we see that it has a similar form to the support vector error function.\nThe key difference is that the ﬂat region in ESV(yt) leads to sparse solutions.\nBoth the logistic error and the hinge loss can be viewed as continuous approx-\nimations to the misclassiﬁcation error. Another continuous error function that has\nsometimes been used to solve classiﬁcation problems is the squared error, which\nis again plotted in Figure 7.5. It has the property, however, of placing increasing\nemphasis on data points that are correctly classiﬁed but that are a long way from\nthe decision boundary on the correct side. Such points will be strongly weighted at\nthe expense of misclassiﬁed points, and so if the objective is to minimize the mis-\nclassiﬁcation rate, then a monotonically decreasing error function would be a better\nchoice.\n7.1.3 Multiclass SVMs\nThe support vector machine is fundamentally a two-class classiﬁer. In practice,\nhowever, we often have to tackle problems involving K> 2 classes. Various meth-\nods have therefore been proposed for combining multiple two-class SVMs in order\nto build a multiclass classiﬁer.\nOne commonly used approach (Vapnik, 1998) is to constructK separate SVMs,\nin which the kth model yk(x) is trained using the data from class Ck as the positive",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 357,
      "page_label": "338"
    }
  },
  {
    "page_content": "in which the kth model yk(x) is trained using the data from class Ck as the positive\nexamples and the data from the remaining K − 1 classes as the negative examples.\nThis is known as the one-versus-the-rest approach. However, in Figure 4.2 we saw\nthat using the decisions of the individual classiﬁers can lead to inconsistent results\nin which an input is assigned to multiple classes simultaneously. This problem is\nsometimes addressed by making predictions for new inputsx using\ny(x)=m a x\nk\nyk(x). (7.49)\nUnfortunately, this heuristic approach suffers from the problem that the different\nclassiﬁers were trained on different tasks, and there is no guarantee that the real-\nvalued quantities yk(x) for different classiﬁers will have appropriate scales.\nAnother problem with the one-versus-the-rest approach is that the training sets\nare imbalanced. For instance, if we have ten classes each with equal numbers of\ntraining data points, then the individual classiﬁers are trained on data sets comprising\n90% negative examples and only 10% positive examples, and the symmetry of the\noriginal problem is lost. A variant of the one-versus-the-rest scheme was proposed\nby Lee et al. (2001) who modify the target values so that the positive class has target\n+1 and the negative class has target −1/(K − 1).\nWeston and Watkins (1999) deﬁne a single objective function for training all\nK SVMs simultaneously, based on maximizing the margin from each to remaining",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 357,
      "page_label": "338"
    }
  },
  {
    "page_content": "K SVMs simultaneously, based on maximizing the margin from each to remaining\nclasses. However, this can result in much slower training because, instead of solving\nK separate optimization problems each over N data points with an overall cost of\nO(KN 2), a single optimization problem of size(K −1)N must be solved giving an\noverall cost of O(K2N2).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 357,
      "page_label": "338"
    }
  },
  {
    "page_content": "7.1. Maximum Margin Classiﬁers 339\nAnother approach is to trainK(K −1)/2 different 2-class SVMs on all possible\npairs of classes, and then to classify test points according to which class has the high-\nest number of ‘votes’, an approach that is sometimes calledone-versus-one. Again,\nwe saw in Figure 4.2 that this can lead to ambiguities in the resulting classiﬁcation.\nAlso, for large K this approach requires signiﬁcantly more training time than the\none-versus-the-rest approach. Similarly, to evaluate test points, signiﬁcantly more\ncomputation is required.\nThe latter problem can be alleviated by organizing the pairwise classiﬁers into\na directed acyclic graph (not to be confused with a probabilistic graphical model)\nleading to the DAGSVM (Platt et al., 2000). For K classes, the DAGSVM has a total\nof K(K − 1)/2 classiﬁers, and to classify a new test point only K − 1 pairwise\nclassiﬁers need to be evaluated, with the particular classiﬁers used depending on\nwhich path through the graph is traversed.\nA different approach to multiclass classiﬁcation, based on error-correcting out-\nput codes, was developed by Dietterich and Bakiri (1995) and applied to support\nvector machines by Allwein et al. (2000). This can be viewed as a generalization of\nthe voting scheme of the one-versus-one approach in which more general partitions\nof the classes are used to train the individual classiﬁers. The K classes themselves",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 358,
      "page_label": "339"
    }
  },
  {
    "page_content": "of the classes are used to train the individual classiﬁers. The K classes themselves\nare represented as particular sets of responses from the two-class classiﬁers chosen,\nand together with a suitable decoding scheme, this gives robustness to errors and to\nambiguity in the outputs of the individual classiﬁers. Although the application of\nSVMs to multiclass classiﬁcation problems remains an open issue, in practice the\none-versus-the-rest approach is the most widely used in spite of its ad-hoc formula-\ntion and its practical limitations.\nThere are also single-class support vector machines, which solve an unsuper-\nvised learning problem related to probability density estimation. Instead of mod-\nelling the density of data, however, these methods aim to ﬁnd a smooth boundary\nenclosing a region of high density. The boundary is chosen to represent a quantile of\nthe density, that is, the probability that a data point drawn from the distribution will\nland inside that region is given by a ﬁxed number between 0 and 1 that is speciﬁed in\nadvance. This is a more restricted problem than estimating the full density but may\nbe sufﬁcient in speciﬁc applications. Two approaches to this problem using support\nvector machines have been proposed. The algorithm of Sch¨olkopf et al. (2001) tries\nto ﬁnd a hyperplane that separates all but a ﬁxed fraction ν of the training data from\nthe origin while at the same time maximizing the distance (margin) of the hyperplane",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 358,
      "page_label": "339"
    }
  },
  {
    "page_content": "the origin while at the same time maximizing the distance (margin) of the hyperplane\nfrom the origin, while Tax and Duin (1999) look for the smallest sphere in feature\nspace that contains all but a fractionν of the data points. For kernels k(x,x′) that\nare functions only of x − x′, the two algorithms are equivalent.\n7.1.4 SVMs for regression\nWe now extend support vector machines to regression problems while at the\nsame time preserving the property of sparseness. In simple linear regression, weSection 3.1.4",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 358,
      "page_label": "339"
    }
  },
  {
    "page_content": "340 7. SPARSE KERNEL MACHINES\nFigure 7.6 Plot of an ϵ-insensitive error function (in\nred) in which the error increases lin-\nearly with distance beyond the insen-\nsitive region. Also shown for compar-\nison is the quadratic error function (in\ngreen).\n0 z\nE(z)\n−ϵ ϵ\nminimize a regularized error function given by\n1\n2\nN∑\nn=1\n{yn − tn}2 + λ\n2∥w∥2. (7.50)\nTo obtain sparse solutions, the quadratic error function is replaced by anϵ-insensitive\nerror function (Vapnik, 1995), which gives zero error if the absolute difference be-\ntween the prediction y(x) and the target t is less than ϵ where ϵ> 0. A simple\nexample of an ϵ-insensitive error function, having a linear cost associated with errors\noutside the insensitive region, is given by\nEϵ(y(x) − t)=\n{\n0, if |y(x) − t| <ϵ ;\n|y(x) − t|− ϵ, otherwise (7.51)\nand is illustrated in Figure 7.6.\nWe therefore minimize a regularized error function given by\nC\nN∑\nn=1\nEϵ(y(xn) − tn)+ 1\n2∥w∥2 (7.52)\nwhere y(x) is given by (7.1). By convention the (inverse) regularization parameter,\ndenoted C, appears in front of the error term.\nAs before, we can re-express the optimization problem by introducing slack\nvariables. For each data point xn, we now need two slack variables ξn ⩾ 0 and\nˆξn ⩾ 0, where ξn > 0 corresponds to a point for which tn >y (xn)+ ϵ, and ˆξn > 0\ncorresponds to a point for which tn <y (xn) − ϵ, as illustrated in Figure 7.7.\nThe condition for a target point to lie inside the ϵ-tube is that yn − ϵ ⩽ tn ⩽",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 359,
      "page_label": "340"
    }
  },
  {
    "page_content": "The condition for a target point to lie inside the ϵ-tube is that yn − ϵ ⩽ tn ⩽\nyn+ϵ, where yn = y(xn). Introducing the slack variables allows points to lie outside\nthe tube provided the slack variables are nonzero, and the corresponding conditions\nare\ntn ⩽ y(xn)+ ϵ + ξn (7.53)\ntn ⩾ y(xn) − ϵ −ˆξn. (7.54)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 359,
      "page_label": "340"
    }
  },
  {
    "page_content": "7.1. Maximum Margin Classiﬁers 341\nFigure 7.7 Illustration of SVM regression, showing\nthe regression curve together with the ϵ-\ninsensitive ‘tube’. Also shown are exam-\nples of the slack variables ξ and bξ. Points\nabove the ϵ-tube have ξ> 0 and bξ =0 ,\npoints below the ϵ-tube have ξ =0 and\nbξ> 0, and points inside the ϵ-tube have\nξ = bξ =0 .\ny\ny + ϵ\ny − ϵ\ny(x)\nx\nˆξ> 0\nξ> 0\nThe error function for support vector regression can then be written as\nC\nN∑\nn=1\n(ξn + ˆξn)+ 1\n2∥w∥2 (7.55)\nwhich must be minimized subject to the constraints ξn ⩾ 0 and ˆξn ⩾ 0 as well as\n(7.53) and (7.54). This can be achieved by introducing Lagrange multipliersan ⩾ 0,\nˆan ⩾ 0, µn ⩾ 0, and ˆµn ⩾ 0 and optimizing the Lagrangian\nL = C\nN∑\nn=1\n(ξn + ˆξn)+ 1\n2∥w∥2 −\nN∑\nn=1\n(µnξn + ˆµnˆξn)\n−\nN∑\nn=1\nan(ϵ + ξn + yn − tn) −\nN∑\nn=1\nˆan(ϵ + ˆξn − yn + tn). (7.56)\nWe now substitute for y(x) using (7.1) and then set the derivatives of the La-\ngrangian with respect to w, b, ξn, and ˆξn to zero, giving\n∂L\n∂w =0 ⇒ w =\nN∑\nn=1\n(an − ˆan)φ(xn) (7.57)\n∂L\n∂b =0 ⇒\nN∑\nn=1\n(an − ˆan)=0 (7.58)\n∂L\n∂ξn\n=0 ⇒ an + µn = C (7.59)\n∂L\n∂ˆξn\n=0 ⇒ ˆan + ˆµn = C. (7.60)\nUsing these results to eliminate the corresponding variables from the Lagrangian, we\nsee that the dual problem involves maximizingExercise 7.7",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 360,
      "page_label": "341"
    }
  },
  {
    "page_content": "342 7. SPARSE KERNEL MACHINES\n˜L(a,ˆa)= −1\n2\nN∑\nn=1\nN∑\nm=1\n(an − ˆan)(am − ˆam)k(xn, xm)\n−ϵ\nN∑\nn=1\n(an + ˆan)+\nN∑\nn=1\n(an − ˆan)tn (7.61)\nwith respect to {an} and {ˆan}, where we have introduced the kernel k(x, x′)=\nφ(x)Tφ(x′). Again, this is a constrained maximization, and to ﬁnd the constraints\nwe note that an ⩾ 0 and ˆan ⩾ 0 are both required because these are Lagrange\nmultipliers. Also µn ⩾ 0 and ˆµn ⩾ 0 together with (7.59) and (7.60), require\nan ⩽ C and ˆan ⩽ C, and so again we have the box constraints\n0 ⩽ an ⩽ C (7.62)\n0 ⩽ ˆan ⩽ C (7.63)\ntogether with the condition (7.58).\nSubstituting (7.57) into (7.1), we see that predictions for new inputs can be made\nusing\ny(x)=\nN∑\nn=1\n(an − ˆan)k(x,xn)+ b (7.64)\nwhich is again expressed in terms of the kernel function.\nThe corresponding Karush-Kuhn-Tucker (KKT) conditions, which state that at\nthe solution the product of the dual variables and the constraints must vanish, are\ngiven by\nan(ϵ + ξn + yn − tn)=0 (7.65)\nˆan(ϵ + ˆξn − yn + tn)=0 (7.66)\n(C − an)ξn =0 (7.67)\n(C − ˆan)ˆξn =0 . (7.68)\nFrom these we can obtain several useful results. First of all, we note that a coefﬁcient\nan can only be nonzero if ϵ + ξn + yn − tn =0 , which implies that the data point\neither lies on the upper boundary of the ϵ-tube (ξn =0 ) or lies above the upper\nboundary (ξn > 0). Similarly, a nonzero value for ˆan implies ϵ+ˆξn − yn + tn =0 ,\nand such points must lie either on or below the lower boundary of the ϵ-tube.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 361,
      "page_label": "342"
    }
  },
  {
    "page_content": "and such points must lie either on or below the lower boundary of the ϵ-tube.\nFurthermore, the two constraints ϵ+ξn +yn −tn =0 and ϵ+ˆξn −yn +tn =0\nare incompatible, as is easily seen by adding them together and noting that ξn and\nˆξn are nonnegative while ϵ is strictly positive, and so for every data point xn, either\nan or ˆan (or both) must be zero.\nThe support vectors are those data points that contribute to predictions given by\n(7.64), in other words those for which eitheran ̸=0or ˆan ̸=0. These are points that\nlie on the boundary of the ϵ-tube or outside the tube. All points within the tube have",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 361,
      "page_label": "342"
    }
  },
  {
    "page_content": "7.1. Maximum Margin Classiﬁers 343\nan = ˆan =0 . We again have a sparse solution, and the only terms that have to be\nevaluated in the predictive model (7.64) are those that involve the support vectors.\nThe parameter b can be found by considering a data point for which 0 <a n <\nC, which from (7.67) must have ξn =0 , and from (7.65) must therefore satisfy\nϵ + yn − tn =0 . Using (7.1) and solving for b, we obtain\nb = tn − ϵ − wTφ(xn)\n= tn − ϵ −\nN∑\nm=1\n(am − ˆam)k(xn, xm) (7.69)\nwhere we have used (7.57). We can obtain an analogous result by considering a point\nfor which 0 < ˆan <C . In practice, it is better to average over all such estimates of\nb.\nAs with the classiﬁcation case, there is an alternative formulation of the SVM\nfor regression in which the parameter governing complexity has a more intuitive\ninterpretation (Sch¨olkopf et al., 2000). In particular, instead of ﬁxing the width ϵ of\nthe insensitive region, we ﬁx instead a parameterν that bounds the fraction of points\nlying outside the tube. This involves maximizing\n˜L(a,ˆa)= −1\n2\nN∑\nn=1\nN∑\nm=1\n(an − ˆan)(am − ˆam)k(xn, xm)\n+\nN∑\nn=1\n(an − ˆan)tn (7.70)\nsubject to the constraints\n0 ⩽ an ⩽ C/N (7.71)\n0 ⩽ ˆan ⩽ C/N (7.72)\nN∑\nn=1\n(an − ˆan)=0 (7.73)\nN∑\nn=1\n(an + ˆan) ⩽ νC. (7.74)\nIt can be shown that there are at most νN data points falling outside the insensitive\ntube, while at least νN data points are support vectors and so lie either on the tube\nor outside it.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 362,
      "page_label": "343"
    }
  },
  {
    "page_content": "tube, while at least νN data points are support vectors and so lie either on the tube\nor outside it.\nThe use of a support vector machine to solve a regression problem is illustrated\nusing the sinusoidal data set in Figure 7.8. Here the parameters ν and C have beenAppendix A\nchosen by hand. In practice, their values would typically be determined by cross-\nvalidation.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 362,
      "page_label": "343"
    }
  },
  {
    "page_content": "344 7. SPARSE KERNEL MACHINES\nFigure 7.8 Illustration of the ν-SVM for re-\ngression applied to the sinusoidal\nsynthetic data set using Gaussian\nkernels. The predicted regression\ncurve is shown by the red line, and\nthe ϵ-insensitive tube corresponds\nto the shaded region. Also, the\ndata points are shown in green,\nand those with support vectors\nare indicated by blue circles.\nx\nt\n0 1\n−1\n0\n1\n7.1.5 Computational learning theory\nHistorically, support vector machines have largely been motivated and analysed\nusing a theoretical framework known as computational learning theory, also some-\ntimes called statistical learning theory(Anthony and Biggs, 1992; Kearns and Vazi-\nrani, 1994; Vapnik, 1995; Vapnik, 1998). This has its origins with Valiant (1984)\nwho formulated the probably approximately correct, or PAC, learning framework.\nThe goal of the PAC framework is to understand how large a data set needs to be in\norder to give good generalization. It also gives bounds for the computational cost of\nlearning, although we do not consider these here.\nSuppose that a data setD of size N is drawn from some joint distributionp(x, t)\nwhere x is the input variable and t represents the class label, and that we restrict\nattention to ‘noise free’ situations in which the class labels are determined by some\n(unknown) deterministic function t = g(x). In PAC learning we say that a function\nf(x;D), drawn from a space F of such functions on the basis of the training set",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 363,
      "page_label": "344"
    }
  },
  {
    "page_content": "f(x;D), drawn from a space F of such functions on the basis of the training set\nD, has good generalization if its expected error rate is below some pre-speciﬁed\nthreshold ϵ, so that\nEx,t [I (f(x;D) ̸= t)] <ϵ (7.75)\nwhere I(·) is the indicator function, and the expectation is with respect to the dis-\ntribution p(x, t). The quantity on the left-hand side is a random variable, because\nit depends on the training set D, and the PAC framework requires that (7.75) holds,\nwith probability greater than 1 − δ, for a data set D drawn randomly from p(x, t).\nHere δ is another pre-speciﬁed parameter, and the terminology ‘probably approxi-\nmately correct’ comes from the requirement that with high probability (greater than\n1−δ), the error rate be small (less thanϵ). For a given choice of model spaceF, and\nfor given parameters ϵ and δ, PAC learning aims to provide bounds on the minimum\nsize N of data set needed to meet this criterion. A key quantity in PAC learning is\nthe V apnik-Chervonenkis dimension, or VC dimension, which provides a measure of\nthe complexity of a space of functions, and which allows the PAC framework to be\nextended to spaces containing an inﬁnite number of functions.\nThe bounds derived within the PAC framework are often described as worst-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 363,
      "page_label": "344"
    }
  },
  {
    "page_content": "7.2. Relevance Vector Machines 345\ncase, because they apply to any choice for the distribution p(x, t), so long as both\nthe training and the test examples are drawn (independently) from the same distribu-\ntion, and forany choice for the functionf(x) so long as it belongs toF. In real-world\napplications of machine learning, we deal with distributions that have signiﬁcant reg-\nularity, for example in which large regions of input space carry the same class label.\nAs a consequence of the lack of any assumptions about the form of the distribution,\nthe PAC bounds are very conservative, in other words they strongly over-estimate\nthe size of data sets required to achieve a given generalization performance. For this\nreason, PAC bounds have found few, if any, practical applications.\nOne attempt to improve the tightness of the PAC bounds is the PAC-Bayesian\nframework (McAllester, 2003), which considers a distribution over the space F of\nfunctions, somewhat analogous to the prior in a Bayesian treatment. This still con-\nsiders any possible choice for p(x, t), and so although the bounds are tighter, they\nare still very conservative.\n7.2. Relevance Vector Machines\nSupport vector machines have been used in a variety of classiﬁcation and regres-\nsion applications. Nevertheless, they suffer from a number of limitations, several\nof which have been highlighted already in this chapter. In particular, the outputs of\nan SVM represent decisions rather than posterior probabilities. Also, the SVM was",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 364,
      "page_label": "345"
    }
  },
  {
    "page_content": "an SVM represent decisions rather than posterior probabilities. Also, the SVM was\noriginally formulated for two classes, and the extension to K> 2 classes is prob-\nlematic. There is a complexity parameterC,o r ν (as well as a parameterϵ in the case\nof regression), that must be found using a hold-out method such as cross-validation.\nFinally, predictions are expressed as linear combinations of kernel functions that are\ncentred on training data points and that are required to be positive deﬁnite.\nThe relevance vector machineor RVM (Tipping, 2001) is a Bayesian sparse ker-\nnel technique for regression and classiﬁcation that shares many of the characteristics\nof the SVM whilst avoiding its principal limitations. Additionally, it typically leads\nto much sparser models resulting in correspondingly faster performance on test data\nwhilst maintaining comparable generalization error.\nIn contrast to the SVM we shall ﬁnd it more convenient to introduce the regres-\nsion form of the RVM ﬁrst and then consider the extension to classiﬁcation tasks.\n7.2.1 RVM for regression\nThe relevance vector machine for regression is a linear model of the form studied\nin Chapter 3 but with a modiﬁed prior that results in sparse solutions. The model\ndeﬁnes a conditional distribution for a real-valued target variable t, given an input\nvector x, which takes the form\np(t|x, w,β )= N(t|y(x),β −1) (7.76)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 364,
      "page_label": "345"
    }
  },
  {
    "page_content": "346 7. SPARSE KERNEL MACHINES\nwhere β = σ−2 is the noise precision (inverse noise variance), and the mean is given\nby a linear model of the form\ny(x)=\nM∑\ni=1\nwiφi(x)= wTφ(x) (7.77)\nwith ﬁxed nonlinear basis functions φi(x), which will typically include a constant\nterm so that the corresponding weight parameter represents a ‘bias’.\nThe relevance vector machine is a speciﬁc instance of this model, which is in-\ntended to mirror the structure of the support vector machine. In particular, the basis\nfunctions are given by kernels, with one kernel associated with each of the data\npoints from the training set. The general expression (7.77) then takes the SVM-like\nform\ny(x)=\nN∑\nn=1\nwnk(x,xn)+ b (7.78)\nwhere b is a bias parameter. The number of parameters in this case is M = N +1 ,\nand y(x) has the same form as the predictive model (7.64) for the SVM, except that\nthe coefﬁcients an are here denoted wn. It should be emphasized that the subsequent\nanalysis is valid for arbitrary choices of basis function, and for generality we shall\nwork with the form (7.77). In contrast to the SVM, there is no restriction to positive-\ndeﬁnite kernels, nor are the basis functions tied in either number or location to the\ntraining data points.\nSuppose we are given a set of N observations of the input vector x, which we\ndenote collectively by a data matrixX whose nth row is xT\nn with n =1 ,...,N . The\ncorresponding target values are given by t =( t1,...,t N )T. Thus, the likelihood\nfunction is given by",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 365,
      "page_label": "346"
    }
  },
  {
    "page_content": "n with n =1 ,...,N . The\ncorresponding target values are given by t =( t1,...,t N )T. Thus, the likelihood\nfunction is given by\np(t|X, w,β )=\nN∏\nn=1\np(tn|xn, w,β −1). (7.79)\nNext we introduce a prior distribution over the parameter vector w and as in\nChapter 3, we shall consider a zero-mean Gaussian prior. However, the key differ-\nence in the RVM is that we introduce a separate hyperparameterαi for each of the\nweight parameters wi instead of a single shared hyperparameter. Thus the weight\nprior takes the form\np(w|α)=\nM∏\ni=1\nN(wi|0,α −1\ni ) (7.80)\nwhere αi represents the precision of the corresponding parameter wi, and α denotes\n(α1,...,α M )T. We shall see that, when we maximize the evidence with respect\nto these hyperparameters, a signiﬁcant proportion of them go to inﬁnity, and the\ncorresponding weight parameters have posterior distributions that are concentrated\nat zero. The basis functions associated with these parameters therefore play no role",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 365,
      "page_label": "346"
    }
  },
  {
    "page_content": "7.2. Relevance Vector Machines 347\nin the predictions made by the model and so are effectively pruned out, resulting in\na sparse model.\nUsing the result (3.49) for linear regression models, we see that the posterior\ndistribution for the weights is again Gaussian and takes the form\np(w|t, X, α,β )= N(w|m, Σ) (7.81)\nwhere the mean and covariance are given by\nm = βΣΦTt (7.82)\nΣ =\n(\nA + βΦTΦ\n)−1\n(7.83)\nwhere Φ is the N × M design matrix with elements Φni = φi(xn), and A =\ndiag(αi). Note that in the speciﬁc case of the model (7.78), we have Φ = K, where\nK is the symmetric (N +1 )× (N +1 )kernel matrix with elements k(xn, xm).\nThe values of α and β are determined using type-2 maximum likelihood, also\nknown as the evidence approximation, in which we maximize the marginal likeli-Section 3.5\nhood function obtained by integrating out the weight parameters\np(t|X, α,β )=\n∫\np(t|X, w,β )p(w|α)d w. (7.84)\nBecause this represents the convolution of two Gaussians, it is readily evaluated toExercise 7.10\ngive the log marginal likelihood in the form\nln p(t|X, α,β )=l n N(t|0,C)\n= −1\n2\n{\nN ln(2π)+l n |C| + tTC−1t\n}\n(7.85)\nwhere t =( t1,...,t N )T, and we have deﬁned the N × N matrix C given by\nC = β−1I + ΦA−1ΦT. (7.86)\nOur goal is now to maximize (7.85) with respect to the hyperparameters α and\nβ. This requires only a small modiﬁcation to the results obtained in Section 3.5 for\nthe evidence approximation in the linear regression model. Again, we can identify",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 366,
      "page_label": "347"
    }
  },
  {
    "page_content": "the evidence approximation in the linear regression model. Again, we can identify\ntwo approaches. In the ﬁrst, we simply set the required derivatives of the marginal\nlikelihood to zero and obtain the following re-estimation equationsExercise 7.12\nαnew\ni = γi\nm2\ni\n(7.87)\n(βnew)−1 = ∥t − Φm∥2\nN − ∑\ni γi\n(7.88)\nwhere mi is the ith component of the posterior mean m deﬁned by (7.82). The\nquantity γi measures how well the corresponding parameter wi is determined by the\ndata and is deﬁned bySection 3.5.3",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 366,
      "page_label": "347"
    }
  },
  {
    "page_content": "348 7. SPARSE KERNEL MACHINES\nγi =1 − αiΣii (7.89)\nin which Σii is the ith diagonal component of the posterior covariance Σ given by\n(7.83). Learning therefore proceeds by choosing initial values for α and β, evalu-\nating the mean and covariance of the posterior using (7.82) and (7.83), respectively,\nand then alternately re-estimating the hyperparameters, using (7.87) and (7.88), and\nre-estimating the posterior mean and covariance, using (7.82) and (7.83), until a suit-\nable convergence criterion is satisﬁed.\nThe second approach is to use the EM algorithm, and is discussed in Sec-\ntion 9.3.4. These two approaches to ﬁnding the values of the hyperparameters that\nmaximize the evidence are formally equivalent. Numerically, however, it is foundExercise 9.23\nthat the direct optimization approach corresponding to (7.87) and (7.88) gives some-\nwhat faster convergence (Tipping, 2001).\nAs a result of the optimization, we ﬁnd that a proportion of the hyperparameters\n{αi} are driven to large (in principle inﬁnite) values, and so the weight parametersSection 7.2.2\nwi corresponding to these hyperparameters have posterior distributions with mean\nand variance both zero. Thus those parameters, and the corresponding basis func-\ntionsφi(x), are removed from the model and play no role in making predictions for\nnew inputs. In the case of models of the form (7.78), the inputs xn corresponding to\nthe remaining nonzero weights are called relevance vectors, because they are iden-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 367,
      "page_label": "348"
    }
  },
  {
    "page_content": "the remaining nonzero weights are called relevance vectors, because they are iden-\ntiﬁed through the mechanism of automatic relevance determination, and are analo-\ngous to the support vectors of an SVM. It is worth emphasizing, however, that this\nmechanism for achieving sparsity in probabilistic models through automatic rele-\nvance determination is quite general and can be applied to any model expressed as\nan adaptive linear combination of basis functions.\nHaving found values α⋆ and β⋆ for the hyperparameters that maximize the\nmarginal likelihood, we can evaluate the predictive distribution over t for a new\ninput x. Using (7.76) and (7.81), this is given byExercise 7.14\np(t|x, X, t, α⋆ ,β ⋆ )=\n∫\np(t|x, w,β ⋆ )p(w|X, t, α⋆ ,β ⋆ )d w\n= N\n(\nt|mTφ(x),σ 2(x)\n)\n. (7.90)\nThus the predictive mean is given by (7.76) with w set equal to the posterior mean\nm, and the variance of the predictive distribution is given by\nσ2(x)=( β⋆ )−1 + φ(x)TΣφ(x) (7.91)\nwhere Σ is given by (7.83) in whichα and β are set to their optimized valuesα⋆ and\nβ⋆ . This is just the familiar result (3.59) obtained in the context of linear regression.\nRecall that for localized basis functions, the predictive variance for linear regression\nmodels becomes small in regions of input space where there are no basis functions.\nIn the case of an RVM with the basis functions centred on data points, the model will\ntherefore become increasingly certain of its predictions when extrapolating outside",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 367,
      "page_label": "348"
    }
  },
  {
    "page_content": "therefore become increasingly certain of its predictions when extrapolating outside\nthe domain of the data (Rasmussen and Qui˜nonero-Candela, 2005), which of course\nis undesirable. The predictive distribution in Gaussian process regression does notSection 6.4.2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 367,
      "page_label": "348"
    }
  },
  {
    "page_content": "7.2. Relevance Vector Machines 349\nFigure 7.9 Illustration of RVM regression us-\ning the same data set, and the\nsame Gaussian kernel functions,\nas used in Figure 7.8 for the\nν-SVM regression model. The\nmean of the predictive distribu-\ntion for the RVM is shown by the\nred line, and the one standard-\ndeviation predictive distribution is\nshown by the shaded region.\nAlso, the data points are shown\nin green, and the relevance vec-\ntors are indicated by blue circles.\nNote that there are only 3 rele-\nvance vectors compared to 7 sup-\nport vectors for the ν-SVM in Fig-\nure 7.8.\nx\nt\n0 1\n−1\n0\n1\nsuffer from this problem. However, the computational cost of making predictions\nwith a Gaussian processes is typically much higher than with an RVM.\nFigure 7.9 shows an example of the RVM applied to the sinusoidal regression\ndata set. Here the noise precision parameter β is also determined through evidence\nmaximization. We see that the number of relevance vectors in the RVM is signif-\nicantly smaller than the number of support vectors used by the SVM. For a wide\nrange of regression and classiﬁcation tasks, the RVM is found to give models that\nare typically an order of magnitude more compact than the corresponding support\nvector machine, resulting in a signiﬁcant improvement in the speed of processing on\ntest data. Remarkably, this greater sparsity is achieved with little or no reduction in\ngeneralization error compared with the corresponding SVM.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 368,
      "page_label": "349"
    }
  },
  {
    "page_content": "test data. Remarkably, this greater sparsity is achieved with little or no reduction in\ngeneralization error compared with the corresponding SVM.\nThe principal disadvantage of the RVM compared to the SVM is that training\ninvolves optimizing a nonconvex function, and training times can be longer than for a\ncomparable SVM. For a model with M basis functions, the RVM requires inversion\nof a matrix of size M × M, which in general requires O(M3) computation. In the\nspeciﬁc case of the SVM-like model (7.78), we haveM = N +1. As we have noted,\nthere are techniques for training SVMs whose cost is roughly quadratic in N.O f\ncourse, in the case of the RVM we always have the option of starting with a smaller\nnumber of basis functions than N +1 . More signiﬁcantly, in the relevance vector\nmachine the parameters governing complexity and noise variance are determined\nautomatically from a single training run, whereas in the support vector machine the\nparameters C and ϵ (or ν) are generally found using cross-validation, which involves\nmultiple training runs. Furthermore, in the next section we shall derive an alternative\nprocedure for training the relevance vector machine that improves training speed\nsigniﬁcantly.\n7.2.2 Analysis of sparsity\nWe have noted earlier that the mechanism ofautomatic relevance determination\ncauses a subset of parameters to be driven to zero. We now examine in more detail",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 368,
      "page_label": "349"
    }
  },
  {
    "page_content": "350 7. SPARSE KERNEL MACHINES\nt1\nt2\nt\nC\nt1\nt2\nt\nC\nϕ\nFigure 7.10 Illustration of the mechanism for sparsity in a Bayesian linear regression model, showing a training\nset vector of target values given by t =( t1,t 2)T, indicated by the cross, for a model with one basis vector\nϕ =( φ(x1),φ (x2))T, which is poorly aligned with the target data vector t. On the left we see a model having\nonly isotropic noise, so that C = β−1I, corresponding to α = ∞, with β set to its most probable value. On\nthe right we see the same model but with a ﬁnite value of α. In each case the red ellipse corresponds to unit\nMahalanobis distance, with |C| taking the same value for both plots, while the dashed green circle shows the\ncontrition arising from the noise term β−1. We see that any ﬁnite value of α reduces the probability of the\nobserved data, and so for the most probable solution the basis vector is removed.\nthe mechanism of sparsity in the context of the relevance vector machine. In the\nprocess, we will arrive at a signiﬁcantly faster procedure for optimizing the hyper-\nparameters compared to the direct techniques given above.\nBefore proceeding with a mathematical analysis, we ﬁrst give some informal\ninsight into the origin of sparsity in Bayesian linear models. Consider a data set\ncomprising N =2 observations t1 and t2, together with a model having a single\nbasis function φ(x), with hyperparameter α, along with isotropic noise having pre-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 369,
      "page_label": "350"
    }
  },
  {
    "page_content": "basis function φ(x), with hyperparameter α, along with isotropic noise having pre-\ncision β. From (7.85), the marginal likelihood is given by p(t|α, β)= N(t|0,C) in\nwhich the covariance matrix takes the form\nC = 1\nβ I + 1\nαϕϕT (7.92)\nwhere ϕ denotes the N-dimensional vector (φ(x1),φ (x2))T, and similarly t =\n(t1,t 2)T. Notice that this is just a zero-mean Gaussian process model over t with\ncovariance C. Given a particular observation for t, our goal is to ﬁnd α⋆ and β⋆ by\nmaximizing the marginal likelihood. We see from Figure 7.10 that, if there is a poor\nalignment between the direction of ϕ and that of the training data vector t, then the\ncorresponding hyperparameter α will be driven to ∞, and the basis vector will be\npruned from the model. This arises because any ﬁnite value for α will always assign\na lower probability to the data, thereby decreasing the value of the density at t, pro-\nvided that β is set to its optimal value. We see that any ﬁnite value forα would cause\nthe distribution to be elongated in a direction away from the data, thereby increasing\nthe probability mass in regions away from the observed data and hence reducing the\nvalue of the density at the target data vector itself. For the more general case of M",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 369,
      "page_label": "350"
    }
  },
  {
    "page_content": "7.2. Relevance Vector Machines 351\nbasis vectors ϕ1,..., ϕM a similar intuition holds, namely that if a particular basis\nvector is poorly aligned with the data vector t, then it is likely to be pruned from the\nmodel.\nWe now investigate the mechanism for sparsity from a more mathematical per-\nspective, for a general case involving M basis functions. To motivate this analysis\nwe ﬁrst note that, in the result (7.87) for re-estimating the parameterαi, the terms on\nthe right-hand side are themselves also functions of αi. These results therefore rep-\nresent implicit solutions, and iteration would be required even to determine a single\nαi with all other αj for j ̸=i ﬁxed.\nThis suggests a different approach to solving the optimization problem for the\nRVM, in which we make explicit all of the dependence of the marginal likelihood\n(7.85) on a particular αi and then determine its stationary points explicitly (Faul and\nTipping, 2002; Tipping and Faul, 2003). To do this, we ﬁrst pull out the contribution\nfromαi in the matrix C deﬁned by (7.86) to give\nC = β−1I +\n∑\nj̸=i\nα−1\nj ϕjϕT\nj + α−1\ni ϕiϕT\ni\n= C−i + α−1\ni ϕiϕT\ni (7.93)\nwhere ϕi denotes the ith column of Φ, in other words theN-dimensional vector with\nelements (φi(x1),...,φ i(xN )), in contrast to φn, which denotes the nth row of Φ.\nThe matrix C−i represents the matrix C with the contribution from basis function i\nremoved. Using the matrix identities (C.7) and (C.15), the determinant and inverse\nof C can then be written",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 370,
      "page_label": "351"
    }
  },
  {
    "page_content": "removed. Using the matrix identities (C.7) and (C.15), the determinant and inverse\nof C can then be written\n|C| = |C−i||1+ α−1\ni ϕT\ni C−1\n−i ϕi| (7.94)\nC−1 = C−1\n−i − C−1\n−i ϕiϕT\ni C−1\n−i\nαi + ϕT\ni C−1\n−i ϕi\n. (7.95)\nUsing these results, we can then write the log marginal likelihood function (7.85) in\nthe formExercise 7.15\nL(α)= L(α−i)+ λ(αi) (7.96)\nwhere L(α−i) is simply the log marginal likelihood with basis function ϕi omitted,\nand the quantity λ(αi) is deﬁned by\nλ(αi)= 1\n2\n[\nlnαi − ln (αi + si)+ q2\ni\nαi + si\n]\n(7.97)\nand contains all of the dependence onαi. Here we have introduced the two quantities\nsi = ϕT\ni C−1\n−i ϕi (7.98)\nqi = ϕT\ni C−1\n−i t. (7.99)\nHere si is called the sparsity and qi is known as the quality of ϕi, and as we shall\nsee, a large value of si relative to the value of qi means that the basis function ϕi",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 370,
      "page_label": "351"
    }
  },
  {
    "page_content": "352 7. SPARSE KERNEL MACHINES\nFigure 7.11 Plots of the log\nmarginal likelihood λ(αi) versus\nln αi showing on the left, the single\nmaximum at a ﬁnite αi for q2\ni =4\nand si =1 (so that q2\ni >s i) and on\nthe right, the maximum at αi = ∞\nfor q2\ni =1 and si =2 (so that\nq2\ni <s i).\n−5 0 5\n−4\n−2\n0\n2\n−5 0 5\n−4\n−2\n0\n2\nis more likely to be pruned from the model. The ‘sparsity’ measures the extent to\nwhich basis function ϕi overlaps with the other basis vectors in the model, and the\n‘quality’ represents a measure of the alignment of the basis vector ϕn with the error\nbetween the training set values t =( t1,...,t N )T and the vector y−i of predictions\nthat would result from the model with the vector ϕi excluded (Tipping and Faul,\n2003).\nThe stationary points of the marginal likelihood with respect to αi occur when\nthe derivative\ndλ(αi)\ndαi\n= α−1\ni s2\ni − (q2\ni − si)\n2(αi + si)2 (7.100)\nis equal to zero. There are two possible forms for the solution. Recalling thatαi ⩾ 0,\nwe see that if q2\ni <s i, then αi →∞ provides a solution. Conversely, if q2\ni >s i,w e\ncan solve for αi to obtain\nαi = s2\ni\nq2\ni − si\n. (7.101)\nThese two solutions are illustrated in Figure 7.11. We see that the relative size of\nthe quality and sparsity terms determines whether a particular basis vector will be\npruned from the model or not. A more complete analysis (Faul and Tipping, 2002),\nbased on the second derivatives of the marginal likelihood, conﬁrms these solutions",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 371,
      "page_label": "352"
    }
  },
  {
    "page_content": "based on the second derivatives of the marginal likelihood, conﬁrms these solutions\nare indeed the unique maxima of λ(αi).Exercise 7.16\nNote that this approach has yielded a closed-form solution for αi, for given\nvalues of the other hyperparameters. As well as providing insight into the origin of\nsparsity in the RVM, this analysis also leads to a practical algorithm for optimizing\nthe hyperparameters that has signiﬁcant speed advantages. This uses a ﬁxed set\nof candidate basis vectors, and then cycles through them in turn to decide whether\neach vector should be included in the model or not. The resulting sequential sparse\nBayesian learning algorithm is described below.\nSequential Sparse Bayesian Learning Algorithm\n1. If solving a regression problem, initialize β.\n2. Initialize using one basis function ϕ1, with hyperparameter α1 set using\n(7.101), with the remaining hyperparameters αj for j ̸= i initialized to\ninﬁnity, so that only ϕ1 is included in the model.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 371,
      "page_label": "352"
    }
  },
  {
    "page_content": "7.2. Relevance Vector Machines 353\n3. Evaluate Σ and m, along with qi and si for all basis functions.\n4. Select a candidate basis function ϕi.\n5. If q2\ni >s i, and αi < ∞, so that the basis vector ϕi is already included in\nthe model, then update αi using (7.101).\n6. If q2\ni >s i, and αi = ∞, then add ϕi to the model, and evaluate hyperpa-\nrameter αi using (7.101).\n7. If q2\ni ⩽ si, and αi < ∞ then remove basis function ϕi from the model,\nand set αi = ∞.\n8. If solving a regression problem, update β.\n9. If converged terminate, otherwise go to 3.\nNote that if q2\ni ⩽ si and αi = ∞, then the basis function ϕi is already excluded\nfrom the model and no action is required.\nIn practice, it is convenient to evaluate the quantities\nQi = ϕT\ni C−1t (7.102)\nSi = ϕT\ni C−1ϕi. (7.103)\nThe quality and sparseness variables can then be expressed in the form\nqi = αiQi\nαi − Si\n(7.104)\nsi = αiSi\nαi − Si\n. (7.105)\nNote that when αi = ∞,w eh a v eqi = Qi and si = Si. Using (C.7), we can writeExercise 7.17\nQi = βϕT\ni t − β2ϕT\ni ΦΣΦTt (7.106)\nSi = βϕT\ni ϕi − β2ϕT\ni ΦΣΦTϕi (7.107)\nwhere Φ and Σ involve only those basis vectors that correspond to ﬁnite hyperpa-\nrameters αi. At each stage the required computations therefore scale like O(M3),\nwhere M is the number of active basis vectors in the model and is typically much\nsmaller than the number N of training patterns.\n7.2.3 RVM for classiﬁcation\nWe can extend the relevance vector machine framework to classiﬁcation prob-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 372,
      "page_label": "353"
    }
  },
  {
    "page_content": "7.2.3 RVM for classiﬁcation\nWe can extend the relevance vector machine framework to classiﬁcation prob-\nlems by applying the ARD prior over weights to a probabilistic linear classiﬁcation\nmodel of the kind studied in Chapter 4. To start with, we consider two-class prob-\nlems with a binary target variable t ∈{ 0, 1}. The model now takes the form of a\nlinear combination of basis functions transformed by a logistic sigmoid function\ny(x,w)= σ\n(\nwTφ(x)\n)\n(7.108)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 372,
      "page_label": "353"
    }
  },
  {
    "page_content": "354 7. SPARSE KERNEL MACHINES\nwhere σ(·) is the logistic sigmoid function deﬁned by (4.59). If we introduce a\nGaussian prior over the weight vector w, then we obtain the model that has been\nconsidered already in Chapter 4. The difference here is that in the RVM, this model\nuses the ARD prior (7.80) in which there is a separate precision hyperparameter\nassociated with each weight parameter.\nIn contrast to the regression model, we can no longer integrate analytically over\nthe parameter vector w. Here we follow Tipping (2001) and use the Laplace ap-\nproximation, which was applied to the closely related problem of Bayesian logisticSection 4.4\nregression in Section 4.5.1.\nWe begin by initializing the hyperparameter vector α. For this given value of\nα, we then build a Gaussian approximation to the posterior distribution and thereby\nobtain an approximation to the marginal likelihood. Maximization of this approxi-\nmate marginal likelihood then leads to a re-estimated value for α, and the process is\nrepeated until convergence.\nLet us consider the Laplace approximation for this model in more detail. For\na ﬁxed value of α, the mode of the posterior distribution over w is obtained by\nmaximizing\nlnp(w|t, α)=l n {p(t|w)p(w|α)}− lnp(t|α)\n=\nN∑\nn=1\n{tn lnyn +( 1− tn)l n ( 1− yn)}− 1\n2wTAw +c o n s t(7.109)\nwhere A =d i a g (αi). This can be done using iterative reweighted least squares\n(IRLS) as discussed in Section 4.3.3. For this, we need the gradient vector and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 373,
      "page_label": "354"
    }
  },
  {
    "page_content": "(IRLS) as discussed in Section 4.3.3. For this, we need the gradient vector and\nHessian matrix of the log posterior distribution, which from (7.109) are given byExercise 7.18\n∇lnp(w|t, α)= ΦT(t − y) − Aw (7.110)\n∇∇lnp(w|t, α)= −\n(\nΦTBΦ + A\n)\n(7.111)\nwhere B is an N × N diagonal matrix with elements bn = yn(1 − yn), the vector\ny =( y1,...,y N )T, and Φ is the design matrix with elements Φni = φi(xn). Here\nwe have used the property (4.88) for the derivative of the logistic sigmoid function.\nAt convergence of the IRLS algorithm, the negative Hessian represents the inverse\ncovariance matrix for the Gaussian approximation to the posterior distribution.\nThe mode of the resulting approximation to the posterior distribution, corre-\nsponding to the mean of the Gaussian approximation, is obtained setting (7.110) to\nzero, giving the mean and covariance of the Laplace approximation in the form\nw⋆ = A−1ΦT(t − y) (7.112)\nΣ =\n(\nΦTBΦ + A\n)−1\n. (7.113)\nWe can now use this Laplace approximation to evaluate the marginal likelihood.\nUsing the general result (4.135) for an integral evaluated using the Laplace approxi-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 373,
      "page_label": "354"
    }
  },
  {
    "page_content": "7.2. Relevance Vector Machines 355\nmation, we have\np(t|α)=\n∫\np(t|w)p(w|α)dw\n≃ p(t|w⋆ )p(w⋆ |α)(2π)M/2|Σ|1/2. (7.114)\nIf we substitute for p(t|w⋆ ) and p(w⋆ |α) and then set the derivative of the marginal\nlikelihood with respect to αi equal to zero, we obtainExercise 7.19\n−1\n2(w⋆\ni )2 + 1\n2αi\n− 1\n2Σii =0 . (7.115)\nDeﬁning γi =1 − αiΣii and rearranging then gives\nαnew\ni = γi\n(w⋆\ni )2 (7.116)\nwhich is identical to the re-estimation formula (7.87) obtained for the regression\nRVM.\nIf we deﬁne\nˆt = Φw⋆ + B−1(t − y) (7.117)\nwe can write the approximate log marginal likelihood in the form\nlnp(t|α,β )= −1\n2\n{\nN ln(2π)+l n |C| +(ˆt)TC−1ˆt\n}\n(7.118)\nwhere\nC = B + ΦAΦT. (7.119)\nThis takes the same form as (7.85) in the regression case, and so we can apply the\nsame analysis of sparsity and obtain the same fast learning algorithm in which we\nfully optimize a single hyperparameter αi at each step.\nFigure 7.12 shows the relevance vector machine applied to a synthetic classiﬁ-\ncation data set. We see that the relevance vectors tend not to lie in the region of theAppendix A\ndecision boundary, in contrast to the support vector machine. This is consistent with\nour earlier discussion of sparsity in the RVM, because a basis functionφi(x) centred\non a data point near the boundary will have a vector ϕi that is poorly aligned with\nthe training data vector t.\nOne of the potential advantages of the relevance vector machine compared with",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 374,
      "page_label": "355"
    }
  },
  {
    "page_content": "the training data vector t.\nOne of the potential advantages of the relevance vector machine compared with\nthe SVM is that it makes probabilistic predictions. For example, this allows the RVM\nto be used to help construct an emission density in a nonlinear extension of the linear\ndynamical system for tracking faces in video sequences (Williams et al., 2005).Section 13.3\nSo far, we have considered the RVM for binary classiﬁcation problems. For\nK> 2 classes, we again make use of the probabilistic approach in Section 4.3.4 in\nwhich there are K linear models of the form\nak = wT\nk x (7.120)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 374,
      "page_label": "355"
    }
  },
  {
    "page_content": "356 7. SPARSE KERNEL MACHINES\n−2 0 2\n−2\n0\n2\nFigure 7.12 Example of the relevance vector machine applied to a synthetic data set, in which the left-hand plot\nshows the decision boundary and the data points, with the relevance vectors indicated by circles. Comparison\nwith the results shown in Figure 7.4 for the corresponding support vector machine shows that the RVM gives a\nmuch sparser model. The right-hand plot shows the posterior probability given by the RVM output in which the\nproportion of red (blue) ink indicates the probability of that point belonging to the red (blue) class.\nwhich are combined using a softmax function to give outputs\nyk(x)= exp(ak)∑\nj\nexp(aj)\n. (7.121)\nThe log likelihood function is then given by\nlnp(T|w1,..., wK)=\nN∏\nn=1\nK∏\nk=1\nytnk\nnk (7.122)\nwhere the target values tnk have a 1-of-K coding for each data point n, and T is a\nmatrix with elements tnk. Again, the Laplace approximation can be used to optimize\nthe hyperparameters (Tipping, 2001), in which the model and its Hessian are found\nusing IRLS. This gives a more principled approach to multiclass classiﬁcation than\nthe pairwise method used in the support vector machine and also provides probabilis-\ntic predictions for new data points. The principal disadvantage is that the Hessian\nmatrix has size MK ×MK , where M is the number of active basis functions, which\ngives an additional factor ofK3 in the computational cost of training compared with\nthe two-class RVM.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 375,
      "page_label": "356"
    }
  },
  {
    "page_content": "gives an additional factor ofK3 in the computational cost of training compared with\nthe two-class RVM.\nThe principal disadvantage of the relevance vector machine is the relatively long\ntraining times compared with the SVM. This is offset, however, by the avoidance of\ncross-validation runs to set the model complexity parameters. Furthermore, because\nit yields sparser models, the computation time on test points, which is usually the\nmore important consideration in practice, is typically much less.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 375,
      "page_label": "356"
    }
  },
  {
    "page_content": "Exercises 357\nExercises\n7.1 (⋆⋆ ) www Suppose we have a data set of input vectors {xn} with corresponding\ntarget values tn ∈{ − 1, 1}, and suppose that we model the density of input vec-\ntors within each class separately using a Parzen kernel density estimator (see Sec-\ntion 2.5.1) with a kernel k(x,x′). Write down the minimum misclassiﬁcation-rate\ndecision rule assuming the two classes have equal prior probability. Show also that,\nif the kernel is chosen to bek(x,x′)= xTx′, then the classiﬁcation rule reduces to\nsimply assigning a new input vector to the class having the closest mean. Finally,\nshow that, if the kernel takes the formk(x,x′)= φ(x)Tφ(x′), that the classiﬁcation\nis based on the closest mean in the feature space φ(x).\n7.2 (⋆) Show that, if the 1 on the right-hand side of the constraint (7.5) is replaced by\nsome arbitrary constant γ> 0, the solution for the maximum margin hyperplane is\nunchanged.\n7.3 (⋆⋆ ) Show that, irrespective of the dimensionality of the data space, a data set\nconsisting of just two data points, one from each class, is sufﬁcient to determine the\nlocation of the maximum-margin hyperplane.\n7.4 (⋆⋆ ) www Show that the value ρ of the margin for the maximum-margin hyper-\nplane is given by\n1\nρ2 =\nN∑\nn=1\nan (7.123)\nwhere {an} are given by maximizing (7.10) subject to the constraints (7.11) and\n(7.12).\n7.5 (⋆⋆ ) Show that the values of ρ and {an} in the previous exercise also satisfy\n1\nρ2 =2 ˜L(a) (7.124)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 376,
      "page_label": "357"
    }
  },
  {
    "page_content": "(7.12).\n7.5 (⋆⋆ ) Show that the values of ρ and {an} in the previous exercise also satisfy\n1\nρ2 =2 ˜L(a) (7.124)\nwhere ˜L(a) is deﬁned by (7.10). Similarly, show that\n1\nρ2 = ∥w∥2. (7.125)\n7.6 (⋆) Consider the logistic regression model with a target variable t ∈{ − 1, 1}.I f\nwe deﬁne p(t =1 |y)= σ(y) where y(x) is given by (7.1), show that the negative\nlog likelihood, with the addition of a quadratic regularization term, takes the form\n(7.47).\n7.7 (⋆) Consider the Lagrangian (7.56) for the regression support vector machine. By\nsetting the derivatives of the Lagrangian with respect to w, b, ξn, and ˆξn to zero and\nthen back substituting to eliminate the corresponding variables, show that the dual\nLagrangian is given by (7.61).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 376,
      "page_label": "357"
    }
  },
  {
    "page_content": "358 7. SPARSE KERNEL MACHINES\n7.8 (⋆) www For the regression support vector machine considered in Section 7.1.4,\nshow that all training data points for which ξn > 0 will have an = C, and similarly\nall points for which ˆξn > 0 will have ˆan = C.\n7.9 (⋆) Verify the results (7.82) and (7.83) for the mean and covariance of the posterior\ndistribution over weights in the regression RVM.\n7.10 (⋆⋆ ) www Derive the result (7.85) for the marginal likelihood function in the\nregression RVM, by performing the Gaussian integral over w in (7.84) using the\ntechnique of completing the square in the exponential.\n7.11 (⋆⋆ ) Repeat the above exercise, but this time make use of the general result (2.115).\n7.12 (⋆⋆ ) www Show that direct maximization of the log marginal likelihood (7.85) for\nthe regression relevance vector machine leads to the re-estimation equations (7.87)\nand (7.88) whereγi is deﬁned by (7.89).\n7.13 (⋆⋆ ) In the evidence framework for RVM regression, we obtained the re-estimation\nformulae (7.87) and (7.88) by maximizing the marginal likelihood given by (7.85).\nExtend this approach by inclusion of hyperpriors given by gamma distributions of\nthe form (B.26) and obtain the corresponding re-estimation formulae forα and β by\nmaximizing the corresponding posterior probability p(t, α,β |X) with respect to α\nand β.\n7.14 (⋆⋆ ) Derive the result (7.90) for the predictive distribution in the relevance vector\nmachine for regression. Show that the predictive variance is given by (7.91).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 377,
      "page_label": "358"
    }
  },
  {
    "page_content": "machine for regression. Show that the predictive variance is given by (7.91).\n7.15 (⋆⋆ ) www Using the results (7.94) and (7.95), show that the marginal likelihood\n(7.85) can be written in the form (7.96), where λ(αn) is deﬁned by (7.97) and the\nsparsity and quality factors are deﬁned by (7.98) and (7.99), respectively.\n7.16 (⋆) By taking the second derivative of the log marginal likelihood (7.97) for the\nregression RVM with respect to the hyperparameter αi, show that the stationary\npoint given by (7.101) is a maximum of the marginal likelihood.\n7.17 (⋆⋆ ) Using (7.83) and (7.86), together with the matrix identity (C.7), show that\nthe quantities Sn and Qn deﬁned by (7.102) and (7.103) can be written in the form\n(7.106) and (7.107).\n7.18 (⋆) www Show that the gradient vector and Hessian matrix of the log poste-\nrior distribution (7.109) for the classiﬁcation relevance vector machine are given by\n(7.110) and (7.111).\n7.19 (⋆⋆ ) Verify that maximization of the approximate log marginal likelihood function\n(7.114) for the classiﬁcation relevance vector machine leads to the result (7.116) for\nre-estimation of the hyperparameters.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 377,
      "page_label": "358"
    }
  },
  {
    "page_content": "8\nGraphical\nModels\nProbabilities play a central role in modern pattern recognition. We have seen in\nChapter 1 that probability theory can be expressed in terms of two simple equations\ncorresponding to the sum rule and the product rule. All of the probabilistic infer-\nence and learning manipulations discussed in this book, no matter how complex,\namount to repeated application of these two equations. We could therefore proceed\nto formulate and solve complicated probabilistic models purely by algebraic ma-\nnipulation. However, we shall ﬁnd it highly advantageous to augment the analysis\nusing diagrammatic representations of probability distributions, called probabilistic\ngraphical models. These offer several useful properties:\n1. They provide a simple way to visualize the structure of a probabilistic model\nand can be used to design and motivate new models.\n2. Insights into the properties of the model, including conditional independence\nproperties, can be obtained by inspection of the graph.\n359",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 378,
      "page_label": "359"
    }
  },
  {
    "page_content": "360 8. GRAPHICAL MODELS\n3. Complex computations, required to perform inference and learning in sophis-\nticated models, can be expressed in terms of graphical manipulations, in which\nunderlying mathematical expressions are carried along implicitly.\nA graph comprises nodes (also called vertices) connected by links (also known\nas edges or arcs). In a probabilistic graphical model, each node represents a random\nvariable (or group of random variables), and the links express probabilistic relation-\nships between these variables. The graph then captures the way in which the joint\ndistribution over all of the random variables can be decomposed into a product of\nfactors each depending only on a subset of the variables. We shall begin by dis-\ncussing Bayesian networks, also known as directed graphical models, in which the\nlinks of the graphs have a particular directionality indicated by arrows. The other\nmajor class of graphical models areMarkov random ﬁelds, also known asundirected\ngraphical models, in which the links do not carry arrows and have no directional\nsigniﬁcance. Directed graphs are useful for expressing causal relationships between\nrandom variables, whereas undirected graphs are better suited to expressing soft con-\nstraints between random variables. For the purposes of solving inference problems,\nit is often convenient to convert both directed and undirected graphs into a different\nrepresentation called a factor graph.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 379,
      "page_label": "360"
    }
  },
  {
    "page_content": "it is often convenient to convert both directed and undirected graphs into a different\nrepresentation called a factor graph.\nIn this chapter, we shall focus on the key aspects of graphical models as needed\nfor applications in pattern recognition and machine learning. More general treat-\nments of graphical models can be found in the books by Whittaker (1990), Lauritzen\n(1996), Jensen (1996), Castillo et al. (1997), Jordan (1999), Cowell et al. (1999),\nand Jordan (2007).\n8.1. Bayesian Networks\nIn order to motivate the use of directed graphs to describe probability distributions,\nconsider ﬁrst an arbitrary joint distributionp(a, b, c) over three variables a, b, and c.\nNote that at this stage, we do not need to specify anything further about these vari-\nables, such as whether they are discrete or continuous. Indeed, one of the powerful\naspects of graphical models is that a speciﬁc graph can make probabilistic statements\nfor a broad class of distributions. By application of the product rule of probability\n(1.11), we can write the joint distribution in the form\np(a, b, c)= p(c|a, b)p(a, b). (8.1)\nA second application of the product rule, this time to the second term on the right-\nhand side of (8.1), gives\np(a, b, c)= p(c|a, b)p(b|a)p(a). (8.2)\nNote that this decomposition holds for any choice of the joint distribution. We now\nrepresent the right-hand side of (8.2) in terms of a simple graphical model as follows.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 379,
      "page_label": "360"
    }
  },
  {
    "page_content": "represent the right-hand side of (8.2) in terms of a simple graphical model as follows.\nFirst we introduce a node for each of the random variablesa, b, and c and associate\neach node with the corresponding conditional distribution on the right-hand side of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 379,
      "page_label": "360"
    }
  },
  {
    "page_content": "8.1. Bayesian Networks 361\nFigure 8.1 A directed graphical model representing the joint probabil-\nity distribution over three variablesa, b, and c, correspond-\ning to the decomposition on the right-hand side of (8.2).\na\nb\nc\n(8.2). Then, for each conditional distribution we add directed links (arrows) to the\ngraph from the nodes corresponding to the variables on which the distribution is\nconditioned. Thus for the factor p(c|a, b), there will be links from nodes a and b to\nnode c, whereas for the factor p(a) there will be no incoming links. The result is the\ngraph shown in Figure 8.1. If there is a link going from a nodea to a node b, then we\nsay that node a is the parent of node b, and we say that node b is the child of node a.\nNote that we shall not make any formal distinction between a node and the variable\nto which it corresponds but will simply use the same symbol to refer to both.\nAn interesting point to note about (8.2) is that the left-hand side is symmetrical\nwith respect to the three variables a, b, and c, whereas the right-hand side is not.\nIndeed, in making the decomposition in (8.2), we have implicitly chosen a particular\nordering, namely a, b, c, and had we chosen a different ordering we would have\nobtained a different decomposition and hence a different graphical representation.\nWe shall return to this point later.\nFor the moment let us extend the example of Figure 8.1 by considering the joint",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 380,
      "page_label": "361"
    }
  },
  {
    "page_content": "We shall return to this point later.\nFor the moment let us extend the example of Figure 8.1 by considering the joint\ndistribution over K variables given by p(x1,...,x K). By repeated application of\nthe product rule of probability, this joint distribution can be written as a product of\nconditional distributions, one for each of the variables\np(x1,...,x K)= p(xK|x1,...,x K−1) ...p (x2|x1)p(x1). (8.3)\nFor a given choice of K, we can again represent this as a directed graph having K\nnodes, one for each conditional distribution on the right-hand side of (8.3), with each\nnode having incoming links from all lower numbered nodes. We say that this graph\nis fully connected because there is a link between every pair of nodes.\nSo far, we have worked with completely general joint distributions, so that the\ndecompositions, and their representations as fully connected graphs, will be applica-\nble to any choice of distribution. As we shall see shortly, it is the absence of links\nin the graph that conveys interesting information about the properties of the class of\ndistributions that the graph represents. Consider the graph shown in Figure 8.2. This\nis not a fully connected graph because, for instance, there is no link fromx1 to x2 or\nfrom x3 to x7.\nWe shall now go from this graph to the corresponding representation of the joint\nprobability distribution written in terms of the product of a set of conditional dis-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 380,
      "page_label": "361"
    }
  },
  {
    "page_content": "probability distribution written in terms of the product of a set of conditional dis-\ntributions, one for each node in the graph. Each such conditional distribution will\nbe conditioned only on the parents of the corresponding node in the graph. For in-\nstance, x5 will be conditioned on x1 and x3. The joint distribution of all 7 variables",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 380,
      "page_label": "361"
    }
  },
  {
    "page_content": "362 8. GRAPHICAL MODELS\nFigure 8.2 Example of a directed acyclic graph describing the joint\ndistribution over variables x1,...,x 7. The corresponding\ndecomposition of the joint distribution is given by (8.4).\nx1\nx2 x3\nx4 x5\nx6 x7\nis therefore given by\np(x1)p(x2)p(x3)p(x4|x1,x2,x3)p(x5|x1,x3)p(x6|x4)p(x7|x4,x5). (8.4)\nThe reader should take a moment to study carefully the correspondence between\n(8.4) and Figure 8.2.\nWe can now state in general terms the relationship between a given directed\ngraph and the corresponding distribution over the variables. The joint distribution\ndeﬁned by a graph is given by the product, over all of the nodes of the graph, of\na conditional distribution for each node conditioned on the variables corresponding\nto the parents of that node in the graph. Thus, for a graph with K nodes, the joint\ndistribution is given by\np(x)=\nK∏\nk=1\np(xk|pak) (8.5)\nwhere pak denotes the set of parents of xk, and x = {x1,...,x K}. This key\nequation expresses the factorization properties of the joint distribution for a directed\ngraphical model. Although we have considered each node to correspond to a single\nvariable, we can equally well associate sets of variables and vector-valued variables\nwith the nodes of a graph. It is easy to show that the representation on the right-\nhand side of (8.5) is always correctly normalized provided the individual conditional\ndistributions are normalized.Exercise 8.1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 381,
      "page_label": "362"
    }
  },
  {
    "page_content": "hand side of (8.5) is always correctly normalized provided the individual conditional\ndistributions are normalized.Exercise 8.1\nThe directed graphs that we are considering are subject to an important restric-\ntion namely that there must be no directed cycles, in other words there are no closed\npaths within the graph such that we can move from node to node along links follow-\ning the direction of the arrows and end up back at the starting node. Such graphs are\nalso called directed acyclic graphs,o r DAGs. This is equivalent to the statement thatExercise 8.2\nthere exists an ordering of the nodes such that there are no links that go from any\nnode to any lower numbered node.\n8.1.1 Example: Polynomial regression\nAs an illustration of the use of directed graphs to describe probability distri-\nbutions, we consider the Bayesian polynomial regression model introduced in Sec-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 381,
      "page_label": "362"
    }
  },
  {
    "page_content": "8.1. Bayesian Networks 363\nFigure 8.3 Directed graphical model representing the joint\ndistribution (8.6) corresponding to the Bayesian\npolynomial regression model introduced in Sec-\ntion 1.2.6.\nw\nt1 tN\ntion 1.2.6. The random variables in this model are the vector of polynomial coefﬁ-\ncients w and the observed data t =( t1,...,t N )T. In addition, this model contains\nthe input data x =( x1,...,x N )T, the noise variance σ2, and the hyperparameter α\nrepresenting the precision of the Gaussian prior over w, all of which are parameters\nof the model rather than random variables. Focussing just on the random variables\nfor the moment, we see that the joint distribution is given by the product of the prior\np(w) and N conditional distributions p(tn|w) for n =1 ,...,N so that\np(t, w)= p(w)\nN∏\nn=1\np(tn|w). (8.6)\nThis joint distribution can be represented by a graphical model shown in Figure 8.3.\nWhen we start to deal with more complex models later in the book, we shall ﬁnd\nit inconvenient to have to write out multiple nodes of the formt1,...,t N explicitly as\nin Figure 8.3. We therefore introduce a graphical notation that allows such multiple\nnodes to be expressed more compactly, in which we draw a single representative\nnode tn and then surround this with a box, called a plate, labelled with N indicating\nthat there are N nodes of this kind. Re-writing the graph of Figure 8.3 in this way,\nwe obtain the graph shown in Figure 8.4.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 382,
      "page_label": "363"
    }
  },
  {
    "page_content": "that there are N nodes of this kind. Re-writing the graph of Figure 8.3 in this way,\nwe obtain the graph shown in Figure 8.4.\nWe shall sometimes ﬁnd it helpful to make the parameters of a model, as well as\nits stochastic variables, explicit. In this case, (8.6) becomes\np(t, w|x,α ,σ2)= p(w|α)\nN∏\nn=1\np(tn|w,xn,σ 2).\nCorrespondingly, we can make x and α explicit in the graphical representation. To\ndo this, we shall adopt the convention that random variables will be denoted by open\ncircles, and deterministic parameters will be denoted by smaller solid circles. If we\ntake the graph of Figure 8.4 and include the deterministic parameters, we obtain the\ngraph shown in Figure 8.5.\nWhen we apply a graphical model to a problem in machine learning or pattern\nrecognition, we will typically set some of the random variables to speciﬁc observed\nFigure 8.4 An alternative, more compact, representation of the graph\nshown in Figure 8.3 in which we have introduced a plate\n(the box labelledN) that representsN nodes of which only\na single example tn is shown explicitly.\ntn\nN\nw",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 382,
      "page_label": "363"
    }
  },
  {
    "page_content": "364 8. GRAPHICAL MODELS\nFigure 8.5 This shows the same model as in Figure 8.4 but\nwith the deterministic parameters shown explicitly\nby the smaller solid nodes.\ntn\nxn\nN\nw\nα\nσ2\nvalues, for example the variables{tn} from the training set in the case of polynomial\ncurve ﬁtting. In a graphical model, we will denote such observed variablesby shad-\ning the corresponding nodes. Thus the graph corresponding to Figure 8.5 in which\nthe variables {tn} are observed is shown in Figure 8.6. Note that the value of w is\nnot observed, and so w is an example of a latent variable, also known as a hidden\nvariable. Such variables play a crucial role in many probabilistic models and will\nform the focus of Chapters 9 and 12.\nHaving observed the values {tn} we can, if desired, evaluate the posterior dis-\ntribution of the polynomial coefﬁcients w as discussed in Section 1.2.5. For the\nmoment, we note that this involves a straightforward application of Bayes’ theorem\np(w|T) ∝ p(w)\nN∏\nn=1\np(tn|w) (8.7)\nwhere again we have omitted the deterministic parameters in order to keep the nota-\ntion uncluttered.\nIn general, model parameters such asw are of little direct interest in themselves,\nbecause our ultimate goal is to make predictions for new input values. Suppose we\nare given a new input valueˆx and we wish to ﬁnd the corresponding probability dis-\ntribution for ˆt conditioned on the observed data. The graphical model that describes",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 383,
      "page_label": "364"
    }
  },
  {
    "page_content": "tribution for ˆt conditioned on the observed data. The graphical model that describes\nthis problem is shown in Figure 8.7, and the corresponding joint distribution of all\nof the random variables in this model, conditioned on the deterministic parameters,\nis then given by\np(ˆt,t, w|ˆx,x,α ,σ2)=\n[N∏\nn=1\np(tn|xn, w,σ 2)\n]\np(w|α)p(ˆt|ˆx, w,σ 2). (8.8)\nFigure 8.6 As in Figure 8.5 but with the nodes {tn} shaded\nto indicate that the corresponding random vari-\nables have been set to their observed (training set)\nvalues.\ntn\nxn\nN\nw\nα\nσ2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 383,
      "page_label": "364"
    }
  },
  {
    "page_content": "8.1. Bayesian Networks 365\nFigure 8.7 The polynomial regression model, corresponding\nto Figure 8.6, showing also a new input value bx\ntogether with the corresponding model prediction\nbt.\ntn\nxn\nN\nw\nα\nˆtσ2 ˆx\nThe required predictive distribution for ˆt is then obtained, from the sum rule of\nprobability, by integrating out the model parameters w so that\np(ˆt|ˆx,x, t,α ,σ2) ∝\n∫\np(ˆt,t, w|ˆx,x,α ,σ2)d w\nwhere we are implicitly setting the random variables in t to the speciﬁc values ob-\nserved in the data set. The details of this calculation were discussed in Chapter 3.\n8.1.2 Generative models\nThere are many situations in which we wish to draw samples from a given prob-\nability distribution. Although we shall devote the whole of Chapter 11 to a detailed\ndiscussion of sampling methods, it is instructive to outline here one technique, called\nancestral sampling, which is particularly relevant to graphical models. Consider a\njoint distribution p(x1,...,x K) over K variables that factorizes according to (8.5)\ncorresponding to a directed acyclic graph. We shall suppose that the variables have\nbeen ordered such that there are no links from any node to any lower numbered node,\nin other words each node has a higher number than any of its parents. Our goal is to\ndraw a sample ˆx1,..., ˆxK from the joint distribution.\nTo do this, we start with the lowest-numbered node and draw a sample from the\ndistribution p(x1), which we call ˆx1. We then work through each of the nodes in or-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 384,
      "page_label": "365"
    }
  },
  {
    "page_content": "distribution p(x1), which we call ˆx1. We then work through each of the nodes in or-\nder, so that for noden we draw a sample from the conditional distributionp(xn|pan)\nin which the parent variables have been set to their sampled values. Note that at each\nstage, these parent values will always be available because they correspond to lower-\nnumbered nodes that have already been sampled. Techniques for sampling from\nspeciﬁc distributions will be discussed in detail in Chapter 11. Once we have sam-\npled from the ﬁnal variable xK, we will have achieved our objective of obtaining a\nsample from the joint distribution. To obtain a sample from some marginal distribu-\ntion corresponding to a subset of the variables, we simply take the sampled values\nfor the required nodes and ignore the sampled values for the remaining nodes. For\nexample, to draw a sample from the distribution p(x2,x4), we simply sample from\nthe full joint distribution and then retain the values ˆx2,ˆx4 and discard the remaining\nvalues {ˆxj̸=2,4}.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 384,
      "page_label": "365"
    }
  },
  {
    "page_content": "366 8. GRAPHICAL MODELS\nFigure 8.8 A graphical model representing the process by which\nimages of objects are created, in which the identity\nof an object (a discrete variable) and the position and\norientation of that object (continuous variables) have\nindependent prior probabilities. The image (a vector\nof pixel intensities) has a probability distribution that\nis dependent on the identity of the object as well as\non its position and orientation.\nImage\nObject OrientationPosition\nFor practical applications of probabilistic models, it will typically be the higher-\nnumbered variables corresponding to terminal nodes of the graph that represent the\nobservations, with lower-numbered nodes corresponding to latent variables. The\nprimary role of the latent variables is to allow a complicated distribution over the\nobserved variables to be represented in terms of a model constructed from simpler\n(typically exponential family) conditional distributions.\nWe can interpret such models as expressing the processes by which the observed\ndata arose. For instance, consider an object recognition task in which each observed\ndata point corresponds to an image (comprising a vector of pixel intensities) of one\nof the objects. In this case, the latent variables might have an interpretation as the\nposition and orientation of the object. Given a particular observed image, our goal is\nto ﬁnd the posterior distribution over objects, in which we integrate over all possible",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 385,
      "page_label": "366"
    }
  },
  {
    "page_content": "to ﬁnd the posterior distribution over objects, in which we integrate over all possible\npositions and orientations. We can represent this problem using a graphical model\nof the form show in Figure 8.8.\nThe graphical model captures the causal process (Pearl, 1988) by which the ob-\nserved data was generated. For this reason, such models are often called generative\nmodels. By contrast, the polynomial regression model described by Figure 8.5 is\nnot generative because there is no probability distribution associated with the input\nvariable x, and so it is not possible to generate synthetic data points from this model.\nWe could make it generative by introducing a suitable prior distribution p(x), at the\nexpense of a more complex model.\nThe hidden variables in a probabilistic model need not, however, have any ex-\nplicit physical interpretation but may be introduced simply to allow a more complex\njoint distribution to be constructed from simpler components. In either case, the\ntechnique of ancestral sampling applied to a generative model mimics the creation\nof the observed data and would therefore give rise to ‘fantasy’ data whose probability\ndistribution (if the model were a perfect representation of reality) would be the same\nas that of the observed data. In practice, producing synthetic observations from a\ngenerative model can prove informative in understanding the form of the probability\ndistribution represented by that model.\n8.1.3 Discrete variables",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 385,
      "page_label": "366"
    }
  },
  {
    "page_content": "generative model can prove informative in understanding the form of the probability\ndistribution represented by that model.\n8.1.3 Discrete variables\nWe have discussed the importance of probability distributions that are members\nof the exponential family, and we have seen that this family includes many well-Section 2.4\nknown distributions as particular cases. Although such distributions are relatively\nsimple, they form useful building blocks for constructing more complex probability",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 385,
      "page_label": "366"
    }
  },
  {
    "page_content": "8.1. Bayesian Networks 367\nFigure 8.9 (a) This fully-connected graph describes a general distribu-\ntion over two K-state discrete variables having a total of\nK2 − 1 parameters. (b) By dropping the link between the\nnodes, the number of parameters is reduced to 2(K − 1).\n(a)\nx1 x2\n(b)\nx1 x2\ndistributions, and the framework of graphical models is very useful in expressing the\nway in which these building blocks are linked together.\nSuch models have particularly nice properties if we choose the relationship be-\ntween each parent-child pair in a directed graph to be conjugate, and we shall ex-\nplore several examples of this shortly. Two cases are particularly worthy of note,\nnamely when the parent and child node each correspond to discrete variables and\nwhen they each correspond to Gaussian variables, because in these two cases the\nrelationship can be extended hierarchically to construct arbitrarily complex directed\nacyclic graphs. We begin by examining the discrete case.\nThe probability distribution p(x|µ) for a single discrete variable x having K\npossible states (using the 1-of-K representation) is given by\np(x|µ)=\nK∏\nk=1\nµxk\nk (8.9)\nand is governed by the parameters µ =( µ1,...,µ K)T. Due to the constraint∑\nk µk =1 , only K − 1 values for µk need to be speciﬁed in order to deﬁne the\ndistribution.\nNow suppose that we have two discrete variables, x1 and x2, each of which has\nK states, and we wish to model their joint distribution. We denote the probability of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 386,
      "page_label": "367"
    }
  },
  {
    "page_content": "K states, and we wish to model their joint distribution. We denote the probability of\nobserving both x1k =1 and x2l =1 by the parameter µkl, where x1k denotes the\nkth component of x1, and similarly for x2l. The joint distribution can be written\np(x1, x2|µ)=\nK∏\nk=1\nK∏\nl=1\nµx1kx2l\nkl .\nBecause the parameters µkl are subject to the constraint ∑\nk\n∑\nl µkl =1 , this distri-\nbution is governed by K2 − 1 parameters. It is easily seen that the total number of\nparameters that must be speciﬁed for an arbitrary joint distribution overM variables\nis KM − 1 and therefore grows exponentially with the number M of variables.\nUsing the product rule, we can factor the joint distributionp(x1, x2) in the form\np(x2|x1)p(x1), which corresponds to a two-node graph with a link going from the\nx1 node to the x2 node as shown in Figure 8.9(a). The marginal distribution p(x1)\nis governed by K − 1 parameters, as before, Similarly, the conditional distribution\np(x2|x1) requires the speciﬁcation of K − 1 parameters for each of the K possible\nvalues of x1. The total number of parameters that must be speciﬁed in the joint\ndistribution is therefore (K − 1) +K(K − 1) = K2 − 1 as before.\nNow suppose that the variables x1 and x2 were independent, corresponding to\nthe graphical model shown in Figure 8.9(b). Each variable is then described by",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 386,
      "page_label": "367"
    }
  },
  {
    "page_content": "368 8. GRAPHICAL MODELS\nFigure 8.10 This chain of M discrete nodes, each\nhaving K states, requires the speciﬁcation of K − 1+\n(M − 1)K(K − 1) parameters, which grows linearly\nwith the length M of the chain. In contrast, a fully con-\nnected graph of M nodes would have KM − 1 param-\neters, which grows exponentially with M.\nx1 x2 xM\na separate multinomial distribution, and the total number of parameters would be\n2(K − 1). For a distribution over M independent discrete variables, each having K\nstates, the total number of parameters would be M(K − 1), which therefore grows\nlinearly with the number of variables. From a graphical perspective, we have reduced\nthe number of parameters by dropping links in the graph, at the expense of having a\nrestricted class of distributions.\nMore generally, if we have M discrete variables x1,..., xM , we can model\nthe joint distribution using a directed graph with one variable corresponding to each\nnode. The conditional distribution at each node is given by a set of nonnegative pa-\nrameters subject to the usual normalization constraint. If the graph is fully connected\nthen we have a completely general distribution havingKM −1 parameters, whereas\nif there are no links in the graph the joint distribution factorizes into the product of\nthe marginals, and the total number of parameters is M(K − 1). Graphs having in-\ntermediate levels of connectivity allow for more general distributions than the fully",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 387,
      "page_label": "368"
    }
  },
  {
    "page_content": "termediate levels of connectivity allow for more general distributions than the fully\nfactorized one while requiring fewer parameters than the general joint distribution.\nAs an illustration, consider the chain of nodes shown in Figure 8.10. The marginal\ndistribution p(x1) requires K − 1 parameters, whereas each of the M − 1 condi-\ntional distributions p(xi|xi−1), for i =2 ,...,M , requires K(K − 1) parameters.\nThis gives a total parameter count ofK −1+( M −1)K(K −1), which is quadratic\nin K and which grows linearly (rather than exponentially) with the length M of the\nchain.\nAn alternative way to reduce the number of independent parameters in a model\nis by sharing parameters (also known as tying of parameters). For instance, in the\nchain example of Figure 8.10, we can arrange that all of the conditional distributions\np(xi|xi−1), for i =2 ,...,M , are governed by the same set ofK(K−1) parameters.\nTogether with theK−1 parameters governing the distribution ofx1, this gives a total\nof K2 − 1 parameters that must be speciﬁed in order to deﬁne the joint distribution.\nWe can turn a graph over discrete variables into a Bayesian model by introduc-\ning Dirichlet priors for the parameters. From a graphical point of view, each node\nthen acquires an additional parent representing the Dirichlet distribution over the pa-\nrameters associated with the corresponding discrete node. This is illustrated for the\nchain model in Figure 8.11. The corresponding model in which we tie the parame-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 387,
      "page_label": "368"
    }
  },
  {
    "page_content": "chain model in Figure 8.11. The corresponding model in which we tie the parame-\nters governing the conditional distributions p(xi|xi−1), for i =2 ,...,M , is shown\nin Figure 8.12.\nAnother way of controlling the exponential growth in the number of parameters\nin models of discrete variables is to use parameterized models for the conditional\ndistributions instead of complete tables of conditional probability values. To illus-\ntrate this idea, consider the graph in Figure 8.13 in which all of the nodes represent\nbinary variables. Each of the parent variables xi is governed by a single parame-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 387,
      "page_label": "368"
    }
  },
  {
    "page_content": "8.1. Bayesian Networks 369\nFigure 8.11 An extension of the model of\nFigure 8.10 to include Dirich-\nlet priors over the param-\neters governing the discrete\ndistributions.\nx1 x2 xM\nµ1 µ2 µM\nFigure 8.12 As in Figure 8.11 but with a sin-\ngle set of parameters µ shared\namongst all of the conditional\ndistributions p(xi|xi−1).\nx1 x2 xM\nµ1 µ\nter µi representing the probability p(xi =1 ), giving M parameters in total for the\nparent nodes. The conditional distribution p(y|x1,...,x M ), however, would require\n2M parameters representing the probability p(y =1 ) for each of the 2M possible\nsettings of the parent variables. Thus in general the number of parameters required\nto specify this conditional distribution will grow exponentially with M. We can ob-\ntain a more parsimonious form for the conditional distribution by using a logistic\nsigmoid function acting on a linear combination of the parent variables, givingSection 2.4\np(y =1 |x1,...,x M )= σ\n(\nw0 +\nM∑\ni=1\nwixi\n)\n= σ(wTx) (8.10)\nwhere σ(a) = (1+exp(−a))−1 is the logistic sigmoid,x =( x0,x1,...,x M )T is an\n(M +1 )-dimensional vector of parent states augmented with an additional variable\nx0 whose value is clamped to 1, and w =( w0,w 1,...,w M )T is a vector of M +1\nparameters. This is a more restricted form of conditional distribution than the general\ncase but is now governed by a number of parameters that grows linearly with M.I n\nthis sense, it is analogous to the choice of a restrictive form of covariance matrix (for",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 388,
      "page_label": "369"
    }
  },
  {
    "page_content": "this sense, it is analogous to the choice of a restrictive form of covariance matrix (for\nexample, a diagonal matrix) in a multivariate Gaussian distribution. The motivation\nfor the logistic sigmoid representation was discussed in Section 4.2.\nFigure 8.13 A graph comprising M parents x1,...,x M and a sin-\ngle child y, used to illustrate the idea of parameterized\nconditional distributions for discrete variables.\ny\nx1 xM",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 388,
      "page_label": "369"
    }
  },
  {
    "page_content": "370 8. GRAPHICAL MODELS\n8.1.4 Linear-Gaussian models\nIn the previous section, we saw how to construct joint probability distributions\nover a set of discrete variables by expressing the variables as nodes in a directed\nacyclic graph. Here we show how a multivariate Gaussian can be expressed as a\ndirected graph corresponding to a linear-Gaussian model over the component vari-\nables. This allows us to impose interesting structure on the distribution, with the\ngeneral Gaussian and the diagonal covariance Gaussian representing opposite ex-\ntremes. Several widely used techniques are examples of linear-Gaussian models,\nsuch as probabilistic principal component analysis, factor analysis, and linear dy-\nnamical systems (Roweis and Ghahramani, 1999). We shall make extensive use of\nthe results of this section in later chapters when we consider some of these techniques\nin detail.\nConsider an arbitrary directed acyclic graph over D variables in which node i\nrepresents a single continuous random variable xi having a Gaussian distribution.\nThe mean of this distribution is taken to be a linear combination of the states of its\nparent nodes pai of node i\np(xi|pai)= N\n⎛\n⎝xi\n⏐⏐⏐⏐⏐⏐\n∑\nj∈pai\nwijxj + bi,vi\n⎞\n⎠ (8.11)\nwhere wij and bi are parameters governing the mean, and vi is the variance of the\nconditional distribution for xi. The log of the joint distribution is then the log of the\nproduct of these conditionals over all nodes in the graph and hence takes the form\nlnp(x)=\nD∑\ni=1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 389,
      "page_label": "370"
    }
  },
  {
    "page_content": "product of these conditionals over all nodes in the graph and hence takes the form\nlnp(x)=\nD∑\ni=1\nlnp(xi|pai) (8.12)\n= −\nD∑\ni=1\n1\n2vi\n⎛\n⎝xi −\n∑\nj∈pai\nwijxj − bi\n⎞\n⎠\n2\n+c o n s t (8.13)\nwhere x =( x1,...,x D)T and ‘const’ denotes terms independent of x. We see that\nthis is a quadratic function of the components of x, and hence the joint distribution\np(x) is a multivariate Gaussian.\nWe can determine the mean and covariance of the joint distribution recursively\nas follows. Each variable xi has (conditional on the states of its parents) a Gaussian\ndistribution of the form (8.11) and so\nxi =\n∑\nj∈pai\nwijxj + bi + √viϵi (8.14)\nwhere ϵi is a zero mean, unit variance Gaussian random variable satisfyingE[ϵi]=0\nand E[ϵiϵj]= Iij, where Iij is the i, j element of the identity matrix. Taking the\nexpectation of (8.14), we have\nE[xi]=\n∑\nj∈pai\nwijE[xj]+ bi. (8.15)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 389,
      "page_label": "370"
    }
  },
  {
    "page_content": "8.1. Bayesian Networks 371\nFigure 8.14 A directed graph over three Gaussian variables,\nwith one missing link.\nx1 x2 x3\nThus we can ﬁnd the components of E[x]=( E[x1],..., E[xD])T by starting at the\nlowest numbered node and working recursively through the graph (here we again\nassume that the nodes are numbered such that each node has a higher number than\nits parents). Similarly, we can use (8.14) and (8.15) to obtain the i, jelement of the\ncovariance matrix for p(x) in the form of a recursion relation\ncov[xi,xj]= E [(xi − E[xi])(xj − E[xj])]\n= E\n⎡\n⎣(xi − E[xi])\n⎧\n⎨\n⎩\n∑\nk∈paj\nwjk(xk − E[xk]) +√vjϵj\n⎫\n⎬\n⎭\n⎤\n⎦\n=\n∑\nk∈paj\nwjkcov[xi,xk]+ Iijvj (8.16)\nand so the covariance can similarly be evaluated recursively starting from the lowest\nnumbered node.\nLet us consider two extreme cases. First of all, suppose that there are no links\nin the graph, which therefore comprises D isolated nodes. In this case, there are no\nparameters wij and so there are just D parameters bi and D parameters vi. From\nthe recursion relations (8.15) and (8.16), we see that the mean of p(x) is given by\n(b1,...,b D)T and the covariance matrix is diagonal of the form diag(v1,...,v D).\nThe joint distribution has a total of 2D parameters and represents a set of D inde-\npendent univariate Gaussian distributions.\nNow consider a fully connected graph in which each node has all lower num-\nbered nodes as parents. The matrix wij then has i − 1 entries on the ith row and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 390,
      "page_label": "371"
    }
  },
  {
    "page_content": "bered nodes as parents. The matrix wij then has i − 1 entries on the ith row and\nhence is a lower triangular matrix (with no entries on the leading diagonal). Then\nthe total number of parameters wij is obtained by taking the number D2 of elements\nin a D×D matrix, subtracting D to account for the absence of elements on the lead-\ning diagonal, and then dividing by 2 because the matrix has elements only below the\ndiagonal, giving a total ofD(D−1)/2. The total number of independent parameters\n{wij} and {vi} in the covariance matrix is therefore D(D +1 )/2 corresponding to\na general symmetric covariance matrix.Section 2.3\nGraphs having some intermediate level of complexity correspond to joint Gaus-\nsian distributions with partially constrained covariance matrices. Consider for ex-\nample the graph shown in Figure 8.14, which has a link missing between variables\nx1 and x3. Using the recursion relations (8.15) and (8.16), we see that the mean and\ncovariance of the joint distribution are given byExercise 8.7\nµ =( b1,b 2 + w21b1,b 3 + w32b2 + w32w21b1)T (8.17)\nΣ =\n( v1 w21v1 w32w21v1\nw21v1 v2 + w2\n21v1 w32(v2 + w2\n21v1)\nw32w21v1 w32(v2 + w2\n21v1) v3 + w2\n32(v2 + w2\n21v1)\n)\n. (8.18)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 390,
      "page_label": "371"
    }
  },
  {
    "page_content": "372 8. GRAPHICAL MODELS\nWe can readily extend the linear-Gaussian graphical model to the case in which\nthe nodes of the graph represent multivariate Gaussian variables. In this case, we can\nwrite the conditional distribution for nodei in the form\np(xi|pai)= N\n⎛\n⎝xi\n⏐⏐⏐⏐⏐⏐\n∑\nj∈pai\nWijxj + bi, Σi\n⎞\n⎠ (8.19)\nwhere now Wij is a matrix (which is nonsquare if xi and xj have different dimen-\nsionalities). Again it is easy to verify that the joint distribution over all variables is\nGaussian.\nNote that we have already encountered a speciﬁc example of the linear-Gaussian\nrelationship when we saw that the conjugate prior for the mean µ of a GaussianSection 2.3.6\nvariable x is itself a Gaussian distribution over µ. The joint distribution over x and\nµ is therefore Gaussian. This corresponds to a simple two-node graph in which\nthe node representing µ is the parent of the node representing x. The mean of the\ndistribution over µ is a parameter controlling a prior, and so it can be viewed as a\nhyperparameter. Because the value of this hyperparameter may itself be unknown,\nwe can again treat it from a Bayesian perspective by introducing a prior over the\nhyperparameter, sometimes called ahyperprior, which is again given by a Gaussian\ndistribution. This type of construction can be extended in principle to any level and is\nan illustration of a hierarchical Bayesian model, of which we shall encounter further\nexamples in later chapters.\n8.2. Conditional Independence",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 391,
      "page_label": "372"
    }
  },
  {
    "page_content": "an illustration of a hierarchical Bayesian model, of which we shall encounter further\nexamples in later chapters.\n8.2. Conditional Independence\nAn important concept for probability distributions over multiple variables is that of\nconditional independence(Dawid, 1980). Consider three variables a, b, and c, and\nsuppose that the conditional distribution of a,g i v e nb and c, is such that it does not\ndepend on the value of b, so that\np(a|b, c)= p(a|c). (8.20)\nWe say that a is conditionally independent of b given c. This can be expressed in a\nslightly different way if we consider the joint distribution of a and b conditioned on\nc, which we can write in the form\np(a, b|c)= p(a|b, c)p(b|c)\n= p(a|c)p(b|c). (8.21)\nwhere we have used the product rule of probability together with (8.20). Thus we\nsee that, conditioned on c, the joint distribution of a and b factorizes into the prod-\nuct of the marginal distribution of a and the marginal distribution of b (again both\nconditioned on c). This says that the variables a and b are statistically independent,\ngiven c. Note that our deﬁnition of conditional independence will require that (8.20),",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 391,
      "page_label": "372"
    }
  },
  {
    "page_content": "8.2. Conditional Independence 373\nFigure 8.15 The ﬁrst of three examples of graphs over three variables\na, b, and c used to discuss conditional independence\nproperties of directed graphical models.\nc\nab\nor equivalently (8.21), must hold for every possible value ofc, and not just for some\nvalues. We shall sometimes use a shorthand notation for conditional independence\n(Dawid, 1979) in which\na ⊥⊥ b | c (8.22)\ndenotes that a is conditionally independent of b given c and is equivalent to (8.20).\nConditional independence properties play an important role in using probabilis-\ntic models for pattern recognition by simplifying both the structure of a model and\nthe computations needed to perform inference and learning under that model. We\nshall see examples of this shortly.\nIf we are given an expression for the joint distribution over a set of variables in\nterms of a product of conditional distributions (i.e., the mathematical representation\nunderlying a directed graph), then we could in principle test whether any poten-\ntial conditional independence property holds by repeated application of the sum and\nproduct rules of probability. In practice, such an approach would be very time con-\nsuming. An important and elegant feature of graphical models is that conditional\nindependence properties of the joint distribution can be read directly from the graph\nwithout having to perform any analytical manipulations. The general framework",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 392,
      "page_label": "373"
    }
  },
  {
    "page_content": "without having to perform any analytical manipulations. The general framework\nfor achieving this is called d-separation, where the ‘d’ stands for ‘directed’ (Pearl,\n1988). Here we shall motivate the concept of d-separation and give a general state-\nment of the d-separation criterion. A formal proof can be found in Lauritzen (1996).\n8.2.1 Three example graphs\nWe begin our discussion of the conditional independence properties of directed\ngraphs by considering three simple examples each involving graphs having just three\nnodes. Together, these will motivate and illustrate the key concepts of d-separation.\nThe ﬁrst of the three examples is shown in Figure 8.15, and the joint distribution\ncorresponding to this graph is easily written down using the general result (8.5) to\ngive\np(a, b, c)= p(a|c)p(b|c)p(c). (8.23)\nIf none of the variables are observed, then we can investigate whether a and b are\nindependent by marginalizing both sides of (8.23) with respect to c to give\np(a, b)=\n∑\nc\np(a|c)p(b|c)p(c). (8.24)\nIn general, this does not factorize into the product p(a)p(b), and so\na ̸⊥⊥ b |∅ (8.25)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 392,
      "page_label": "373"
    }
  },
  {
    "page_content": "374 8. GRAPHICAL MODELS\nFigure 8.16 As in Figure 8.15 but where we have conditioned on the\nvalue of variable c.\nc\nab\nwhere ∅ denotes the empty set, and the symbol ̸⊥⊥ means that the conditional inde-\npendence property does not hold in general. Of course, it may hold for a particular\ndistribution by virtue of the speciﬁc numerical values associated with the various\nconditional probabilities, but it does not follow in general from the structure of the\ngraph.\nNow suppose we condition on the variable c, as represented by the graph of\nFigure 8.16. From (8.23), we can easily write down the conditional distribution of a\nand b,g i v e nc, in the form\np(a, b|c)= p(a, b, c)\np(c)\n= p(a|c)p(b|c)\nand so we obtain the conditional independence property\na ⊥⊥ b | c.\nWe can provide a simple graphical interpretation of this result by considering\nthe path from node a to node b via c. The node c is said to be tail-to-tail with re-\nspect to this path because the node is connected to the tails of the two arrows, and\nthe presence of such a path connecting nodes a and b causes these nodes to be de-\npendent. However, when we condition on node c, as in Figure 8.16, the conditioned\nnode ‘blocks’ the path from a to b and causes a and b to become (conditionally)\nindependent.\nWe can similarly consider the graph shown in Figure 8.17. The joint distribution\ncorresponding to this graph is again obtained from our general formula (8.5) to give\np(a, b, c)= p(a)p(c|a)p(b|c). (8.26)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 393,
      "page_label": "374"
    }
  },
  {
    "page_content": "corresponding to this graph is again obtained from our general formula (8.5) to give\np(a, b, c)= p(a)p(c|a)p(b|c). (8.26)\nFirst of all, suppose that none of the variables are observed. Again, we can test to\nsee if a and b are independent by marginalizing over c to give\np(a, b)= p(a)\n∑\nc\np(c|a)p(b|c)= p(a)p(b|a).\nFigure 8.17 The second of our three examples of 3-node\ngraphs used to motivate the conditional indepen-\ndence framework for directed graphical models.\nac b",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 393,
      "page_label": "374"
    }
  },
  {
    "page_content": "8.2. Conditional Independence 375\nFigure 8.18 As in Figure 8.17 but now conditioning on node c. ac b\nwhich in general does not factorize into p(a)p(b), and so\na ̸⊥⊥ b |∅ (8.27)\nas before.\nNow suppose we condition on node c, as shown in Figure 8.18. Using Bayes’\ntheorem, together with (8.26), we obtain\np(a, b|c)= p(a, b, c)\np(c)\n= p(a)p(c|a)p(b|c)\np(c)\n= p(a|c)p(b|c)\nand so again we obtain the conditional independence property\na ⊥⊥ b | c.\nAs before, we can interpret these results graphically. The node c is said to be\nhead-to-tail with respect to the path from node a to node b. Such a path connects\nnodes a and b and renders them dependent. If we now observe c, as in Figure 8.18,\nthen this observation ‘blocks’ the path from a to b and so we obtain the conditional\nindependence property a ⊥⊥ b | c.\nFinally, we consider the third of our 3-node examples, shown by the graph in\nFigure 8.19. As we shall see, this has a more subtle behaviour than the two previous\ngraphs.\nThe joint distribution can again be written down using our general result (8.5) to\ngive\np(a, b, c)= p(a)p(b)p(c|a, b). (8.28)\nConsider ﬁrst the case where none of the variables are observed. Marginalizing both\nsides of (8.28) over c we obtain\np(a, b)= p(a)p(b)\nFigure 8.19 The last of our three examples of 3-node graphs used to\nexplore conditional independence properties in graphi-\ncal models. This graph has rather different properties\nfrom the two previous examples.\nc\nab",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 394,
      "page_label": "375"
    }
  },
  {
    "page_content": "376 8. GRAPHICAL MODELS\nFigure 8.20 As in Figure 8.19 but conditioning on the value of node\nc. In this graph, the act of conditioning induces a depen-\ndence between a and b.\nc\nab\nand so a and b are independent with no variables observed, in contrast to the two\nprevious examples. We can write this result as\na ⊥⊥ b |∅ . (8.29)\nNow suppose we condition on c, as indicated in Figure 8.20. The conditional distri-\nbution of a and b is then given by\np(a, b|c)= p(a, b, c)\np(c)\n= p(a)p(b)p(c|a, b)\np(c)\nwhich in general does not factorize into the product p(a)p(b), and so\na ̸⊥⊥ b | c.\nThus our third example has the opposite behaviour from the ﬁrst two. Graphically,\nwe say that node c is head-to-head with respect to the path from a to b because it\nconnects to the heads of the two arrows. When node c is unobserved, it ‘blocks’\nthe path, and the variables a and b are independent. However, conditioning on c\n‘unblocks’ the path and renders a and b dependent.\nThere is one more subtlety associated with this third example that we need to\nconsider. First we introduce some more terminology. We say that node y is a de-\nscendant of node x if there is a path from x to y in which each step of the path\nfollows the directions of the arrows. Then it can be shown that a head-to-head path\nwill become unblocked if either the node, or any of its descendants, is observed.Exercise 8.10\nIn summary, a tail-to-tail node or a head-to-tail node leaves a path unblocked",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 395,
      "page_label": "376"
    }
  },
  {
    "page_content": "In summary, a tail-to-tail node or a head-to-tail node leaves a path unblocked\nunless it is observed in which case it blocks the path. By contrast, a head-to-head\nnode blocks a path if it is unobserved, but once the node, and/or at least one of its\ndescendants, is observed the path becomes unblocked.\nIt is worth spending a moment to understand further the unusual behaviour of the\ngraph of Figure 8.20. Consider a particular instance of such a graph corresponding\nto a problem with three binary random variables relating to the fuel system on a car,\nas shown in Figure 8.21. The variables are called B, representing the state of a\nbattery that is either charged ( B =1 )o rﬂ a t( B =0 ), F representing the state of\nthe fuel tank that is either full of fuel ( F =1 ) or empty ( F =0 ), and G, which is\nthe state of an electric fuel gauge and which indicates either full ( G =1 ) or empty",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 395,
      "page_label": "376"
    }
  },
  {
    "page_content": "8.2. Conditional Independence 377\nG\nBF\nG\nBF\nG\nBF\nFigure 8.21 An example of a 3-node graph used to illustrate the phenomenon of ‘explaining away’. The three\nnodes represent the state of the battery ( B), the state of the fuel tank ( F) and the reading on the electric fuel\ngauge (G). See the text for details.\n(G =0 ). The battery is either charged or ﬂat, and independently the fuel tank is\neither full or empty, with prior probabilities\np(B =1 ) = 0 .9\np(F =1 ) = 0 .9.\nGiven the state of the fuel tank and the battery, the fuel gauge reads full with proba-\nbilities given by\np(G =1 |B =1 ,F =1 ) = 0 .8\np(G =1 |B =1 ,F =0 ) = 0 .2\np(G =1 |B =0 ,F =1 ) = 0 .2\np(G =1 |B =0 ,F =0 ) = 0 .1\nso this is a rather unreliable fuel gauge! All remaining probabilities are determined\nby the requirement that probabilities sum to one, and so we have a complete speciﬁ-\ncation of the probabilistic model.\nBefore we observe any data, the prior probability of the fuel tank being empty\nis p(F =0 )=0 .1. Now suppose that we observe the fuel gauge and discover that\nit reads empty, i.e., G =0 , corresponding to the middle graph in Figure 8.21. We\ncan use Bayes’ theorem to evaluate the posterior probability of the fuel tank being\nempty. First we evaluate the denominator for Bayes’ theorem given by\np(G =0 )=\n∑\nB∈{0,1}\n∑\nF∈{0,1}\np(G =0 |B,F )p(B)p(F)=0 .315 (8.30)\nand similarly we evaluate\np(G =0 |F =0 )=\n∑\nB∈{0,1}\np(G =0 |B,F =0 )p(B)=0 .81 (8.31)\nand using these results we have",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 396,
      "page_label": "377"
    }
  },
  {
    "page_content": "and similarly we evaluate\np(G =0 |F =0 )=\n∑\nB∈{0,1}\np(G =0 |B,F =0 )p(B)=0 .81 (8.31)\nand using these results we have\np(F =0 |G =0 )= p(G =0 |F =0 )p(F =0 )\np(G =0 ) ≃ 0.257 (8.32)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 396,
      "page_label": "377"
    }
  },
  {
    "page_content": "378 8. GRAPHICAL MODELS\nand so p(F =0 |G =0 ) >p (F =0 ). Thus observing that the gauge reads empty\nmakes it more likely that the tank is indeed empty, as we would intuitively expect.\nNext suppose that we also check the state of the battery and ﬁnd that it is ﬂat, i.e.,\nB =0 . We have now observed the states of both the fuel gauge and the battery, as\nshown by the right-hand graph in Figure 8.21. The posterior probability that the fuel\ntank is empty given the observations of both the fuel gauge and the battery state is\nthen given by\np(F =0 |G =0 ,B =0 )= p(G =0 |B =0 ,F =0 )p(F =0 )∑\nF∈{0,1} p(G =0 |B =0 ,F )p(F) ≃ 0.111 (8.33)\nwhere the prior probability p(B =0 ) has cancelled between numerator and denom-\ninator. Thus the probability that the tank is empty has decreased (from 0.257 to\n0.111) as a result of the observation of the state of the battery. This accords with our\nintuition that ﬁnding out that the battery is ﬂatexplains awaythe observation that the\nfuel gauge reads empty. We see that the state of the fuel tank and that of the battery\nhave indeed become dependent on each other as a result of observing the reading\non the fuel gauge. In fact, this would also be the case if, instead of observing the\nfuel gauge directly, we observed the state of some descendant of G. Note that the\nprobability p(F =0 |G =0 ,B =0 ) ≃ 0.111 is greater than the prior probability\np(F =0 )=0 .1 because the observation that the fuel gauge reads zero still provides",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 397,
      "page_label": "378"
    }
  },
  {
    "page_content": "p(F =0 )=0 .1 because the observation that the fuel gauge reads zero still provides\nsome evidence in favour of an empty fuel tank.\n8.2.2 D-separation\nWe now give a general statement of the d-separation property (Pearl, 1988) for\ndirected graphs. Consider a general directed graph in which A, B, and C are arbi-\ntrary nonintersecting sets of nodes (whose union may be smaller than the complete\nset of nodes in the graph). We wish to ascertain whether a particular conditional\nindependence statement A ⊥⊥ B | C is implied by a given directed acyclic graph. To\ndo so, we consider all possible paths from any node inA to any node in B. Any such\npath is said to be blocked if it includes a node such that either\n(a) the arrows on the path meet either head-to-tail or tail-to-tail at the node, and the\nnode is in the set C,o r\n(b) the arrows meet head-to-head at the node, and neither the node, nor any of its\ndescendants, is in the set C.\nIf all paths are blocked, then A is said to be d-separated from B by C, and the joint\ndistribution over all of the variables in the graph will satisfy A ⊥⊥ B | C.\nThe concept of d-separation is illustrated in Figure 8.22. In graph (a), the path\nfrom a to b is not blocked by node f because it is a tail-to-tail node for this path\nand is not observed, nor is it blocked by node e because, although the latter is a\nhead-to-head node, it has a descendant c because is in the conditioning set. Thus",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 397,
      "page_label": "378"
    }
  },
  {
    "page_content": "head-to-head node, it has a descendant c because is in the conditioning set. Thus\nthe conditional independence statement a ⊥⊥ b | c does not follow from this graph.\nIn graph (b), the path from a to b is blocked by node f because this is a tail-to-tail\nnode that is observed, and so the conditional independence property a ⊥⊥ b | f will",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 397,
      "page_label": "378"
    }
  },
  {
    "page_content": "8.2. Conditional Independence 379\nFigure 8.22 Illustration of the con-\ncept of d-separation. See the text for\ndetails.\nf\ne b\na\nc\n(a)\nf\ne b\na\nc\n(b)\nbe satisﬁed by any distribution that factorizes according to this graph. Note that this\npath is also blocked by node e because e is a head-to-head node and neither it nor its\ndescendant are in the conditioning set.\nFor the purposes of d-separation, parameters such as α and σ2 in Figure 8.5,\nindicated by small ﬁlled circles, behave in the same was as observed nodes. How-\never, there are no marginal distributions associated with such nodes. Consequently\nparameter nodes never themselves have parents and so all paths through these nodes\nwill always be tail-to-tail and hence blocked. Consequently they play no role in\nd-separation.\nAnother example of conditional independence and d-separation is provided by\nthe concept of i.i.d. (independent identically distributed) data introduced in Sec-\ntion 1.2.4. Consider the problem of ﬁnding the posterior distribution for the mean\nof a univariate Gaussian distribution. This can be represented by the directed graphSection 2.3\nshown in Figure 8.23 in which the joint distribution is deﬁned by a prior p(µ) to-\ngether with a set of conditional distributions p(xn|µ) for n =1 ,...,N . In practice,\nwe observe D = {x1,...,x N } and our goal is to infer µ. Suppose, for a moment,\nthat we condition on µ and consider the joint distribution of the observations. Using",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 398,
      "page_label": "379"
    }
  },
  {
    "page_content": "that we condition on µ and consider the joint distribution of the observations. Using\nd-separation, we note that there is a unique path from any xi to any other xj̸=i and\nthat this path is tail-to-tail with respect to the observed node µ. Every such path is\nblocked and so the observationsD = {x1,...,x N } are independent given µ, so that\np(D|µ)=\nN∏\nn=1\np(xn|µ). (8.34)\nFigure 8.23 (a) Directed graph corre-\nsponding to the problem\nof inferring the mean µ of\na univariate Gaussian dis-\ntribution from observations\nx1,...,x N . (b) The same\ngraph drawn using the plate\nnotation.\nµ\nx1 xN\n(a)\nxn\nN\nN\nµ\n(b)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 398,
      "page_label": "379"
    }
  },
  {
    "page_content": "380 8. GRAPHICAL MODELS\nFigure 8.24 A graphical representation of the ‘naive Bayes’\nmodel for classiﬁcation. Conditioned on the\nclass label z, the components of the observed\nvector x =( x1,...,x D)T are assumed to be\nindependent.\nz\nx1 xD\nHowever, if we integrate over µ, the observations are in general no longer indepen-\ndent\np(D)=\n∫ ∞\n0\np(D|µ)p(µ)d µ ̸=\nN∏\nn=1\np(xn). (8.35)\nHere µ is a latent variable, because its value is not observed.\nAnother example of a model representing i.i.d. data is the graph in Figure 8.7\ncorresponding to Bayesian polynomial regression. Here the stochastic nodes corre-\nspond to {tn}, w and ˆt. We see that the node for w is tail-to-tail with respect to\nthe path from ˆt to any one of the nodes tn and so we have the following conditional\nindependence property\nˆt ⊥⊥ tn | w. (8.36)\nThus, conditioned on the polynomial coefﬁcients w, the predictive distribution for\nˆt is independent of the training data {t1,...,t N }. We can therefore ﬁrst use the\ntraining data to determine the posterior distribution over the coefﬁcients w and then\nwe can discard the training data and use the posterior distribution for w to make\npredictions of ˆt for new input observations ˆx.Section 3.3\nA related graphical structure arises in an approach to classiﬁcation called the\nnaive Bayes model, in which we use conditional independence assumptions to sim-\nplify the model structure. Suppose our observed variable consists of aD-dimensional",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 399,
      "page_label": "380"
    }
  },
  {
    "page_content": "plify the model structure. Suppose our observed variable consists of aD-dimensional\nvector x =( x1,...,x D)T, and we wish to assign observed values of x to one of K\nclasses. Using the 1-of-K encoding scheme, we can represent these classes by a K-\ndimensional binary vector z. We can then deﬁne a generative model by introducing\na multinomial prior p(z|µ) over the class labels, where the kth component µk of µ\nis the prior probability of class Ck, together with a conditional distribution p(x|z)\nfor the observed vector x. The key assumption of the naive Bayes model is that,\nconditioned on the class z, the distributions of the input variables x1,...,x D are in-\ndependent. The graphical representation of this model is shown in Figure 8.24. We\nsee that observation of z blocks the path between xi and xj for j ̸= i (because such\npaths are tail-to-tail at the node z) and so xi and xj are conditionally independent\ngiven z. If, however, we marginalize out z (so that z is unobserved) the tail-to-tail\npath from xi to xj is no longer blocked. This tells us that in general the marginal\ndensity p(x) will not factorize with respect to the components ofx. We encountered\na simple application of the naive Bayes model in the context of fusing data from\ndifferent sources for medical diagnosis in Section 1.5.\nIf we are given a labelled training set, comprising inputs {x1,..., xN } together\nwith their class labels, then we can ﬁt the naive Bayes model to the training data",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 399,
      "page_label": "380"
    }
  },
  {
    "page_content": "8.2. Conditional Independence 381\nusing maximum likelihood assuming that the data are drawn independently from\nthe model. The solution is obtained by ﬁtting the model for each class separately\nusing the correspondingly labelled data. As an example, suppose that the probability\ndensity within each class is chosen to be Gaussian. In this case, the naive Bayes\nassumption then implies that the covariance matrix for each Gaussian is diagonal,\nand the contours of constant density within each class will be axis-aligned ellipsoids.\nThe marginal density, however, is given by a superposition of diagonal Gaussians\n(with weighting coefﬁcients given by the class priors) and so will no longer factorize\nwith respect to its components.\nThe naive Bayes assumption is helpful when the dimensionality D of the input\nspace is high, making density estimation in the full D-dimensional space more chal-\nlenging. It is also useful if the input vector contains both discrete and continuous\nvariables, since each can be represented separately using appropriate models (e.g.,\nBernoulli distributions for binary observations or Gaussians for real-valued vari-\nables). The conditional independence assumption of this model is clearly a strong\none that may lead to rather poor representations of the class-conditional densities.\nNevertheless, even if this assumption is not precisely satisﬁed, the model may still\ngive good classiﬁcation performance in practice because the decision boundaries can",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 400,
      "page_label": "381"
    }
  },
  {
    "page_content": "give good classiﬁcation performance in practice because the decision boundaries can\nbe insensitive to some of the details in the class-conditional densities, as illustrated\nin Figure 1.27.\nWe have seen that a particular directed graph represents a speciﬁc decomposition\nof a joint probability distribution into a product of conditional probabilities. The\ngraph also expresses a set of conditional independence statements obtained through\nthe d-separation criterion, and the d-separation theorem is really an expression of the\nequivalence of these two properties. In order to make this clear, it is helpful to think\nof a directed graph as a ﬁlter. Suppose we consider a particular joint probability\ndistribution p(x) over the variables x corresponding to the (nonobserved) nodes of\nthe graph. The ﬁlter will allow this distribution to pass through if, and only if, it can\nbe expressed in terms of the factorization (8.5) implied by the graph. If we present to\nthe ﬁlter the set of all possible distributions p(x) over the set of variables x, then the\nsubset of distributions that are passed by the ﬁlter will be denoted DF,f o rdirected\nfactorization. This is illustrated in Figure 8.25. Alternatively, we can use the graph as\na different kind of ﬁlter by ﬁrst listing all of the conditional independence properties\nobtained by applying the d-separation criterion to the graph, and then allowing a\ndistribution to pass only if it satisﬁes all of these properties. If we present all possible",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 400,
      "page_label": "381"
    }
  },
  {
    "page_content": "distribution to pass only if it satisﬁes all of these properties. If we present all possible\ndistributions p(x) to this second kind of ﬁlter, then the d-separation theorem tells us\nthat the set of distributions that will be allowed through is precisely the set DF.\nIt should be emphasized that the conditional independence properties obtained\nfrom d-separation apply to any probabilistic model described by that particular di-\nrected graph. This will be true, for instance, whether the variables are discrete or\ncontinuous or a combination of these. Again, we see that a particular graph is de-\nscribing a whole family of probability distributions.\nAt one extreme we have a fully connected graph that exhibits no conditional in-\ndependence properties at all, and which can represent any possible joint probability\ndistribution over the given variables. The set DF will contain all possible distribu-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 400,
      "page_label": "381"
    }
  },
  {
    "page_content": "382 8. GRAPHICAL MODELS\np(x) DF\nFigure 8.25 We can view a graphical model (in this case a directed graph) as a ﬁlter in which a prob-\nability distribution p(x) is allowed through the ﬁlter if, and only if, it satisﬁes the directed\nfactorization property (8.5). The set of all possible probability distributions p(x) that pass\nthrough the ﬁlter is denoted DF. We can alternatively use the graph to ﬁlter distributions\naccording to whether they respect all of the conditional independencies implied by the\nd-separation properties of the graph. The d-separation theorem says that it is the same\nset of distributions DF that will be allowed through this second kind of ﬁlter.\ntions p(x). At the other extreme, we have the fully disconnected graph, i.e., one\nhaving no links at all. This corresponds to joint distributions which factorize into the\nproduct of the marginal distributions over the variables comprising the nodes of the\ngraph.\nNote that for any given graph, the set of distributions DF will include any dis-\ntributions that have additional independence properties beyond those described by\nthe graph. For instance, a fully factorized distribution will always be passed through\nthe ﬁlter implied by any graph over the corresponding set of variables.\nWe end our discussion of conditional independence properties by exploring the\nconcept of a Markov blanket or Markov boundary. Consider a joint distribution\np(x1,..., xD) represented by a directed graph having D nodes, and consider the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 401,
      "page_label": "382"
    }
  },
  {
    "page_content": "p(x1,..., xD) represented by a directed graph having D nodes, and consider the\nconditional distribution of a particular node with variables xi conditioned on all of\nthe remaining variables xj̸=i. Using the factorization property (8.5), we can express\nthis conditional distribution in the form\np(xi|x{j̸=i})= p(x1,..., xD)∫\np(x1,..., xD)d xi\n=\n∏\nk\np(xk|pak)\n∫ ∏\nk\np(xk|pak)d xi\nin which the integral is replaced by a summation in the case of discrete variables. We\nnow observe that any factorp(xk|pak) that does not have any functional dependence\non xi can be taken outside the integral over xi, and will therefore cancel between\nnumerator and denominator. The only factors that remain will be the conditional\ndistribution p(xi|pai) for node xi itself, together with the conditional distributions\nfor any nodes xk such that node xi is in the conditioning set of p(xk|pak), in other\nwords for which xi is a parent of xk. The conditional p(xi|pai) will depend on the\nparents of node xi, whereas the conditionals p(xk|pak) will depend on the children",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 401,
      "page_label": "382"
    }
  },
  {
    "page_content": "8.3. Markov Random Fields 383\nFigure 8.26 The Markov blanket of a node xi comprises the set\nof parents, children and co-parents of the node. It\nhas the property that the conditional distribution of\nxi, conditioned on all the remaining variables in the\ngraph, is dependent only on the variables in the\nMarkov blanket. xi\nof xi as well as on the co-parents, in other words variables corresponding to parents\nof node xk other than node xi. The set of nodes comprising the parents, the children\nand the co-parents is called the Markov blanket and is illustrated in Figure 8.26. We\ncan think of the Markov blanket of a node xi as being the minimal set of nodes that\nisolates xi from the rest of the graph. Note that it is not sufﬁcient to include only the\nparents and children of node xi because the phenomenon of explaining away means\nthat observations of the child nodes will not block paths to the co-parents. We must\ntherefore observe the co-parent nodes also.\n8.3. Markov Random Fields\nWe have seen that directed graphical models specify a factorization of the joint dis-\ntribution over a set of variables into a product of local conditional distributions. They\nalso deﬁne a set of conditional independence properties that must be satisﬁed by any\ndistribution that factorizes according to the graph. We turn now to the second ma-\njor class of graphical models that are described by undirected graphs and that again\nspecify both a factorization and a set of conditional independence relations.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 402,
      "page_label": "383"
    }
  },
  {
    "page_content": "specify both a factorization and a set of conditional independence relations.\nA Markov random ﬁeld, also known as a Markov network or an undirected\ngraphical model (Kindermann and Snell, 1980), has a set of nodes each of which\ncorresponds to a variable or group of variables, as well as a set of links each of\nwhich connects a pair of nodes. The links are undirected, that is they do not carry\narrows. In the case of undirected graphs, it is convenient to begin with a discussion\nof conditional independence properties.\n8.3.1 Conditional independence properties\nIn the case of directed graphs, we saw that it was possible to test whether a par-Section 8.2\nticular conditional independence property holds by applying a graphical test called\nd-separation. This involved testing whether or not the paths connecting two sets of\nnodes were ‘blocked’. The deﬁnition of blocked, however, was somewhat subtle\ndue to the presence of paths having head-to-head nodes. We might ask whether it\nis possible to deﬁne an alternative graphical semantics for probability distributions\nsuch that conditional independence is determined by simple graph separation. This\nis indeed the case and corresponds to undirected graphical models. By removing the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 402,
      "page_label": "383"
    }
  },
  {
    "page_content": "384 8. GRAPHICAL MODELS\nFigure 8.27 An example of an undirected graph in\nwhich every path from any node in set\nA to any node in set B passes through\nat least one node in set C. Conse-\nquently the conditional independence\nproperty A ⊥⊥ B | C holds for any\nprobability distribution described by this\ngraph.\nA\nC\nB\ndirectionality from the links of the graph, the asymmetry between parent and child\nnodes is removed, and so the subtleties associated with head-to-head nodes no longer\narise.\nSuppose that in an undirected graph we identify three sets of nodes, denoted A,\nB, and C, and that we consider the conditional independence property\nA ⊥⊥ B | C. (8.37)\nTo test whether this property is satisﬁed by a probability distribution deﬁned by a\ngraph we consider all possible paths that connect nodes in set A to nodes in set\nB. If all such paths pass through one or more nodes in set C, then all such paths are\n‘blocked’ and so the conditional independence property holds. However, if there is at\nleast one such path that is not blocked, then the property does not necessarily hold, or\nmore precisely there will exist at least some distributions corresponding to the graph\nthat do not satisfy this conditional independence relation. This is illustrated with an\nexample in Figure 8.27. Note that this is exactly the same as the d-separation crite-\nrion except that there is no ‘explaining away’ phenomenon. Testing for conditional",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 403,
      "page_label": "384"
    }
  },
  {
    "page_content": "rion except that there is no ‘explaining away’ phenomenon. Testing for conditional\nindependence in undirected graphs is therefore simpler than in directed graphs.\nAn alternative way to view the conditional independence test is to imagine re-\nmoving all nodes in set C from the graph together with any links that connect to\nthose nodes. We then ask if there exists a path that connects any node in A to any\nnode in B. If there are no such paths, then the conditional independence property\nmust hold.\nThe Markov blanket for an undirected graph takes a particularly simple form,\nbecause a node will be conditionally independent of all other nodes conditioned only\non the neighbouring nodes, as illustrated in Figure 8.28.\n8.3.2 Factorization properties\nWe now seek a factorization rule for undirected graphs that will correspond to\nthe above conditional independence test. Again, this will involve expressing the joint\ndistribution p(x) as a product of functions deﬁned over sets of variables that are local\nto the graph. We therefore need to decide what is the appropriate notion of locality\nin this case.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 403,
      "page_label": "384"
    }
  },
  {
    "page_content": "8.3. Markov Random Fields 385\nFigure 8.28 For an undirected graph, the Markov blanket of a node\nxi consists of the set of neighbouring nodes. It has the\nproperty that the conditional distribution of xi, conditioned\non all the remaining variables in the graph, is dependent\nonly on the variables in the Markov blanket.\nIf we consider two nodes xi and xj that are not connected by a link, then these\nvariables must be conditionally independent given all other nodes in the graph. This\nfollows from the fact that there is no direct path between the two nodes, and all other\npaths pass through nodes that are observed, and hence those paths are blocked. This\nconditional independence property can be expressed as\np(xi,xj|x\\{i,j})= p(xi|x\\{i,j})p(xj|x\\{i,j}) (8.38)\nwhere x\\{i,j} denotes the set x of all variables with xi and xj removed. The factor-\nization of the joint distribution must therefore be such that xi and xj do not appear\nin the same factor in order for the conditional independence property to hold for all\npossible distributions belonging to the graph.\nThis leads us to consider a graphical concept called a clique, which is deﬁned\nas a subset of the nodes in a graph such that there exists a link between all pairs of\nnodes in the subset. In other words, the set of nodes in a clique is fully connected.\nFurthermore, a maximal clique is a clique such that it is not possible to include any\nother nodes from the graph in the set without it ceasing to be a clique. These concepts",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 404,
      "page_label": "385"
    }
  },
  {
    "page_content": "other nodes from the graph in the set without it ceasing to be a clique. These concepts\nare illustrated by the undirected graph over four variables shown in Figure 8.29. This\ngraph has ﬁve cliques of two nodes given by {x1,x2}, {x2,x3}, {x3,x4}, {x4,x2},\nand {x1,x3}, as well as two maximal cliques given by{x1,x2,x3} and {x2,x3,x4}.\nThe set {x1,x2,x3,x4} is not a clique because of the missing link from x1 to x4.\nWe can therefore deﬁne the factors in the decomposition of the joint distribution\nto be functions of the variables in the cliques. In fact, we can consider functions\nof the maximal cliques, without loss of generality, because other cliques must be\nsubsets of maximal cliques. Thus, if {x1,x2,x3} is a maximal clique and we deﬁne\nan arbitrary function over this clique, then including another factor deﬁned over a\nsubset of these variables would be redundant.\nLet us denote a clique by C and the set of variables in that clique by xC. Then\nFigure 8.29 A four-node undirected graph showing a clique (outlined in\ngreen) and a maximal clique (outlined in blue). x1\nx2\nx3\nx4",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 404,
      "page_label": "385"
    }
  },
  {
    "page_content": "386 8. GRAPHICAL MODELS\nthe joint distribution is written as a product of potential functions ψC(xC) over the\nmaximal cliques of the graph\np(x)= 1\nZ\n∏\nC\nψC(xC). (8.39)\nHere the quantity Z, sometimes called the partition function, is a normalization con-\nstant and is given by\nZ =\n∑\nx\n∏\nC\nψC(xC) (8.40)\nwhich ensures that the distribution p(x) given by (8.39) is correctly normalized.\nBy considering only potential functions which satisfy ψC(xC) ⩾ 0 we ensure that\np(x) ⩾ 0. In (8.40) we have assumed that x comprises discrete variables, but the\nframework is equally applicable to continuous variables, or a combination of the two,\nin which the summation is replaced by the appropriate combination of summation\nand integration.\nNote that we do not restrict the choice of potential functions to those that have a\nspeciﬁc probabilistic interpretation as marginal or conditional distributions. This is\nin contrast to directed graphs in which each factor represents the conditional distribu-\ntion of the corresponding variable, conditioned on the state of its parents. However,\nin special cases, for instance where the undirected graph is constructed by starting\nwith a directed graph, the potential functions may indeed have such an interpretation,\nas we shall see shortly.\nOne consequence of the generality of the potential functions ψC(xC) is that\ntheir product will in general not be correctly normalized. We therefore have to in-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 405,
      "page_label": "386"
    }
  },
  {
    "page_content": "their product will in general not be correctly normalized. We therefore have to in-\ntroduce an explicit normalization factor given by (8.40). Recall that for directed\ngraphs, the joint distribution was automatically normalized as a consequence of the\nnormalization of each of the conditional distributions in the factorization.\nThe presence of this normalization constant is one of the major limitations of\nundirected graphs. If we have a model with M discrete nodes each having K states,\nthen the evaluation of the normalization term involves summing overKM states and\nso (in the worst case) is exponential in the size of the model. The partition function\nis needed for parameter learning because it will be a function of any parameters that\ngovern the potential functionsψC(xC). However, for evaluation of local conditional\ndistributions, the partition function is not needed because a conditional is the ratio\nof two marginals, and the partition function cancels between numerator and denom-\ninator when evaluating this ratio. Similarly, for evaluating local marginal probabil-\nities we can work with the unnormalized joint distribution and then normalize the\nmarginals explicitly at the end. Provided the marginals only involves a small number\nof variables, the evaluation of their normalization coefﬁcient will be feasible.\nSo far, we have discussed the notion of conditional independence based on sim-\nple graph separation and we have proposed a factorization of the joint distribution",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 405,
      "page_label": "386"
    }
  },
  {
    "page_content": "ple graph separation and we have proposed a factorization of the joint distribution\nthat is intended to correspond to this conditional independence structure. However,\nwe have not made any formal connection between conditional independence and\nfactorization for undirected graphs. To do so we need to restrict attention to poten-\ntial functions ψC(xC) that are strictly positive (i.e., never zero or negative for any",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 405,
      "page_label": "386"
    }
  },
  {
    "page_content": "8.3. Markov Random Fields 387\nchoice of xC). Given this restriction, we can make a precise relationship between\nfactorization and conditional independence.\nTo do this we again return to the concept of a graphical model as a ﬁlter, corre-\nsponding to Figure 8.25. Consider the set of all possible distributions deﬁned over\na ﬁxed set of variables corresponding to the nodes of a particular undirected graph.\nWe can deﬁneUI to be the set of such distributions that are consistent with the set\nof conditional independence statements that can be read from the graph using graph\nseparation. Similarly, we can deﬁne UF to be the set of such distributions that can\nbe expressed as a factorization of the form (8.39) with respect to the maximal cliques\nof the graph. The Hammersley-Clifford theorem (Clifford, 1990) states that the sets\nUI and UF are identical.\nBecause we are restricted to potential functions which are strictly positive it is\nconvenient to express them as exponentials, so that\nψC(xC) = exp{−E(xC)} (8.41)\nwhere E(xC) is called an energy function, and the exponential representation is\ncalled the Boltzmann distribution. The joint distribution is deﬁned as the product of\npotentials, and so the total energy is obtained by adding the energies of each of the\nmaximal cliques.\nIn contrast to the factors in the joint distribution for a directed graph, the po-\ntentials in an undirected graph do not have a speciﬁc probabilistic interpretation.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 406,
      "page_label": "387"
    }
  },
  {
    "page_content": "tentials in an undirected graph do not have a speciﬁc probabilistic interpretation.\nAlthough this gives greater ﬂexibility in choosing the potential functions, because\nthere is no normalization constraint, it does raise the question of how to motivate a\nchoice of potential function for a particular application. This can be done by view-\ning the potential function as expressing which conﬁgurations of the local variables\nare preferred to others. Global conﬁgurations that have a relatively high probability\nare those that ﬁnd a good balance in satisfying the (possibly conﬂicting) inﬂuences\nof the clique potentials. We turn now to a speciﬁc example to illustrate the use of\nundirected graphs.\n8.3.3 Illustration: Image de-noising\nWe can illustrate the application of undirected graphs using an example of noise\nremoval from a binary image (Besag, 1974; Geman and Geman, 1984; Besag, 1986).\nAlthough a very simple example, this is typical of more sophisticated applications.\nLet the observed noisy image be described by an array of binary pixel values yi ∈\n{−1, +1}, where the index i =1 ,...,D runs over all pixels. We shall suppose\nthat the image is obtained by taking an unknown noise-free image, described by\nbinary pixel values xi ∈{ − 1, +1} and randomly ﬂipping the sign of pixels with\nsome small probability. An example binary image, together with a noise corrupted\nimage obtained by ﬂipping the sign of the pixels with probability 10%, is shown in",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 406,
      "page_label": "387"
    }
  },
  {
    "page_content": "image obtained by ﬂipping the sign of the pixels with probability 10%, is shown in\nFigure 8.30. Given the noisy image, our goal is to recover the original noise-free\nimage.\nBecause the noise level is small, we know that there will be a strong correlation\nbetween xi and yi. We also know that neighbouring pixels xi and xj in an image\nare strongly correlated. This prior knowledge can be captured using the Markov",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 406,
      "page_label": "387"
    }
  },
  {
    "page_content": "388 8. GRAPHICAL MODELS\nFigure 8.30 Illustration of image de-noising using a Markov random ﬁeld. The top row shows the original\nbinary image on the left and the corrupted image after randomly changing 10% of the pixels on the right. The\nbottom row shows the restored images obtained using iterated conditional models (ICM) on the left and using\nthe graph-cut algorithm on the right. ICM produces an image where 96% of the pixels agree with the original\nimage, whereas the corresponding number for graph-cut is 99%.\nrandom ﬁeld model whose undirected graph is shown in Figure 8.31. This graph has\ntwo types of cliques, each of which contains two variables. The cliques of the form\n{xi,y i} have an associated energy function that expresses the correlation between\nthese variables. We choose a very simple energy function for these cliques of the\nform −ηxiyi where η is a positive constant. This has the desired effect of giving a\nlower energy (thus encouraging a higher probability) when xi and yi have the same\nsign and a higher energy when they have the opposite sign.\nThe remaining cliques comprise pairs of variables {xi,xj} where i and j are\nindices of neighbouring pixels. Again, we want the energy to be lower when the\npixels have the same sign than when they have the opposite sign, and so we choose\nan energy given by −βxixj where β is a positive constant.\nBecause a potential function is an arbitrary, nonnegative function over a maximal",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 407,
      "page_label": "388"
    }
  },
  {
    "page_content": "an energy given by −βxixj where β is a positive constant.\nBecause a potential function is an arbitrary, nonnegative function over a maximal\nclique, we can multiply it by any nonnegative functions of subsets of the clique, or",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 407,
      "page_label": "388"
    }
  },
  {
    "page_content": "8.3. Markov Random Fields 389\nFigure 8.31 An undirected graphical model representing a\nMarkov random ﬁeld for image de-noising, in\nwhich xi is a binary variable denoting the state\nof pixel i in the unknown noise-free image, and yi\ndenotes the corresponding value of pixel i in the\nobserved noisy image.\nxi\nyi\nequivalently we can add the corresponding energies. In this example, this allows us\nto add an extra term hxi for each pixel i in the noise-free image. Such a term has\nthe effect of biasing the model towards pixel values that have one particular sign in\npreference to the other.\nThe complete energy function for the model then takes the form\nE(x,y)= h\n∑\ni\nxi − β\n∑\n{i,j}\nxixj − η\n∑\ni\nxiyi (8.42)\nwhich deﬁnes a joint distribution over x and y given by\np(x, y)= 1\nZ exp{−E(x,y)}. (8.43)\nWe now ﬁx the elements of y to the observed values given by the pixels of the\nnoisy image, which implicitly deﬁnes a conditional distribution p(x|y) over noise-\nfree images. This is an example of theIsing model, which has been widely studied in\nstatistical physics. For the purposes of image restoration, we wish to ﬁnd an imagex\nhaving a high probability (ideally the maximum probability). To do this we shall use\na simple iterative technique called iterated conditional modes,o r ICM (Kittler and\nF¨oglein, 1984), which is simply an application of coordinate-wise gradient ascent.\nThe idea is ﬁrst to initialize the variables {xi}, which we do by simply setting xi =",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 408,
      "page_label": "389"
    }
  },
  {
    "page_content": "The idea is ﬁrst to initialize the variables {xi}, which we do by simply setting xi =\nyi for all i. Then we take one node xj at a time and we evaluate the total energy\nfor the two possible states xj =+ 1 and xj = −1, keeping all other node variables\nﬁxed, and set xj to whichever state has the lower energy. This will either leave\nthe probability unchanged, if xj is unchanged, or will increase it. Because only\none variable is changed, this is a simple local computation that can be performedExercise 8.13\nefﬁciently. We then repeat the update for another site, and so on, until some suitable\nstopping criterion is satisﬁed. The nodes may be updated in a systematic way, for\ninstance by repeatedly raster scanning through the image, or by choosing nodes at\nrandom.\nIf we have a sequence of updates in which every site is visited at least once,\nand in which no changes to the variables are made, then by deﬁnition the algorithm",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 408,
      "page_label": "389"
    }
  },
  {
    "page_content": "390 8. GRAPHICAL MODELS\nFigure 8.32 (a) Example of a directed\ngraph. (b) The equivalent undirected\ngraph.\n(a)\nx1 x2 xN−1 xN\n(b)\nx1 x2 xN−1xN\nwill have converged to a local maximum of the probability. This need not, however,\ncorrespond to the global maximum.\nFor the purposes of this simple illustration, we have ﬁxed the parameters to be\nβ =1 .0, η =2 .1 and h =0 . Note that leaving h =0 simply means that the prior\nprobabilities of the two states ofxi are equal. Starting with the observed noisy image\nas the initial conﬁguration, we run ICM until convergence, leading to the de-noised\nimage shown in the lower left panel of Figure 8.30. Note that if we set β =0 ,\nwhich effectively removes the links between neighbouring pixels, then the global\nmost probable solution is given by xi = yi for all i, corresponding to the observed\nnoisy image.Exercise 8.14\nLater we shall discuss a more effective algorithm for ﬁnding high probability so-\nlutions called the max-product algorithm, which typically leads to better solutions,Section 8.4\nalthough this is still not guaranteed to ﬁnd the global maximum of the posterior dis-\ntribution. However, for certain classes of model, including the one given by (8.42),\nthere exist efﬁcient algorithms based on graph cuts that are guaranteed to ﬁnd the\nglobal maximum (Greig et al., 1989; Boykov et al., 2001; Kolmogorov and Zabih,\n2004). The lower right panel of Figure 8.30 shows the result of applying a graph-cut\nalgorithm to the de-noising problem.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 409,
      "page_label": "390"
    }
  },
  {
    "page_content": "2004). The lower right panel of Figure 8.30 shows the result of applying a graph-cut\nalgorithm to the de-noising problem.\n8.3.4 Relation to directed graphs\nWe have introduced two graphical frameworks for representing probability dis-\ntributions, corresponding to directed and undirected graphs, and it is instructive to\ndiscuss the relation between these. Consider ﬁrst the problem of taking a model that\nis speciﬁed using a directed graph and trying to convert it to an undirected graph. In\nsome cases this is straightforward, as in the simple example in Figure 8.32. Here the\njoint distribution for the directed graph is given as a product of conditionals in the\nform\np(x)= p(x1)p(x2|x1)p(x3|x2) ··· p(xN |xN−1). (8.44)\nNow let us convert this to an undirected graph representation, as shown in Fig-\nure 8.32. In the undirected graph, the maximal cliques are simply the pairs of neigh-\nbouring nodes, and so from (8.39) we wish to write the joint distribution in the form\np(x)= 1\nZ ψ1,2(x1,x2)ψ2,3(x2,x3) ··· ψN−1,N (xN−1,xN ). (8.45)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 409,
      "page_label": "390"
    }
  },
  {
    "page_content": "8.3. Markov Random Fields 391\nFigure 8.33 Example of a simple\ndirected graph (a) and the corre-\nsponding moral graph (b).\nx1 x3\nx4\nx2\n(a)\nx1 x3\nx4\nx2\n(b)\nThis is easily done by identifying\nψ1,2(x1,x2)= p(x1)p(x2|x1)\nψ2,3(x2,x3)= p(x3|x2)\n...\nψN−1,N (xN−1,xN )= p(xN |xN−1)\nwhere we have absorbed the marginal p(x1) for the ﬁrst node into the ﬁrst potential\nfunction. Note that in this case, the partition function Z =1 .\nLet us consider how to generalize this construction, so that we can convert any\ndistribution speciﬁed by a factorization over a directed graph into one speciﬁed by a\nfactorization over an undirected graph. This can be achieved if the clique potentials\nof the undirected graph are given by the conditional distributions of the directed\ngraph. In order for this to be valid, we must ensure that the set of variables that\nappears in each of the conditional distributions is a member of at least one clique of\nthe undirected graph. For nodes on the directed graph having just one parent, this is\nachieved simply by replacing the directed link with an undirected link. However, for\nnodes in the directed graph having more than one parent, this is not sufﬁcient. These\nare nodes that have ‘head-to-head’ paths encountered in our discussion of conditional\nindependence. Consider a simple directed graph over 4 nodes shown in Figure 8.33.\nThe joint distribution for the directed graph takes the form\np(x)= p(x1)p(x2)p(x3)p(x4|x1,x2,x3). (8.46)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 410,
      "page_label": "391"
    }
  },
  {
    "page_content": "The joint distribution for the directed graph takes the form\np(x)= p(x1)p(x2)p(x3)p(x4|x1,x2,x3). (8.46)\nWe see that the factor p(x4|x1,x2,x3) involves the four variables x1, x2, x3, and\nx4, and so these must all belong to a single clique if this conditional distribution is\nto be absorbed into a clique potential. To ensure this, we add extra links between\nall pairs of parents of the node x4. Anachronistically, this process of ‘marrying\nthe parents’ has become known as moralization, and the resulting undirected graph,\nafter dropping the arrows, is called the moral graph. It is important to observe that\nthe moral graph in this example is fully connected and so exhibits no conditional\nindependence properties, in contrast to the original directed graph.\nThus in general to convert a directed graph into an undirected graph, we ﬁrst add\nadditional undirected links between all pairs of parents for each node in the graph and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 410,
      "page_label": "391"
    }
  },
  {
    "page_content": "392 8. GRAPHICAL MODELS\nthen drop the arrows on the original links to give the moral graph. Then we initialize\nall of the clique potentials of the moral graph to 1. We then take each conditional\ndistribution factor in the original directed graph and multiply it into one of the clique\npotentials. There will always exist at least one maximal clique that contains all of\nthe variables in the factor as a result of the moralization step. Note that in all cases\nthe partition function is given by Z =1 .\nThe process of converting a directed graph into an undirected graph plays an\nimportant role in exact inference techniques such as the junction tree algorithm.Section 8.4\nConverting from an undirected to a directed representation is much less common\nand in general presents problems due to the normalization constraints.\nWe saw that in going from a directed to an undirected representation we had to\ndiscard some conditional independence properties from the graph. Of course, we\ncould always trivially convert any distribution over a directed graph into one over an\nundirected graph by simply using a fully connected undirected graph. This would,\nhowever, discard all conditional independence properties and so would be vacuous.\nThe process of moralization adds the fewest extra links and so retains the maximum\nnumber of independence properties.\nWe have seen that the procedure for determining the conditional independence",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 411,
      "page_label": "392"
    }
  },
  {
    "page_content": "number of independence properties.\nWe have seen that the procedure for determining the conditional independence\nproperties is different between directed and undirected graphs. It turns out that the\ntwo types of graph can express different conditional independence properties, and\nit is worth exploring this issue in more detail. To do so, we return to the view of\na speciﬁc (directed or undirected) graph as a ﬁlter, so that the set of all possibleSection 8.2\ndistributions over the given variables could be reduced to a subset that respects the\nconditional independencies implied by the graph. A graph is said to be a D map\n(for ‘dependency map’) of a distribution if every conditional independence statement\nsatisﬁed by the distribution is reﬂected in the graph. Thus a completely disconnected\ngraph (no links) will be a trivial D map for any distribution.\nAlternatively, we can consider a speciﬁc distribution and ask which graphs have\nthe appropriate conditional independence properties. If every conditional indepen-\ndence statement implied by a graph is satisﬁed by a speciﬁc distribution, then the\ngraph is said to be an I map (for ‘independence map’) of that distribution. Clearly a\nfully connected graph will be a trivial I map for any distribution.\nIf it is the case that every conditional independence property of the distribution\nis reﬂected in the graph, and vice versa, then the graph is said to be aperfect mapfor\nFigure 8.34 Venn diagram illustrating the set of all distributions",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 411,
      "page_label": "392"
    }
  },
  {
    "page_content": "Figure 8.34 Venn diagram illustrating the set of all distributions\nP over a given set of variables, together with the\nset of distributions D that can be represented as a\nperfect map using a directed graph, and the set U\nthat can be represented as a perfect map using an\nundirected graph.\nPUD",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 411,
      "page_label": "392"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 393\nFigure 8.35 A directed graph whose conditional independence\nproperties cannot be expressed using an undirected\ngraph over the same three variables.\nC\nAB\nthat distribution. A perfect map is therefore both an I map and a D map.\nConsider the set of distributions such that for each distribution there exists a\ndirected graph that is a perfect map. This set is distinct from the set of distributions\nsuch that for each distribution there exists an undirected graph that is a perfect map.\nIn addition there are distributions for which neither directed nor undirected graphs\noffer a perfect map. This is illustrated as a Venn diagram in Figure 8.34.\nFigure 8.35 shows an example of a directed graph that is a perfect map for\na distribution satisfying the conditional independence properties A ⊥⊥ B |∅ and\nA ̸⊥⊥ B | C. There is no corresponding undirected graph over the same three vari-\nables that is a perfect map.\nConversely, consider the undirected graph over four variables shown in Fig-\nure 8.36. This graph exhibits the properties A ̸⊥⊥ B |∅ , C ⊥⊥ D | A ∪ B and\nA ⊥⊥ B | C ∪D. There is no directed graph over four variables that implies the same\nset of conditional independence properties.\nThe graphical framework can be extended in a consistent way to graphs that\ninclude both directed and undirected links. These are called chain graphs(Lauritzen\nand Wermuth, 1989; Frydenberg, 1990), and contain the directed and undirected",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 412,
      "page_label": "393"
    }
  },
  {
    "page_content": "and Wermuth, 1989; Frydenberg, 1990), and contain the directed and undirected\ngraphs considered so far as special cases. Although such graphs can represent a\nbroader class of distributions than either directed or undirected alone, there remain\ndistributions for which even a chain graph cannot provide a perfect map. Chain\ngraphs are not discussed further in this book.\nFigure 8.36 An undirected graph whose conditional independence\nproperties cannot be expressed in terms of a directed\ngraph over the same variables.\nA\nC\nB\nD\n8.4. Inference in Graphical Models\nWe turn now to the problem of inference in graphical models, in which some of\nthe nodes in a graph are clamped to observed values, and we wish to compute the\nposterior distributions of one or more subsets of other nodes. As we shall see, we\ncan exploit the graphical structure both to ﬁnd efﬁcient algorithms for inference, and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 412,
      "page_label": "393"
    }
  },
  {
    "page_content": "394 8. GRAPHICAL MODELS\nFigure 8.37 A graphical representation of Bayes’ theorem.\nSee the text for details.\nx\ny\nx\ny\nx\ny\n(a) (b) (c)\nto make the structure of those algorithms transparent. Speciﬁcally, we shall see that\nmany algorithms can be expressed in terms of the propagation of local messages\naround the graph. In this section, we shall focus primarily on techniques for exact\ninference, and in Chapter 10 we shall consider a number of approximate inference\nalgorithms.\nTo start with, let us consider the graphical interpretation of Bayes’ theorem.\nSuppose we decompose the joint distribution p(x, y) over two variables x and y into\na product of factors in the form p(x, y)= p(x)p(y|x). This can be represented by\nthe directed graph shown in Figure 8.37(a). Now suppose we observe the value of\ny, as indicated by the shaded node in Figure 8.37(b). We can view the marginal\ndistribution p(x) as a prior over the latent variable x, and our goal is to infer the\ncorresponding posterior distribution over x. Using the sum and product rules of\nprobability we can evaluate\np(y)=\n∑\nx′\np(y|x′)p(x′) (8.47)\nwhich can then be used in Bayes’ theorem to calculate\np(x|y)= p(y|x)p(x)\np(y) . (8.48)\nThus the joint distribution is now expressed in terms of p(y) and p(x|y). From a\ngraphical perspective, the joint distribution p(x, y) is now represented by the graph\nshown in Figure 8.37(c), in which the direction of the arrow is reversed. This is the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 413,
      "page_label": "394"
    }
  },
  {
    "page_content": "shown in Figure 8.37(c), in which the direction of the arrow is reversed. This is the\nsimplest example of an inference problem for a graphical model.\n8.4.1 Inference on a chain\nNow consider a more complex problem involving the chain of nodes of the form\nshown in Figure 8.32. This example will lay the foundation for a discussion of exact\ninference in more general graphs later in this section.\nSpeciﬁcally, we shall consider the undirected graph in Figure 8.32(b). We have\nalready seen that the directed chain can be transformed into an equivalent undirected\nchain. Because the directed graph does not have any nodes with more than one\nparent, this does not require the addition of any extra links, and the directed and\nundirected versions of this graph express exactly the same set of conditional inde-\npendence statements.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 413,
      "page_label": "394"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 395\nThe joint distribution for this graph takes the form\np(x)= 1\nZ ψ1,2(x1,x2)ψ2,3(x2,x3) ··· ψN−1,N (xN−1,xN ). (8.49)\nWe shall consider the speciﬁc case in which the N nodes represent discrete vari-\nables each having K states, in which case each potential function ψn−1,n(xn−1,xn)\ncomprises an K × K table, and so the joint distribution has (N − 1)K2 parameters.\nLet us consider the inference problem of ﬁnding the marginal distributionp(xn)\nfor a speciﬁc node xn that is part way along the chain. Note that, for the moment,\nthere are no observed nodes. By deﬁnition, the required marginal is obtained by\nsumming the joint distribution over all variables except xn, so that\np(xn)=\n∑\nx1\n···\n∑\nxn−1\n∑\nxn+1\n···\n∑\nxN\np(x). (8.50)\nIn a naive implementation, we would ﬁrst evaluate the joint distribution and\nthen perform the summations explicitly. The joint distribution can be represented as\na set of numbers, one for each possible value for x. Because there are N variables\neach with K states, there are KN values for x and so evaluation and storage of the\njoint distribution, as well as marginalization to obtain p(xn), all involve storage and\ncomputation that scale exponentially with the length N of the chain.\nWe can, however, obtain a much more efﬁcient algorithm by exploiting the con-\nditional independence properties of the graphical model. If we substitute the factor-\nized expression (8.49) for the joint distribution into (8.50), then we can rearrange the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 414,
      "page_label": "395"
    }
  },
  {
    "page_content": "ized expression (8.49) for the joint distribution into (8.50), then we can rearrange the\norder of the summations and the multiplications to allow the required marginal to be\nevaluated much more efﬁciently. Consider for instance the summation overxN . The\npotential ψN−1,N (xN−1,xN ) is the only one that depends on xN , and so we can\nperform the summation ∑\nxN\nψN−1,N (xN−1,xN ) (8.51)\nﬁrst to give a function of xN−1. We can then use this to perform the summation\nover xN−1, which will involve only this new function together with the potential\nψN−2,N−1(xN−2,xN−1), because this is the only other place that xN−1 appears.\nSimilarly, the summation over x1 involves only the potential ψ1,2(x1,x2) and so\ncan be performed separately to give a function of x2, and so on. Because each\nsummation effectively removes a variable from the distribution, this can be viewed\nas the removal of a node from the graph.\nIf we group the potentials and summations together in this way, we can express",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 414,
      "page_label": "395"
    }
  },
  {
    "page_content": "396 8. GRAPHICAL MODELS\nthe desired marginal in the form\np(xn)= 1\nZ⎡\n⎣∑\nxn−1\nψn−1,n(xn−1,xn) ···\n[∑\nx2\nψ2,3(x2,x3)\n[∑\nx1\nψ1,2(x1,x2)\n]]\n···\n⎤\n⎦\n  \nµα(xn)⎡\n⎣∑\nxn+1\nψn,n+1(xn,xn+1) ···\n[∑\nxN\nψN−1,N (xN−1,xN )\n]\n···\n⎤\n⎦\n  \nµβ(xn)\n. (8.52)\nThe reader is encouraged to study this re-ordering carefully as the underlying idea\nforms the basis for the later discussion of the general sum-product algorithm. Here\nthe key concept that we are exploiting is that multiplication is distributive over addi-\ntion, so that\nab + ac = a(b + c) (8.53)\nin which the left-hand side involves three arithmetic operations whereas the right-\nhand side reduces this to two operations.\nLet us work out the computational cost of evaluating the required marginal using\nthis re-ordered expression. We have to perform N − 1 summations each of which is\nover K states and each of which involves a function of two variables. For instance,\nthe summation over x1 involves only the function ψ1,2(x1,x2), which is a table of\nK × K numbers. We have to sum this table over x1 for each value of x2 and so this\nhas O(K2) cost. The resulting vector of K numbers is multiplied by the matrix of\nnumbers ψ2,3(x2,x3) and so is again O(K2). Because there are N − 1 summations\nand multiplications of this kind, the total cost of evaluating the marginal p(xn) is\nO(NK 2). This is linear in the length of the chain, in contrast to the exponential cost\nof a naive approach. We have therefore been able to exploit the many conditional",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 415,
      "page_label": "396"
    }
  },
  {
    "page_content": "of a naive approach. We have therefore been able to exploit the many conditional\nindependence properties of this simple graph in order to obtain an efﬁcient calcula-\ntion. If the graph had been fully connected, there would have been no conditional\nindependence properties, and we would have been forced to work directly with the\nfull joint distribution.\nWe now give a powerful interpretation of this calculation in terms of the passing\nof local messages around on the graph. From (8.52) we see that the expression for the\nmarginal p(xn) decomposes into the product of two factors times the normalization\nconstant\np(xn)= 1\nZ µα(xn)µβ(xn). (8.54)\nWe shall interpret µα(xn) as a message passed forwards along the chain from node\nxn−1 to node xn. Similarly, µβ(xn) can be viewed as a message passed backwards",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 415,
      "page_label": "396"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 397\nFigure 8.38 The marginal distribution\np(xn) for a node xn along the chain is ob-\ntained by multiplying the two messages\nµα(xn) and µβ(xn), and then normaliz-\ning. These messages can themselves\nbe evaluated recursively by passing mes-\nsages from both ends of the chain to-\nwards node xn.\nx1 xn−1 xn xn+1 xN\nµα(xn−1) µα(xn) µβ(xn) µβ(xn+1)\nalong the chain to node xn from node xn+1. Note that each of the messages com-\nprises a set of K values, one for each choice of xn, and so the product of two mes-\nsages should be interpreted as the point-wise multiplication of the elements of the\ntwo messages to give another set of K values.\nThe message µα(xn) can be evaluated recursively because\nµα(xn)=\n∑\nxn−1\nψn−1,n(xn−1,xn)\n⎡\n⎣∑\nxn−2\n···\n⎤\n⎦\n=\n∑\nxn−1\nψn−1,n(xn−1,xn)µα(xn−1). (8.55)\nWe therefore ﬁrst evaluate\nµα(x2)=\n∑\nx1\nψ1,2(x1,x2) (8.56)\nand then apply (8.55) repeatedly until we reach the desired node. Note carefully the\nstructure of the message passing equation. The outgoing message µα(xn) in (8.55)\nis obtained by multiplying the incoming message µα(xn−1) by the local potential\ninvolving the node variable and the outgoing variable and then summing over the\nnode variable.\nSimilarly, the message µβ(xn) can be evaluated recursively by starting with\nnode xN and using\nµβ(xn)=\n∑\nxn+1\nψn+1,n(xn+1,xn)\n⎡\n⎣∑\nxn+2\n···\n⎤\n⎦\n=\n∑\nxn+1\nψn+1,n(xn+1,xn)µβ(xn+1). (8.57)\nThis recursive message passing is illustrated in Figure 8.38. The normalization con-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 416,
      "page_label": "397"
    }
  },
  {
    "page_content": "⎡\n⎣∑\nxn+2\n···\n⎤\n⎦\n=\n∑\nxn+1\nψn+1,n(xn+1,xn)µβ(xn+1). (8.57)\nThis recursive message passing is illustrated in Figure 8.38. The normalization con-\nstant Z is easily evaluated by summing the right-hand side of (8.54) over all states\nof xn, an operation that requires only O(K) computation.\nGraphs of the form shown in Figure 8.38 are called Markov chains, and the\ncorresponding message passing equations represent an example of the Chapman-\nKolmogorov equations for Markov processes (Papoulis, 1984).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 416,
      "page_label": "397"
    }
  },
  {
    "page_content": "398 8. GRAPHICAL MODELS\nNow suppose we wish to evaluate the marginals p(xn) for every node n ∈\n{1,...,N } in the chain. Simply applying the above procedure separately for each\nnode will have computational cost that is O(N2M2). However, such an approach\nwould be very wasteful of computation. For instance, to ﬁnd p(x1) we need to prop-\nagate a message µβ(·) from node xN back to node x2. Similarly, to evaluate p(x2)\nwe need to propagate a messages µβ(·) from node xN back to node x3. This will\ninvolve much duplicated computation because most of the messages will be identical\nin the two cases.\nSuppose instead we ﬁrst launch a message µβ(xN−1) starting from node xN\nand propagate corresponding messages all the way back to nodex1, and suppose we\nsimilarly launch a message µα(x2) starting from node x1 and propagate the corre-\nsponding messages all the way forward to node xN . Provided we store all of the\nintermediate messages along the way, then any node can evaluate its marginal sim-\nply by applying (8.54). The computational cost is only twice that for ﬁnding the\nmarginal of a single node, rather than N times as much. Observe that a message\nhas passed once in each direction across each link in the graph. Note also that the\nnormalization constantZ need be evaluated only once, using any convenient node.\nIf some of the nodes in the graph are observed, then the corresponding variables\nare simply clamped to their observed values and there is no summation. To see",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 417,
      "page_label": "398"
    }
  },
  {
    "page_content": "are simply clamped to their observed values and there is no summation. To see\nthis, note that the effect of clamping a variablexn to an observed value ˆxn can\nbe expressed by multiplying the joint distribution by (one or more copies of) an\nadditional function I(xn,ˆxn), which takes the value 1 when xn = ˆxn and the value\n0 otherwise. One such function can then be absorbed into each of the potentials that\ncontain xn. Summations over xn then contain only one term in which xn = ˆxn.\nNow suppose we wish to calculate the joint distribution p(xn−1,xn) for two\nneighbouring nodes on the chain. This is similar to the evaluation of the marginal\nfor a single node, except that there are now two variables that are not summed out.\nA few moments thought will show that the required joint distribution can be writtenExercise 8.15\nin the form\np(xn−1,xn)= 1\nZ µα(xn−1)ψn−1,n(xn−1,xn)µβ(xn). (8.58)\nThus we can obtain the joint distributions over all of the sets of variables in each\nof the potentials directly once we have completed the message passing required to\nobtain the marginals.\nThis is a useful result because in practice we may wish to use parametric forms\nfor the clique potentials, or equivalently for the conditional distributions if we started\nfrom a directed graph. In order to learn the parameters of these potentials in situa-\ntions where not all of the variables are observed, we can employ the EM algorithm,Chapter 9",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 417,
      "page_label": "398"
    }
  },
  {
    "page_content": "tions where not all of the variables are observed, we can employ the EM algorithm,Chapter 9\nand it turns out that the local joint distributions of the cliques, conditioned on any\nobserved data, is precisely what is needed in the E step. We shall consider some\nexamples of this in detail in Chapter 13.\n8.4.2 Trees\nWe have seen that exact inference on a graph comprising a chain of nodes can be\nperformed efﬁciently in time that is linear in the number of nodes, using an algorithm",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 417,
      "page_label": "398"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 399\nFigure 8.39 Examples of tree-\nstructured graphs, showing (a) an\nundirected tree, (b) a directed tree,\nand (c) a directed polytree.\n(a) (b) (c)\nthat can be interpreted in terms of messages passed along the chain. More generally,\ninference can be performed efﬁciently using local message passing on a broader\nclass of graphs calledtrees. In particular, we shall shortly generalize the message\npassing formalism derived above for chains to give thesum-product algorithm, which\nprovides an efﬁcient framework for exact inference in tree-structured graphs.\nIn the case of an undirected graph, a tree is deﬁned as a graph in which there\nis one, and only one, path between any pair of nodes. Such graphs therefore do not\nhave loops. In the case of directed graphs, a tree is deﬁned such that there is a single\nnode, called the root, which has no parents, and all other nodes have one parent. If\nwe convert a directed tree into an undirected graph, we see that the moralization step\nwill not add any links as all nodes have at most one parent, and as a consequence the\ncorresponding moralized graph will be an undirected tree. Examples of undirected\nand directed trees are shown in Figure 8.39(a) and 8.39(b). Note that a distribution\nrepresented as a directed tree can easily be converted into one represented by an\nundirected tree, and vice versa.Exercise 8.18\nIf there are nodes in a directed graph that have more than one parent, but there is",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 418,
      "page_label": "399"
    }
  },
  {
    "page_content": "undirected tree, and vice versa.Exercise 8.18\nIf there are nodes in a directed graph that have more than one parent, but there is\nstill only one path (ignoring the direction of the arrows) between any two nodes, then\nthe graph is a called a polytree, as illustrated in Figure 8.39(c). Such a graph will\nhave more than one node with the property of having no parents, and furthermore,\nthe corresponding moralized undirected graph will have loops.\n8.4.3 Factor graphs\nThe sum-product algorithm that we derive in the next section is applicable to\nundirected and directed trees and to polytrees. It can be cast in a particularly simple\nand general form if we ﬁrst introduce a new graphical construction called a factor\ngraph (Frey, 1998; Kschischnang et al., 2001).\nBoth directed and undirected graphs allow a global function of several vari-\nables to be expressed as a product of factors over subsets of those variables. Factor\ngraphs make this decomposition explicit by introducing additional nodes for the fac-\ntors themselves in addition to the nodes representing the variables. They also allow\nus to be more explicit about the details of the factorization, as we shall see.\nLet us write the joint distribution over a set of variables in the form of a product\nof factors\np(x)=\n∏\ns\nfs(xs) (8.59)\nwhere xs denotes a subset of the variables. For convenience, we shall denote the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 418,
      "page_label": "399"
    }
  },
  {
    "page_content": "400 8. GRAPHICAL MODELS\nFigure 8.40 Example of a factor graph, which corresponds\nto the factorization (8.60).\nx1 x2 x3\nfa fb fc fd\nindividual variables by xi, however, as in earlier discussions, these can comprise\ngroups of variables (such as vectors or matrices). Each factor fs is a function of a\ncorresponding set of variables xs.\nDirected graphs, whose factorization is deﬁned by (8.5), represent special cases\nof (8.59) in which the factors fs(xs) are local conditional distributions. Similarly,\nundirected graphs, given by (8.39), are a special case in which the factors are po-\ntential functions over the maximal cliques (the normalizing coefﬁcient 1/Z can be\nviewed as a factor deﬁned over the empty set of variables).\nIn a factor graph, there is a node (depicted as usual by a circle) for every variable\nin the distribution, as was the case for directed and undirected graphs. There are also\nadditional nodes (depicted by small squares) for each factor fs(xs) in the joint dis-\ntribution. Finally, there are undirected links connecting each factor node to all of the\nvariables nodes on which that factor depends. Consider, for example, a distribution\nthat is expressed in terms of the factorization\np(x)= fa(x1,x2)fb(x1,x2)fc(x2,x3)fd(x3). (8.60)\nThis can be expressed by the factor graph shown in Figure 8.40. Note that there are\ntwo factors fa(x1,x2) and fb(x1,x2) that are deﬁned over the same set of variables.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 419,
      "page_label": "400"
    }
  },
  {
    "page_content": "two factors fa(x1,x2) and fb(x1,x2) that are deﬁned over the same set of variables.\nIn an undirected graph, the product of two such factors would simply be lumped\ntogether into the same clique potential. Similarly, fc(x2,x3) and fd(x3) could be\ncombined into a single potential over x2 and x3. The factor graph, however, keeps\nsuch factors explicit and so is able to convey more detailed information about the\nunderlying factorization.\nx1 x2\nx3\n(a)\nx1 x2\nx3\nf\n(b)\nx1 x2\nx3\nfa\nfb\n(c)\nFigure 8.41 (a) An undirected graph with a single clique potential ψ(x1,x 2,x 3). (b) A factor graph with factor\nf(x1,x 2,x 3)= ψ(x1,x 2,x 3) representing the same distribution as the undirected graph. (c) A different factor\ngraph representing the same distribution, whose factors satisfy fa(x1,x 2,x 3)fb(x1,x 2)= ψ(x1,x 2,x 3).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 419,
      "page_label": "400"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 401\nx1 x2\nx3\n(a)\nx1 x2\nx3\nf\n(b)\nx1 x2\nx3\nfc\nfa fb\n(c)\nFigure 8.42 (a) A directed graph with the factorization p(x1)p(x2)p(x3|x1,x 2). (b) A factor graph representing\nthe same distribution as the directed graph, whose factor satisﬁes f(x1,x 2,x 3)= p(x1)p(x2)p(x3|x1,x 2). (c)\nA different factor graph representing the same distribution with factors fa(x1)= p(x1), fb(x2)= p(x2) and\nfc(x1,x 2,x 3)= p(x3|x1,x 2).\nFactor graphs are said to be bipartite because they consist of two distinct kinds\nof nodes, and all links go between nodes of opposite type. In general, factor graphs\ncan therefore always be drawn as two rows of nodes (variable nodes at the top and\nfactor nodes at the bottom) with links between the rows, as shown in the example in\nFigure 8.40. In some situations, however, other ways of laying out the graph may\nbe more intuitive, for example when the factor graph is derived from a directed or\nundirected graph, as we shall see.\nIf we are given a distribution that is expressed in terms of an undirected graph,\nthen we can readily convert it to a factor graph. To do this, we create variable nodes\ncorresponding to the nodes in the original undirected graph, and then create addi-\ntional factor nodes corresponding to the maximal cliques xs. The factors fs(xs) are\nthen set equal to the clique potentials. Note that there may be several different factor\ngraphs that correspond to the same undirected graph. These concepts are illustrated",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 420,
      "page_label": "401"
    }
  },
  {
    "page_content": "graphs that correspond to the same undirected graph. These concepts are illustrated\nin Figure 8.41.\nSimilarly, to convert a directed graph to a factor graph, we simply create variable\nnodes in the factor graph corresponding to the nodes of the directed graph, and then\ncreate factor nodes corresponding to the conditional distributions, and then ﬁnally\nadd the appropriate links. Again, there can be multiple factor graphs all of which\ncorrespond to the same directed graph. The conversion of a directed graph to a\nfactor graph is illustrated in Figure 8.42.\nWe have already noted the importance of tree-structured graphs for performing\nefﬁcient inference. If we take a directed or undirected tree and convert it into a factor\ngraph, then the result will again be a tree (in other words, the factor graph will have\nno loops, and there will be one and only one path connecting any two nodes). In\nthe case of a directed polytree, conversion to an undirected graph results in loops\ndue to the moralization step, whereas conversion to a factor graph again results in a\ntree, as illustrated in Figure 8.43. In fact, local cycles in a directed graph due to\nlinks connecting parents of a node can be removed on conversion to a factor graph\nby deﬁning the appropriate factor function, as shown in Figure 8.44.\nWe have seen that multiple different factor graphs can represent the same di-\nrected or undirected graph. This allows factor graphs to be more speciﬁc about the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 420,
      "page_label": "401"
    }
  },
  {
    "page_content": "402 8. GRAPHICAL MODELS\n(a) (b) (c)\nFigure 8.43 (a) A directed polytree. (b) The result of converting the polytree into an undirected graph showing\nthe creation of loops. (c) The result of converting the polytree into a factor graph, which retains the tree structure.\nprecise form of the factorization. Figure 8.45 shows an example of a fully connected\nundirected graph along with two different factor graphs. In (b), the joint distri-\nbution is given by a general form p(x)= f(x1,x2,x3), whereas in (c), it is given\nby the more speciﬁc factorization p(x)= fa(x1,x2)fb(x1,x3)fc(x2,x3). It should\nbe emphasized that the factorization in (c) does not correspond to any conditional\nindependence properties.\n8.4.4 The sum-product algorithm\nWe shall now make use of the factor graph framework to derive a powerful class\nof efﬁcient, exact inference algorithms that are applicable to tree-structured graphs.\nHere we shall focus on the problem of evaluating local marginals over nodes or\nsubsets of nodes, which will lead us to the sum-product algorithm. Later we shall\nmodify the technique to allow the most probable state to be found, giving rise to the\nmax-sum algorithm.\nAlso we shall suppose that all of the variables in the model are discrete, and\nso marginalization corresponds to performing sums. The framework, however, is\nequally applicable to linear-Gaussian models in which case marginalization involves\nintegration, and we shall consider an example of this in detail when we discuss linear",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 421,
      "page_label": "402"
    }
  },
  {
    "page_content": "integration, and we shall consider an example of this in detail when we discuss linear\ndynamical systems.Section 13.3\nFigure 8.44 (a) A fragment of a di-\nrected graph having a lo-\ncal cycle. (b) Conversion\nto a fragment of a factor\ngraph having a tree struc-\nture, in whichf(x1,x 2,x 3)=\np(x1)p(x2|x1)p(x3|x1,x 2).\nx1 x2\nx3\n(a)\nx1 x2\nx3\nf(x1,x2,x3)\n(b)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 421,
      "page_label": "402"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 403\nx1 x2\nx3\n(a)\nx1 x2\nx3\nf(x1,x2,x3)\n(b)\nx1 x2\nx3\nfa\nfcfb\n(c)\nFigure 8.45 (a) A fully connected undirected graph. (b) and (c) Two factor graphs each of which corresponds\nto the undirected graph in (a).\nThere is an algorithm for exact inference on directed graphs without loops known\nas belief propagation(Pearl, 1988; Lauritzen and Spiegelhalter, 1988), and is equiv-\nalent to a special case of the sum-product algorithm. Here we shall consider only the\nsum-product algorithm because it is simpler to derive and to apply, as well as being\nmore general.\nWe shall assume that the original graph is an undirected tree or a directed tree or\npolytree, so that the corresponding factor graph has a tree structure. We ﬁrst convert\nthe original graph into a factor graph so that we can deal with both directed and\nundirected models using the same framework. Our goal is to exploit the structure of\nthe graph to achieve two things: (i) to obtain an efﬁcient, exact inference algorithm\nfor ﬁnding marginals; (ii) in situations where several marginals are required to allow\ncomputations to be shared efﬁciently.\nWe begin by considering the problem of ﬁnding the marginal p(x) for partic-\nular variable node x. For the moment, we shall suppose that all of the variables\nare hidden. Later we shall see how to modify the algorithm to incorporate evidence\ncorresponding to observed variables. By deﬁnition, the marginal is obtained by sum-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 422,
      "page_label": "403"
    }
  },
  {
    "page_content": "corresponding to observed variables. By deﬁnition, the marginal is obtained by sum-\nming the joint distribution over all variables except x so that\np(x)=\n∑\nx\\x\np(x) (8.61)\nwhere x \\ x denotes the set of variables in x with variable x omitted. The idea is\nto substitute for p(x) using the factor graph expression (8.59) and then interchange\nsummations and products in order to obtain an efﬁcient algorithm. Consider the\nfragment of graph shown in Figure 8.46 in which we see that the tree structure of\nthe graph allows us to partition the factors in the joint distribution into groups, with\none group associated with each of the factor nodes that is a neighbour of the variable\nnode x. We see that the joint distribution can be written as a product of the form\np(x)=\n∏\ns∈ne(x)\nFs(x, Xs) (8.62)\nne(x) denotes the set of factor nodes that are neighbours of x, and Xs denotes the\nset of all variables in the subtree connected to the variable nodex via the factor node",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 422,
      "page_label": "403"
    }
  },
  {
    "page_content": "404 8. GRAPHICAL MODELS\nFigure 8.46 A fragment of a factor graph illustrating the\nevaluation of the marginal p(x).\nxfs\nµfs→x(x)\nFs(x, Xs)\nfs, and Fs(x, Xs) represents the product of all the factors in the group associated\nwith factor fs.\nSubstituting (8.62) into (8.61) and interchanging the sums and products, we ob-\ntain\np(x)=\n∏\ns∈ne(x)\n[∑\nXs\nFs(x, Xs)\n]\n=\n∏\ns∈ne(x)\nµfs→x(x). (8.63)\nHere we have introduced a set of functions µfs→x(x), deﬁned by\nµfs→x(x) ≡\n∑\nXs\nFs(x, Xs) (8.64)\nwhich can be viewed as messages from the factor nodes fs to the variable node x.\nWe see that the required marginal p(x) is given by the product of all the incoming\nmessages arriving at node x.\nIn order to evaluate these messages, we again turn to Figure 8.46 and note that\neach factor Fs(x, Xs) is described by a factor (sub-)graph and so can itself be fac-\ntorized. In particular, we can write\nFs(x, Xs)= fs(x, x1,...,x M )G1 (x1,X s1)...G M (xM ,X sM ) (8.65)\nwhere, for convenience, we have denoted the variables associated with factor fx,i n\naddition to x,b y x1,...,x M . This factorization is illustrated in Figure 8.47. Note\nthat the set of variables {x, x1,...,x M } is the set of variables on which the factor\nfs depends, and so it can also be denoted xs, using the notation of (8.59).\nSubstituting (8.65) into (8.64) we obtain\nµfs→x(x)=\n∑\nx1\n...\n∑\nxM\nfs(x, x1,...,x M )\n∏\nm∈ne(fs)\\x\n[∑\nXxm\nGm(xm,X sm)\n]\n=\n∑\nx1\n...\n∑\nxM\nfs(x, x1,...,x M )\n∏\nm∈ne(fs)\\x\nµxm→fs (xm) (8.66)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 423,
      "page_label": "404"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 405\nFigure 8.47 Illustration of the factorization of the subgraph as-\nsociated with factor node fs.\nxm\nxM\nx\nfs\nµxM →fs (xM )\nµfs→x(x)\nGm(xm,X sm)\nwhere ne(fs) denotes the set of variable nodes that are neighbours of the factor node\nfs, and ne(fs) \\ x denotes the same set but with node x removed. Here we have\ndeﬁned the following messages from variable nodes to factor nodes\nµxm→fs (xm) ≡\n∑\nXsm\nGm(xm,X sm). (8.67)\nWe have therefore introduced two distinct kinds of message, those that go from factor\nnodes to variable nodes denoted µf→x(x), and those that go from variable nodes to\nfactor nodes denoted µx→f (x). In each case, we see that messages passed along a\nlink are always a function of the variable associated with the variable node that link\nconnects to.\nThe result (8.66) says that to evaluate the message sent by a factor node to a vari-\nable node along the link connecting them, take the product of the incoming messages\nalong all other links coming into the factor node, multiply by the factor associated\nwith that node, and then marginalize over all of the variables associated with the\nincoming messages. This is illustrated in Figure 8.47. It is important to note that\na factor node can send a message to a variable node once it has received incoming\nmessages from all other neighbouring variable nodes.\nFinally, we derive an expression for evaluating the messages from variable nodes",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 424,
      "page_label": "405"
    }
  },
  {
    "page_content": "messages from all other neighbouring variable nodes.\nFinally, we derive an expression for evaluating the messages from variable nodes\nto factor nodes, again by making use of the (sub-)graph factorization. From Fig-\nure 8.48, we see that term Gm(xm,X sm) associated with node xm is given by a\nproduct of terms Fl(xm,X ml) each associated with one of the factor nodesfl that is\nlinked to node xm (excluding node fs), so that\nGm(xm,X sm)=\n∏\nl∈ne(xm)\\fs\nFl(xm,X ml) (8.68)\nwhere the product is taken over all neighbours of node xm except for node fs. Note\nthat each of the factors Fl(xm,X ml) represents a subtree of the original graph of\nprecisely the same kind as introduced in (8.62). Substituting (8.68) into (8.67), we",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 424,
      "page_label": "405"
    }
  },
  {
    "page_content": "406 8. GRAPHICAL MODELS\nFigure 8.48 Illustration of the evaluation of the message sent by a\nvariable node to an adjacent factor node.\nxm\nfl\nfL\nfs\nFl(xm,X ml)\nthen obtain\nµxm→fs (xm)=\n∏\nl∈ne(xm)\\fs\n[∑\nXml\nFl(xm,X ml)\n]\n=\n∏\nl∈ne(xm)\\fs\nµfl→xm (xm) (8.69)\nwhere we have used the deﬁnition (8.64) of the messages passed from factor nodes to\nvariable nodes. Thus to evaluate the message sent by a variable node to an adjacent\nfactor node along the connecting link, we simply take the product of the incoming\nmessages along all of the other links. Note that any variable node that has only\ntwo neighbours performs no computation but simply passes messages through un-\nchanged. Also, we note that a variable node can send a message to a factor node\nonce it has received incoming messages from all other neighbouring factor nodes.\nRecall that our goal is to calculate the marginal for variable nodex, and that this\nmarginal is given by the product of incoming messages along all of the links arriving\nat that node. Each of these messages can be computed recursively in terms of other\nmessages. In order to start this recursion, we can view the node x as the root of the\ntree and begin at the leaf nodes. From the deﬁnition (8.69), we see that if a leaf node\nis a variable node, then the message that it sends along its one and only link is given\nby\nµx→f (x)=1 (8.70)\nas illustrated in Figure 8.49(a). Similarly, if the leaf node is a factor node, we see\nfrom (8.66) that the message sent should take the form",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 425,
      "page_label": "406"
    }
  },
  {
    "page_content": "as illustrated in Figure 8.49(a). Similarly, if the leaf node is a factor node, we see\nfrom (8.66) that the message sent should take the form\nµf→x(x)= f(x) (8.71)\nFigure 8.49 The sum-product algorithm\nbegins with messages sent\nby the leaf nodes, which de-\npend on whether the leaf\nnode is (a) a variable node,\nor (b) a factor node.\nx f\nµx→f (x)=1\n(a)\nxf\nµf→x(x)= f(x)\n(b)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 425,
      "page_label": "406"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 407\nas illustrated in Figure 8.49(b).\nAt this point, it is worth pausing to summarize the particular version of the sum-\nproduct algorithm obtained so far for evaluating the marginal p(x). We start by\nviewing the variable node x as the root of the factor graph and initiating messages\nat the leaves of the graph using (8.70) and (8.71). The message passing steps (8.66)\nand (8.69) are then applied recursively until messages have been propagated along\nevery link, and the root node has received messages from all of its neighbours. Each\nnode can send a message towards the root once it has received messages from all\nof its other neighbours. Once the root node has received messages from all of its\nneighbours, the required marginal can be evaluated using (8.63). We shall illustrate\nthis process shortly.\nTo see that each node will always receive enough messages to be able to send out\na message, we can use a simple inductive argument as follows. Clearly, for a graph\ncomprising a variable root node connected directly to several factor leaf nodes, the\nalgorithm trivially involves sending messages of the form (8.71) directly from the\nleaves to the root. Now imagine building up a general graph by adding nodes one at\na time, and suppose that for some particular graph we have a valid algorithm. When\none more (variable or factor) node is added, it can be connected only by a single",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 426,
      "page_label": "407"
    }
  },
  {
    "page_content": "one more (variable or factor) node is added, it can be connected only by a single\nlink because the overall graph must remain a tree, and so the new node will be a leaf\nnode. It therefore sends a message to the node to which it is linked, which in turn\nwill therefore receive all the messages it requires in order to send its own message\ntowards the root, and so again we have a valid algorithm, thereby completing the\nproof.\nNow suppose we wish to ﬁnd the marginals for every variable node in the graph.\nThis could be done by simply running the above algorithm afresh for each such node.\nHowever, this would be very wasteful as many of the required computations would\nbe repeated. We can obtain a much more efﬁcient procedure by ‘overlaying’ these\nmultiple message passing algorithms to obtain the general sum-product algorithm\nas follows. Arbitrarily pick any (variable or factor) node and designate it as the\nroot. Propagate messages from the leaves to the root as before. At this point, the\nroot node will have received messages from all of its neighbours. It can therefore\nsend out messages to all of its neighbours. These in turn will then have received\nmessages from all of their neighbours and so can send out messages along the links\ngoing away from the root, and so on. In this way, messages are passed outwards\nfrom the root all the way to the leaves. By now, a message will have passed in\nboth directions across every link in the graph, and every node will have received",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 426,
      "page_label": "407"
    }
  },
  {
    "page_content": "both directions across every link in the graph, and every node will have received\na message from all of its neighbours. Again a simple inductive argument can be\nused to verify the validity of this message passing protocol. Because every variableExercise 8.20\nnode will have received messages from all of its neighbours, we can readily calculate\nthe marginal distribution for every variable in the graph. The number of messages\nthat have to be computed is given by twice the number of links in the graph and\nso involves only twice the computation involved in ﬁnding a single marginal. By\ncomparison, if we had run the sum-product algorithm separately for each node, the\namount of computation would grow quadratically with the size of the graph. Note\nthat this algorithm is in fact independent of which node was designated as the root,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 426,
      "page_label": "407"
    }
  },
  {
    "page_content": "408 8. GRAPHICAL MODELS\nFigure 8.50 The sum-product algorithm can be viewed\npurely in terms of messages sent out by factor\nnodes to other factor nodes. In this example,\nthe outgoing message shown by the blue arrow\nis obtained by taking the product of all the in-\ncoming messages shown by green arrows, mul-\ntiplying by the factor fs, and marginalizing over\nthe variables x1 and x2. fs\nx1\nx2\nx3\nand indeed the notion of one node having a special status was introduced only as a\nconvenient way to explain the message passing protocol.\nNext suppose we wish to ﬁnd the marginal distributions p(xs) associated with\nthe sets of variables belonging to each of the factors. By a similar argument to that\nused above, it is easy to see that the marginal associated with a factor is given by theExercise 8.21\nproduct of messages arriving at the factor node and the local factor at that node\np(xs)= fs(xs)\n∏\ni∈ne(fs)\nµxi→fs (xi) (8.72)\nin complete analogy with the marginals at the variable nodes. If the factors are\nparameterized functions and we wish to learn the values of the parameters using\nthe EM algorithm, then these marginals are precisely the quantities we will need to\ncalculate in the E step, as we shall see in detail when we discuss the hidden Markov\nmodel in Chapter 13.\nThe message sent by a variable node to a factor node, as we have seen, is simply\nthe product of the incoming messages on other links. We can if we wish view the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 427,
      "page_label": "408"
    }
  },
  {
    "page_content": "the product of the incoming messages on other links. We can if we wish view the\nsum-product algorithm in a slightly different form by eliminating messages from\nvariable nodes to factor nodes and simply considering messages that are sent out by\nfactor nodes. This is most easily seen by considering the example in Figure 8.50.\nSo far, we have rather neglected the issue of normalization. If the factor graph\nwas derived from a directed graph, then the joint distribution is already correctly nor-\nmalized, and so the marginals obtained by the sum-product algorithm will similarly\nbe normalized correctly. However, if we started from an undirected graph, then in\ngeneral there will be an unknown normalization coefﬁcient 1/Z. As with the simple\nchain example of Figure 8.38, this is easily handled by working with an unnormal-\nized version ˜p(x) of the joint distribution, where p(x)= ˜p(x)/Z. We ﬁrst run the\nsum-product algorithm to ﬁnd the corresponding unnormalized marginals˜p(xi). The\ncoefﬁcient 1/Z is then easily obtained by normalizing any one of these marginals,\nand this is computationally efﬁcient because the normalization is done over a single\nvariable rather than over the entire set of variables as would be required to normalize\n˜p(x) directly.\nAt this point, it may be helpful to consider a simple example to illustrate the\noperation of the sum-product algorithm. Figure 8.51 shows a simple 4-node factor",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 427,
      "page_label": "408"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 409\nFigure 8.51 A simple factor graph used to illustrate the\nsum-product algorithm.\nx1 x2 x3\nx4\nfa fb\nfc\ngraph whose unnormalized joint distribution is given by\n˜p(x)= fa(x1,x2)fb(x2,x3)fc(x2,x4). (8.73)\nIn order to apply the sum-product algorithm to this graph, let us designate node x3\nas the root, in which case there are two leaf nodes x1 and x4. Starting with the leaf\nnodes, we then have the following sequence of six messages\nµx1→fa (x1)=1 (8.74)\nµfa→x2 (x2)=\n∑\nx1\nfa(x1,x2) (8.75)\nµx4→fc (x4)=1 (8.76)\nµfc→x2 (x2)=\n∑\nx4\nfc(x2,x4) (8.77)\nµx2→fb (x2)= µfa→x2 (x2)µfc→x2 (x2) (8.78)\nµfb→x3 (x3)=\n∑\nx2\nfb(x2,x3)µx2→fb . (8.79)\nThe direction of ﬂow of these messages is illustrated in Figure 8.52. Once this mes-\nsage propagation is complete, we can then propagate messages from the root node\nout to the leaf nodes, and these are given by\nµx3→fb (x3)=1 (8.80)\nµfb→x2 (x2)=\n∑\nx3\nfb(x2,x3) (8.81)\nµx2→fa (x2)= µfb→x2 (x2)µfc→x2 (x2) (8.82)\nµfa→x1 (x1)=\n∑\nx2\nfa(x1,x2)µx2→fa (x2) (8.83)\nµx2→fc (x2)= µfa→x2 (x2)µfb→x2 (x2) (8.84)\nµfc→x4 (x4)=\n∑\nx2\nfc(x2,x4)µx2→fc (x2). (8.85)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 428,
      "page_label": "409"
    }
  },
  {
    "page_content": "410 8. GRAPHICAL MODELS\nx1 x2 x3\nx4\n(a)\nx1 x2 x3\nx4\n(b)\nFigure 8.52 Flow of messages for the sum-product algorithm applied to the example graph in Figure 8.51. (a)\nFrom the leaf nodes x1 and x4 towards the root node x3. (b) From the root node towards the leaf nodes.\nOne message has now passed in each direction across each link, and we can now\nevaluate the marginals. As a simple check, let us verify that the marginal p(x2) is\ngiven by the correct expression. Using (8.63) and substituting for the messages using\nthe above results, we have\n˜p(x2)= µfa→x2 (x2)µfb→x2 (x2)µfc→x2 (x2)\n=\n[∑\nx1\nfa(x1,x2)\n][∑\nx3\nfb(x2,x3)\n][∑\nx4\nfc(x2,x4)\n]\n=\n∑\nx1\n∑\nx2\n∑\nx4\nfa(x1,x2)fb(x2,x3)fc(x2,x4)\n=\n∑\nx1\n∑\nx3\n∑\nx4\n˜p(x) (8.86)\nas required.\nSo far, we have assumed that all of the variables in the graph are hidden. In most\npractical applications, a subset of the variables will be observed, and we wish to cal-\nculate posterior distributions conditioned on these observations. Observed nodes are\neasily handled within the sum-product algorithm as follows. Suppose we partition x\ninto hidden variables h and observed variables v, and that the observed value of v\nis denoted ˆv. Then we simply multiply the joint distribution p(x) by ∏\ni I(vi,ˆvi),\nwhere I(v, ˆv)=1 if v = ˆv and I(v, ˆv)=0 otherwise. This product corresponds\nto p(h, v = ˆv) and hence is an unnormalized version of p(h|v = ˆv). By run-\nning the sum-product algorithm, we can efﬁciently calculate the posterior marginals",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 429,
      "page_label": "410"
    }
  },
  {
    "page_content": "ning the sum-product algorithm, we can efﬁciently calculate the posterior marginals\np(hi|v = ˆv) up to a normalization coefﬁcient whose value can be found efﬁciently\nusing a local computation. Any summations over variables in v then collapse into a\nsingle term.\nWe have assumed throughout this section that we are dealing with discrete vari-\nables. However, there is nothing speciﬁc to discrete variables either in the graphical\nframework or in the probabilistic construction of the sum-product algorithm. For",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 429,
      "page_label": "410"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 411\nTable 8.1 Example of a joint distribution over two binary variables for\nwhich the maximum of the joint distribution occurs for dif-\nferent variable values compared to the maxima of the two\nmarginals.\nx =0 x =1\ny =0 0.3 0.4\ny =1 0.3 0.0\ncontinuous variables the summations are simply replaced by integrations. We shall\ngive an example of the sum-product algorithm applied to a graph of linear-Gaussian\nvariables when we consider linear dynamical systems.Section 13.3\n8.4.5 The max-sum algorithm\nThe sum-product algorithm allows us to take a joint distribution p(x) expressed\nas a factor graph and efﬁciently ﬁnd marginals over the component variables. Two\nother common tasks are to ﬁnd a setting of the variables that has the largest prob-\nability and to ﬁnd the value of that probability. These can be addressed through a\nclosely related algorithm calledmax-sum, which can be viewed as an application of\ndynamic programmingin the context of graphical models (Cormen et al., 2001).\nA simple approach to ﬁnding latent variable values having high probability\nwould be to run the sum-product algorithm to obtain the marginals p(xi) for ev-\nery variable, and then, for each marginal in turn, to ﬁnd the valuex⋆\ni that maximizes\nthat marginal. However, this would give the set of values that are individually the\nmost probable. In practice, we typically wish to ﬁnd the set of values that jointly",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 430,
      "page_label": "411"
    }
  },
  {
    "page_content": "most probable. In practice, we typically wish to ﬁnd the set of values that jointly\nhave the largest probability, in other words the vector xmax that maximizes the joint\ndistribution, so that\nxmax =a r gm a x\nx\np(x) (8.87)\nfor which the corresponding value of the joint probability will be given by\np(xmax) = max\nx\np(x). (8.88)\nIn general, xmax is not the same as the set of x⋆\ni values, as we can easily show using\na simple example. Consider the joint distribution p(x, y) over two binary variables\nx, y∈{ 0, 1} given in Table 8.1. The joint distribution is maximized by setting x =\n1 and y =0 , corresponding the value 0.4. However, the marginal for p(x), obtained\nby summing over both values of y,i sg i v e nb yp(x =0 )=0 .6 and p(x =1 )=0 .4,\nand similarly the marginal for y is given by p(y =0 )=0 .7 and p(y =1 )=0 .3,\nand so the marginals are maximized by x =0 and y =0 , which corresponds to a\nvalue of 0.3 for the joint distribution. In fact, it is not difﬁcult to construct examples\nfor which the set of individually most probable values has probability zero under the\njoint distribution.Exercise 8.27\nWe therefore seek an efﬁcient algorithm for ﬁnding the value of x that maxi-\nmizes the joint distribution p(x) and that will allow us to obtain the value of the\njoint distribution at its maximum. To address the second of these problems, we shall\nsimply write out the max operator in terms of its components\nmax\nx\np(x)=m a x\nx1\n... max\nxM\np(x) (8.89)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 430,
      "page_label": "411"
    }
  },
  {
    "page_content": "412 8. GRAPHICAL MODELS\nwhere M is the total number of variables, and then substitute for p(x) using its\nexpansion in terms of a product of factors. In deriving the sum-product algorithm,\nwe made use of the distributive law (8.53) for multiplication. Here we make use of\nthe analogous law for the max operator\nmax(ab, ac)= amax(b, c) (8.90)\nwhich holds if a ⩾ 0 (as will always be the case for the factors in a graphical model).\nThis allows us to exchange products with maximizations.\nConsider ﬁrst the simple example of a chain of nodes described by (8.49). The\nevaluation of the probability maximum can be written as\nmax\nx\np(x)= 1\nZ max\nx1\n··· max\nxN\n[ψ1,2(x1,x2) ··· ψN−1,N (xN−1,xN )]\n= 1\nZ max\nx1\n[\nψ1,2(x1,x2)\n[\n··· max\nxN\nψN−1,N (xN−1,xN )\n]]\n.\nAs with the calculation of marginals, we see that exchanging the max and product\noperators results in a much more efﬁcient computation, and one that is easily inter-\npreted in terms of messages passed from nodexN backwards along the chain to node\nx1.\nWe can readily generalize this result to arbitrary tree-structured factor graphs\nby substituting the expression (8.59) for the factor graph expansion into (8.89) and\nagain exchanging maximizations with products. The structure of this calculation is\nidentical to that of the sum-product algorithm, and so we can simply translate those\nresults into the present context. In particular, suppose that we designate a particular",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 431,
      "page_label": "412"
    }
  },
  {
    "page_content": "results into the present context. In particular, suppose that we designate a particular\nvariable node as the ‘root’ of the graph. Then we start a set of messages propagating\ninwards from the leaves of the tree towards the root, with each node sending its\nmessage towards the root once it has received all incoming messages from its other\nneighbours. The ﬁnal maximization is performed over the product of all messages\narriving at the root node, and gives the maximum value forp(x). This could be called\nthe max-product algorithm and is identical to the sum-product algorithm except that\nsummations are replaced by maximizations. Note that at this stage, messages have\nbeen sent from leaves to the root, but not in the other direction.\nIn practice, products of many small probabilities can lead to numerical under-\nﬂow problems, and so it is convenient to work with the logarithm of the joint distri-\nbution. The logarithm is a monotonic function, so that if a>b then ln a> lnb, and\nhence the max operator and the logarithm function can be interchanged, so that\nln\n(\nmax\nx\np(x)\n)\n=m a x\nx\nlnp(x). (8.91)\nThe distributive property is preserved because\nmax(a + b, a+ c)= a +m a x (b, c). (8.92)\nThus taking the logarithm simply has the effect of replacing the products in the\nmax-product algorithm with sums, and so we obtain the max-sum algorithm. From",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 431,
      "page_label": "412"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 413\nthe results (8.66) and (8.69) derived earlier for the sum-product algorithm, we can\nreadily write down the max-sum algorithm in terms of message passing simply by\nreplacing ‘sum’ with ‘max’ and replacing products with sums of logarithms to give\nµf→x(x) = max\nx1,...,xM\n⎡\n⎣lnf(x, x1,...,x M )+\n∑\nm∈ne(fs)\\x\nµxm→f (xm)\n⎤\n⎦(8.93)\nµx→f (x)=\n∑\nl∈ne(x)\\f\nµfl→x(x). (8.94)\nThe initial messages sent by the leaf nodes are obtained by analogy with (8.70) and\n(8.71) and are given by\nµx→f (x)=0 (8.95)\nµf→x(x)=l n f(x) (8.96)\nwhile at the root node the maximum probability can then be computed, by analogy\nwith (8.63), using\npmax =m a x\nx\n⎡\n⎣ ∑\ns∈ne(x)\nµfs→x(x)\n⎤\n⎦. (8.97)\nSo far, we have seen how to ﬁnd the maximum of the joint distribution by prop-\nagating messages from the leaves to an arbitrarily chosen root node. The result will\nbe the same irrespective of which node is chosen as the root. Now we turn to the\nsecond problem of ﬁnding the conﬁguration of the variables for which the joint dis-\ntribution attains this maximum value. So far, we have sent messages from the leaves\nto the root. The process of evaluating (8.97) will also give the value xmax for the\nmost probable value of the root node variable, deﬁned by\nxmax =a r gm a x\nx\n⎡\n⎣ ∑\ns∈ne(x)\nµfs→x(x)\n⎤\n⎦. (8.98)\nAt this point, we might be tempted simply to continue with the message passing al-\ngorithm and send messages from the root back out to the leaves, using (8.93) and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 432,
      "page_label": "413"
    }
  },
  {
    "page_content": "gorithm and send messages from the root back out to the leaves, using (8.93) and\n(8.94), then apply (8.98) to all of the remaining variable nodes. However, because\nwe are now maximizing rather than summing, it is possible that there may be mul-\ntiple conﬁgurations of x all of which give rise to the maximum value for p(x).I n\nsuch cases, this strategy can fail because it is possible for the individual variable\nvalues obtained by maximizing the product of messages at each node to belong to\ndifferent maximizing conﬁgurations, giving an overall conﬁguration that no longer\ncorresponds to a maximum.\nThe problem can be resolved by adopting a rather different kind of message\npassing from the root node to the leaves. To see how this works, let us return once\nagain to the simple chain example of N variables x1,...,x N each having K states,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 432,
      "page_label": "413"
    }
  },
  {
    "page_content": "414 8. GRAPHICAL MODELS\nFigure 8.53 A lattice, or trellis, diagram show-\ning explicitly the K possible states (one per row\nof the diagram) for each of the variables xn in the\nchain model. In this illustration K =3 . The ar-\nrow shows the direction of message passing in the\nmax-product algorithm. For every state k of each\nvariable xn (corresponding to column n of the dia-\ngram) the function φ(xn) deﬁnes a unique state at\nthe previous variable, indicated by the black lines.\nThe two paths through the lattice correspond to\nconﬁgurations that give the global maximum of the\njoint probability distribution, and either of these\ncan be found by tracing back along the black lines\nin the opposite direction to the arrow.\nk =1\nk =2\nk =3\nn − 2 n − 1 nn +1\ncorresponding to the graph shown in Figure 8.38. Suppose we take node xN to be\nthe root node. Then in the ﬁrst phase, we propagate messages from the leaf node x1\nto the root node using\nµxn→fn,n+1 (xn)= µfn−1,n→xn (xn)\nµfn−1,n→xn (xn) = max\nxn−1\n[\nlnfn−1,n(xn−1,xn)+ µxn−1→fn−1,n(xn)\n]\nwhich follow from applying (8.94) and (8.93) to this particular graph. The initial\nmessage sent from the leaf node is simply\nµx1→f1,2 (x1)=0 . (8.99)\nThe most probable value for xN is then given by\nxmax\nN =a r gm a x\nxN\n[\nµfN−1,N →xN (xN )\n]\n. (8.100)\nNow we need to determine the states of the previous variables that correspond to the\nsame maximizing conﬁguration. This can be done by keeping track of which values",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 433,
      "page_label": "414"
    }
  },
  {
    "page_content": "same maximizing conﬁguration. This can be done by keeping track of which values\nof the variables gave rise to the maximum state of each variable, in other words by\nstoring quantities given by\nφ(xn) = arg max\nxn−1\n[\nlnfn−1,n(xn−1,xn)+ µxn−1→fn−1,n(xn)\n]\n. (8.101)\nTo understand better what is happening, it is helpful to represent the chain of vari-\nables in terms of a lattice or trellis diagram as shown in Figure 8.53. Note that this\nis not a probabilistic graphical model because the nodes represent individual states\nof variables, while each variable corresponds to a column of such states in the di-\nagram. For each state of a given variable, there is a unique state of the previous\nvariable that maximizes the probability (ties are broken either systematically or at\nrandom), corresponding to the function φ(xn) given by (8.101), and this is indicated",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 433,
      "page_label": "414"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 415\nby the lines connecting the nodes. Once we know the most probable value of the ﬁ-\nnal node xN , we can then simply follow the link back to ﬁnd the most probable state\nof node xN−1 and so on back to the initial nodex1. This corresponds to propagating\na message back down the chain using\nxmax\nn−1 = φ(xmax\nn ) (8.102)\nand is known as back-tracking. Note that there could be several values of xn−1 all\nof which give the maximum value in (8.101). Provided we chose one of these values\nwhen we do the back-tracking, we are assured of a globally consistent maximizing\nconﬁguration.\nIn Figure 8.53, we have indicated two paths, each of which we shall suppose\ncorresponds to a global maximum of the joint probability distribution. If k =2\nand k =3 each represent possible values of xmax\nN , then starting from either state\nand tracing back along the black lines, which corresponds to iterating (8.102), we\nobtain a valid global maximum conﬁguration. Note that if we had run a forward\npass of max-sum message passing followed by a backward pass and then applied\n(8.98) at each node separately, we could end up selecting some states from one path\nand some from the other path, giving an overall conﬁguration that is not a global\nmaximizer. We see that it is necessary instead to keep track of the maximizing states\nduring the forward pass using the functionsφ(xn) and then use back-tracking to ﬁnd\na consistent solution.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 434,
      "page_label": "415"
    }
  },
  {
    "page_content": "during the forward pass using the functionsφ(xn) and then use back-tracking to ﬁnd\na consistent solution.\nThe extension to a general tree-structured factor graph should now be clear. If\na message is sent from a factor node f to a variable node x, a maximization is\nperformed over all other variable nodes x1,...,x M that are neighbours of that fac-\ntor node, using (8.93). When we perform this maximization, we keep a record of\nwhich values of the variables x1,...,x M gave rise to the maximum. Then in the\nback-tracking step, having found xmax, we can then use these stored values to as-\nsign consistent maximizing states xmax\n1 ,...,x max\nM . The max-sum algorithm, with\nback-tracking, gives an exact maximizing conﬁguration for the variables provided\nthe factor graph is a tree. An important application of this technique is for ﬁnding\nthe most probable sequence of hidden states in a hidden Markov model, in which\ncase it is known as the Viterbi algorithm.Section 13.2\nAs with the sum-product algorithm, the inclusion of evidence in the form of\nobserved variables is straightforward. The observed variables are clamped to their\nobserved values, and the maximization is performed over the remaining hidden vari-\nables. This can be shown formally by including identity functions for the observed\nvariables into the factor functions, as we did for the sum-product algorithm.\nIt is interesting to compare max-sum with the iterated conditional modes (ICM)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 434,
      "page_label": "415"
    }
  },
  {
    "page_content": "It is interesting to compare max-sum with the iterated conditional modes (ICM)\nalgorithm described on page 389. Each step in ICM is computationally simpler be-\ncause the ‘messages’ that are passed from one node to the next comprise a single\nvalue consisting of the new state of the node for which the conditional distribution\nis maximized. The max-sum algorithm is more complex because the messages are\nfunctions of node variables x and hence comprise a set of K values for each pos-\nsible state of x. Unlike max-sum, however, ICM is not guaranteed to ﬁnd a global\nmaximum even for tree-structured graphs.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 434,
      "page_label": "415"
    }
  },
  {
    "page_content": "416 8. GRAPHICAL MODELS\n8.4.6 Exact inference in general graphs\nThe sum-product and max-sum algorithms provide efﬁcient and exact solutions\nto inference problems in tree-structured graphs. For many practical applications,\nhowever, we have to deal with graphs having loops.\nThe message passing framework can be generalized to arbitrary graph topolo-\ngies, giving an exact inference procedure known as thejunction tree algorithm(Lau-\nritzen and Spiegelhalter, 1988; Jordan, 2007). Here we give a brief outline of the\nkey steps involved. This is not intended to convey a detailed understanding of the\nalgorithm, but rather to give a ﬂavour of the various stages involved. If the starting\npoint is a directed graph, it is ﬁrst converted to an undirected graph by moraliza-\ntion, whereas if starting from an undirected graph this step is not required. Next the\ngraph is triangulated, which involves ﬁnding chord-less cycles containing four or\nmore nodes and adding extra links to eliminate such chord-less cycles. For instance,\nin the graph in Figure 8.36, the cycleA–C–B–D–A is chord-less a link could be\nadded between A and B or alternatively between C and D. Note that the joint dis-\ntribution for the resulting triangulated graph is still deﬁned by a product of the same\npotential functions, but these are now considered to be functions over expanded sets\nof variables. Next the triangulated graph is used to construct a new tree-structured",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 435,
      "page_label": "416"
    }
  },
  {
    "page_content": "of variables. Next the triangulated graph is used to construct a new tree-structured\nundirected graph called a join tree, whose nodes correspond to the maximal cliques\nof the triangulated graph, and whose links connect pairs of cliques that have vari-\nables in common. The selection of which pairs of cliques to connect in this way is\nimportant and is done so as to give a maximal spanning treedeﬁned as follows. Of\nall possible trees that link up the cliques, the one that is chosen is one for which the\nweight of the tree is largest, where the weight for a link is the number of nodes shared\nby the two cliques it connects, and the weight for the tree is the sum of the weights\nfor the links. If the tree is condensed, so that any clique that is a subset of another\nclique is absorbed into the larger clique, this gives ajunction tree. As a consequence\nof the triangulation step, the resulting tree satisﬁes therunning intersection property,\nwhich means that if a variable is contained in two cliques, then it must also be con-\ntained in every clique on the path that connects them. This ensures that inference\nabout variables will be consistent across the graph. Finally, a two-stage message\npassing algorithm, essentially equivalent to the sum-product algorithm, can now be\napplied to this junction tree in order to ﬁnd marginals and conditionals. Although\nthe junction tree algorithm sounds complicated, at its heart is the simple idea that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 435,
      "page_label": "416"
    }
  },
  {
    "page_content": "the junction tree algorithm sounds complicated, at its heart is the simple idea that\nwe have used already of exploiting the factorization properties of the distribution to\nallow sums and products to be interchanged so that partial summations can be per-\nformed, thereby avoiding having to work directly with the joint distribution. The\nrole of the junction tree is to provide a precise and efﬁcient way to organize these\ncomputations. It is worth emphasizing that this is achieved using purely graphical\noperations!\nThe junction tree is exact for arbitrary graphs and is efﬁcient in the sense that\nfor a given graph there does not in general exist a computationally cheaper approach.\nUnfortunately, the algorithm must work with the joint distributions within each node\n(each of which corresponds to a clique of the triangulated graph) and so the compu-\ntational cost of the algorithm is determined by the number of variables in the largest",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 435,
      "page_label": "416"
    }
  },
  {
    "page_content": "8.4. Inference in Graphical Models 417\nclique and will grow exponentially with this number in the case of discrete variables.\nAn important concept is the treewidth of a graph (Bodlaender, 1993), which is de-\nﬁned in terms of the number of variables in the largest clique. In fact, it is deﬁned to\nbe as one less than the size of the largest clique, to ensure that a tree has a treewidth\nof 1. Because there in general there can be multiple different junction trees that can\nbe constructed from a given starting graph, the treewidth is deﬁned by the junction\ntree for which the largest clique has the fewest variables. If the treewidth of the\noriginal graph is high, the junction tree algorithm becomes impractical.\n8.4.7 Loopy belief propagation\nFor many problems of practical interest, it will not be feasible to use exact in-\nference, and so we need to exploit effective approximation methods. An important\nclass of such approximations, that can broadly be calledvariational methods, will be\ndiscussed in detail in Chapter 10. Complementing these deterministic approaches is\na wide range ofsampling methods, also called Monte Carlomethods, that are based\non stochastic numerical sampling from distributions and that will be discussed at\nlength in Chapter 11.\nHere we consider one simple approach to approximate inference in graphs with\nloops, which builds directly on the previous discussion of exact inference in trees.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 436,
      "page_label": "417"
    }
  },
  {
    "page_content": "loops, which builds directly on the previous discussion of exact inference in trees.\nThe idea is simply to apply the sum-product algorithm even though there is no guar-\nantee that it will yield good results. This approach is known asloopy belief propa-\ngation (Frey and MacKay, 1998) and is possible because the message passing rules\n(8.66) and (8.69) for the sum-product algorithm are purely local. However, because\nthe graph now has cycles, information can ﬂow many times around the graph. For\nsome models, the algorithm will converge, whereas for others it will not.\nIn order to apply this approach, we need to deﬁne a message passing schedule.\nLet us assume that one message is passed at a time on any given link and in any\ngiven direction. Each message sent from a node replaces any previous message sent\nin the same direction across the same link and will itself be a function only of the\nmost recent messages received by that node at previous steps of the algorithm.\nWe have seen that a message can only be sent across a link from a node when\nall other messages have been received by that node across its other links. Because\nthere are loops in the graph, this raises the problem of how to initiate the message\npassing algorithm. To resolve this, we suppose that an initial message given by the\nunit function has been passed across every link in each direction. Every node is then\nin a position to send a message.\nThere are now many possible ways to organize the message passing schedule.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 436,
      "page_label": "417"
    }
  },
  {
    "page_content": "in a position to send a message.\nThere are now many possible ways to organize the message passing schedule.\nFor example, the ﬂooding schedule simultaneously passes a message across every\nlink in both directions at each time step, whereas schedules that pass one message at\na time are called serial schedules.\nFollowing Kschischnang et al. (2001), we will say that a (variable or factor)\nnode a has a message pending on its link to a node b if node a has received any\nmessage on any of its other links since the last time it send a message to b. Thus,\nwhen a node receives a message on one of its links, this creates pending messages\non all of its other links. Only pending messages need to be transmitted because",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 436,
      "page_label": "417"
    }
  },
  {
    "page_content": "418 8. GRAPHICAL MODELS\nother messages would simply duplicate the previous message on the same link. For\ngraphs that have a tree structure, any schedule that sends only pending messages\nwill eventually terminate once a message has passed in each direction across every\nlink. At this point, there are no pending messages, and the product of the receivedExercise 8.29\nmessages at every variable give the exact marginal. In graphs having loops, however,\nthe algorithm may never terminate because there might always be pending messages,\nalthough in practice it is generally found to converge within a reasonable time for\nmost applications. Once the algorithm has converged, or once it has been stopped\nif convergence is not observed, the (approximate) local marginals can be computed\nusing the product of the most recently received incoming messages to each variable\nnode or factor node on every link.\nIn some applications, the loopy belief propagation algorithm can give poor re-\nsults, whereas in other applications it has proven to be very effective. In particular,\nstate-of-the-art algorithms for decoding certain kinds of error-correcting codes are\nequivalent to loopy belief propagation (Gallager, 1963; Berrouet al., 1993; McEliece\net al., 1998; MacKay and Neal, 1999; Frey, 1998).\n8.4.8 Learning the graph structure\nIn our discussion of inference in graphical models, we have assumed that the\nstructure of the graph is known and ﬁxed. However, there is also interest in go-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 437,
      "page_label": "418"
    }
  },
  {
    "page_content": "structure of the graph is known and ﬁxed. However, there is also interest in go-\ning beyond the inference problem and learning the graph structure itself from data\n(Friedman and Koller, 2003). This requires that we deﬁne a space of possible struc-\ntures as well as a measure that can be used to score each structure.\nFrom a Bayesian viewpoint, we would ideally like to compute a posterior dis-\ntribution over graph structures and to make predictions by averaging with respect\nto this distribution. If we have a prior p(m) over graphs indexed by m, then the\nposterior distribution is given by\np(m|D) ∝ p(m)p(D|m) (8.103)\nwhere D is the observed data set. The model evidence p(D|m) then provides the\nscore for each model. However, evaluation of the evidence involves marginalization\nover the latent variables and presents a challenging computational problem for many\nmodels.\nExploring the space of structures can also be problematic. Because the number\nof different graph structures grows exponentially with the number of nodes, it is\noften necessary to resort to heuristics to ﬁnd good candidates.\nExercises\n8.1 (⋆) www By marginalizing out the variables in order, show that the representation\n(8.5) for the joint distribution of a directed graph is correctly normalized, provided\neach of the conditional distributions is normalized.\n8.2 (⋆) www Show that the property of there being no directed cycles in a directed",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 437,
      "page_label": "418"
    }
  },
  {
    "page_content": "each of the conditional distributions is normalized.\n8.2 (⋆) www Show that the property of there being no directed cycles in a directed\ngraph follows from the statement that there exists an ordered numbering of the nodes\nsuch that for each node there are no links going to a lower-numbered node.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 437,
      "page_label": "418"
    }
  },
  {
    "page_content": "Exercises 419\nTable 8.2 The joint distribution over three binary variables. a b c p(a, b, c)\n0 0 0 0.192\n0 0 1 0.144\n0 1 0 0.048\n0 1 1 0.216\n1 0 0 0.192\n1 0 1 0.064\n1 1 0 0.048\n1 1 1 0.096\n8.3 (⋆⋆ ) Consider three binary variables a, b, c∈{ 0, 1} having the joint distribution\ngiven in Table 8.2. Show by direct evaluation that this distribution has the property\nthat a and b are marginally dependent, so that p(a, b) ̸=p(a)p(b), but that they\nbecome independent when conditioned on c, so that p(a, b|c)= p(a|c)p(b|c) for\nboth c =0 and c =1 .\n8.4 (⋆⋆ ) Evaluate the distributions p(a), p(b|c), and p(c|a) corresponding to the joint\ndistribution given in Table 8.2. Hence show by direct evaluation that p(a, b, c)=\np(a)p(c|a)p(b|c). Draw the corresponding directed graph.\n8.5 (⋆) www Draw a directed probabilistic graphical model corresponding to the\nrelevance vector machine described by (7.79) and (7.80).\n8.6 (⋆) For the model shown in Figure 8.13, we have seen that the number of parameters\nrequired to specify the conditional distribution p(y|x1,...,x M ), where xi ∈{ 0, 1},\ncould be reduced from 2M to M +1 by making use of the logistic sigmoid represen-\ntation (8.10). An alternative representation (Pearl, 1988) is given by\np(y =1 |x1,...,x M )=1 − (1 − µ0)\nM∏\ni=1\n(1 − µi)xi (8.104)\nwhere the parametersµi represent the probabilitiesp(xi =1 ), and µ0 is an additional\nparameters satisfying 0 ⩽ µ0 ⩽ 1. The conditional distribution (8.104) is known as",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 438,
      "page_label": "419"
    }
  },
  {
    "page_content": "parameters satisfying 0 ⩽ µ0 ⩽ 1. The conditional distribution (8.104) is known as\nthe noisy-OR. Show that this can be interpreted as a ‘soft’ (probabilistic) form of the\nlogical OR function (i.e., the function that gives y =1 whenever at least one of the\nxi =1 ). Discuss the interpretation of µ0.\n8.7 (⋆⋆ ) Using the recursion relations (8.15) and (8.16), show that the mean and covari-\nance of the joint distribution for the graph shown in Figure 8.14 are given by (8.17)\nand (8.18), respectively.\n8.8 (⋆) www Show that a ⊥⊥ b, c| d implies a ⊥⊥ b | d.\n8.9 (⋆) www Using the d-separation criterion, show that the conditional distribution\nfor a node x in a directed graph, conditioned on all of the nodes in the Markov\nblanket, is independent of the remaining variables in the graph.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 438,
      "page_label": "419"
    }
  },
  {
    "page_content": "420 8. GRAPHICAL MODELS\nFigure 8.54 Example of a graphical model used to explore the con-\nditional independence properties of the head-to-head\npath a–c–b when a descendant of c, namely the node\nd, is observed.\nc\nab\nd\n8.10 (⋆) Consider the directed graph shown in Figure 8.54 in which none of the variables\nis observed. Show that a ⊥⊥ b |∅ . Suppose we now observe the variable d. Show\nthat in general a ̸⊥⊥ b | d.\n8.11 (⋆⋆ ) Consider the example of the car fuel system shown in Figure 8.21, and suppose\nthat instead of observing the state of the fuel gauge G directly, the gauge is seen by\nthe driver D who reports to us the reading on the gauge. This report is either that the\ngauge shows full D =1 or that it shows emptyD =0 . Our driver is a bit unreliable,\nas expressed through the following probabilities\np(D =1 |G =1 ) = 0 .9 (8.105)\np(D =0 |G =0 ) = 0 .9. (8.106)\nSuppose that the driver tells us that the fuel gauge shows empty, in other words\nthat we observe D =0 . Evaluate the probability that the tank is empty given only\nthis observation. Similarly, evaluate the corresponding probability given also the\nobservation that the battery is ﬂat, and note that this second probability is lower.\nDiscuss the intuition behind this result, and relate the result to Figure 8.54.\n8.12 (⋆) www Show that there are 2M(M−1)/2 distinct undirected graphs over a set of\nM distinct random variables. Draw the 8 possibilities for the case of M =3 .",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 439,
      "page_label": "420"
    }
  },
  {
    "page_content": "M distinct random variables. Draw the 8 possibilities for the case of M =3 .\n8.13 (⋆) Consider the use of iterated conditional modes (ICM) to minimize the energy\nfunction given by (8.42). Write down an expression for the difference in the values\nof the energy associated with the two states of a particular variablexj, with all other\nvariables held ﬁxed, and show that it depends only on quantities that are local to xj\nin the graph.\n8.14 (⋆) Consider a particular case of the energy function given by (8.42) in which the\ncoefﬁcients β = h =0 . Show that the most probable conﬁguration of the latent\nvariables is given by xi = yi for all i.\n8.15 (⋆⋆ ) www Show that the joint distribution p(xn−1,xn) for two neighbouring\nnodes in the graph shown in Figure 8.38 is given by an expression of the form (8.58).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 439,
      "page_label": "420"
    }
  },
  {
    "page_content": "Exercises 421\n8.16 (⋆⋆ ) Consider the inference problem of evaluating p(xn|xN ) for the graph shown\nin Figure 8.38, for all nodes n ∈{ 1,...,N − 1}. Show that the message passing\nalgorithm discussed in Section 8.4.1 can be used to solve this efﬁciently, and discuss\nwhich messages are modiﬁed and in what way.\n8.17 (⋆⋆ ) Consider a graph of the form shown in Figure 8.38 having N =5 nodes, in\nwhich nodes x3 and x5 are observed. Use d-separation to show that x2 ⊥⊥ x5 | x3.\nShow that if the message passing algorithm of Section 8.4.1 is applied to the evalu-\nation of p(x2|x3,x5), the result will be independent of the value of x5.\n8.18 (⋆⋆ ) www Show that a distribution represented by a directed tree can trivially\nbe written as an equivalent distribution over the corresponding undirected tree. Also\nshow that a distribution expressed as an undirected tree can, by suitable normaliza-\ntion of the clique potentials, be written as a directed tree. Calculate the number of\ndistinct directed trees that can be constructed from a given undirected tree.\n8.19 (⋆⋆ ) Apply the sum-product algorithm derived in Section 8.4.4 to the chain-of-\nnodes model discussed in Section 8.4.1 and show that the results (8.54), (8.55), and\n(8.57) are recovered as a special case.\n8.20 (⋆) www Consider the message passing protocol for the sum-product algorithm on\na tree-structured factor graph in which messages are ﬁrst propagated from the leaves",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 440,
      "page_label": "421"
    }
  },
  {
    "page_content": "a tree-structured factor graph in which messages are ﬁrst propagated from the leaves\nto an arbitrarily chosen root node and then from the root node out to the leaves. Use\nproof by induction to show that the messages can be passed in such an order that\nat every step, each node that must send a message has received all of the incoming\nmessages necessary to construct its outgoing messages.\n8.21 (⋆⋆ ) www Show that the marginal distributions p(xs) over the sets of variables\nxs associated with each of the factors fx(xs) in a factor graph can be found by ﬁrst\nrunning the sum-product message passing algorithm and then evaluating the required\nmarginals using (8.72).\n8.22 (⋆) Consider a tree-structured factor graph, in which a given subset of the variable\nnodes form a connected subgraph (i.e., any variable node of the subset is connected\nto at least one of the other variable nodes via a single factor node). Show how the\nsum-product algorithm can be used to compute the marginal distribution over that\nsubset.\n8.23 (⋆⋆ ) www In Section 8.4.4, we showed that the marginal distribution p(xi) for a\nvariable node xi in a factor graph is given by the product of the messages arriving at\nthis node from neighbouring factor nodes in the form (8.63). Show that the marginal\np(xi) can also be written as the product of the incoming message along any one of\nthe links with the outgoing message along the same link.\n8.24 (⋆⋆ ) Show that the marginal distribution for the variables xs in a factor fs(xs) in",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 440,
      "page_label": "421"
    }
  },
  {
    "page_content": "the links with the outgoing message along the same link.\n8.24 (⋆⋆ ) Show that the marginal distribution for the variables xs in a factor fs(xs) in\na tree-structured factor graph, after running the sum-product message passing algo-\nrithm, can be written as the product of the message arriving at the factor node along\nall its links, times the local factor f(xs), in the form (8.72).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 440,
      "page_label": "421"
    }
  },
  {
    "page_content": "422 8. GRAPHICAL MODELS\n8.25 (⋆⋆ ) In (8.86), we veriﬁed that the sum-product algorithm run on the graph in\nFigure 8.51 with node x3 designated as the root node gives the correct marginal for\nx2. Show that the correct marginals are obtained also for x1 and x3. Similarly, show\nthat the use of the result (8.72) after running the sum-product algorithm on this graph\ngives the correct joint distribution for x1,x2.\n8.26 (⋆) Consider a tree-structured factor graph over discrete variables, and suppose we\nwish to evaluate the joint distributionp(xa,xb) associated with two variables xa and\nxb that do not belong to a common factor. Deﬁne a procedure for using the sum-\nproduct algorithm to evaluate this joint distribution in which one of the variables is\nsuccessively clamped to each of its allowed values.\n8.27 (⋆⋆ ) Consider two discrete variables x and y each having three possible states, for\nexample x, y ∈{ 0, 1, 2}. Construct a joint distribution p(x, y) over these variables\nhaving the property that the value ˆx that maximizes the marginal p(x), along with\nthe value ˆy that maximizes the marginal p(y), together have probability zero under\nthe joint distribution, so that p(ˆx,ˆy)=0 .\n8.28 (⋆⋆ ) www The concept of a pending message in the sum-product algorithm for\na factor graph was deﬁned in Section 8.4.7. Show that if the graph has one or more\ncycles, there will always be at least one pending message irrespective of how long\nthe algorithm runs.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 441,
      "page_label": "422"
    }
  },
  {
    "page_content": "cycles, there will always be at least one pending message irrespective of how long\nthe algorithm runs.\n8.29 (⋆⋆ ) www Show that if the sum-product algorithm is run on a factor graph with a\ntree structure (no loops), then after a ﬁnite number of messages have been sent, there\nwill be no pending messages.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 441,
      "page_label": "422"
    }
  },
  {
    "page_content": "9\nMixture Models\nand EM\nIf we deﬁne a joint distribution over observed and latent variables, the correspond-\ning distribution of the observed variables alone is obtained by marginalization. This\nallows relatively complex marginal distributions over observed variables to be ex-\npressed in terms of more tractable joint distributions over the expanded space of\nobserved and latent variables. The introduction of latent variables thereby allows\ncomplicated distributions to be formed from simpler components. In this chapter,\nwe shall see that mixture distributions, such as the Gaussian mixture discussed in\nSection 2.3.9, can be interpreted in terms of discrete latent variables. Continuous\nlatent variables will form the subject of Chapter 12.\nAs well as providing a framework for building more complex probability dis-\ntributions, mixture models can also be used to cluster data. We therefore begin our\ndiscussion of mixture distributions by considering the problem of ﬁnding clusters\nin a set of data points, which we approach ﬁrst using a nonprobabilistic technique\ncalled the K-means algorithm (Lloyd, 1982). Then we introduce the latent variableSection 9.1\n423",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 442,
      "page_label": "423"
    }
  },
  {
    "page_content": "424 9. MIXTURE MODELS AND EM\nview of mixture distributions in which the discrete latent variables can be interpreted\nas deﬁning assignments of data points to speciﬁc components of the mixture. A gen-Section 9.2\neral technique for ﬁnding maximum likelihood estimators in latent variable models\nis the expectation-maximization (EM) algorithm. We ﬁrst of all use the Gaussian\nmixture distribution to motivate the EM algorithm in a fairly informal way, and then\nwe give a more careful treatment based on the latent variable viewpoint. We shallSection 9.3\nsee that the K-means algorithm corresponds to a particular nonprobabilistic limit of\nEM applied to mixtures of Gaussians. Finally, we discuss EM in some generality.Section 9.4\nGaussian mixture models are widely used in data mining, pattern recognition,\nmachine learning, and statistical analysis. In many applications, their parameters are\ndetermined by maximum likelihood, typically using the EM algorithm. However, as\nwe shall see there are some signiﬁcant limitations to the maximum likelihood ap-\nproach, and in Chapter 10 we shall show that an elegant Bayesian treatment can be\ngiven using the framework of variational inference. This requires little additional\ncomputation compared with EM, and it resolves the principal difﬁculties of maxi-\nmum likelihood while also allowing the number of components in the mixture to be\ninferred automatically from the data.\n9.1. K-means Clustering",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 443,
      "page_label": "424"
    }
  },
  {
    "page_content": "mum likelihood while also allowing the number of components in the mixture to be\ninferred automatically from the data.\n9.1. K-means Clustering\nWe begin by considering the problem of identifying groups, or clusters, of data points\nin a multidimensional space. Suppose we have a data set {x1,..., xN } consisting\nof N observations of a random D-dimensional Euclidean variable x. Our goal is to\npartition the data set into some number K of clusters, where we shall suppose for\nthe moment that the value of K is given. Intuitively, we might think of a cluster as\ncomprising a group of data points whose inter-point distances are small compared\nwith the distances to points outside of the cluster. We can formalize this notion by\nﬁrst introducing a set ofD-dimensional vectors µk, where k =1 ,...,K , in which\nµk is a prototype associated with the kth cluster. As we shall see shortly, we can\nthink of the µk as representing the centres of the clusters. Our goal is then to ﬁnd\nan assignment of data points to clusters, as well as a set of vectors {µk}, such that\nthe sum of the squares of the distances of each data point to its closest vector µk,i s\na minimum.\nIt is convenient at this point to deﬁne some notation to describe the assignment\nof data points to clusters. For each data point xn, we introduce a corresponding set\nof binary indicator variables rnk ∈{ 0, 1}, where k =1 ,...,K describing which of\nthe K clusters the data point xn is assigned to, so that if data point xn is assigned to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 443,
      "page_label": "424"
    }
  },
  {
    "page_content": "the K clusters the data point xn is assigned to, so that if data point xn is assigned to\ncluster k then rnk =1 , and rnj =0 for j ̸=k. This is known as the 1-of-K coding\nscheme. We can then deﬁne an objective function, sometimes called a distortion\nmeasure, given by\nJ =\nN∑\nn=1\nK∑\nk=1\nrnk∥xn − µk∥2 (9.1)\nwhich represents the sum of the squares of the distances of each data point to its",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 443,
      "page_label": "424"
    }
  },
  {
    "page_content": "9.1. K-means Clustering 425\nassigned vector µk. Our goal is to ﬁnd values for the {rnk} and the {µk} so as to\nminimize J. We can do this through an iterative procedure in which each iteration\ninvolves two successive steps corresponding to successive optimizations with respect\nto the rnk and the µk. First we choose some initial values for theµk. Then in the ﬁrst\nphase we minimize J with respect to the rnk, keeping the µk ﬁxed. In the second\nphase we minimize J with respect to the µk, keeping rnk ﬁxed. This two-stage\noptimization is then repeated until convergence. We shall see that these two stages\nof updating rnk and updating µk correspond respectively to the E (expectation) and\nM (maximization) steps of the EM algorithm, and to emphasize this we shall use theSection 9.4\nterms E step and M step in the context of the K-means algorithm.\nConsider ﬁrst the determination of the rnk. Because J in (9.1) is a linear func-\ntion of rnk, this optimization can be performed easily to give a closed form solution.\nThe terms involving different n are independent and so we can optimize for each\nn separately by choosing rnk to be 1 for whichever value of k gives the minimum\nvalue of ∥xn − µk∥2. In other words, we simply assign the nth data point to the\nclosest cluster centre. More formally, this can be expressed as\nrnk =\n{1 if k =a r gm i nj ∥xn − µj∥2\n0 otherwise. (9.2)\nNow consider the optimization of the µk with the rnk held ﬁxed. The objective",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 444,
      "page_label": "425"
    }
  },
  {
    "page_content": "rnk =\n{1 if k =a r gm i nj ∥xn − µj∥2\n0 otherwise. (9.2)\nNow consider the optimization of the µk with the rnk held ﬁxed. The objective\nfunction J is a quadratic function of µk, and it can be minimized by setting its\nderivative with respect to µk to zero giving\n2\nN∑\nn=1\nrnk(xn − µk)=0 (9.3)\nwhich we can easily solve for µk to give\nµk =\n∑\nn rnkxn∑\nn rnk\n. (9.4)\nThe denominator in this expression is equal to the number of points assigned to\ncluster k, and so this result has a simple interpretation, namely set µk equal to the\nmean of all of the data pointsxn assigned to cluster k. For this reason, the procedure\nis known as the K-means algorithm.\nThe two phases of re-assigning data points to clusters and re-computing the clus-\nter means are repeated in turn until there is no further change in the assignments (or\nuntil some maximum number of iterations is exceeded). Because each phase reduces\nthe value of the objective functionJ, convergence of the algorithm is assured. How-Exercise 9.1\never, it may converge to a local rather than global minimum of J. The convergence\nproperties of the K-means algorithm were studied by MacQueen (1967).\nThe K-means algorithm is illustrated using the Old Faithful data set in Fig-Appendix A\nure 9.1. For the purposes of this example, we have made a linear re-scaling of the\ndata, known asstandardizing, such that each of the variables has zero mean and\nunit standard deviation. For this example, we have chosen K =2 , and so in this",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 444,
      "page_label": "425"
    }
  },
  {
    "page_content": "426 9. MIXTURE MODELS AND EM\n(a)\n−2 0 2\n−2\n0\n2 (b)\n−2 0 2\n−2\n0\n2 (c)\n−2 0 2\n−2\n0\n2\n(d)\n−2 0 2\n−2\n0\n2 (e)\n−2 0 2\n−2\n0\n2 (f)\n−2 0 2\n−2\n0\n2\n(g)\n−2 0 2\n−2\n0\n2 (h)\n−2 0 2\n−2\n0\n2 (i)\n−2 0 2\n−2\n0\n2\nFigure 9.1 Illustration of the K-means algorithm using the re-scaled Old Faithful data set. (a) Green points\ndenote the data set in a two-dimensional Euclidean space. The initial choices for centres µ1 and µ2 are shown\nby the red and blue crosses, respectively. (b) In the initial E step, each data point is assigned either to the red\ncluster or to the blue cluster, according to which cluster centre is nearer. This is equivalent to classifying the\npoints according to which side of the perpendicular bisector of the two cluster centres, shown by the magenta\nline, they lie on. (c) In the subsequent M step, each cluster centre is re-computed to be the mean of the points\nassigned to the corresponding cluster. (d)–(i) show successive E and M steps through to ﬁnal convergence of\nthe algorithm.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 445,
      "page_label": "426"
    }
  },
  {
    "page_content": "9.1. K-means Clustering 427\nFigure 9.2 Plot of the cost function J given by\n(9.1) after each E step (blue points)\nand M step (red points) of the K-\nmeans algorithm for the example\nshown in Figure 9.1. The algo-\nrithm has converged after the third\nM step, and the ﬁnal EM cycle pro-\nduces no changes in either the as-\nsignments or the prototype vectors.\nJ\n1 2 3 4\n0\n500\n1000\ncase, the assignment of each data point to the nearest cluster centre is equivalent to a\nclassiﬁcation of the data points according to which side they lie of the perpendicular\nbisector of the two cluster centres. A plot of the cost function J given by (9.1) for\nthe Old Faithful example is shown in Figure 9.2.\nNote that we have deliberately chosen poor initial values for the cluster centres\nso that the algorithm takes several steps before convergence. In practice, a better\ninitialization procedure would be to choose the cluster centres µk to be equal to a\nrandom subset of K data points. It is also worth noting that the K-means algorithm\nitself is often used to initialize the parameters in a Gaussian mixture model before\napplying the EM algorithm.Section 9.2.2\nA direct implementation of the K-means algorithm as discussed here can be\nrelatively slow, because in each E step it is necessary to compute the Euclidean dis-\ntance between every prototype vector and every data point. Various schemes have\nbeen proposed for speeding up the K-means algorithm, some of which are based on",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 446,
      "page_label": "427"
    }
  },
  {
    "page_content": "been proposed for speeding up the K-means algorithm, some of which are based on\nprecomputing a data structure such as a tree such that nearby points are in the same\nsubtree (Ramasubramanian and Paliwal, 1990; Moore, 2000). Other approaches\nmake use of the triangle inequality for distances, thereby avoiding unnecessary dis-\ntance calculations (Hodgson, 1998; Elkan, 2003).\nSo far, we have considered a batch version ofK-means in which the whole data\nset is used together to update the prototype vectors. We can also derive an on-line\nstochastic algorithm (MacQueen, 1967) by applying the Robbins-Monro procedureSection 2.3.5\nto the problem of ﬁnding the roots of the regression function given by the derivatives\nof J in (9.1) with respect to µk. This leads to a sequential update in which, for eachExercise 9.2\ndata point xn in turn, we update the nearest prototype µk using\nµnew\nk = µold\nk + ηn(xn − µold\nk ) (9.5)\nwhere ηn is the learning rate parameter, which is typically made to decrease mono-\ntonically as more data points are considered.\nThe K-means algorithm is based on the use of squared Euclidean distance as the\nmeasure of dissimilarity between a data point and a prototype vector. Not only does\nthis limit the type of data variables that can be considered (it would be inappropriate\nfor cases where some or all of the variables represent categorical labels for instance),",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 446,
      "page_label": "427"
    }
  },
  {
    "page_content": "428 9. MIXTURE MODELS AND EM\nbut it can also make the determination of the cluster means nonrobust to outliers. WeSection 2.3.7\ncan generalize the K-means algorithm by introducing a more general dissimilarity\nmeasure V(x,x′) between two vectors x and x′and then minimizing the following\ndistortion measure\n˜J =\nN∑\nn=1\nK∑\nk=1\nrnkV(xn, µk) (9.6)\nwhich gives the K-medoids algorithm. The E step again involves, for given cluster\nprototypes µk, assigning each data point to the cluster for which the dissimilarity to\nthe corresponding prototype is smallest. The computational cost of this is O(KN ),\nas is the case for the standard K-means algorithm. For a general choice of dissimi-\nlarity measure, the M step is potentially more complex than for K-means, and so it\nis common to restrict each cluster prototype to be equal to one of the data vectors as-\nsigned to that cluster, as this allows the algorithm to be implemented for any choice\nof dissimilarity measure V(·, ·) so long as it can be readily evaluated. Thus the M\nstep involves, for each clusterk, a discrete search over theNk points assigned to that\ncluster, which requires O(N2\nk) evaluations of V(·, ·).\nOne notable feature of the K-means algorithm is that at each iteration, every\ndata point is assigned uniquely to one, and only one, of the clusters. Whereas some\ndata points will be much closer to a particular centre µk than to any other centre,\nthere may be other data points that lie roughly midway between cluster centres. In",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 447,
      "page_label": "428"
    }
  },
  {
    "page_content": "there may be other data points that lie roughly midway between cluster centres. In\nthe latter case, it is not clear that the hard assignment to the nearest cluster is the\nmost appropriate. We shall see in the next section that by adopting a probabilistic\napproach, we obtain ‘soft’ assignments of data points to clusters in a way that reﬂects\nthe level of uncertainty over the most appropriate assignment. This probabilistic\nformulation brings with it numerous beneﬁts.\n9.1.1 Image segmentation and compression\nAs an illustration of the application of the K-means algorithm, we consider\nthe related problems of image segmentation and image compression. The goal of\nsegmentation is to partition an image into regions each of which has a reasonably\nhomogeneous visual appearance or which corresponds to objects or parts of objects\n(Forsyth and Ponce, 2003). Each pixel in an image is a point in a 3-dimensional space\ncomprising the intensities of the red, blue, and green channels, and our segmentation\nalgorithm simply treats each pixel in the image as a separate data point. Note that\nstrictly this space is not Euclidean because the channel intensities are bounded by\nthe interval[0,1]. Nevertheless, we can apply the K-means algorithm without difﬁ-\nculty. We illustrate the result of runningK-means to convergence, for any particular\nvalue of K, by re-drawing the image replacing each pixel vector with the{R, G, B}",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 447,
      "page_label": "428"
    }
  },
  {
    "page_content": "value of K, by re-drawing the image replacing each pixel vector with the{R, G, B}\nintensity triplet given by the centreµk to which that pixel has been assigned. Results\nfor various values of K are shown in Figure 9.3. We see that for a given value ofK,\nthe algorithm is representing the image using a palette of only K colours. It should\nbe emphasized that this use of K-means is not a particularly sophisticated approach\nto image segmentation, not least because it takes no account of the spatial proximity\nof different pixels. The image segmentation problem is in general extremely difﬁcult",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 447,
      "page_label": "428"
    }
  },
  {
    "page_content": "9.1. K-means Clustering 429\nK =2\n K =3\n K =1 0\n Original image\nFigure 9.3 Two examples of the application of theK-means clustering algorithm to image segmentation show-\ning the initial images together with their K-means segmentations obtained using various values of K. This\nalso illustrates of the use of vector quantization for data compression, in which smaller values of K give higher\ncompression at the expense of poorer image quality.\nand remains the subject of active research and is introduced here simply to illustrate\nthe behaviour of the K-means algorithm.\nWe can also use the result of a clustering algorithm to perform data compres-\nsion. It is important to distinguish between lossless data compression, in which\nthe goal is to be able to reconstruct the original data exactly from the compressed\nrepresentation, and lossy data compression, in which we accept some errors in the\nreconstruction in return for higher levels of compression than can be achieved in the\nlossless case. We can apply the K-means algorithm to the problem of lossy data\ncompression as follows. For each of the N data points, we store only the identity\nk of the cluster to which it is assigned. We also store the values of the K clus-\nter centres µk, which typically requires signiﬁcantly less data, provided we choose\nK ≪ N. Each data point is then approximated by its nearest centre µk. New data\npoints can similarly be compressed by ﬁrst ﬁnding the nearest µk and then storing",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 448,
      "page_label": "429"
    }
  },
  {
    "page_content": "points can similarly be compressed by ﬁrst ﬁnding the nearest µk and then storing\nthe label k instead of the original data vector. This framework is often called vector\nquantization, and the vectors µk are called code-book vectors.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 448,
      "page_label": "429"
    }
  },
  {
    "page_content": "430 9. MIXTURE MODELS AND EM\nThe image segmentation problem discussed above also provides an illustration\nof the use of clustering for data compression. Suppose the original image has N\npixels comprising {R, G, B} values each of which is stored with 8 bits of precision.\nThen to transmit the whole image directly would cost 24N bits. Now suppose we\nﬁrst run K-means on the image data, and then instead of transmitting the original\npixel intensity vectors we transmit the identity of the nearest vector µk. Because\nthere are K such vectors, this requires log2 K bits per pixel. We must also transmit\nthe K code book vectors µk, which requires 24 K bits, and so the total number of\nbits required to transmit the image is 24K + N log2 K (rounding up to the nearest\ninteger). The original image shown in Figure 9.3 has 240 × 180 = 43, 200 pixels\nand so requires 24 × 43, 200 = 1, 036, 800 bits to transmit directly. By comparison,\nthe compressed images require 43, 248 bits (K =2 ), 86, 472 bits (K =3 ), and\n173, 040 bits (K =1 0), respectively, to transmit. These represent compression ratios\ncompared to the original image of 4.2%, 8.3%, and 16.7%, respectively. We see that\nthere is a trade-off between degree of compression and image quality. Note that our\naim in this example is to illustrate the K-means algorithm. If we had been aiming to\nproduce a good image compressor, then it would be more fruitful to consider small",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 449,
      "page_label": "430"
    }
  },
  {
    "page_content": "produce a good image compressor, then it would be more fruitful to consider small\nblocks of adjacent pixels, for instance5×5, and thereby exploit the correlations that\nexist in natural images between nearby pixels.\n9.2. Mixtures of Gaussians\nIn Section 2.3.9 we motivated the Gaussian mixture model as a simple linear super-\nposition of Gaussian components, aimed at providing a richer class of density mod-\nels than the single Gaussian. We now turn to a formulation of Gaussian mixtures in\nterms of discrete latent variables. This will provide us with a deeper insight into this\nimportant distribution, and will also serve to motivate the expectation-maximization\nalgorithm.\nRecall from (2.188) that the Gaussian mixture distribution can be written as a\nlinear superposition of Gaussians in the form\np(x)=\nK∑\nk=1\nπkN(x|µk, Σk). (9.7)\nLet us introduce a K-dimensional binary random variable z having a 1-of-K repre-\nsentation in which a particular element zk is equal to 1 and all other elements are\nequal to 0. The values of zk therefore satisfy zk ∈{ 0, 1} and ∑\nk zk =1 , and we\nsee that there are K possible states for the vector z according to which element is\nnonzero. We shall deﬁne the joint distribution p(x, z) in terms of a marginal dis-\ntribution p(z) and a conditional distribution p(x|z), corresponding to the graphical\nmodel in Figure 9.4. The marginal distribution over z is speciﬁed in terms of the\nmixing coefﬁcients πk, such that\np(zk =1 )= πk",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 449,
      "page_label": "430"
    }
  },
  {
    "page_content": "9.2. Mixtures of Gaussians 431\nFigure 9.4 Graphical representation of a mixture model, in which\nthe joint distribution is expressed in the form p(x, z)=\np(z)p(x|z).\nx\nz\nwhere the parameters {πk} must satisfy\n0 ⩽ πk ⩽ 1 (9.8)\ntogether with\nK∑\nk=1\nπk =1 (9.9)\nin order to be valid probabilities. Because z uses a 1-of- K representation, we can\nalso write this distribution in the form\np(z)=\nK∏\nk=1\nπzk\nk . (9.10)\nSimilarly, the conditional distribution ofx given a particular value forz is a Gaussian\np(x|zk =1 )= N(x|µk, Σk)\nwhich can also be written in the form\np(x|z)=\nK∏\nk=1\nN(x|µk, Σk)zk . (9.11)\nThe joint distribution is given by p(z)p(x|z), and the marginal distribution of x is\nthen obtained by summing the joint distribution over all possible states of z to giveExercise 9.3\np(x)=\n∑\nz\np(z)p(x|z)=\nK∑\nk=1\nπkN(x|µk, Σk) (9.12)\nwhere we have made use of (9.10) and (9.11). Thus the marginal distribution of x is\na Gaussian mixture of the form (9.7). If we have several observations x1,..., xN ,\nthen, because we have represented the marginal distribution in the form p(x)=∑\nz p(x, z), it follows that for every observed data pointxn there is a corresponding\nlatent variable zn.\nWe have therefore found an equivalent formulation of the Gaussian mixture in-\nvolving an explicit latent variable. It might seem that we have not gained much\nby doing so. However, we are now able to work with the joint distribution p(x, z)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 450,
      "page_label": "431"
    }
  },
  {
    "page_content": "432 9. MIXTURE MODELS AND EM\ninstead of the marginal distribution p(x), and this will lead to signiﬁcant simpliﬁca-\ntions, most notably through the introduction of the expectation-maximization (EM)\nalgorithm.\nAnother quantity that will play an important role is the conditional probability\nof z given x. We shall use γ(zk) to denote p(zk =1 |x), whose value can be found\nusing Bayes’ theorem\nγ(zk) ≡ p(zk =1 |x)= p(zk =1 )p(x|zk =1 )\nK∑\nj=1\np(zj =1 )p(x|zj =1 )\n= πkN(x|µk, Σk)\nK∑\nj=1\nπjN(x|µj, Σj)\n. (9.13)\nWe shall view πk as the prior probability of zk =1 , and the quantity γ(zk) as the\ncorresponding posterior probability once we have observed x. As we shall see later,\nγ(zk) can also be viewed as the responsibility that component k takes for ‘explain-\ning’ the observation x.\nWe can use the technique of ancestral sampling to generate random samplesSection 8.1.2\ndistributed according to the Gaussian mixture model. To do this, we ﬁrst generate a\nvalue forz, which we denoteˆz, from the marginal distributionp(z) and then generate\na value for x from the conditional distributionp(x|ˆz). Techniques for sampling from\nstandard distributions are discussed in Chapter 11. We can depict samples from the\njoint distribution p(x, z) by plotting points at the corresponding values of x and\nthen colouring them according to the value of z, in other words according to which\nGaussian component was responsible for generating them, as shown in Figure 9.5(a).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 451,
      "page_label": "432"
    }
  },
  {
    "page_content": "Gaussian component was responsible for generating them, as shown in Figure 9.5(a).\nSimilarly samples from the marginal distribution p(x) are obtained by taking the\nsamples from the joint distribution and ignoring the values ofz. These are illustrated\nin Figure 9.5(b) by plotting the x values without any coloured labels.\nWe can also use this synthetic data set to illustrate the ‘responsibilities’ by eval-\nuating, for every data point, the posterior probability for each component in the\nmixture distribution from which this data set was generated. In particular, we can\nrepresent the value of the responsibilities γ(znk) associated with data point xn by\nplotting the corresponding point using proportions of red, blue, and green ink given\nbyγ(znk) for k =1 , 2, 3, respectively, as shown in Figure 9.5(c). So, for instance,\na data point for which γ(zn1)=1 will be coloured red, whereas one for which\nγ(zn2)= γ(zn3)=0 .5 will be coloured with equal proportions of blue and green\nink and so will appear cyan. This should be compared with Figure 9.5(a) in which\nthe data points were labelled using the true identity of the component from which\nthey were generated.\n9.2.1 Maximum likelihood\nSuppose we have a data set of observations{x1,..., xN }, and we wish to model\nthis data using a mixture of Gaussians. We can represent this data set as an N × D",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 451,
      "page_label": "432"
    }
  },
  {
    "page_content": "9.2. Mixtures of Gaussians 433\n(a)\n0 0.5 1\n0\n0.5\n1 (b)\n0 0.5 1\n0\n0.5\n1 (c)\n0 0.5 1\n0\n0.5\n1\nFigure 9.5 Example of 500 points drawn from the mixture of 3 Gaussians shown in Figure 2.23. (a) Samples\nfrom the joint distribution p(z)p(x|z) in which the three states of z, corresponding to the three components of the\nmixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution\np(x), which is obtained by simply ignoring the values of z and just plotting the x values. The data set in (a) is\nsaid to be complete, whereas that in (b) is incomplete. (c) The same samples in which the colours represent the\nvalue of the responsibilities γ(znk) associated with data point xn, obtained by plotting the corresponding point\nusing proportions of red, blue, and green ink given by γ(znk) for k =1 , 2, 3, respectively\nmatrix X in which the nth row is given by xT\nn. Similarly, the corresponding latent\nvariables will be denoted by an N × K matrix Z with rows zT\nn. If we assume that\nthe data points are drawn independently from the distribution, then we can express\nthe Gaussian mixture model for this i.i.d. data set using the graphical representation\nshown in Figure 9.6. From (9.7) the log of the likelihood function is given by\nlnp(X|π, µ, Σ)=\nN∑\nn=1\nln\n{ K∑\nk=1\nπkN(xn|µk, Σk)\n}\n. (9.14)\nBefore discussing how to maximize this function, it is worth emphasizing that\nthere is a signiﬁcant problem associated with the maximum likelihood framework",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 452,
      "page_label": "433"
    }
  },
  {
    "page_content": "there is a signiﬁcant problem associated with the maximum likelihood framework\napplied to Gaussian mixture models, due to the presence of singularities. For sim-\nplicity, consider a Gaussian mixture whose components have covariance matrices\ngiven by Σk = σ2\nkI, where I is the unit matrix, although the conclusions will hold\nfor general covariance matrices. Suppose that one of the components of the mixture\nmodel, let us say the jth component, has its mean µj exactly equal to one of the data\nFigure 9.6 Graphical representation of a Gaussian mixture model\nfor a set of N i.i.d. data points {xn}, with corresponding\nlatent points {zn}, where n =1 ,...,N .\nxn\nzn\nN\nµ Σ\nπ",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 452,
      "page_label": "433"
    }
  },
  {
    "page_content": "434 9. MIXTURE MODELS AND EM\nFigure 9.7 Illustration of how singularities in the\nlikelihood function arise with mixtures\nof Gaussians. This should be com-\npared with the case of a single Gaus-\nsian shown in Figure 1.14 for which no\nsingularities arise.\nx\np(x)\npoints so that µj = xn for some value of n. This data point will then contribute a\nterm in the likelihood function of the form\nN(xn|xn,σ 2\nj I)= 1\n(2π)1/2\n1\nσj\n. (9.15)\nIf we consider the limit σj → 0, then we see that this term goes to inﬁnity and\nso the log likelihood function will also go to inﬁnity. Thus the maximization of\nthe log likelihood function is not a well posed problem because such singularities\nwill always be present and will occur whenever one of the Gaussian components\n‘collapses’ onto a speciﬁc data point. Recall that this problem did not arise in the\ncase of a single Gaussian distribution. To understand the difference, note that if a\nsingle Gaussian collapses onto a data point it will contribute multiplicative factors\nto the likelihood function arising from the other data points and these factors will go\nto zero exponentially fast, giving an overall likelihood that goes to zero rather than\ninﬁnity. However, once we have (at least) two components in the mixture, one of\nthe components can have a ﬁnite variance and therefore assign ﬁnite probability to\nall of the data points while the other component can shrink onto one speciﬁc data",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 453,
      "page_label": "434"
    }
  },
  {
    "page_content": "all of the data points while the other component can shrink onto one speciﬁc data\npoint and thereby contribute an ever increasing additive value to the log likelihood.\nThis is illustrated in Figure 9.7. These singularities provide another example of the\nsevere over-ﬁtting that can occur in a maximum likelihood approach. We shall see\nthat this difﬁculty does not occur if we adopt a Bayesian approach. For the moment,Section 10.1\nhowever, we simply note that in applying maximum likelihood to Gaussian mixture\nmodels we must take steps to avoid ﬁnding such pathological solutions and instead\nseek local maxima of the likelihood function that are well behaved. We can hope to\navoid the singularities by using suitable heuristics, for instance by detecting when a\nGaussian component is collapsing and resetting its mean to a randomly chosen value\nwhile also resetting its covariance to some large value, and then continuing with the\noptimization.\nA further issue in ﬁnding maximum likelihood solutions arises from the fact\nthat for any given maximum likelihood solution, a K-component mixture will have\na total of K! equivalent solutions corresponding to the K! ways of assigning K\nsets of parameters to K components. In other words, for any given (nondegenerate)\npoint in the space of parameter values there will be a furtherK!−1 additional points\nall of which give rise to exactly the same distribution. This problem is known as",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 453,
      "page_label": "434"
    }
  },
  {
    "page_content": "9.2. Mixtures of Gaussians 435\nidentiﬁability (Casella and Berger, 2002) and is an important issue when we wish to\ninterpret the parameter values discovered by a model. Identiﬁability will also arise\nwhen we discuss models having continuous latent variables in Chapter 12. However,\nfor the purposes of ﬁnding a good density model, it is irrelevant because any of the\nequivalent solutions is as good as any other.\nMaximizing the log likelihood function (9.14) for a Gaussian mixture model\nturns out to be a more complex problem than for the case of a single Gaussian. The\ndifﬁculty arises from the presence of the summation over k that appears inside the\nlogarithm in (9.14), so that the logarithm function no longer acts directly on the\nGaussian. If we set the derivatives of the log likelihood to zero, we will no longer\nobtain a closed form solution, as we shall see shortly.\nOne approach is to apply gradient-based optimization techniques (Fletcher, 1987;\nNocedal and Wright, 1999; Bishop and Nabney, 2008). Although gradient-based\ntechniques are feasible, and indeed will play an important role when we discuss\nmixture density networks in Chapter 5, we now consider an alternative approach\nknown as the EM algorithm which has broad applicability and which will lay the\nfoundations for a discussion of variational inference techniques in Chapter 10.\n9.2.2 EM for Gaussian mixtures\nAn elegant and powerful method for ﬁnding maximum likelihood solutions for",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 454,
      "page_label": "435"
    }
  },
  {
    "page_content": "9.2.2 EM for Gaussian mixtures\nAn elegant and powerful method for ﬁnding maximum likelihood solutions for\nmodels with latent variables is called theexpectation-maximization algorithm, or EM\nalgorithm (Dempster et al., 1977; McLachlan and Krishnan, 1997). Later we shall\ngive a general treatment of EM, and we shall also show how EM can be generalized\nto obtain the variational inference framework. Initially, we shall motivate the EMSection 10.1\nalgorithm by giving a relatively informal treatment in the context of the Gaussian\nmixture model. We emphasize, however, that EM has broad applicability, and indeed\nit will be encountered in the context of a variety of different models in this book.\nLet us begin by writing down the conditions that must be satisﬁed at a maximum\nof the likelihood function. Setting the derivatives of lnp(X|π, µ, Σ) in (9.14) with\nrespect to the means µk of the Gaussian components to zero, we obtain\n0= −\nN∑\nn=1\nπkN(xn|µk, Σk)∑\nj πjN(xn|µj, Σj)\n  \nγ(znk)\nΣk(xn − µk) (9.16)\nwhere we have made use of the form (2.43) for the Gaussian distribution. Note that\nthe posterior probabilities, or responsibilities, given by (9.13) appear naturally on\nthe right-hand side. Multiplying by Σ−1\nk (which we assume to be nonsingular) and\nrearranging we obtain\nµk = 1\nNk\nN∑\nn=1\nγ(znk)xn (9.17)\nwhere we have deﬁned\nNk =\nN∑\nn=1\nγ(znk). (9.18)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 454,
      "page_label": "435"
    }
  },
  {
    "page_content": "436 9. MIXTURE MODELS AND EM\nWe can interpret Nk as the effective number of points assigned to cluster k. Note\ncarefully the form of this solution. We see that the mean µk for the kth Gaussian\ncomponent is obtained by taking a weighted mean of all of the points in the data set,\nin which the weighting factor for data point xn is given by the posterior probability\nγ(znk) that component k was responsible for generating xn.\nIf we set the derivative oflnp(X|π, µ, Σ) with respect toΣk to zero, and follow\na similar line of reasoning, making use of the result for the maximum likelihood\nsolution for the covariance matrix of a single Gaussian, we obtainSection 2.3.4\nΣk = 1\nNk\nN∑\nn=1\nγ(znk)(xn − µk)(xn − µk)T (9.19)\nwhich has the same form as the corresponding result for a single Gaussian ﬁtted to\nthe data set, but again with each data point weighted by the corresponding poste-\nrior probability and with the denominator given by the effective number of points\nassociated with the corresponding component.\nFinally, we maximize ln p(X|π, µ, Σ) with respect to the mixing coefﬁcients\nπk. Here we must take account of the constraint (9.9), which requires the mixing\ncoefﬁcients to sum to one. This can be achieved using a Lagrange multiplier andAppendix E\nmaximizing the following quantity\nlnp(X|π, µ, Σ)+ λ\n( K∑\nk=1\nπk − 1\n)\n(9.20)\nwhich gives\n0=\nN∑\nn=1\nN(xn|µk, Σk)∑\nj πjN(xn|µj, Σj) + λ (9.21)\nwhere again we see the appearance of the responsibilities. If we now multiply both",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 455,
      "page_label": "436"
    }
  },
  {
    "page_content": "which gives\n0=\nN∑\nn=1\nN(xn|µk, Σk)∑\nj πjN(xn|µj, Σj) + λ (9.21)\nwhere again we see the appearance of the responsibilities. If we now multiply both\nsides by πk and sum over k making use of the constraint (9.9), we ﬁnd λ = −N.\nUsing this to eliminate λ and rearranging we obtain\nπk = Nk\nN (9.22)\nso that the mixing coefﬁcient for the kth component is given by the average respon-\nsibility which that component takes for explaining the data points.\nIt is worth emphasizing that the results (9.17), (9.19), and (9.22) do not con-\nstitute a closed-form solution for the parameters of the mixture model because the\nresponsibilities γ(znk) depend on those parameters in a complex way through (9.13).\nHowever, these results do suggest a simple iterative scheme for ﬁnding a solution to\nthe maximum likelihood problem, which as we shall see turns out to be an instance\nof the EM algorithm for the particular case of the Gaussian mixture model. We\nﬁrst choose some initial values for the means, covariances, and mixing coefﬁcients.\nThen we alternate between the following two updates that we shall call the E step",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 455,
      "page_label": "436"
    }
  },
  {
    "page_content": "9.2. Mixtures of Gaussians 437\n(a)−2 0 2\n−2\n0\n2\n(b)−2 0 2\n−2\n0\n2\n(c)\nL =1\n−2 0 2\n−2\n0\n2\n(d)\nL =2\n−2 0 2\n−2\n0\n2\n(e)\nL =5\n−2 0 2\n−2\n0\n2\n(f)\nL =2 0\n−2 0 2\n−2\n0\n2\nFigure 9.8 Illustration of the EM algorithm using the Old Faithful set as used for the illustration of theK-means\nalgorithm in Figure 9.1. See the text for details.\nand the M step, for reasons that will become apparent shortly. In the expectation\nstep, or E step, we use the current values for the parameters to evaluate the posterior\nprobabilities, or responsibilities, given by (9.13). We then use these probabilities in\nthe maximization step, or M step, to re-estimate the means, covariances, and mix-\ning coefﬁcients using the results (9.17), (9.19), and (9.22). Note that in so doing\nwe ﬁrst evaluate the new means using (9.17) and then use these new values to ﬁnd\nthe covariances using (9.19), in keeping with the corresponding result for a single\nGaussian distribution. We shall show that each update to the parameters resulting\nfrom an E step followed by an M step is guaranteed to increase the log likelihood\nfunction. In practice, the algorithm is deemed to have converged when the changeSection 9.4\nin the log likelihood function, or alternatively in the parameters, falls below some\nthreshold. We illustrate the EM algorithm for a mixture of two Gaussians applied to\nthe rescaled Old Faithful data set in Figure 9.8. Here a mixture of two Gaussians",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 456,
      "page_label": "437"
    }
  },
  {
    "page_content": "the rescaled Old Faithful data set in Figure 9.8. Here a mixture of two Gaussians\nis used, with centres initialized using the same values as for the K-means algorithm\nin Figure 9.1, and with precision matrices initialized to be proportional to the unit\nmatrix. Plot (a) shows the data points in green, together with the initial conﬁgura-\ntion of the mixture model in which the one standard-deviation contours for the two",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 456,
      "page_label": "437"
    }
  },
  {
    "page_content": "438 9. MIXTURE MODELS AND EM\nGaussian components are shown as blue and red circles. Plot (b) shows the result\nof the initial E step, in which each data point is depicted using a proportion of blue\nink equal to the posterior probability of having been generated from the blue com-\nponent, and a corresponding proportion of red ink given by the posterior probability\nof having been generated by the red component. Thus, points that have a signiﬁcant\nprobability for belonging to either cluster appear purple. The situation after the ﬁrst\nM step is shown in plot (c), in which the mean of the blue Gaussian has moved to\nthe mean of the data set, weighted by the probabilities of each data point belonging\nto the blue cluster, in other words it has moved to the centre of mass of the blue ink.\nSimilarly, the covariance of the blue Gaussian is set equal to the covariance of the\nblue ink. Analogous results hold for the red component. Plots (d), (e), and (f) show\nthe results after 2, 5, and 20 complete cycles of EM, respectively. In plot (f) the\nalgorithm is close to convergence.\nNote that the EM algorithm takes many more iterations to reach (approximate)\nconvergence compared with the K-means algorithm, and that each cycle requires\nsigniﬁcantly more computation. It is therefore common to run the K-means algo-\nrithm in order to ﬁnd a suitable initialization for a Gaussian mixture model that is\nsubsequently adapted using EM. The covariance matrices can conveniently be ini-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 457,
      "page_label": "438"
    }
  },
  {
    "page_content": "subsequently adapted using EM. The covariance matrices can conveniently be ini-\ntialized to the sample covariances of the clusters found by the K-means algorithm,\nand the mixing coefﬁcients can be set to the fractions of data points assigned to the\nrespective clusters. As with gradient-based approaches for maximizing the log like-\nlihood, techniques must be employed to avoid singularities of the likelihood function\nin which a Gaussian component collapses onto a particular data point. It should be\nemphasized that there will generally be multiple local maxima of the log likelihood\nfunction, and that EM is not guaranteed to ﬁnd the largest of these maxima. Because\nthe EM algorithm for Gaussian mixtures plays such an important role, we summarize\nit below.\nEM for Gaussian Mixtures\nGiven a Gaussian mixture model, the goal is to maximize the likelihood function\nwith respect to the parameters (comprising the means and covariances of the\ncomponents and the mixing coefﬁcients).\n1. Initialize the means µk, covariances Σk and mixing coefﬁcients πk, and\nevaluate the initial value of the log likelihood.\n2. E step. Evaluate the responsibilities using the current parameter values\nγ(znk)= πkN(xn|µk, Σk)\nK∑\nj=1\nπjN(xn|µj, Σj)\n. (9.23)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 457,
      "page_label": "438"
    }
  },
  {
    "page_content": "9.3. An Alternative View of EM 439\n3. M step. Re-estimate the parameters using the current responsibilities\nµnew\nk = 1\nNk\nN∑\nn=1\nγ(znk)xn (9.24)\nΣnew\nk = 1\nNk\nN∑\nn=1\nγ(znk)(xn − µnew\nk )( xn − µnew\nk )T (9.25)\nπnew\nk = Nk\nN (9.26)\nwhere\nNk =\nN∑\nn=1\nγ(znk). (9.27)\n4. Evaluate the log likelihood\nlnp(X|µ, Σ, π)=\nN∑\nn=1\nln\n{ K∑\nk=1\nπkN(xn|µk, Σk)\n}\n(9.28)\nand check for convergence of either the parameters or the log likelihood. If\nthe convergence criterion is not satisﬁed return to step 2.\n9.3. An Alternative View of EM\nIn this section, we present a complementary view of the EM algorithm that recog-\nnizes the key role played by latent variables. We discuss this approach ﬁrst of all\nin an abstract setting, and then for illustration we consider once again the case of\nGaussian mixtures.\nThe goal of the EM algorithm is to ﬁnd maximum likelihood solutions for mod-\nels having latent variables. We denote the set of all observed data byX, in which the\nnth row represents xT\nn, and similarly we denote the set of all latent variables by Z,\nwith a corresponding row zT\nn. The set of all model parameters is denoted by θ, and\nso the log likelihood function is given by\nlnp(X|θ)=l n\n{∑\nZ\np(X, Z|θ)\n}\n. (9.29)\nNote that our discussion will apply equally well to continuous latent variables simply\nby replacing the sum over Z with an integral.\nA key observation is that the summation over the latent variables appears inside",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 458,
      "page_label": "439"
    }
  },
  {
    "page_content": "by replacing the sum over Z with an integral.\nA key observation is that the summation over the latent variables appears inside\nthe logarithm. Even if the joint distribution p(X, Z|θ) belongs to the exponential",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 458,
      "page_label": "439"
    }
  },
  {
    "page_content": "440 9. MIXTURE MODELS AND EM\nfamily, the marginal distribution p(X|θ) typically does not as a result of this sum-\nmation. The presence of the sum prevents the logarithm from acting directly on the\njoint distribution, resulting in complicated expressions for the maximum likelihood\nsolution.\nNow suppose that, for each observation in X, we were told the corresponding\nvalue of the latent variable Z. We shall call {X, Z} the complete data set, and we\nshall refer to the actual observed data X as incomplete, as illustrated in Figure 9.5.\nThe likelihood function for the complete data set simply takes the formln p(X, Z|θ),\nand we shall suppose that maximization of this complete-data log likelihood function\nis straightforward.\nIn practice, however, we are not given the complete data set {X, Z}, but only\nthe incomplete data X. Our state of knowledge of the values of the latent variables\nin Z is given only by the posterior distribution p(Z|X, θ). Because we cannot use\nthe complete-data log likelihood, we consider instead its expected value under the\nposterior distribution of the latent variable, which corresponds (as we shall see) to the\nE step of the EM algorithm. In the subsequent M step, we maximize this expectation.\nIf the current estimate for the parameters is denotedθold, then a pair of successive\nE and M steps gives rise to a revised estimate θnew. The algorithm is initialized by\nchoosing some starting value for the parameters θ0. The use of the expectation may",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 459,
      "page_label": "440"
    }
  },
  {
    "page_content": "choosing some starting value for the parameters θ0. The use of the expectation may\nseem somewhat arbitrary. However, we shall see the motivation for this choice when\nwe give a deeper treatment of EM in Section 9.4.\nIn the E step, we use the current parameter values θold to ﬁnd the posterior\ndistribution of the latent variables given by p(Z|X, θold). We then use this posterior\ndistribution to ﬁnd the expectation of the complete-data log likelihood evaluated for\nsome general parameter value θ. This expectation, denoted Q(θ, θold),i sg i v e nb y\nQ(θ, θold)=\n∑\nZ\np(Z|X, θold)l np(X, Z|θ). (9.30)\nIn the M step, we determine the revised parameter estimateθnew by maximizing this\nfunction\nθnew =a r gm a x\nθ\nQ(θ, θold). (9.31)\nNote that in the deﬁnition of Q(θ, θold), the logarithm acts directly on the joint\ndistribution p(X, Z|θ), and so the corresponding M-step maximization will, by sup-\nposition, be tractable.\nThe general EM algorithm is summarized below. It has the property, as we shall\nshow later, that each cycle of EM will increase the incomplete-data log likelihood\n(unless it is already at a local maximum).Section 9.4\nThe General EM Algorithm\nGiven a joint distribution p(X, Z|θ) over observed variables X and latent vari-\nables Z, governed by parameters θ, the goal is to maximize the likelihood func-\ntion p(X|θ) with respect to θ.\n1. Choose an initial setting for the parameters θold.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 459,
      "page_label": "440"
    }
  },
  {
    "page_content": "9.3. An Alternative View of EM 441\n2. E stepEvaluate p(Z|X, θold).\n3. M stepEvaluate θnew given by\nθnew =a r gm a x\nθ\nQ(θ, θold) (9.32)\nwhere\nQ(θ, θold)=\n∑\nZ\np(Z|X, θold)l np(X, Z|θ). (9.33)\n4. Check for convergence of either the log likelihood or the parameter values.\nIf the convergence criterion is not satisﬁed, then let\nθold ← θnew (9.34)\nand return to step 2.\nThe EM algorithm can also be used to ﬁnd MAP (maximum posterior) solutions\nfor models in which a prior p(θ) is deﬁned over the parameters. In this case the EExercise 9.4\nstep remains the same as in the maximum likelihood case, whereas in the M step the\nquantity to be maximized is given by Q(θ, θold)+l n p(θ). Suitable choices for the\nprior will remove the singularities of the kind illustrated in Figure 9.7.\nHere we have considered the use of the EM algorithm to maximize a likelihood\nfunction when there are discrete latent variables. However, it can also be applied\nwhen the unobserved variables correspond to missing values in the data set. The\ndistribution of the observed values is obtained by taking the joint distribution of all\nthe variables and then marginalizing over the missing ones. EM can then be used\nto maximize the corresponding likelihood function. We shall show an example of\nthe application of this technique in the context of principal component analysis in\nFigure 12.11. This will be a valid procedure if the data values aremissing at random,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 460,
      "page_label": "441"
    }
  },
  {
    "page_content": "Figure 12.11. This will be a valid procedure if the data values aremissing at random,\nmeaning that the mechanism causing values to be missing does not depend on the\nunobserved values. In many situations this will not be the case, for instance if a\nsensor fails to return a value whenever the quantity it is measuring exceeds some\nthreshold.\n9.3.1 Gaussian mixtures revisited\nWe now consider the application of this latent variable view of EM to the spe-\nciﬁc case of a Gaussian mixture model. Recall that our goal is to maximize the log\nlikelihood function (9.14), which is computed using the observed data setX, and we\nsaw that this was more difﬁcult than for the case of a single Gaussian distribution\ndue to the presence of the summation over k that occurs inside the logarithm. Sup-\npose then that in addition to the observed data set X, we were also given the values\nof the corresponding discrete variables Z. Recall that Figure 9.5(a) shows a ‘com-\nplete’ data set (i.e., one that includes labels showing which component generated\neach data point) while Figure 9.5(b) shows the corresponding ‘incomplete’ data set.\nThe graphical model for the complete data is shown in Figure 9.9.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 460,
      "page_label": "441"
    }
  },
  {
    "page_content": "442 9. MIXTURE MODELS AND EM\nFigure 9.9 This shows the same graph as in Figure 9.6 except that\nwe now suppose that the discrete variables zn are ob-\nserved, as well as the data variables xn.\nxn\nzn\nN\nµ Σ\nπ\nNow consider the problem of maximizing the likelihood for the complete data\nset {X, Z}. From (9.10) and (9.11), this likelihood function takes the form\np(X, Z|µ, Σ, π)=\nN∏\nn=1\nK∏\nk=1\nπznk\nk N(xn|µk, Σk)znk (9.35)\nwhere znk denotes the kth component of zn. Taking the logarithm, we obtain\nlnp(X, Z|µ, Σ, π)=\nN∑\nn=1\nK∑\nk=1\nznk {lnπk +l nN(xn|µk, Σk)}. (9.36)\nComparison with the log likelihood function (9.14) for the incomplete data shows\nthat the summation over k and the logarithm have been interchanged. The loga-\nrithm now acts directly on the Gaussian distribution, which itself is a member of\nthe exponential family. Not surprisingly, this leads to a much simpler solution to\nthe maximum likelihood problem, as we now show. Consider ﬁrst the maximization\nwith respect to the means and covariances. Because zn is a K-dimensional vec-\ntor with all elements equal to 0 except for a single element having the value 1, the\ncomplete-data log likelihood function is simply a sum of K independent contribu-\ntions, one for each mixture component. Thus the maximization with respect to a\nmean or a covariance is exactly as for a single Gaussian, except that it involves only\nthe subset of data points that are ‘assigned’ to that component. For the maximization",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 461,
      "page_label": "442"
    }
  },
  {
    "page_content": "the subset of data points that are ‘assigned’ to that component. For the maximization\nwith respect to the mixing coefﬁcients, we note that these are coupled for different\nvalues of k by virtue of the summation constraint (9.9). Again, this can be enforced\nusing a Lagrange multiplier as before, and leads to the result\nπk = 1\nN\nN∑\nn=1\nznk (9.37)\nso that the mixing coefﬁcients are equal to the fractions of data points assigned to\nthe corresponding components.\nThus we see that the complete-data log likelihood function can be maximized\ntrivially in closed form. In practice, however, we do not have values for the latent\nvariables so, as discussed earlier, we consider the expectation, with respect to the\nposterior distribution of the latent variables, of the complete-data log likelihood.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 461,
      "page_label": "442"
    }
  },
  {
    "page_content": "9.3. An Alternative View of EM 443\nUsing (9.10) and (9.11) together with Bayes’ theorem, we see that this posterior\ndistribution takes the form\np(Z|X, µ, Σ, π) ∝\nN∏\nn=1\nK∏\nk=1\n[πkN(xn|µk, Σk)]znk . (9.38)\nand hence factorizes over n so that under the posterior distribution the {zn} are\nindependent. This is easily veriﬁed by inspection of the directed graph in Figure 9.6Exercise 9.5\nand making use of the d-separation criterion. The expected value of the indicatorSection 8.2\nvariable znk under this posterior distribution is then given by\nE[znk]=\n∑\nznk\nznk [πkN(xn|µk, Σk)]znk\n∑\nznj\n[\nπjN(xn|µj, Σj)\n]znj\n= πkN(xn|µk, Σk)\nK∑\nj=1\nπjN(xn|µj, Σj)\n= γ(znk) (9.39)\nwhich is just the responsibility of componentk for data pointxn. The expected value\nof the complete-data log likelihood function is therefore given by\nEZ[lnp(X, Z|µ, Σ, π)] =\nN∑\nn=1\nK∑\nk=1\nγ(znk) {lnπk +l nN(xn|µk, Σk)}. (9.40)\nWe can now proceed as follows. First we choose some initial values for the param-\neters µold, Σold and πold, and use these to evaluate the responsibilities (the E step).\nWe then keep the responsibilities ﬁxed and maximize (9.40) with respect to µk, Σk\nand πk (the M step). This leads to closed form solutions for µnew, Σnew and πnew\ngiven by (9.17), (9.19), and (9.22) as before. This is precisely the EM algorithm forExercise 9.8\nGaussian mixtures as derived earlier. We shall gain more insight into the role of the\nexpected complete-data log likelihood function when we give a proof of convergence",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 462,
      "page_label": "443"
    }
  },
  {
    "page_content": "expected complete-data log likelihood function when we give a proof of convergence\nof the EM algorithm in Section 9.4.\n9.3.2 Relation to K-means\nComparison of the K-means algorithm with the EM algorithm for Gaussian\nmixtures shows that there is a close similarity. Whereas the K-means algorithm\nperforms a hard assignment of data points to clusters, in which each data point is\nassociated uniquely with one cluster, the EM algorithm makes a soft assignment\nbased on the posterior probabilities. In fact, we can derive the K-means algorithm\nas a particular limit of EM for Gaussian mixtures as follows.\nConsider a Gaussian mixture model in which the covariance matrices of the\nmixture components are given by ϵI, where ϵ is a variance parameter that is shared",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 462,
      "page_label": "443"
    }
  },
  {
    "page_content": "444 9. MIXTURE MODELS AND EM\nby all of the components, and I is the identity matrix, so that\np(x|µk, Σk)= 1\n(2πϵ)1/2 exp\n{\n− 1\n2ϵ∥x − µk∥2\n}\n. (9.41)\nWe now consider the EM algorithm for a mixture of K Gaussians of this form in\nwhich we treat ϵ as a ﬁxed constant, instead of a parameter to be re-estimated. From\n(9.13) the posterior probabilities, or responsibilities, for a particular data point xn,\nare given by\nγ(znk)= πk exp{−∥xn − µk∥2/2ϵ}∑\nj πj exp\n{\n−∥xn − µj∥2/2ϵ\n}. (9.42)\nIf we consider the limit ϵ → 0, we see that in the denominator the term for which\n∥xn − µj∥2 is smallest will go to zero most slowly, and hence the responsibilities\nγ(znk) for the data point xn all go to zero except for term j, for which the responsi-\nbility γ(znj) will go to unity. Note that this holds independently of the values of the\nπk so long as none of the πk is zero. Thus, in this limit, we obtain a hard assignment\nof data points to clusters, just as in the K-means algorithm, so that γ(znk) → rnk\nwhere rnk is deﬁned by (9.2). Each data point is thereby assigned to the cluster\nhaving the closest mean.\nThe EM re-estimation equation for the µk, given by (9.17), then reduces to the\nK-means result (9.4). Note that the re-estimation formula for the mixing coefﬁcients\n(9.22) simply re-sets the value ofπk to be equal to the fraction of data points assigned\nto cluster k, although these parameters no longer play an active role in the algorithm.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 463,
      "page_label": "444"
    }
  },
  {
    "page_content": "to cluster k, although these parameters no longer play an active role in the algorithm.\nFinally, in the limit ϵ → 0 the expected complete-data log likelihood, given by\n(9.40), becomesExercise 9.11\nEZ[lnp(X, Z|µ, Σ, π)] →− 1\n2\nN∑\nn=1\nK∑\nk=1\nrnk∥xn − µk∥2 +c o n s t. (9.43)\nThus we see that in this limit, maximizing the expected complete-data log likelihood\nis equivalent to minimizing the distortion measure J for the K-means algorithm\ngiven by (9.1).\nNote that the K-means algorithm does not estimate the covariances of the clus-\nters but only the cluster means. A hard-assignment version of the Gaussian mixture\nmodel with general covariance matrices, known as theelliptical K-means algorithm,\nhas been considered by Sung and Poggio (1994).\n9.3.3 Mixtures of Bernoulli distributions\nSo far in this chapter, we have focussed on distributions over continuous vari-\nables described by mixtures of Gaussians. As a further example of mixture mod-\nelling, and to illustrate the EM algorithm in a different context, we now discuss mix-\ntures of discrete binary variables described by Bernoulli distributions. This model\nis also known as latent class analysis(Lazarsfeld and Henry, 1968; McLachlan and\nPeel, 2000). As well as being of practical importance in its own right, our discus-\nsion of Bernoulli mixtures will also lay the foundation for a consideration of hidden\nMarkov models over discrete variables.Section 13.2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 463,
      "page_label": "444"
    }
  },
  {
    "page_content": "9.3. An Alternative View of EM 445\nConsider a set of D binary variables xi, where i =1 ,...,D , each of which is\ngoverned by a Bernoulli distribution with parameter µi, so that\np(x|µ)=\nD∏\ni=1\nµxi\ni (1 − µi)(1−xi) (9.44)\nwhere x =( x1,...,x D)T and µ =( µ1,...,µ D)T. We see that the individual\nvariables xi are independent, given µ. The mean and covariance of this distribution\nare easily seen to be\nE[x]= µ (9.45)\ncov[x]=d i a g {µi(1 − µi)}. (9.46)\nNow let us consider a ﬁnite mixture of these distributions given by\np(x|µ, π)=\nK∑\nk=1\nπkp(x|µk) (9.47)\nwhere µ = {µ1,..., µK}, π = {π1,...,π K}, and\np(x|µk)=\nD∏\ni=1\nµxi\nki(1 − µki)(1−xi). (9.48)\nThe mean and covariance of this mixture distribution are given byExercise 9.12\nE[x]=\nK∑\nk=1\nπkµk (9.49)\ncov[x]=\nK∑\nk=1\nπk\n{\nΣk + µkµT\nk\n}\n− E[x]E[x]T (9.50)\nwhere Σk =d i a g{µki(1 − µki)}. Because the covariance matrix cov[x] is no\nlonger diagonal, the mixture distribution can capture correlations between the vari-\nables, unlike a single Bernoulli distribution.\nIf we are given a data set X = {x1,..., xN } then the log likelihood function\nfor this model is given by\nln p(X|µ, π)=\nN∑\nn=1\nln\n{ K∑\nk=1\nπkp(xn|µk)\n}\n. (9.51)\nAgain we see the appearance of the summation inside the logarithm, so that the\nmaximum likelihood solution no longer has closed form.\nWe now derive the EM algorithm for maximizing the likelihood function for\nthe mixture of Bernoulli distributions. To do this, we ﬁrst introduce an explicit latent",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 464,
      "page_label": "445"
    }
  },
  {
    "page_content": "446 9. MIXTURE MODELS AND EM\nvariable z associated with each instance ofx. As in the case of the Gaussian mixture,\nz =( z1,...,z K)T is a binary K-dimensional variable having a single component\nequal to 1, with all other components equal to 0. We can then write the conditional\ndistribution of x, given the latent variable, as\np(x|z, µ)=\nK∏\nk=1\np(x|µk)zk (9.52)\nwhile the prior distribution for the latent variables is the same as for the mixture of\nGaussians model, so that\np(z|π)=\nK∏\nk=1\nπzk\nk . (9.53)\nIf we form the product ofp(x|z, µ) and p(z|π) and then marginalize overz, then we\nrecover (9.47).Exercise 9.14\nIn order to derive the EM algorithm, we ﬁrst write down the complete-data log\nlikelihood function, which is given by\nlnp(X, Z|µ, π)=\nN∑\nn=1\nK∑\nk=1\nznk\n{\nln πk\n+\nD∑\ni=1\n[xni ln µki +( 1− xni)l n ( 1− µki)]\n}\n(9.54)\nwhere X = {xn} and Z = {zn}. Next we take the expectation of the complete-data\nlog likelihood with respect to the posterior distribution of the latent variables to give\nEZ[ln p(X, Z|µ, π)] =\nN∑\nn=1\nK∑\nk=1\nγ(znk)\n{\nln πk\n+\nD∑\ni=1\n[xni ln µki +( 1− xni)l n ( 1− µki)]\n}\n(9.55)\nwhere γ(znk)= E[znk] is the posterior probability, or responsibility, of component\nk given data pointxn. In the E step, these responsibilities are evaluated using Bayes’\ntheorem, which takes the form\nγ(znk)= E[znk]=\n∑\nznk\nznk [πkp(xn|µk)]znk\n∑\nznj\n[\nπjp(xn|µj)\n]znj\n= πkp(xn|µk)\nK∑\nj=1\nπjp(xn|µj)\n. (9.56)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 465,
      "page_label": "446"
    }
  },
  {
    "page_content": "9.3. An Alternative View of EM 447\nIf we consider the sum over n in (9.55), we see that the responsibilities enter\nonly through two terms, which can be written as\nNk =\nN∑\nn=1\nγ(znk) (9.57)\nxk = 1\nNk\nN∑\nn=1\nγ(znk)xn (9.58)\nwhere Nk is the effective number of data points associated with componentk. In the\nM step, we maximize the expected complete-data log likelihood with respect to the\nparameters µk and π. If we set the derivative of (9.55) with respect to µk equal to\nzero and rearrange the terms, we obtainExercise 9.15\nµk = xk. (9.59)\nWe see that this sets the mean of component k equal to a weighted mean of the\ndata, with weighting coefﬁcients given by the responsibilities that componentk takes\nfor data points. For the maximization with respect to πk, we need to introduce a\nLagrange multiplier to enforce the constraint ∑\nk πk =1 . Following analogous\nsteps to those used for the mixture of Gaussians, we then obtainExercise 9.16\nπk = Nk\nN (9.60)\nwhich represents the intuitively reasonable result that the mixing coefﬁcient for com-\nponent k is given by the effective fraction of points in the data set explained by that\ncomponent.\nNote that in contrast to the mixture of Gaussians, there are no singularities in\nwhich the likelihood function goes to inﬁnity. This can be seen by noting that the\nlikelihood function is bounded above because 0 ⩽ p(xn|µk) ⩽ 1. There existExercise 9.17\nsingularities at which the likelihood function goes to zero, but these will not be",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 466,
      "page_label": "447"
    }
  },
  {
    "page_content": "singularities at which the likelihood function goes to zero, but these will not be\nfound by EM provided it is not initialized to a pathological starting point, because\nthe EM algorithm always increases the value of the likelihood function, until a local\nmaximum is found. We illustrate the Bernoulli mixture model in Figure 9.10 bySection 9.4\nusing it to model handwritten digits. Here the digit images have been turned into\nbinary vectors by setting all elements whose values exceed0.5 to 1 and setting the\nremaining elements to 0. We now ﬁt a data set of N = 600 such digits, comprising\nthe digits ‘2’, ‘3’, and ‘4’, with a mixture of K =3 Bernoulli distributions by\nrunning 10 iterations of the EM algorithm. The mixing coefﬁcients were initialized\nto πk =1 /K, and the parametersµkj were set to random values chosen uniformly in\nthe range (0.25,0.75) and then normalized to satisfy the constraint that∑\nj µkj =1 .\nWe see that a mixture of 3 Bernoulli distributions is able to ﬁnd the three clusters in\nthe data set corresponding to the different digits.\nThe conjugate prior for the parameters of a Bernoulli distribution is given by\nthe beta distribution, and we have seen that a beta prior is equivalent to introducing",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 466,
      "page_label": "447"
    }
  },
  {
    "page_content": "448 9. MIXTURE MODELS AND EM\nFigure 9.10 Illustration of the Bernoulli mixture model in which the top row shows examples from the digits data\nset after converting the pixel values from grey scale to binary using a threshold of0.5. On the bottom row the ﬁrst\nthree images show the parametersµki for each of the three components in the mixture model. As a comparison,\nwe also ﬁt the same data set using a single multivariate Bernoulli distribution, again using maximum likelihood.\nThis amounts to simply averaging the counts in each pixel and is shown by the right-most image on the bottom\nrow.\nadditional effective observations of x. We can similarly introduce priors into theSection 2.1.1\nBernoulli mixture model, and use EM to maximize the posterior probability distri-\nbutions.Exercise 9.18\nIt is straightforward to extend the analysis of Bernoulli mixtures to the case of\nmultinomial binary variables havingM> 2 states by making use of the discrete dis-Exercise 9.19\ntribution (2.26). Again, we can introduce Dirichlet priors over the model parameters\nif desired.\n9.3.4 EM for Bayesian linear regression\nAs a third example of the application of EM, we return to the evidence ap-\nproximation for Bayesian linear regression. In Section 3.5.2, we obtained the re-\nestimation equations for the hyperparametersα and β by evaluation of the evidence\nand then setting the derivatives of the resulting expression to zero. We now turn to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 467,
      "page_label": "448"
    }
  },
  {
    "page_content": "and then setting the derivatives of the resulting expression to zero. We now turn to\nan alternative approach for ﬁnding α and β based on the EM algorithm. Recall that\nour goal is to maximize the evidence function p(t|α, β) given by (3.77) with respect\nto α and β. Because the parameter vector w is marginalized out, we can regard it as\na latent variable, and hence we can optimize this marginal likelihood function using\nEM. In the E step, we compute the posterior distribution ofw given the current set-\nting of the parameters α and β and then use this to ﬁnd the expected complete-data\nlog likelihood. In the M step, we maximize this quantity with respect toα and β.W e\nhave already derived the posterior distribution of w because this is given by (3.49).\nThe complete-data log likelihood function is then given by\nln p(t, w|α, β)=l n p(t|w,β )+l n p(w|α) (9.61)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 467,
      "page_label": "448"
    }
  },
  {
    "page_content": "9.3. An Alternative View of EM 449\nwhere the likelihood p(t|w,β ) and the prior p(w|α) are given by (3.10) and (3.52),\nrespectively, and y(x,w) is given by (3.3). Taking the expectation with respect to\nthe posterior distribution of w then gives\nE [lnp(t, w|α, β)] = M\n2 ln\n( α\n2π\n)\n− α\n2 E\n[\nwTw\n]\n+ N\n2 ln\n( β\n2π\n)\n−β\n2\nN∑\nn=1\nE\n[\n(tn − wTφn)2]\n. (9.62)\nSetting the derivatives with respect to α to zero, we obtain the M step re-estimation\nequationExercise 9.20\nα = M\nE [wTw] = M\nmT\nN mN + Tr(SN ). (9.63)\nAn analogous result holds for β.Exercise 9.21\nNote that this re-estimation equation takes a slightly different form from the\ncorresponding result (3.92) derived by direct evaluation of the evidence function.\nHowever, they each involve computation and inversion (or eigen decomposition) of\nan M × M matrix and hence will have comparable computational cost per iteration.\nThese two approaches to determining α should of course converge to the same\nresult (assuming they ﬁnd the same local maximum of the evidence function). This\ncan be veriﬁed by ﬁrst noting that the quantity γ is deﬁned by\nγ = M − α\nM∑\ni=1\n1\nλi + α = M − αTr(SN ). (9.64)\nAt a stationary point of the evidence function, the re-estimation equation (3.92) will\nbe self-consistently satisﬁed, and hence we can substitute for γ to give\nαmT\nN mN = γ = M − αTr(SN ) (9.65)\nand solving forα we obtain (9.63), which is precisely the EM re-estimation equation.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 468,
      "page_label": "449"
    }
  },
  {
    "page_content": "αmT\nN mN = γ = M − αTr(SN ) (9.65)\nand solving forα we obtain (9.63), which is precisely the EM re-estimation equation.\nAs a ﬁnal example, we consider a closely related model, namely the relevance\nvector machine for regression discussed in Section 7.2.1. There we used direct max-\nimization of the marginal likelihood to derive re-estimation equations for the hyper-\nparametersα and β. Here we consider an alternative approach in which we view the\nweight vector w as a latent variable and apply the EM algorithm. The E step involves\nﬁnding the posterior distribution over the weights, and this is given by (7.81). In the\nM step we maximize the expected complete-data log likelihood, which is deﬁned by\nEw [lnp(t|X, w,β )p(w|α)] (9.66)\nwhere the expectation is taken with respect to the posterior distribution computed\nusing the ‘old’ parameter values. To compute the new parameter values we maximize\nwith respect to α and β to giveExercise 9.22",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 468,
      "page_label": "449"
    }
  },
  {
    "page_content": "450 9. MIXTURE MODELS AND EM\nαnew\ni = 1\nm2\ni +Σ ii\n(9.67)\n(βnew)−1 = ∥t − ΦmN ∥2 + β−1 ∑\ni γi\nN (9.68)\nThese re-estimation equations are formally equivalent to those obtained by direct\nmaxmization.Exercise 9.23\n9.4. The EM Algorithm in General\nThe expectation maximizationalgorithm, or EM algorithm, is a general technique for\nﬁnding maximum likelihood solutions for probabilistic models having latent vari-\nables (Dempster et al., 1977; McLachlan and Krishnan, 1997). Here we give a very\ngeneral treatment of the EM algorithm and in the process provide a proof that the\nEM algorithm derived heuristically in Sections 9.2 and 9.3 for Gaussian mixtures\ndoes indeed maximize the likelihood function (Csisz `ar and Tusn `ady, 1984; Hath-\naway, 1986; Neal and Hinton, 1999). Our discussion will also form the basis for the\nderivation of the variational inference framework.Section 10.1\nConsider a probabilistic model in which we collectively denote all of the ob-\nserved variables by X and all of the hidden variables by Z. The joint distribution\np(X, Z|θ) is governed by a set of parameters denoted θ. Our goal is to maximize\nthe likelihood function that is given by\np(X|θ)=\n∑\nZ\np(X, Z|θ). (9.69)\nHere we are assuming Z is discrete, although the discussion is identical if Z com-\nprises continuous variables or a combination of discrete and continuous variables,\nwith summation replaced by integration as appropriate.\nWe shall suppose that direct optimization of p(X|θ) is difﬁcult, but that opti-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 469,
      "page_label": "450"
    }
  },
  {
    "page_content": "with summation replaced by integration as appropriate.\nWe shall suppose that direct optimization of p(X|θ) is difﬁcult, but that opti-\nmization of the complete-data likelihood function p(X, Z|θ) is signiﬁcantly easier.\nNext we introduce a distribution q(Z) deﬁned over the latent variables, and we ob-\nserve that, for any choice of q(Z), the following decomposition holds\nln p(X|θ)= L(q, θ)+K L (q∥p) (9.70)\nwhere we have deﬁned\nL(q, θ)=\n∑\nZ\nq(Z)l n\n{p(X, Z|θ)\nq(Z)\n}\n(9.71)\nKL(q∥p)= −\n∑\nZ\nq(Z)l n\n{p(Z|X, θ)\nq(Z)\n}\n. (9.72)\nNote that L(q, θ) is a functional (see Appendix D for a discussion of functionals)\nof the distribution q(Z), and a function of the parameters θ. It is worth studying",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 469,
      "page_label": "450"
    }
  },
  {
    "page_content": "9.4. The EM Algorithm in General 451\nFigure 9.11 Illustration of the decomposition given\nby (9.70), which holds for any choice\nof distribution q(Z). Because the\nKullback-Leibler divergence satisﬁes\nKL(q∥p) ⩾ 0, we see that the quan-\ntity L(q, θ) is a lower bound on the log\nlikelihood function ln p(X|θ).\nlnp(X|θ)L(q, θ)\nKL(q||p)\ncarefully the forms of the expressions (9.71) and (9.72), and in particular noting that\nthey differ in sign and also that L(q, θ) contains the joint distribution of X and Z\nwhile KL(q∥p) contains the conditional distribution of Z given X. To verify the\ndecomposition (9.70), we ﬁrst make use of the product rule of probability to giveExercise 9.24\nln p(X, Z|θ)=l n p(Z|X, θ)+l n p(X|θ) (9.73)\nwhich we then substitute into the expression forL(q, θ). This gives rise to two terms,\none of which cancels KL(q∥p) while the other gives the required log likelihood\nlnp(X|θ) after noting that q(Z) is a normalized distribution that sums to 1.\nFrom (9.72), we see that KL(q∥p) is the Kullback-Leibler divergence between\nq(Z) and the posterior distribution p(Z|X, θ). Recall that the Kullback-Leibler di-\nvergence satisﬁes KL(q∥p) ⩾ 0, with equality if, and only if, q(Z)= p(Z|X, θ).I tSection 1.6.1\ntherefore follows from (9.70) that L(q, θ) ⩽ lnp(X|θ), in other words that L(q, θ)\nis a lower bound on lnp(X|θ). The decomposition (9.70) is illustrated in Fig-\nure 9.11.\nThe EM algorithm is a two-stage iterative optimization technique for ﬁnding",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 470,
      "page_label": "451"
    }
  },
  {
    "page_content": "ure 9.11.\nThe EM algorithm is a two-stage iterative optimization technique for ﬁnding\nmaximum likelihood solutions. We can use the decomposition (9.70) to deﬁne the\nEM algorithm and to demonstrate that it does indeed maximize the log likelihood.\nSuppose that the current value of the parameter vector is θold. In the E step, the\nlower bound L(q, θold) is maximized with respect to q(Z) while holding θold ﬁxed.\nThe solution to this maximization problem is easily seen by noting that the value\nof lnp(X|θold) does not depend on q(Z) and so the largest value of L(q, θold) will\noccur when the Kullback-Leibler divergence vanishes, in other words when q(Z) is\nequal to the posterior distribution p(Z|X, θold). In this case, the lower bound will\nequal the log likelihood, as illustrated in Figure 9.12.\nIn the subsequent M step, the distributionq(Z) is held ﬁxed and the lower bound\nL(q, θ) is maximized with respect to θ to give some new value θnew. This will\ncause the lower bound L to increase (unless it is already at a maximum), which will\nnecessarily cause the corresponding log likelihood function to increase. Because the\ndistribution q is determined using the old parameter values rather than the new values\nand is held ﬁxed during the M step, it will not equal the new posterior distribution\np(Z|X, θnew), and hence there will be a nonzero KL divergence. The increase in the\nlog likelihood function is therefore greater than the increase in the lower bound, as",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 470,
      "page_label": "451"
    }
  },
  {
    "page_content": "452 9. MIXTURE MODELS AND EM\nFigure 9.12 Illustration of the E step of\nthe EM algorithm. The q\ndistribution is set equal to\nthe posterior distribution for\nthe current parameter val-\nues θold, causing the lower\nbound to move up to the\nsame value as the log like-\nlihood function, with the KL\ndivergence vanishing. lnp(X|θold)L(q, θold)\nKL(q||p)=0\nshown in Figure 9.13. If we substitute q(Z)= p(Z|X, θold) into (9.71), we see that,\nafter the E step, the lower bound takes the form\nL(q, θ)=\n∑\nZ\np(Z|X, θold)l np(X, Z|θ) −\n∑\nZ\np(Z|X, θold)l np(Z|X, θold)\n= Q(θ, θold)+c o n s t (9.74)\nwhere the constant is simply the negative entropy of the q distribution and is there-\nfore independent ofθ. Thus in the M step, the quantity that is being maximized is the\nexpectation of the complete-data log likelihood, as we saw earlier in the case of mix-\ntures of Gaussians. Note that the variable θ over which we are optimizing appears\nonly inside the logarithm. If the joint distribution p(Z, X|θ) comprises a member of\nthe exponential family, or a product of such members, then we see that the logarithm\nwill cancel the exponential and lead to an M step that will be typically much simpler\nthan the maximization of the corresponding incomplete-data log likelihood function\np(X|θ).\nThe operation of the EM algorithm can also be viewed in the space of parame-\nters, as illustrated schematically in Figure 9.14. Here the red curve depicts the (in-\nFigure 9.13 Illustration of the M step of the EM",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 471,
      "page_label": "452"
    }
  },
  {
    "page_content": "ters, as illustrated schematically in Figure 9.14. Here the red curve depicts the (in-\nFigure 9.13 Illustration of the M step of the EM\nalgorithm. The distribution q(Z)\nis held ﬁxed and the lower bound\nL(q, θ) is maximized with respect\nto the parameter vector θ to give\na revised value θnew. Because the\nKL divergence is nonnegative, this\ncauses the log likelihood ln p(X|θ)\nto increase by at least as much as\nthe lower bound does.\nlnp(X|θnew)L(q, θnew)\nKL(q||p)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 471,
      "page_label": "452"
    }
  },
  {
    "page_content": "9.4. The EM Algorithm in General 453\nFigure 9.14 The EM algorithm involves alter-\nnately computing a lower bound\non the log likelihood for the cur-\nrent parameter values and then\nmaximizing this bound to obtain\nthe new parameter values. See\nthe text for a full discussion.\nθold θnew\nL (q,θ )\nlnp(X|θ)\ncomplete data) log likelihood function whose value we wish to maximize. We start\nwith some initial parameter value θold, and in the ﬁrst E step we evaluate the poste-\nrior distribution over latent variables, which gives rise to a lower bound L(θ, θ(old))\nwhose value equals the log likelihood atθ(old), as shown by the blue curve. Note that\nthe bound makes a tangential contact with the log likelihood at θ(old), so that both\ncurves have the same gradient. This bound is a convex function having a uniqueExercise 9.25\nmaximum (for mixture components from the exponential family). In the M step, the\nbound is maximized giving the value θ(new), which gives a larger value of log likeli-\nhood than θ(old). The subsequent E step then constructs a bound that is tangential at\nθ(new) as shown by the green curve.\nFor the particular case of an independent, identically distributed data set, X\nwill comprise N data points {xn} while Z will comprise N corresponding latent\nvariables {zn}, where n =1 ,...,N . From the independence assumption, we have\np(X, Z)= ∏\nn p(xn, zn) and, by marginalizing over the {zn} we have p(X)=∏\nn p(xn). Using the sum and product rules, we see that the posterior probability",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 472,
      "page_label": "453"
    }
  },
  {
    "page_content": "n p(xn, zn) and, by marginalizing over the {zn} we have p(X)=∏\nn p(xn). Using the sum and product rules, we see that the posterior probability\nthat is evaluated in the E step takes the form\np(Z|X, θ)= p(X, Z|θ)∑\nZ\np(X, Z|θ)\n=\nN∏\nn=1\np(xn, zn|θ)\n∑\nZ\nN∏\nn=1\np(xn, zn|θ)\n=\nN∏\nn=1\np(zn|xn, θ) (9.75)\nand so the posterior distribution also factorizes with respect to n. In the case of\nthe Gaussian mixture model this simply says that the responsibility that each of the\nmixture components takes for a particular data point xn depends only on the value\nof xn and on the parameters θ of the mixture components, not on the values of the\nother data points.\nWe have seen that both the E and the M steps of the EM algorithm are increas-\ning the value of a well-deﬁned bound on the log likelihood function and that the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 472,
      "page_label": "453"
    }
  },
  {
    "page_content": "454 9. MIXTURE MODELS AND EM\ncomplete EM cycle will change the model parameters in such a way as to cause\nthe log likelihood to increase (unless it is already at a maximum, in which case the\nparameters remain unchanged).\nWe can also use the EM algorithm to maximize the posterior distributionp(θ|X)\nfor models in which we have introduced a priorp(θ) over the parameters. To see this,\nwe note that as a function of θ,w eh a v ep(θ|X)= p(θ, X)/p(X) and so\nlnp(θ|X)=l n p(θ, X) − ln p(X). (9.76)\nMaking use of the decomposition (9.70), we have\nln p(θ|X)= L(q, θ)+K L (q∥p)+l n p(θ) − lnp(X)\n⩾ L(q, θ)+l n p(θ) − ln p(X). (9.77)\nwhere ln p(X) is a constant. We can again optimize the right-hand side alternately\nwith respect to q and θ. The optimization with respect to q gives rise to the same E-\nstep equations as for the standard EM algorithm, because q only appears in L(q, θ).\nThe M-step equations are modiﬁed through the introduction of the prior termlnp(θ),\nwhich typically requires only a small modiﬁcation to the standard maximum likeli-\nhood M-step equations.\nThe EM algorithm breaks down the potentially difﬁcult problem of maximizing\nthe likelihood function into two stages, the E step and the M step, each of which will\noften prove simpler to implement. Nevertheless, for complex models it may be the\ncase that either the E step or the M step, or indeed both, remain intractable. This\nleads to two possible extensions of the EM algorithm, as follows.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 473,
      "page_label": "454"
    }
  },
  {
    "page_content": "case that either the E step or the M step, or indeed both, remain intractable. This\nleads to two possible extensions of the EM algorithm, as follows.\nThe generalized EM,o r GEM, algorithm addresses the problem of an intractable\nM step. Instead of aiming to maximize L(q, θ) with respect to θ, it seeks instead\nto change the parameters in such a way as to increase its value. Again, because\nL(q, θ) is a lower bound on the log likelihood function, each complete EM cycle of\nthe GEM algorithm is guaranteed to increase the value of the log likelihood (unless\nthe parameters already correspond to a local maximum). One way to exploit the\nGEM approach would be to use one of the nonlinear optimization strategies, such\nas the conjugate gradients algorithm, during the M step. Another form of GEM\nalgorithm, known as the expectation conditional maximization, or ECM, algorithm,\ninvolves making several constrained optimizations within each M step (Meng and\nRubin, 1993). For instance, the parameters might be partitioned into groups, and the\nM step is broken down into multiple steps each of which involves optimizing one of\nthe subset with the remainder held ﬁxed.\nWe can similarly generalize the E step of the EM algorithm by performing a\npartial, rather than complete, optimization of L(q, θ) with respect to q(Z) (Neal and\nHinton, 1999). As we have seen, for any given value ofθ there is a unique maximum\nof L(q, θ) with respect toq(Z) that corresponds to the posterior distributionqθ(Z)=",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 473,
      "page_label": "454"
    }
  },
  {
    "page_content": "of L(q, θ) with respect toq(Z) that corresponds to the posterior distributionqθ(Z)=\np(Z|X, θ) and that for this choice of q(Z) the bound L(q, θ) is equal to the log\nlikelihood function lnp(X|θ). It follows that any algorithm that converges to the\nglobal maximum of L(q, θ) will ﬁnd a value of θ that is also a global maximum\nof the log likelihood lnp(X|θ). Provided p(X, Z|θ) is a continuous function of θ",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 473,
      "page_label": "454"
    }
  },
  {
    "page_content": "Exercises 455\nthen, by continuity, any local maximum of L(q, θ) will also be a local maximum of\nlnp(X|θ).\nConsider the case of N independent data points x1,..., xN with corresponding\nlatent variables z1,..., zN . The joint distribution p(X, Z|θ) factorizes over the data\npoints, and this structure can be exploited in an incremental form of EM in which\nat each EM cycle only one data point is processed at a time. In the E step, instead\nof recomputing the responsibilities for all of the data points, we just re-evaluate the\nresponsibilities for one data point. It might appear that the subsequent M step would\nrequire computation involving the responsibilities for all of the data points. How-\never, if the mixture components are members of the exponential family, then the\nresponsibilities enter only through simple sufﬁcient statistics, and these can be up-\ndated efﬁciently. Consider, for instance, the case of a Gaussian mixture, and suppose\nwe perform an update for data point m in which the corresponding old and new\nvalues of the responsibilities are denoted γold(zmk) and γnew(zmk). In the M step,\nthe required sufﬁcient statistics can be updated incrementally. For instance, for the\nmeans the sufﬁcient statistics are deﬁned by (9.17) and (9.18) from which we obtainExercise 9.26\nµnew\nk = µold\nk +\n( γnew(zmk) − γold(zmk)\nNnew\nk\n) (\nxm − µold\nk\n)\n(9.78)\ntogether with\nNnew\nk = Nold\nk + γnew(zmk) − γold(zmk). (9.79)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 474,
      "page_label": "455"
    }
  },
  {
    "page_content": "µnew\nk = µold\nk +\n( γnew(zmk) − γold(zmk)\nNnew\nk\n) (\nxm − µold\nk\n)\n(9.78)\ntogether with\nNnew\nk = Nold\nk + γnew(zmk) − γold(zmk). (9.79)\nThe corresponding results for the covariances and the mixing coefﬁcients are analo-\ngous.\nThus both the E step and the M step take ﬁxed time that is independent of the\ntotal number of data points. Because the parameters are revised after each data point,\nrather than waiting until after the whole data set is processed, this incremental ver-\nsion can converge faster than the batch version. Each E or M step in this incremental\nalgorithm is increasing the value of L(q, θ) and, as we have shown above, if the\nalgorithm converges to a local (or global) maximum of L(q, θ), this will correspond\nto a local (or global) maximum of the log likelihood function lnp(X|θ).\nExercises\n9.1 (⋆) www Consider the K-means algorithm discussed in Section 9.1. Show that as\na consequence of there being a ﬁnite number of possible assignments for the set of\ndiscrete indicator variables rnk, and that for each such assignment there is a unique\noptimum for the {µk}, the K-means algorithm must converge after a ﬁnite number\nof iterations.\n9.2 (⋆) Apply the Robbins-Monro sequential estimation procedure described in Sec-\ntion 2.3.5 to the problem of ﬁnding the roots of the regression function given by\nthe derivatives ofJ in (9.1) with respect to µk. Show that this leads to a stochastic\nK-means algorithm in which, for each data point xn, the nearest prototype µk is",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 474,
      "page_label": "455"
    }
  },
  {
    "page_content": "K-means algorithm in which, for each data point xn, the nearest prototype µk is\nupdated using (9.5).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 474,
      "page_label": "455"
    }
  },
  {
    "page_content": "456 9. MIXTURE MODELS AND EM\n9.3 (⋆) www Consider a Gaussian mixture model in which the marginal distribution\np(z) for the latent variable is given by (9.10), and the conditional distributionp(x|z)\nfor the observed variable is given by (9.11). Show that the marginal distribution\np(x), obtained by summing p(z)p(x|z) over all possible values of z, is a Gaussian\nmixture of the form (9.7).\n9.4 (⋆) Suppose we wish to use the EM algorithm to maximize the posterior distri-\nbution over parameters p(θ|X) for a model containing latent variables, where X is\nthe observed data set. Show that the E step remains the same as in the maximum\nlikelihood case, whereas in the M step the quantity to be maximized is given by\nQ(θ, θold)+l n p(θ) where Q(θ, θold) is deﬁned by (9.30).\n9.5 (⋆) Consider the directed graph for a Gaussian mixture model shown in Figure 9.6.\nBy making use of the d-separation criterion discussed in Section 8.2, show that the\nposterior distribution of the latent variables factorizes with respect to the different\ndata points so that\np(Z|X, µ, Σ, π)=\nN∏\nn=1\np(zn|xn, µ, Σ, π). (9.80)\n9.6 (⋆⋆ ) Consider a special case of a Gaussian mixture model in which the covari-\nance matrices Σk of the components are all constrained to have a common value\nΣ. Derive the EM equations for maximizing the likelihood function under such a\nmodel.\n9.7 (⋆) www Verify that maximization of the complete-data log likelihood (9.36) for",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 475,
      "page_label": "456"
    }
  },
  {
    "page_content": "model.\n9.7 (⋆) www Verify that maximization of the complete-data log likelihood (9.36) for\na Gaussian mixture model leads to the result that the means and covariances of each\ncomponent are ﬁtted independently to the corresponding group of data points, and\nthe mixing coefﬁcients are given by the fractions of points in each group.\n9.8 (⋆) www Show that if we maximize (9.40) with respect to µk while keeping the\nresponsibilities γ(znk) ﬁxed, we obtain the closed form solution given by (9.17).\n9.9 (⋆) Show that if we maximize (9.40) with respect to Σk and πk while keeping the\nresponsibilities γ(znk) ﬁxed, we obtain the closed form solutions given by (9.19)\nand (9.22).\n9.10 (⋆⋆ ) Consider a density model given by a mixture distribution\np(x)=\nK∑\nk=1\nπkp(x|k) (9.81)\nand suppose that we partition the vector x into two parts so that x =( xa, xb).\nShow that the conditional density p(xb|xa) is itself a mixture distribution and ﬁnd\nexpressions for the mixing coefﬁcients and for the component densities.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 475,
      "page_label": "456"
    }
  },
  {
    "page_content": "Exercises 457\n9.11 (⋆) In Section 9.3.2, we obtained a relationship between K means and EM for\nGaussian mixtures by considering a mixture model in which all components have\ncovarianceϵI. Show that in the limit ϵ → 0, maximizing the expected complete-\ndata log likelihood for this model, given by (9.40), is equivalent to minimizing the\ndistortion measure J for the K-means algorithm given by (9.1).\n9.12 (⋆) www Consider a mixture distribution of the form\np(x)=\nK∑\nk=1\nπkp(x|k) (9.82)\nwhere the elements of x could be discrete or continuous or a combination of these.\nDenote the mean and covariance of p(x|k) by µk and Σk, respectively. Show that\nthe mean and covariance of the mixture distribution are given by (9.49) and (9.50).\n9.13 (⋆⋆ ) Using the re-estimation equations for the EM algorithm, show that a mix-\nture of Bernoulli distributions, with its parameters set to values corresponding to a\nmaximum of the likelihood function, has the property that\nE[x]= 1\nN\nN∑\nn=1\nxn ≡ x. (9.83)\nHence show that if the parameters of this model are initialized such that all compo-\nnents have the same mean µk = ˆµ for k =1 ,...,K , then the EM algorithm will\nconverge after one iteration, for any choice of the initial mixing coefﬁcients, and that\nthis solution has the property µk = x. Note that this represents a degenerate case of\nthe mixture model in which all of the components are identical, and in practice we\ntry to avoid such solutions by using an appropriate initialization.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 476,
      "page_label": "457"
    }
  },
  {
    "page_content": "try to avoid such solutions by using an appropriate initialization.\n9.14 (⋆) Consider the joint distribution of latent and observed variables for the Bernoulli\ndistribution obtained by forming the product ofp(x|z, µ) given by (9.52) andp(z|π)\ngiven by (9.53). Show that if we marginalize this joint distribution with respect toz,\nthen we obtain (9.47).\n9.15 (⋆) www Show that if we maximize the expected complete-data log likelihood\nfunction (9.55) for a mixture of Bernoulli distributions with respect toµk, we obtain\nthe M step equation (9.59).\n9.16 (⋆) Show that if we maximize the expected complete-data log likelihood function\n(9.55) for a mixture of Bernoulli distributions with respect to the mixing coefﬁcients\nπk, using a Lagrange multiplier to enforce the summation constraint, we obtain the\nM step equation (9.60).\n9.17 (⋆) www Show that as a consequence of the constraint 0 ⩽ p(xn|µk) ⩽ 1 for\nthe discrete variable xn, the incomplete-data log likelihood function for a mixture\nof Bernoulli distributions is bounded above, and hence that there are no singularities\nfor which the likelihood goes to inﬁnity.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 476,
      "page_label": "457"
    }
  },
  {
    "page_content": "458 9. MIXTURE MODELS AND EM\n9.18 (⋆⋆ ) Consider a Bernoulli mixture model as discussed in Section 9.3.3, together\nwith a prior distribution p(µk|ak,b k) over each of the parameter vectors µk given\nby the beta distribution (2.13), and a Dirichlet prior p(π|α) given by (2.38). Derive\nthe EM algorithm for maximizing the posterior probability p(µ, π|X).\n9.19 (⋆⋆ ) Consider a D-dimensional variable x each of whose components i is itself a\nmultinomial variable of degree M so that x is a binary vector with components xij\nwhere i =1 ,...,D and j =1 ,...,M , subject to the constraint that ∑\nj xij =1 for\nall i. Suppose that the distribution of these variables is described by a mixture of the\ndiscrete multinomial distributions considered in Section 2.2 so that\np(x)=\nK∑\nk=1\nπkp(x|µk) (9.84)\nwhere\np(x|µk)=\nD∏\ni=1\nM∏\nj=1\nµxij\nkij. (9.85)\nThe parameters µkij represent the probabilities p(xij =1 |µk) and must satisfy\n0 ⩽ µkij ⩽ 1 together with the constraint ∑\nj µkij =1 for all values of k and i.\nGiven an observed data set {xn}, where n =1 ,...,N , derive the E and M step\nequations of the EM algorithm for optimizing the mixing coefﬁcients πk and the\ncomponent parameters µkij of this distribution by maximum likelihood.\n9.20 (⋆) www Show that maximization of the expected complete-data log likelihood\nfunction (9.62) for the Bayesian linear regression model leads to the M step re-\nestimation result (9.63) for α.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 477,
      "page_label": "458"
    }
  },
  {
    "page_content": "function (9.62) for the Bayesian linear regression model leads to the M step re-\nestimation result (9.63) for α.\n9.21 (⋆⋆ ) Using the evidence framework of Section 3.5, derive the M-step re-estimation\nequations for the parameter β in the Bayesian linear regression model, analogous to\nthe result (9.63) for α.\n9.22 (⋆⋆ ) By maximization of the expected complete-data log likelihood deﬁned by\n(9.66), derive the M step equations (9.67) and (9.68) for re-estimating the hyperpa-\nrameters of the relevance vector machine for regression.\n9.23 (⋆⋆ ) www In Section 7.2.1 we used direct maximization of the marginal like-\nlihood to derive the re-estimation equations (7.87) and (7.88) for ﬁnding values of\nthe hyperparameters α and β for the regression RVM. Similarly, in Section 9.3.4\nwe used the EM algorithm to maximize the same marginal likelihood, giving the\nre-estimation equations (9.67) and (9.68). Show that these two sets of re-estimation\nequations are formally equivalent.\n9.24 (⋆) Verify the relation (9.70) in which L(q, θ) and KL(q∥p) are deﬁned by (9.71)\nand (9.72), respectively.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 477,
      "page_label": "458"
    }
  },
  {
    "page_content": "Exercises 459\n9.25 (⋆) www Show that the lower bound L(q, θ) given by (9.71), with q(Z)=\np(Z|X, θ(old)), has the same gradient with respect toθ as the log likelihood function\nlnp(X|θ) at the point θ = θ(old).\n9.26 (⋆) www Consider the incremental form of the EM algorithm for a mixture of\nGaussians, in which the responsibilities are recomputed only for a speciﬁc data point\nxm. Starting from the M-step formulae (9.17) and (9.18), derive the results (9.78)\nand (9.79) for updating the component means.\n9.27 (⋆⋆ ) Derive M-step formulae for updating the covariance matrices and mixing\ncoefﬁcients in a Gaussian mixture model when the responsibilities are updated in-\ncrementally, analogous to the result (9.78) for updating the means.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 478,
      "page_label": "459"
    }
  },
  {
    "page_content": "10\nApproximate\nInference\nA central task in the application of probabilistic models is the evaluation of the pos-\nterior distribution p(Z|X) of the latent variables Z given the observed (visible) data\nvariables X, and the evaluation of expectations computed with respect to this dis-\ntribution. The model might also contain some deterministic parameters, which we\nwill leave implicit for the moment, or it may be a fully Bayesian model in which any\nunknown parameters are given prior distributions and are absorbed into the set of\nlatent variables denoted by the vector Z. For instance, in the EM algorithm we need\nto evaluate the expectation of the complete-data log likelihood with respect to the\nposterior distribution of the latent variables. For many models of practical interest, it\nwill be infeasible to evaluate the posterior distribution or indeed to compute expec-\ntations with respect to this distribution. This could be because the dimensionality of\nthe latent space is too high to work with directly or because the posterior distribution\nhas a highly complex form for which expectations are not analytically tractable. In\nthe case of continuous variables, the required integrations may not have closed-form\n461",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 480,
      "page_label": "461"
    }
  },
  {
    "page_content": "462 10. APPROXIMATE INFERENCE\nanalytical solutions, while the dimensionality of the space and the complexity of the\nintegrand may prohibit numerical integration. For discrete variables, the marginal-\nizations involve summing over all possible conﬁgurations of the hidden variables,\nand though this is always possible in principle, we often ﬁnd in practice that there\nmay be exponentially many hidden states so that exact calculation is prohibitively\nexpensive.\nIn such situations, we need to resort to approximation schemes, and these fall\nbroadly into two classes, according to whether they rely on stochastic or determin-\nistic approximations. Stochastic techniques such as Markov chain Monte Carlo, de-\nscribed in Chapter 11, have enabled the widespread use of Bayesian methods across\nmany domains. They generally have the property that given inﬁnite computational\nresource, they can generate exact results, and the approximation arises from the use\nof a ﬁnite amount of processor time. In practice, sampling methods can be compu-\ntationally demanding, often limiting their use to small-scale problems. Also, it can\nbe difﬁcult to know whether a sampling scheme is generating independent samples\nfrom the required distribution.\nIn this chapter, we introduce a range of deterministic approximation schemes,\nsome of which scale well to large applications. These are based on analytical ap-\nproximations to the posterior distribution, for example by assuming that it factorizes",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 481,
      "page_label": "462"
    }
  },
  {
    "page_content": "proximations to the posterior distribution, for example by assuming that it factorizes\nin a particular way or that it has a speciﬁc parametric form such as a Gaussian. As\nsuch, they can never generate exact results, and so their strengths and weaknesses\nare complementary to those of sampling methods.\nIn Section 4.4, we discussed the Laplace approximation, which is based on a\nlocal Gaussian approximation to a mode (i.e., a maximum) of the distribution. Here\nwe turn to a family of approximation techniques calledvariational inferenceor vari-\national Bayes, which use more global criteria and which have been widely applied.\nWe conclude with a brief introduction to an alternative variational framework known\nas expectation propagation.\n10.1. Variational Inference\nVariational methods have their origins in the 18 th century with the work of Euler,\nLagrange, and others on the calculus of variations. Standard calculus is concerned\nwith ﬁnding derivatives of functions. We can think of a function as a mapping that\ntakes the value of a variable as the input and returns the value of the function as the\noutput. The derivative of the function then describes how the output value varies\nas we make inﬁnitesimal changes to the input value. Similarly, we can deﬁne a\nfunctional as a mapping that takes a function as the input and that returns the value\nof the functional as the output. An example would be the entropy H[p], which takes",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 481,
      "page_label": "462"
    }
  },
  {
    "page_content": "of the functional as the output. An example would be the entropy H[p], which takes\na probability distribution p(x) as the input and returns the quantity\nH[p]=\n∫\np(x)l np(x)d x (10.1)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 481,
      "page_label": "462"
    }
  },
  {
    "page_content": "10.1. Variational Inference 463\nas the output. We can the introduce the concept of afunctional derivative, which ex-\npresses how the value of the functional changes in response to inﬁnitesimal changes\nto the input function (Feynmanet al., 1964). The rules for the calculus of variations\nmirror those of standard calculus and are discussed in Appendix D. Many problems\ncan be expressed in terms of an optimization problem in which the quantity being\noptimized is a functional. The solution is obtained by exploring all possible input\nfunctions to ﬁnd the one that maximizes, or minimizes, the functional. Variational\nmethods have broad applicability and include such areas as ﬁnite element methods\n(Kapur, 1989) and maximum entropy (Schwarz, 1988).\nAlthough there is nothing intrinsically approximate about variational methods,\nthey do naturally lend themselves to ﬁnding approximate solutions. This is done\nby restricting the range of functions over which the optimization is performed, for\ninstance by considering only quadratic functions or by considering functions com-\nposed of a linear combination of ﬁxed basis functions in which only the coefﬁcients\nof the linear combination can vary. In the case of applications to probabilistic in-\nference, the restriction may for example take the form of factorization assumptions\n(Jordanet al., 1999; Jaakkola, 2001).\nNow let us consider in more detail how the concept of variational optimization",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 482,
      "page_label": "463"
    }
  },
  {
    "page_content": "(Jordanet al., 1999; Jaakkola, 2001).\nNow let us consider in more detail how the concept of variational optimization\ncan be applied to the inference problem. Suppose we have a fully Bayesian model in\nwhich all parameters are given prior distributions. The model may also have latent\nvariables as well as parameters, and we shall denote the set of all latent variables\nand parameters by Z. Similarly, we denote the set of all observed variables by X.\nFor example, we might have a set of N independent, identically distributed data,\nfor which X = {x1,..., xN } and Z = {z1,..., zN }. Our probabilistic model\nspeciﬁes the joint distribution p(X, Z), and our goal is to ﬁnd an approximation for\nthe posterior distribution p(Z|X) as well as for the model evidence p(X).A si no u r\ndiscussion of EM, we can decompose the log marginal probability using\nln p(X)= L(q) + KL(q∥p) (10.2)\nwhere we have deﬁned\nL(q)=\n∫\nq(Z)l n\n{p(X, Z)\nq(Z)\n}\ndZ (10.3)\nKL(q∥p)= −\n∫\nq(Z)l n\n{p(Z|X)\nq(Z)\n}\ndZ. (10.4)\nThis differs from our discussion of EM only in that the parameter vectorθ no longer\nappears, because the parameters are now stochastic variables and are absorbed into\nZ. Since in this chapter we will mainly be interested in continuous variables we have\nused integrations rather than summations in formulating this decomposition. How-\never, the analysis goes through unchanged if some or all of the variables are discrete\nsimply by replacing the integrations with summations as required. As before, we",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 482,
      "page_label": "463"
    }
  },
  {
    "page_content": "simply by replacing the integrations with summations as required. As before, we\ncan maximize the lower bound L(q) by optimization with respect to the distribution\nq(Z), which is equivalent to minimizing the KL divergence. If we allow any possible\nchoice for q(Z), then the maximum of the lower bound occurs when the KL diver-\ngence vanishes, which occurs when q(Z) equals the posterior distribution p(Z|X).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 482,
      "page_label": "463"
    }
  },
  {
    "page_content": "464 10. APPROXIMATE INFERENCE\n−2 −1 0 1 2 3 4\n0\n0.2\n0.4\n0.6\n0.8\n1\n−2 −1 0 1 2 3 4\n0\n10\n20\n30\n40\nFigure 10.1 Illustration of the variational approximation for the example considered earlier in Figure 4.14. The\nleft-hand plot shows the original distribution (yellow) along with the Laplace (red) and variational (green) approx-\nimations, and the right-hand plot shows the negative logarithms of the corresponding curves.\nHowever, we shall suppose the model is such that working with the true posterior\ndistribution is intractable.\nWe therefore consider instead a restricted family of distributions q(Z) and then\nseek the member of this family for which the KL divergence is minimized. Our goal\nis to restrict the family sufﬁciently that they comprise only tractable distributions,\nwhile at the same time allowing the family to be sufﬁciently rich and ﬂexible that it\ncan provide a good approximation to the true posterior distribution. It is important to\nemphasize that the restriction is imposed purely to achieve tractability, and that sub-\nject to this requirement we should use as rich a family of approximating distributions\nas possible. In particular, there is no ‘over-ﬁtting’ associated with highly ﬂexible dis-\ntributions. Using more ﬂexible approximations simply allows us to approach the true\nposterior distribution more closely.\nOne way to restrict the family of approximating distributions is to use a paramet-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 483,
      "page_label": "464"
    }
  },
  {
    "page_content": "posterior distribution more closely.\nOne way to restrict the family of approximating distributions is to use a paramet-\nric distribution q(Z|ω) governed by a set of parameters ω. The lower bound L(q)\nthen becomes a function of ω, and we can exploit standard nonlinear optimization\ntechniques to determine the optimal values for the parameters. An example of this\napproach, in which the variational distribution is a Gaussian and we have optimized\nwith respect to its mean and variance, is shown in Figure 10.1.\n10.1.1 Factorized distributions\nHere we consider an alternative way in which to restrict the family of distri-\nbutions q(Z). Suppose we partition the elements of Z into disjoint groups that we\ndenote by Zi where i =1 ,...,M . We then assume that the q distribution factorizes\nwith respect to these groups, so that\nq(Z)=\nM∏\ni=1\nqi(Zi). (10.5)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 483,
      "page_label": "464"
    }
  },
  {
    "page_content": "10.1. Variational Inference 465\nIt should be emphasized that we are making no further assumptions about the distri-\nbution. In particular, we place no restriction on the functional forms of the individual\nfactorsqi(Zi). This factorized form of variational inference corresponds to an ap-\nproximation framework developed in physics calledmean ﬁeld theory(Parisi, 1988).\nAmongst all distributions q(Z) having the form (10.5), we now seek that distri-\nbution for which the lower bound L(q) is largest. We therefore wish to make a free\nform (variational) optimization ofL(q) with respect to all of the distributionsqi(Zi),\nwhich we do by optimizing with respect to each of the factors in turn. To achieve\nthis, we ﬁrst substitute (10.5) into (10.3) and then dissect out the dependence on one\nof the factorsqj(Zj). Denoting qj(Zj) by simply qj to keep the notation uncluttered,\nwe then obtain\nL(q)=\n∫ ∏\ni\nqi\n{\nlnp(X, Z) −\n∑\ni\nlnqi\n}\ndZ\n=\n∫\nqj\n{∫\nlnp(X, Z)\n∏\ni̸=j\nqi dZi\n}\ndZj −\n∫\nqj lnqj dZj +c o n s t\n=\n∫\nqj ln˜p(X, Zj)d Zj −\n∫\nqj lnqj dZj +c o n s t (10.6)\nwhere we have deﬁned a new distribution ˜p(X, Zj) by the relation\nln˜p(X, Zj)= Ei̸=j[lnp(X, Z)] + const. (10.7)\nHere the notation Ei̸=j[··· ] denotes an expectation with respect to theq distributions\nover all variables zi for i ̸=j, so that\nEi̸=j[lnp(X, Z)] =\n∫\nln p(X, Z)\n∏\ni̸=j\nqi dZi. (10.8)\nNow suppose we keep the {qi̸=j} ﬁxed and maximize L(q) in (10.6) with re-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 484,
      "page_label": "465"
    }
  },
  {
    "page_content": "Ei̸=j[lnp(X, Z)] =\n∫\nln p(X, Z)\n∏\ni̸=j\nqi dZi. (10.8)\nNow suppose we keep the {qi̸=j} ﬁxed and maximize L(q) in (10.6) with re-\nspect to all possible forms for the distribution qj(Zj). This is easily done by rec-\nognizing that (10.6) is a negative Kullback-Leibler divergence between qj(Zj) and\n˜p(X, Zj). Thus maximizing (10.6) is equivalent to minimizing the Kullback-Leibler\nLeonhard Euler\n1707–1783\nEuler was a Swiss mathematician\nand physicist who worked in St.\nPetersburg and Berlin and who is\nwidely considered to be one of the\ngreatest mathematicians of all time.\nHe is certainly the most proliﬁc, and\nhis collected works ﬁll 75 volumes. Amongst his many\ncontributions, he formulated the modern theory of the\nfunction, he developed (together with Lagrange) the\ncalculus of variations, and he discovered the formula\neiπ = −1, which relates four of the most important\nnumbers in mathematics. During the last 17 years of\nhis life, he was almost totally blind, and yet he pro-\nduced nearly half of his results during this period.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 484,
      "page_label": "465"
    }
  },
  {
    "page_content": "466 10. APPROXIMATE INFERENCE\ndivergence, and the minimum occurs when qj(Zj)= ˜p(X, Zj). Thus we obtain a\ngeneral expression for the optimal solution q⋆\nj (Zj) given by\nlnq⋆\nj (Zj)= Ei̸=j[lnp(X, Z)] + const. (10.9)\nIt is worth taking a few moments to study the form of this solution as it provides the\nbasis for applications of variational methods. It says that the log of the optimal so-\nlution for factor qj is obtained simply by considering the log of the joint distribution\nover all hidden and visible variables and then taking the expectation with respect to\nall of the other factors {qi} for i ̸=j.\nThe additive constant in (10.9) is set by normalizing the distribution q⋆\nj (Zj).\nThus if we take the exponential of both sides and normalize, we have\nq⋆\nj (Zj)= exp (Ei̸=j[lnp(X, Z)])∫\nexp (Ei̸=j[lnp(X, Z)]) dZj\n.\nIn practice, we shall ﬁnd it more convenient to work with the form (10.9) and then re-\ninstate the normalization constant (where required) by inspection. This will become\nclear from subsequent examples.\nThe set of equations given by (10.9) for j =1 ,...,M represent a set of con-\nsistency conditions for the maximum of the lower bound subject to the factorization\nconstraint. However, they do not represent an explicit solution because the expres-\nsion on the right-hand side of (10.9) for the optimumq⋆\nj (Zj) depends on expectations\ncomputed with respect to the other factors qi(Zi) for i ̸=j. We will therefore seek",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 485,
      "page_label": "466"
    }
  },
  {
    "page_content": "j (Zj) depends on expectations\ncomputed with respect to the other factors qi(Zi) for i ̸=j. We will therefore seek\na consistent solution by ﬁrst initializing all of the factors qi(Zi) appropriately and\nthen cycling through the factors and replacing each in turn with a revised estimate\ngiven by the right-hand side of (10.9) evaluated using the current estimates for all of\nthe other factors. Convergence is guaranteed because bound is convex with respect\nto each of the factorsqi(Zi) (Boyd and Vandenberghe, 2004).\n10.1.2 Properties of factorized approximations\nOur approach to variational inference is based on a factorized approximation to\nthe true posterior distribution. Let us consider for a moment the problem of approx-\nimating a general distribution by a factorized distribution. To begin with, we discuss\nthe problem of approximating a Gaussian distribution using a factorized Gaussian,\nwhich will provide useful insight into the types of inaccuracy introduced in using\nfactorized approximations. Consider a Gaussian distribution p(z)= N(z|µ, Λ−1)\nover two correlated variables z =( z1,z 2) in which the mean and precision have\nelements\nµ =\n(\nµ1\nµ2\n)\n, Λ =\n(\nΛ11 Λ12\nΛ21 Λ22\n)\n(10.10)\nand Λ21 =Λ 12 due to the symmetry of the precision matrix. Now suppose we\nwish to approximate this distribution using a factorized Gaussian of the formq(z)=\nq1(z1)q2(z2). We ﬁrst apply the general result (10.9) to ﬁnd an expression for the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 485,
      "page_label": "466"
    }
  },
  {
    "page_content": "10.1. Variational Inference 467\noptimal factor q⋆\n1(z1). In doing so it is useful to note that on the right-hand side we\nonly need to retain those terms that have some functional dependence on z1 because\nall other terms can be absorbed into the normalization constant. Thus we have\nlnq⋆\n1(z1)= Ez2 [lnp(z) ]+c o n s t\n= Ez2\n[\n−1\n2(z1 − µ1)2Λ11 − (z1 − µ1)Λ12(z2 − µ2)\n]\n+c o n s t\n= −1\n2z2\n1Λ11 + z1µ1Λ11 − z1Λ12 (E[z2] − µ2) + const. (10.11)\nNext we observe that the right-hand side of this expression is a quadratic function of\nz1, and so we can identify q⋆ (z1) as a Gaussian distribution. It is worth emphasizing\nthat we did not assume that q(zi) is Gaussian, but rather we derived this result by\nvariational optimization of the KL divergence over all possible distributions q(zi).\nNote also that we do not need to consider the additive constant in (10.9) explicitly\nbecause it represents the normalization constant that can be found at the end by\ninspection if required. Using the technique of completing the square, we can identifySection 2.3.1\nthe mean and precision of this Gaussian, giving\nq⋆ (z1)= N(z1|m1, Λ−1\n11 ) (10.12)\nwhere\nm1 = µ1 − Λ−1\n11 Λ12 (E[z2] − µ2) . (10.13)\nBy symmetry, q⋆\n2(z2) is also Gaussian and can be written as\nq⋆\n2(z2)= N(z2|m2, Λ−1\n22 ) (10.14)\nin which\nm2 = µ2 − Λ−1\n22 Λ21 (E[z1] − µ1) . (10.15)\nNote that these solutions are coupled, so that q⋆ (z1) depends on expectations com-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 486,
      "page_label": "467"
    }
  },
  {
    "page_content": "22 ) (10.14)\nin which\nm2 = µ2 − Λ−1\n22 Λ21 (E[z1] − µ1) . (10.15)\nNote that these solutions are coupled, so that q⋆ (z1) depends on expectations com-\nputed with respect to q⋆ (z2) and vice versa. In general, we address this by treating\nthe variational solutions as re-estimation equations and cycling through the variables\nin turn updating them until some convergence criterion is satisﬁed. We shall see\nan example of this shortly. Here, however, we note that the problem is sufﬁciently\nsimple that a closed form solution can be found. In particular, because E[z1]= m1\nand E[z2]= m2, we see that the two equations are satisﬁed if we take E[z1]= µ1\nand E[z2]= µ2, and it is easily shown that this is the only solution provided the dis-\ntribution is nonsingular. This result is illustrated in Figure 10.2(a). We see that theExercise 10.2\nmean is correctly captured but that the variance ofq(z) is controlled by the direction\nof smallest variance of p(z), and that the variance along the orthogonal direction is\nsigniﬁcantly under-estimated. It is a general result that a factorized variational ap-\nproximation tends to give approximations to the posterior distribution that are too\ncompact.\nBy way of comparison, suppose instead that we had been minimizing the reverse\nKullback-Leibler divergence KL(p∥q). As we shall see, this form of KL divergence",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 486,
      "page_label": "467"
    }
  },
  {
    "page_content": "468 10. APPROXIMATE INFERENCE\nFigure 10.2 Comparison of\nthe two alternative forms for the\nKullback-Leibler divergence. The\ngreen contours corresponding to\n1, 2, and 3 standard deviations for\na correlated Gaussian distribution\np(z) over two variables z1 and z2,\nand the red contours represent\nthe corresponding levels for an\napproximating distribution q(z)\nover the same variables given by\nthe product of two independent\nunivariate Gaussian distributions\nwhose parameters are obtained by\nminimization of (a) the Kullback-\nLeibler divergence KL(q∥p), and\n(b) the reverse Kullback-Leibler\ndivergence KL(p∥q).\nz1\nz2\n(a)\n0 0.5 1\n0\n0.5\n1\nz1\nz2\n(b)\n0 0.5 1\n0\n0.5\n1\nis used in an alternative approximate inference framework called expectation prop-\nagation. We therefore consider the general problem of minimizing KL(p∥q) whenSection 10.7\nq(Z) is a factorized approximation of the form (10.5). The KL divergence can then\nbe written in the form\nKL(p∥q)= −\n∫\np(Z)\n[M∑\ni=1\nlnqi(Zi)\n]\ndZ +c o n s t (10.16)\nwhere the constant term is simply the entropy of p(Z) and so does not depend on\nq(Z). We can now optimize with respect to each of the factors qj(Zj), which is\neasily done using a Lagrange multiplier to giveExercise 10.3\nq⋆\nj (Zj)=\n∫\np(Z)\n∏\ni̸=j\ndZi = p(Zj). (10.17)\nIn this case, we ﬁnd that the optimal solution for qj(Zj) is just given by the corre-\nsponding marginal distribution of p(Z). Note that this is a closed-form solution and\nso does not require iteration.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 487,
      "page_label": "468"
    }
  },
  {
    "page_content": "sponding marginal distribution of p(Z). Note that this is a closed-form solution and\nso does not require iteration.\nTo apply this result to the illustrative example of a Gaussian distribution p(z)\nover a vector z we can use (2.98), which gives the result shown in Figure 10.2(b).\nWe see that once again the mean of the approximation is correct, but that it places\nsigniﬁcant probability mass in regions of variable space that have very low probabil-\nity.\nThe difference between these two results can be understood by noting that there\nis a large positive contribution to the Kullback-Leibler divergence\nKL(q∥p)= −\n∫\nq(Z)l n\n{p(Z)\nq(Z)\n}\ndZ (10.18)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 487,
      "page_label": "468"
    }
  },
  {
    "page_content": "10.1. Variational Inference 469\n(a) (b) (c)\nFigure 10.3 Another comparison of the two alternative forms for the Kullback-Leibler divergence. (a) The blue\ncontours show a bimodal distributionp(Z) given by a mixture of two Gaussians, and the red contours correspond\nto the single Gaussian distribution q(Z) that best approximates p(Z) in the sense of minimizing the Kullback-\nLeibler divergence KL(p∥q). (b) As in (a) but now the red contours correspond to a Gaussian distribution q(Z)\nfound by numerical minimization of the Kullback-Leibler divergenceKL(q∥p). (c) As in (b) but showing a different\nlocal minimum of the Kullback-Leibler divergence.\nfrom regions of Z space in which p(Z) is near zero unless q(Z) is also close to\nzero. Thus minimizing this form of KL divergence leads to distributions q(Z) that\navoid regions in which p(Z) is small. Conversely, the Kullback-Leibler divergence\nKL(p∥q) is minimized by distributions q(Z) that are nonzero in regions where p(Z)\nis nonzero.\nWe can gain further insight into the different behaviour of the two KL diver-\ngences if we consider approximating a multimodal distribution by a unimodal one,\nas illustrated in Figure 10.3. In practical applications, the true posterior distri-\nbution will often be multimodal, with most of the posterior mass concentrated in\nsome number of relatively small regions of parameter space. These multiple modes\nmay arise through nonidentiﬁability in the latent space or through complex nonlin-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 488,
      "page_label": "469"
    }
  },
  {
    "page_content": "may arise through nonidentiﬁability in the latent space or through complex nonlin-\near dependence on the parameters. Both types of multimodality were encountered in\nChapter 9 in the context of Gaussian mixtures, where they manifested themselves as\nmultiple maxima in the likelihood function, and a variational treatment based on the\nminimization ofKL(q∥p) will tend to ﬁnd one of these modes. By contrast, if we\nwere to minimize KL(p∥q), the resulting approximations would average across all\nof the modes and, in the context of the mixture model, would lead to poor predictive\ndistributions (because the average of two good parameter values is typically itself\nnot a good parameter value). It is possible to make use ofKL(p∥q) to deﬁne a useful\ninference procedure, but this requires a rather different approach to the one discussed\nhere, and will be considered in detail when we discuss expectation propagation.Section 10.7\nThe two forms of Kullback-Leibler divergence are members of thealpha family",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 488,
      "page_label": "469"
    }
  },
  {
    "page_content": "470 10. APPROXIMATE INFERENCE\nof divergences (Ali and Silvey, 1966; Amari, 1985; Minka, 2005) deﬁned by\nDα(p∥q)= 4\n1 − α2\n(\n1 −\n∫\np(x)(1+α)/2q(x)(1−α)/2 dx\n)\n(10.19)\nwhere −∞ <α< ∞ is a continuous parameter. The Kullback-Leibler divergence\nKL(p∥q) corresponds to the limit α → 1, whereas KL(q∥p) corresponds to the limit\nα →− 1. For all values of α we have Dα(p∥q) ⩾ 0, with equality if, and only if,Exercise 10.6\np(x)= q(x). Suppose p(x) is a ﬁxed distribution, and we minimize Dα(p∥q) with\nrespect to some set of distributions q(x). Then for α ⩽ −1 the divergence is zero\nforcing, so that any values ofx for which p(x)=0 will have q(x)=0 , and typically\nq(x) will under-estimate the support of p(x) and will tend to seek the mode with the\nlargest mass. Conversely for α ⩾ 1 the divergence is zero-avoiding, so that values\nof x for which p(x) > 0 will have q(x) > 0, and typically q(x) will stretch to cover\nall of p(x), and will over-estimate the support of p(x). When α =0 we obtain a\nsymmetric divergence that is linearly related to the Hellinger distancegiven by\nDH(p∥q)=\n∫ (\np(x)1/2 − q(x)1/2)\ndx. (10.20)\nThe square root of the Hellinger distance is a valid distance metric.\n10.1.3 Example: The univariate Gaussian\nWe now illustrate the factorized variational approximation using a Gaussian dis-\ntribution over a single variable x (MacKay, 2003). Our goal is to infer the posterior\ndistribution for the mean µ and precision τ, given a data set D = {x1,...,x N } of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 489,
      "page_label": "470"
    }
  },
  {
    "page_content": "distribution for the mean µ and precision τ, given a data set D = {x1,...,x N } of\nobserved values of x which are assumed to be drawn independently from the Gaus-\nsian. The likelihood function is given by\np(D|µ, τ)=\n( τ\n2π\n)N/2\nexp\n{\n−τ\n2\nN∑\nn=1\n(xn − µ)2\n}\n. (10.21)\nWe now introduce conjugate prior distributions forµ and τ given by\np(µ|τ)= N\n(\nµ|µ0, (λ0τ)−1)\n(10.22)\np(τ)=G a m ( τ|a0,b 0) (10.23)\nwhere Gam(τ|a0,b 0) is the gamma distribution deﬁned by (2.146). Together these\ndistributions constitute a Gaussian-Gamma conjugate prior distribution.Section 2.3.6\nFor this simple problem the posterior distribution can be found exactly, and again\ntakes the form of a Gaussian-gamma distribution. However, for tutorial purposesExercise 2.44\nwe will consider a factorized variational approximation to the posterior distribution\ngiven by\nq(µ, τ)= qµ(µ)qτ (τ). (10.24)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 489,
      "page_label": "470"
    }
  },
  {
    "page_content": "10.1. Variational Inference 471\nNote that the true posterior distribution does not factorize in this way. The optimum\nfactors qµ(µ) and qτ (τ) can be obtained from the general result (10.9) as follows.\nFor qµ(µ) we have\nlnq⋆\nµ(µ)= Eτ [ln p(D|µ, τ)+l n p(µ|τ) ]+c o n s t\n= −E[τ]\n2\n{\nλ0(µ − µ0)2 +\nN∑\nn=1\n(xn − µ)2\n}\n+c o n s t. (10.25)\nCompleting the square over µ we see that qµ(µ) is a Gaussian N\n(\nµ|µN ,λ −1\nN\n)\nwith\nmean and precision given byExercise 10.7\nµN = λ0µ0 + Nx\nλ0 + N (10.26)\nλN =( λ0 + N)E[τ]. (10.27)\nNote that for N →∞ this gives the maximum likelihood result in which µN = x\nand the precision is inﬁnite.\nSimilarly, the optimal solution for the factor qτ (τ) is given by\nlnq⋆\nτ (τ)= Eµ [lnp(D|µ, τ)+l n p(µ|τ) ]+l np(τ)+c o n s t\n=( a0 − 1) lnτ − b0τ + N\n2 ln τ\n−τ\n2Eµ\n[N∑\nn=1\n(xn − µ)2 + λ0(µ − µ0)2\n]\n+c o n s t(10.28)\nand hence qτ (τ) is a gamma distribution Gam(τ|aN ,b N ) with parameters\naN = a0 + N\n2 (10.29)\nbN = b0 + 1\n2Eµ\n[N∑\nn=1\n(xn − µ)2 + λ0(µ − µ0)2\n]\n. (10.30)\nAgain this exhibits the expected behaviour when N →∞ .Exercise 10.8\nIt should be emphasized that we did not assume these speciﬁc functional forms\nfor the optimal distributionsqµ(µ) and qτ (τ). They arose naturally from the structure\nof the likelihood function and the corresponding conjugate priors.Section 10.4.1\nThus we have expressions for the optimal distributions qµ(µ) and qτ (τ) each of\nwhich depends on moments evaluated with respect to the other distribution. One ap-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 490,
      "page_label": "471"
    }
  },
  {
    "page_content": "which depends on moments evaluated with respect to the other distribution. One ap-\nproach to ﬁnding a solution is therefore to make an initial guess for, say, the moment\nE[τ] and use this to re-compute the distribution qµ(µ). Given this revised distri-\nbution we can then extract the required moments E[µ] and E[µ2], and use these to\nrecompute the distribution qτ (τ), and so on. Since the space of hidden variables for\nthis example is only two dimensional, we can illustrate the variational approxima-\ntion to the posterior distribution by plotting contours of both the true posterior and\nthe factorized approximation, as illustrated in Figure 10.4.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 490,
      "page_label": "471"
    }
  },
  {
    "page_content": "472 10. APPROXIMATE INFERENCE\nµ\nτ\n(a)\n−1 0 1\n0\n1\n2\nµ\nτ\n(b)\n−1 0 1\n0\n1\n2\nµ\nτ\n(c)\n−1 0 1\n0\n1\n2\nµ\nτ\n(d)\n−1 0 1\n0\n1\n2\nFigure 10.4 Illustration of variational inference for the meanµ and precision τ of a univariate Gaussian distribu-\ntion. Contours of the true posterior distribution p(µ, τ|D) are shown in green. (a) Contours of the initial factorized\napproximation qµ(µ)qτ (τ) are shown in blue. (b) After re-estimating the factor qµ(µ). (c) After re-estimating the\nfactor qτ (τ). (d) Contours of the optimal factorized approximation, to which the iterative scheme converges, are\nshown in red.\nIn general, we will need to use an iterative approach such as this in order to\nsolve for the optimal factorized posterior distribution. For the very simple example\nwe are considering here, however, we can ﬁnd an explicit solution by solving the\nsimultaneous equations for the optimal factors qµ(µ) and qτ (τ). Before doing this,\nwe can simplify these expressions by considering broad, noninformative priors in\nwhich µ0 = a0 = b0 = λ0 =0 . Although these parameter settings correspond to\nimproper priors, we see that the posterior distribution is still well deﬁned. Using the\nstandard result E[τ]= aN /bN for the mean of a gamma distribution, together withAppendix B\n(10.29) and (10.30), we have\n1\nE[τ] = E\n[\n1\nN\nN∑\nn=1\n(xn − µ)2\n]\n= x2 − 2xE[µ]+ E[µ2]. (10.31)\nThen, using (10.26) and (10.27), we obtain the ﬁrst and second order moments of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 491,
      "page_label": "472"
    }
  },
  {
    "page_content": "10.1. Variational Inference 473\nqµ(µ) in the form\nE[µ]= x, E[µ2]= x2 + 1\nNE[τ]. (10.32)\nWe can now substitute these moments into (10.31) and then solve for E[τ] to giveExercise 10.9\n1\nE[τ] = 1\nN − 1(x2 − x2)\n= 1\nN − 1\nN∑\nn=1\n(xn − x)2. (10.33)\nWe recognize the right-hand side as the familiar unbiased estimator for the variance\nof a univariate Gaussian distribution, and so we see that the use of a Bayesian ap-\nproach has avoided the bias of the maximum likelihood solution.Section 1.2.4\n10.1.4 Model comparison\nAs well as performing inference over the hidden variables Z, we may also\nwish to compare a set of candidate models, labelled by the index m, and having\nprior probabilities p(m). Our goal is then to approximate the posterior probabilities\np(m|X), where X is the observed data. This is a slightly more complex situation\nthan that considered so far because different models may have different structure\nand indeed different dimensionality for the hidden variables Z. We cannot there-\nfore simply consider a factorized approximation q(Z)q(m), but must instead recog-\nnize that the posterior over Z must be conditioned on m, and so we must consider\nq(Z,m )= q(Z|m)q(m). We can readily verify the following decomposition based\non this variational distributionExercise 10.10\nlnp(X)= Lm −\n∑\nm\n∑\nZ\nq(Z|m)q(m)l n\n{ p(Z,m |X)\nq(Z|m)q(m)\n}\n(10.34)\nwhere the Lm is a lower bound on lnp(X) and is given by\nLm =\n∑\nm\n∑\nZ\nq(Z|m)q(m)l n\n{ p(Z, X,m )\nq(Z|m)q(m)\n}\n. (10.35)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 492,
      "page_label": "473"
    }
  },
  {
    "page_content": "q(Z|m)q(m)\n}\n(10.34)\nwhere the Lm is a lower bound on lnp(X) and is given by\nLm =\n∑\nm\n∑\nZ\nq(Z|m)q(m)l n\n{ p(Z, X,m )\nq(Z|m)q(m)\n}\n. (10.35)\nHere we are assuming discrete Z, but the same analysis applies to continuous latent\nvariables provided the summations are replaced with integrations. We can maximize\nLm with respect to the distribution q(m) using a Lagrange multiplier, with the resultExercise 10.11\nq(m) ∝ p(m)e x p{Lm}. (10.36)\nHowever, if we maximize Lm with respect to the q(Z|m), we ﬁnd that the solutions\nfor different m are coupled, as we expect because they are conditioned on m.W e\nproceed instead by ﬁrst optimizing each of the q(Z|m) individually by optimization",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 492,
      "page_label": "473"
    }
  },
  {
    "page_content": "474 10. APPROXIMATE INFERENCE\nof (10.35), and then subsequently determining the q(m) using (10.36). After nor-\nmalization the resulting values for q(m) can be used for model selection or model\naveraging in the usual way.\n10.2. Illustration: Variational Mixture of Gaussians\nWe now return to our discussion of the Gaussian mixture model and apply the vari-\national inference machinery developed in the previous section. This will provide a\ngood illustration of the application of variational methods and will also demonstrate\nhow a Bayesian treatment elegantly resolves many of the difﬁculties associated with\nthe maximum likelihood approach (Attias, 1999b). The reader is encouraged to work\nthrough this example in detail as it provides many insights into the practical appli-\ncation of variational methods. Many Bayesian models, corresponding to much more\nsophisticated distributions, can be solved by straightforward extensions and general-\nizations of this analysis.\nOur starting point is the likelihood function for the Gaussian mixture model, il-\nlustrated by the graphical model in Figure 9.6. For each observation xn we have\na corresponding latent variable zn comprising a 1-of- K binary vector with ele-\nments znk for k =1 ,...,K . As before we denote the observed data set by X =\n{x1,..., xN }, and similarly we denote the latent variables by Z = {z1,..., zN }.\nFrom (9.10) we can write down the conditional distribution of Z, given the mixing\ncoefﬁcients π, in the form\np(Z|π)=\nN∏\nn=1\nK∏",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 493,
      "page_label": "474"
    }
  },
  {
    "page_content": "From (9.10) we can write down the conditional distribution of Z, given the mixing\ncoefﬁcients π, in the form\np(Z|π)=\nN∏\nn=1\nK∏\nk=1\nπznk\nk . (10.37)\nSimilarly, from (9.11), we can write down the conditional distribution of the ob-\nserved data vectors, given the latent variables and the component parameters\np(X|Z, µ, Λ)=\nN∏\nn=1\nK∏\nk=1\nN\n(\nxn|µk, Λ−1\nk\n)znk\n(10.38)\nwhere µ = {µk} and Λ = {Λk}. Note that we are working in terms of precision\nmatrices rather than covariance matrices as this somewhat simpliﬁes the mathemat-\nics.\nNext we introduce priors over the parameters µ, Λ and π. The analysis is con-\nsiderably simpliﬁed if we use conjugate prior distributions. We therefore choose aSection 10.4.1\nDirichlet distribution over the mixing coefﬁcients π\np(π)=D i r (π|α0)= C(α0)\nK∏\nk=1\nπα0−1\nk (10.39)\nwhere by symmetry we have chosen the same parameter α0 for each of the compo-\nnents, and C(α0) is the normalization constant for the Dirichlet distribution deﬁned",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 493,
      "page_label": "474"
    }
  },
  {
    "page_content": "10.2. Illustration: Variational Mixture of Gaussians 475\nFigure 10.5 Directed acyclic graph representing the Bayesian mix-\nture of Gaussians model, in which the box (plate) de-\nnotes a set of N i.i.d. observations. Here µ denotes\n{µk} and Λ denotes {Λk}.\nxn\nzn\nN\nπ\nµ\nΛ\nby (B.23). As we have seen, the parameter α0 can be interpreted as the effectiveSection 2.2.1\nprior number of observations associated with each component of the mixture. If the\nvalue of α0 is small, then the posterior distribution will be inﬂuenced primarily by\nthe data rather than by the prior.\nSimilarly, we introduce an independent Gaussian-Wishart prior governing the\nmean and precision of each Gaussian component, given by\np(µ, Λ)= p(µ|Λ)p(Λ)\n=\nK∏\nk=1\nN\n(\nµk|m0, (β0Λk)−1)\nW(Λk|W0,ν 0) (10.40)\nbecause this represents the conjugate prior distribution when both the mean and pre-\ncision are unknown. Typically we would choose m0 = 0 by symmetry.Section 2.3.6\nThe resulting model can be represented as a directed graph as shown in Fig-\nure 10.5. Note that there is a link from Λ to µ since the variance of the distribution\nover µ in (10.40) is a function of Λ.\nThis example provides a nice illustration of the distinction between latent vari-\nables and parameters. Variables such as zn that appear inside the plate are regarded\nas latent variables because the number of such variables grows with the size of the\ndata set. By contrast, variables such as µ that are outside the plate are ﬁxed in",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 494,
      "page_label": "475"
    }
  },
  {
    "page_content": "data set. By contrast, variables such as µ that are outside the plate are ﬁxed in\nnumber independently of the size of the data set, and so are regarded as parameters.\nFrom the perspective of graphical models, however, there is really no fundamental\ndifference between them.\n10.2.1 Variational distribution\nIn order to formulate a variational treatment of this model, we next write down\nthe joint distribution of all of the random variables, which is given by\np(X, Z, π, µ, Λ)= p(X|Z, µ, Λ)p(Z|π)p(π)p(µ|Λ)p(Λ) (10.41)\nin which the various factors are deﬁned above. The reader should take a moment to\nverify that this decomposition does indeed correspond to the probabilistic graphical\nmodel shown in Figure 10.5. Note that only the variables X = {x1,..., xN } are\nobserved.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 494,
      "page_label": "475"
    }
  },
  {
    "page_content": "476 10. APPROXIMATE INFERENCE\nWe now consider a variational distribution which factorizes between the latent\nvariables and the parameters so that\nq(Z, π, µ, Λ)= q(Z)q(π, µ, Λ). (10.42)\nIt is remarkable that this is the only assumption that we need to make in order to\nobtain a tractable practical solution to our Bayesian mixture model. In particular, the\nfunctional form of the factorsq(Z) and q(π, µ, Λ) will be determined automatically\nby optimization of the variational distribution. Note that we are omitting the sub-\nscripts on the q distributions, much as we do with the p distributions in (10.41), and\nare relying on the arguments to distinguish the different distributions.\nThe corresponding sequential update equations for these factors can be easily\nderived by making use of the general result (10.9). Let us consider the derivation of\nthe update equation for the factor q(Z). The log of the optimized factor is given by\nln q⋆ (Z)= Eπ,µ,Λ[lnp(X, Z, π, µ, Λ) ]+c o n s t. (10.43)\nWe now make use of the decomposition (10.41). Note that we are only interested in\nthe functional dependence of the right-hand side on the variable Z. Thus any terms\nthat do not depend on Z can be absorbed into the additive normalization constant,\ngiving\nln q⋆ (Z)= Eπ[lnp(Z|π)] +Eµ,Λ[lnp(X|Z, µ, Λ) ]+c o n s t. (10.44)\nSubstituting for the two conditional distributions on the right-hand side, and again\nabsorbing any terms that are independent of Z into the additive constant, we have\nlnq⋆ (Z)=\nN∑\nn=1\nK∑\nk=1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 495,
      "page_label": "476"
    }
  },
  {
    "page_content": "absorbing any terms that are independent of Z into the additive constant, we have\nlnq⋆ (Z)=\nN∑\nn=1\nK∑\nk=1\nznk lnρnk +c o n s t (10.45)\nwhere we have deﬁned\nlnρnk = E[lnπk]+ 1\n2E [ln|Λk|] − D\n2 ln(2π)\n−1\n2Eµk,Λk\n[\n(xn − µk)TΛk(xn − µk)\n]\n(10.46)\nwhere D is the dimensionality of the data variable x. Taking the exponential of both\nsides of (10.45) we obtain\nq⋆ (Z) ∝\nN∏\nn=1\nK∏\nk=1\nρznk\nnk . (10.47)\nRequiring that this distribution be normalized, and noting that for each value of n\nthe quantities znk are binary and sum to 1 over all values of k, we obtainExercise 10.12\nq⋆ (Z)=\nN∏\nn=1\nK∏\nk=1\nrznk\nnk (10.48)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 495,
      "page_label": "476"
    }
  },
  {
    "page_content": "10.2. Illustration: Variational Mixture of Gaussians 477\nwhere\nrnk = ρnk\nK∑\nj=1\nρnj\n. (10.49)\nWe see that the optimal solution for the factor q(Z) takes the same functional form\nas the prior p(Z|π). Note that because ρnk is given by the exponential of a real\nquantity, the quantities rnk will be nonnegative and will sum to one, as required.\nFor the discrete distribution q⋆ (Z) we have the standard result\nE[znk]= rnk (10.50)\nfrom which we see that the quantities rnk are playing the role of responsibilities.\nNote that the optimal solution forq⋆ (Z) depends on moments evaluated with respect\nto the distributions of other variables, and so again the variational update equations\nare coupled and must be solved iteratively.\nAt this point, we shall ﬁnd it convenient to deﬁne three statistics of the observed\ndata set evaluated with respect to the responsibilities, given by\nNk =\nN∑\nn=1\nrnk (10.51)\nxk = 1\nNk\nN∑\nn=1\nrnkxn (10.52)\nSk = 1\nNk\nN∑\nn=1\nrnk(xn − xk)(xn − xk)T. (10.53)\nNote that these are analogous to quantities evaluated in the maximum likelihood EM\nalgorithm for the Gaussian mixture model.\nNow let us consider the factor q(π, µ, Λ) in the variational posterior distribu-\ntion. Again using the general result (10.9) we have\nln q⋆ (π, µ, Λ)=l n p(π)+\nK∑\nk=1\nlnp(µk, Λk)+ EZ [lnp(Z|π)]\n+\nK∑\nk=1\nN∑\nn=1\nE[znk]l nN\n(\nxn|µk, Λ−1\nk\n)\n+c o n s t. (10.54)\nWe observe that the right-hand side of this expression decomposes into a sum of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 496,
      "page_label": "477"
    }
  },
  {
    "page_content": "+\nK∑\nk=1\nN∑\nn=1\nE[znk]l nN\n(\nxn|µk, Λ−1\nk\n)\n+c o n s t. (10.54)\nWe observe that the right-hand side of this expression decomposes into a sum of\nterms involving only π together with terms only involving µ and Λ, which implies\nthat the variational posterior q(π, µ, Λ) factorizes to give q(π)q(µ, Λ). Further-\nmore, the terms involving µ and Λ themselves comprise a sum over k of terms\ninvolving µk and Λk leading to the further factorization\nq(π, µ, Λ)= q(π)\nK∏\nk=1\nq(µk, Λk). (10.55)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 496,
      "page_label": "477"
    }
  },
  {
    "page_content": "478 10. APPROXIMATE INFERENCE\nIdentifying the terms on the right-hand side of (10.54) that depend on π,w eh a v e\nlnq⋆ (π)=( α0 − 1)\nK∑\nk=1\nlnπk +\nK∑\nk=1\nN∑\nn=1\nrnk lnπk +c o n s t (10.56)\nwhere we have used (10.50). Taking the exponential of both sides, we recognize\nq⋆ (π) as a Dirichlet distribution\nq⋆ (π)=D i r (π|α) (10.57)\nwhere α has components αk given by\nαk = α0 + Nk. (10.58)\nFinally, the variational posterior distribution q⋆ (µk, Λk) does not factorize into\nthe product of the marginals, but we can always use the product rule to write it in the\nform q⋆ (µk, Λk)= q⋆ (µk|Λk)q⋆ (Λk). The two factors can be found by inspecting\n(10.54) and reading off those terms that involveµk and Λk. The result, as expected,\nis a Gaussian-Wishart distribution and is given byExercise 10.13\nq⋆ (µk, Λk)= N\n(\nµk|mk, (βkΛk)−1)\nW(Λk|Wk,ν k) (10.59)\nwhere we have deﬁned\nβk = β0 + Nk (10.60)\nmk = 1\nβk\n(β0m0 + Nkxk) (10.61)\nW−1\nk = W−1\n0 + NkSk + β0Nk\nβ0 + Nk\n(xk − m0)(xk − m0)T (10.62)\nνk = ν0 + Nk. (10.63)\nThese update equations are analogous to the M-step equations of the EM algorithm\nfor the maximum likelihood solution of the mixture of Gaussians. We see that the\ncomputations that must be performed in order to update the variational posterior\ndistribution over the model parameters involve evaluation of the same sums over the\ndata set, as arose in the maximum likelihood treatment.\nIn order to perform this variational M step, we need the expectations E[znk]=",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 497,
      "page_label": "478"
    }
  },
  {
    "page_content": "data set, as arose in the maximum likelihood treatment.\nIn order to perform this variational M step, we need the expectations E[znk]=\nrnk representing the responsibilities. These are obtained by normalizing theρnk that\nare given by (10.46). We see that this expression involves expectations with respect\nto the variational distributions of the parameters, and these are easily evaluated to\ngiveExercise 10.14\nEµk,Λk\n[\n(xn − µk)TΛk(xn − µk)\n]\n= Dβ−1\nk + νk(xn − mk)TWk(xn − mk) (10.64)\nln ˜Λk ≡ E [ln|Λk|]=\nD∑\ni=1\nψ\n( νk +1 − i\n2\n)\n+ D ln 2 + ln|Wk| (10.65)\nln ˜πk ≡ E [lnπk]= ψ(αk) − ψ(ˆα) (10.66)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 497,
      "page_label": "478"
    }
  },
  {
    "page_content": "10.2. Illustration: Variational Mixture of Gaussians 479\nwhere we have introduced deﬁnitions of˜Λk and ˜πk, and ψ(·) is the digamma function\ndeﬁned by (B.25), with ˆα = ∑\nk αk. The results (10.65) and (10.66) follow from\nthe standard properties of the Wishart and Dirichlet distributions.Appendix B\nIf we substitute (10.64), (10.65), and (10.66) into (10.46) and make use of\n(10.49), we obtain the following result for the responsibilities\nrnk ∝ ˜πk ˜Λ1/2\nk exp\n{\n− D\n2βk\n− νk\n2 (xn − mk)TWk(xn − mk)\n}\n. (10.67)\nNotice the similarity to the corresponding result for the responsibilities in maximum\nlikelihood EM, which from (9.13) can be written in the form\nrnk ∝ πk|Λk|1/2 exp\n{\n−1\n2(xn − µk)TΛk(xn − µk)\n}\n(10.68)\nwhere we have used the precision in place of the covariance to highlight the similarity\nto (10.67).\nThus the optimization of the variational posterior distribution involves cycling\nbetween two stages analogous to the E and M steps of the maximum likelihood EM\nalgorithm. In the variational equivalent of the E step, we use the current distributions\nover the model parameters to evaluate the moments in (10.64), (10.65), and (10.66)\nand hence evaluateE[znk]= rnk. Then in the subsequent variational equivalent\nof the M step, we keep these responsibilities ﬁxed and use them to re-compute the\nvariational distribution over the parameters using (10.57) and (10.59). In each case,\nwe see that the variational posterior distribution has the same functional form as the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 498,
      "page_label": "479"
    }
  },
  {
    "page_content": "we see that the variational posterior distribution has the same functional form as the\ncorresponding factor in the joint distribution (10.41). This is a general result and is\na consequence of the choice of conjugate distributions.Section 10.4.1\nFigure 10.6 shows the results of applying this approach to the rescaled Old Faith-\nful data set for a Gaussian mixture model having K =6 components. We see that\nafter convergence, there are only two components for which the expected values\nof the mixing coefﬁcients are numerically distinguishable from their prior values.\nThis effect can be understood qualitatively in terms of the automatic trade-off in a\nBayesian model between ﬁtting the data and the complexity of the model, in whichSection 3.4\nthe complexity penalty arises from components whose parameters are pushed away\nfrom their prior values. Components that take essentially no responsibility for ex-\nplaining the data points havernk ≃ 0 and hence Nk ≃ 0. From (10.58), we see\nthat αk ≃ α0 and from (10.60)–(10.63) we see that the other parameters revert to\ntheir prior values. In principle such components are ﬁtted slightly to the data points,\nbut for broad priors this effect is too small to be seen numerically. For the varia-\ntional Gaussian mixture model the expected values of the mixing coefﬁcients in the\nposterior distribution are given byExercise 10.15\nE[πk]= αk + Nk\nKα0 + N . (10.69)\nConsider a component for which Nk ≃ 0 and αk ≃ α0. If the prior is broad so that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 498,
      "page_label": "479"
    }
  },
  {
    "page_content": "E[πk]= αk + Nk\nKα0 + N . (10.69)\nConsider a component for which Nk ≃ 0 and αk ≃ α0. If the prior is broad so that\nα0 → 0, then E[πk] → 0 and the component plays no role in the model, whereas if",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 498,
      "page_label": "479"
    }
  },
  {
    "page_content": "480 10. APPROXIMATE INFERENCE\nFigure 10.6 Variational Bayesian\nmixture of K =6 Gaussians ap-\nplied to the Old Faithful data set, in\nwhich the ellipses denote the one\nstandard-deviation density contours\nfor each of the components, and the\ndensity of red ink inside each ellipse\ncorresponds to the mean value of\nthe mixing coefﬁcient for each com-\nponent. The number in the top left\nof each diagram shows the num-\nber of iterations of variational infer-\nence. Components whose expected\nmixing coefﬁcient are numerically in-\ndistinguishable from zero are not\nplotted.\n0 15\n60 120\nthe prior tightly constrains the mixing coefﬁcients so that α0 →∞ , then E[πk] →\n1/K.\nIn Figure 10.6, the prior over the mixing coefﬁcients is a Dirichlet of the form\n(10.39). Recall from Figure 2.5 that for α0 < 1 the prior favours solutions in which\nsome of the mixing coefﬁcients are zero. Figure 10.6 was obtained usingα0 =1 0−3,\nand resulted in two components having nonzero mixing coefﬁcients. If instead we\nchoose α0 =1 we obtain three components with nonzero mixing coefﬁcients, and\nfor α =1 0 all six components have nonzero mixing coefﬁcients.\nAs we have seen there is a close similarity between the variational solution for\nthe Bayesian mixture of Gaussians and the EM algorithm for maximum likelihood.\nIn fact if we consider the limitN →∞ then the Bayesian treatment converges to the\nmaximum likelihood EM algorithm. For anything other than very small data sets,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 499,
      "page_label": "480"
    }
  },
  {
    "page_content": "maximum likelihood EM algorithm. For anything other than very small data sets,\nthe dominant computational cost of the variational algorithm for Gaussian mixtures\narises from the evaluation of the responsibilities, together with the evaluation and\ninversion of the weighted data covariance matrices. These computations mirror pre-\ncisely those that arise in the maximum likelihood EM algorithm, and so there is little\ncomputational overhead in using this Bayesian approach as compared to the tradi-\ntional maximum likelihood one. There are, however, some substantial advantages.\nFirst of all, the singularities that arise in maximum likelihood when a Gaussian com-\nponent ‘collapses’ onto a speciﬁc data point are absent in the Bayesian treatment.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 499,
      "page_label": "480"
    }
  },
  {
    "page_content": "10.2. Illustration: Variational Mixture of Gaussians 481\nIndeed, these singularities are removed if we simply introduce a prior and then use a\nMAP estimate instead of maximum likelihood. Furthermore, there is no over-ﬁtting\nif we choose a large numberK of components in the mixture, as we saw in Fig-\nure 10.6. Finally, the variational treatment opens up the possibility of determining\nthe optimal number of components in the mixture without resorting to techniques\nsuch as cross validation.Section 10.2.4\n10.2.2 Variational lower bound\nWe can also straightforwardly evaluate the lower bound (10.3) for this model.\nIn practice, it is useful to be able to monitor the bound during the re-estimation in\norder to test for convergence. It can also provide a valuable check on both the math-\nematical expressions for the solutions and their software implementation, because at\neach step of the iterative re-estimation procedure the value of this bound should not\ndecrease. We can take this a stage further to provide a deeper test of the correctness\nof both the mathematical derivation of the update equations and of their software im-\nplementation by using ﬁnite differences to check that each update does indeed give\na (constrained) maximum of the bound (Svens´en and Bishop, 2004).\nFor the variational mixture of Gaussians, the lower bound (10.3) is given by\nL =\n∑\nZ\n∫∫∫\nq(Z, π, µ, Λ)l n\n{p(X, Z, π, µ, Λ)\nq(Z, π, µ, Λ)\n}\ndπ dµdΛ\n= E[lnp(X, Z, π, µ, Λ)] − E[lnq(Z, π, µ, Λ)]",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 500,
      "page_label": "481"
    }
  },
  {
    "page_content": "L =\n∑\nZ\n∫∫∫\nq(Z, π, µ, Λ)l n\n{p(X, Z, π, µ, Λ)\nq(Z, π, µ, Λ)\n}\ndπ dµdΛ\n= E[lnp(X, Z, π, µ, Λ)] − E[lnq(Z, π, µ, Λ)]\n= E[lnp(X|Z, µ, Λ)] +E[lnp(Z|π)] +E[lnp(π)] +E[lnp(µ, Λ)]\n−E[lnq(Z)] − E[lnq(π)] − E[lnq(µ, Λ)] (10.70)\nwhere, to keep the notation uncluttered, we have omitted the ⋆ superscript on the\nq distributions, along with the subscripts on the expectation operators because each\nexpectation is taken with respect to all of the random variables in its argument. The\nvarious terms in the bound are easily evaluated to give the following resultsExercise 10.16\nE[lnp(X|Z, µ, Λ)] = 1\n2\nK∑\nk=1\nNk\n{\nln ˜Λk − Dβ−1\nk − νkTr(SkWk)\n−νk(xk − mk)TWk(xk − mk) − D ln(2π)\n}\n(10.71)\nE[lnp(Z|π)] =\nN∑\nn=1\nK∑\nk=1\nrnk ln ˜πk (10.72)\nE[lnp(π)] = ln C(α0)+( α0 − 1)\nK∑\nk=1\nln ˜πk (10.73)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 500,
      "page_label": "481"
    }
  },
  {
    "page_content": "482 10. APPROXIMATE INFERENCE\nE[lnp(µ, Λ)] = 1\n2\nK∑\nk=1\n{\nD ln(β0/2π)+l n ˜Λk − Dβ0\nβk\n−β0νk(mk − m0)TWk(mk − m0)\n}\n+ K lnB(W0,ν 0)\n+(ν0 − D − 1)\n2\nK∑\nk=1\nln ˜Λk − 1\n2\nK∑\nk=1\nνkTr(W−1\n0 Wk) (10.74)\nE[lnq(Z)] =\nN∑\nn=1\nK∑\nk=1\nrnk lnrnk (10.75)\nE[lnq(π)] =\nK∑\nk=1\n(αk − 1) ln˜πk +l nC(α) (10.76)\nE[lnq(µ, Λ)] =\nK∑\nk=1\n{1\n2 ln ˜Λk + D\n2 ln\n( βk\n2π\n)\n− D\n2 − H[ q(Λk)]\n}\n(10.77)\nwhere D is the dimensionality of x, H[q(Λk)] is the entropy of the Wishart distribu-\ntion given by (B.82), and the coefﬁcients C(α) and B(W,ν ) are deﬁned by (B.23)\nand (B.79), respectively. Note that the terms involving expectations of the logs of the\nqdistributions simply represent the negative entropies of those distributions. Some\nsimpliﬁcations and combination of terms can be performed when these expressions\nare summed to give the lower bound. However, we have kept the expressions sepa-\nrate for ease of understanding.\nFinally, it is worth noting that the lower bound provides an alternative approach\nfor deriving the variational re-estimation equations obtained in Section 10.2.1. To do\nthis we use the fact that, since the model has conjugate priors, the functional form of\nthe factors in the variational posterior distribution is known, namely discrete for Z,\nDirichlet for π, and Gaussian-Wishart for (µk, Λk). By taking general parametric\nforms for these distributions we can derive the form of the lower bound as a function\nof the parameters of the distributions. Maximizing the bound with respect to these",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 501,
      "page_label": "482"
    }
  },
  {
    "page_content": "of the parameters of the distributions. Maximizing the bound with respect to these\nparameters then gives the required re-estimation equations.Exercise 10.18\n10.2.3 Predictive density\nIn applications of the Bayesian mixture of Gaussians model we will often be\ninterested in the predictive density for a new value ˆx of the observed variable. As-\nsociated with this observation will be a corresponding latent variable ˆz, and the pre-\ndictive density is then given by\np(ˆx|X)=\n∑\nbz\n∫∫∫\np(ˆx|ˆz, µ, Λ)p(ˆz|π)p(π, µ, Λ|X)d π dµ dΛ (10.78)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 501,
      "page_label": "482"
    }
  },
  {
    "page_content": "10.2. Illustration: Variational Mixture of Gaussians 483\nwhere p(π, µ, Λ|X) is the (unknown) true posterior distribution of the parameters.\nUsing (10.37) and (10.38) we can ﬁrst perform the summation over ˆz to give\np(ˆx|X)=\nK∑\nk=1\n∫∫∫\nπkN\n(\nˆx|µk, Λ−1\nk\n)\np(π, µ, Λ|X)d π dµdΛ. (10.79)\nBecause the remaining integrations are intractable, we approximate the predictive\ndensity by replacing the true posterior distribution p(π, µ, Λ|X) with its variational\napproximation q(π)q(µ, Λ) to give\np(ˆx|X)=\nK∑\nk=1\n∫∫∫\nπkN\n(ˆx|µk, Λ−1\nk\n)\nq(π)q(µk, Λk)d π dµk dΛk (10.80)\nwhere we have made use of the factorization (10.55) and in each term we have im-\nplicitly integrated out all variables {µj, Λj} for j ̸=k The remaining integrations\ncan now be evaluated analytically giving a mixture of Student’s t-distributionsExercise 10.19\np(ˆx|X)= 1\nˆα\nK∑\nk=1\nαkSt(ˆx|mk, Lk,ν k +1 − D) (10.81)\nin which the kth component has mean mk, and the precision is given by\nLk = (νk +1 − D)βk\n(1 +βk) Wk (10.82)\nin which νk is given by (10.63). When the sizeN of the data set is large the predictive\ndistribution (10.81) reduces to a mixture of Gaussians.Exercise 10.20\n10.2.4 Determining the number of components\nWe have seen that the variational lower bound can be used to determine a pos-\nterior distribution over the number K of components in the mixture model. ThereSection 10.1.4\nis, however, one subtlety that needs to be addressed. For any given setting of the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 502,
      "page_label": "483"
    }
  },
  {
    "page_content": "is, however, one subtlety that needs to be addressed. For any given setting of the\nparameters in a Gaussian mixture model (except for speciﬁc degenerate settings),\nthere will exist other parameter settings for which the density over the observed vari-\nables will be identical. These parameter values differ only through a re-labelling of\nthe components. For instance, consider a mixture of two Gaussians and a single ob-\nserved variablex, in which the parameters have the values π1 = a, π2 = b, µ1 = c,\nµ2 = d, σ1 = e, σ2 = f. Then the parameter values π1 = b, π2 = a, µ1 = d,\nµ2 = c, σ1 = f, σ2 = e, in which the two components have been exchanged, will\nby symmetry give rise to the same value of p(x). If we have a mixture model com-\nprising K components, then each parameter setting will be a member of a family of\nK! equivalent settings.Exercise 10.21\nIn the context of maximum likelihood, this redundancy is irrelevant because the\nparameter optimization algorithm (for example EM) will, depending on the initial-\nization of the parameters, ﬁnd one speciﬁc solution, and the other equivalent solu-\ntions play no role. In a Bayesian setting, however, we marginalize over all possible",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 502,
      "page_label": "483"
    }
  },
  {
    "page_content": "484 10. APPROXIMATE INFERENCE\nFigure 10.7 Plot of the variational lower bound\nL versus the number K of com-\nponents in the Gaussian mixture\nmodel, for the Old Faithful data,\nshowing a distinct peak at K =\n2 components. For each value\nof K, the model is trained from\n100 different random starts, and\nthe results shown as ‘+’ symbols\nplotted with small random hori-\nzontal perturbations so that they\ncan be distinguished. Note that\nsome solutions ﬁnd suboptimal\nlocal maxima, but that this hap-\npens infrequently.\nK\np(D|K)\n1 2 3 4 5 6\nparameter values. We have seen in Figure 10.2 that if the true posterior distribution\nis multimodal, variational inference based on the minimization ofKL(q∥p) will tend\nto approximate the distribution in the neighbourhood of one of the modes and ignore\nthe others. Again, because equivalent modes have equivalent predictive densities,\nthis is of no concern provided we are considering a model having a speciﬁc number\nK of components. If, however, we wish to compare different values of K, then we\nneed to take account of this multimodality. A simple approximate solution is to add\na term lnK! onto the lower bound when used for model comparison and averaging.Exercise 10.22\nFigure 10.7 shows a plot of the lower bound, including the multimodality fac-\ntor, versus the number K of components for the Old Faithful data set. It is worth\nemphasizing once again that maximum likelihood would lead to values of the likeli-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 503,
      "page_label": "484"
    }
  },
  {
    "page_content": "emphasizing once again that maximum likelihood would lead to values of the likeli-\nhood function that increase monotonically with K (assuming the singular solutions\nhave been avoided, and discounting the effects of local maxima) and so cannot be\nused to determine an appropriate model complexity. By contrast, Bayesian inference\nautomatically makes the trade-off between model complexity and ﬁtting the data.Section 3.4\nThis approach to the determination of K requires that a range of models having\ndifferent K values be trained and compared. An alternative approach to determining\na suitable value for K is to treat the mixing coefﬁcients π as parameters and make\npoint estimates of their values by maximizing the lower bound (Corduneanu and\nBishop, 2001) with respect to π instead of maintaining a probability distribution\nover them as in the fully Bayesian approach. This leads to the re-estimation equationExercise 10.23\nπk = 1\nN\nN∑\nn=1\nrnk (10.83)\nand this maximization is interleaved with the variational updates for theq distribution\nover the remaining parameters. Components that provide insufﬁcient contribution",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 503,
      "page_label": "484"
    }
  },
  {
    "page_content": "10.2. Illustration: Variational Mixture of Gaussians 485\nto explaining the data will have their mixing coefﬁcients driven to zero during the\noptimization, and so they are effectively removed from the model throughautomatic\nrelevance determination. This allows us to make a single training run in which we\nstart with a relatively large initial value of K, and allow surplus components to be\npruned out of the model. The origins of the sparsity when optimizing with respect to\nhyperparameters is discussed in detail in the context of the relevance vector machine.Section 7.2.2\n10.2.5 Induced factorizations\nIn deriving these variational update equations for the Gaussian mixture model,\nwe assumed a particular factorization of the variational posterior distribution given\nby (10.42). However, the optimal solutions for the various factors exhibit additional\nfactorizations. In particular, the solution for q⋆ (µ, Λ) is given by the product of an\nindependent distribution q⋆ (µk, Λk) over each of the components k of the mixture,\nwhereas the variational posterior distribution q⋆ (Z) over the latent variables, given\nby (10.48), factorizes into an independent distributionq⋆ (zn) for each observation n\n(note that it does not further factorize with respect to k because, for each value of n,\nthe znk are constrained to sum to one over k). These additional factorizations are a\nconsequence of the interaction between the assumed factorization and the conditional",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 504,
      "page_label": "485"
    }
  },
  {
    "page_content": "consequence of the interaction between the assumed factorization and the conditional\nindependence properties of the true distribution, as characterized by the directed\ngraph in Figure 10.5.\nWe shall refer to these additional factorizations as induced factorizations be-\ncause they arise from an interaction between the factorization assumed in the varia-\ntional posterior distribution and the conditional independence properties of the true\njoint distribution. In a numerical implementation of the variational approach it is\nimportant to take account of such additional factorizations. For instance, it would\nbe very inefﬁcient to maintain a full precision matrix for the Gaussian distribution\nover a set of variables if the optimal form for that distribution always had a diago-\nnal precision matrix (corresponding to a factorization with respect to the individual\nvariables described by that Gaussian).\nSuch induced factorizations can easily be detected using a simple graphical test\nbased on d-separation as follows. We partition the latent variables into three disjoint\ngroups A, B, C and then let us suppose that we are assuming a factorization between\nC and the remaining latent variables, so that\nq(A, B, C)= q(A, B)q(C). (10.84)\nUsing the general result (10.9), together with the product rule for probabilities, we\nsee that the optimal solution for q(A, B) is given by\nlnq⋆ (A, B)= EC[lnp(X, A, B, C)] + const\n= EC[lnp(A, B|X, C)] + const. (10.85)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 504,
      "page_label": "485"
    }
  },
  {
    "page_content": "see that the optimal solution for q(A, B) is given by\nlnq⋆ (A, B)= EC[lnp(X, A, B, C)] + const\n= EC[lnp(A, B|X, C)] + const. (10.85)\nWe now ask whether this resulting solution will factorize between A and B,i n\nother words whether q⋆ (A, B) = q⋆ (A)q⋆ (B). This will happen if, and only if,\nlnp(A, B|X, C)=l n p(A|X, C)+l n p(B|X, C), that is, if the conditional inde-\npendence relation\nA ⊥⊥ B | X, C (10.86)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 504,
      "page_label": "485"
    }
  },
  {
    "page_content": "486 10. APPROXIMATE INFERENCE\nis satisﬁed. We can test to see if this relation does hold, for any choice of A and B\nby making use of the d-separation criterion.\nTo illustrate this, consider again the Bayesian mixture of Gaussians represented\nby the directed graph in Figure 10.5, in which we are assuming a variational fac-\ntorization given by (10.42). We can see immediately that the variational posterior\ndistribution over the parameters must factorize betweenπ and the remaining param-\neters µ and Λ because all paths connecting π to either µ or Λ must pass through\none of the nodes zn all of which are in the conditioning set for our conditional inde-\npendence test and all of which are head-to-tail with respect to such paths.\n10.3. Variational Linear Regression\nAs a second illustration of variational inference, we return to the Bayesian linear\nregression model of Section 3.3. In the evidence framework, we approximated the\nintegration over α and β by making point estimates obtained by maximizing the log\nmarginal likelihood. A fully Bayesian approach would integrate over the hyperpa-\nrameters as well as over the parameters. Although exact integration is intractable,\nwe can use variational methods to ﬁnd a tractable approximation. In order to sim-\nplify the discussion, we shall suppose that the noise precision parameterβ is known,\nand is ﬁxed to its true value, although the framework is easily extended to include",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 505,
      "page_label": "486"
    }
  },
  {
    "page_content": "and is ﬁxed to its true value, although the framework is easily extended to include\nthe distribution overβ. For the linear regression model, the variational treatmentExercise 10.26\nwill turn out to be equivalent to the evidence framework. Nevertheless, it provides a\ngood exercise in the use of variational methods and will also lay the foundation for\nvariational treatment of Bayesian logistic regression in Section 10.6.\nRecall that the likelihood function for w, and the prior over w, are given by\np(t|w)=\nN∏\nn=1\nN(tn|wTφn,β −1) (10.87)\np(w|α)= N(w|0,α −1I) (10.88)\nwhere φn = φ(xn). We now introduce a prior distribution over α. From our dis-\ncussion in Section 2.3.6, we know that the conjugate prior for the precision of a\nGaussian is given by a gamma distribution, and so we choose\np(α)=G a m (α|a0,b 0) (10.89)\nwhere Gam(·|·, ·) is deﬁned by (B.26). Thus the joint distribution of all the variables\nis given by\np(t, w,α )= p(t|w)p(w|α)p(α). (10.90)\nThis can be represented as a directed graphical model as shown in Figure 10.8.\n10.3.1 Variational distribution\nOur ﬁrst goal is to ﬁnd an approximation to the posterior distribution p(w,α |t).\nTo do this, we employ the variational framework of Section 10.1, with a variational",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 505,
      "page_label": "486"
    }
  },
  {
    "page_content": "10.3. Variational Linear Regression 487\nFigure 10.8 Probabilistic graphical model representing the joint dis-\ntribution (10.90) for the Bayesian linear regression\nmodel.\ntn\nφn\nN\nw\nα\nβ\nposterior distribution given by the factorized expression\nq(w,α )= q(w)q(α). (10.91)\nWe can ﬁnd re-estimation equations for the factors in this distribution by making use\nof the general result (10.9). Recall that for each factor, we take the log of the joint\ndistribution over all variables and then average with respect to those variables not in\nthat factor. Consider ﬁrst the distribution over α. Keeping only terms that have a\nfunctional dependence on α,w eh a v e\nlnq⋆ (α)=l n p(α)+ Ew [lnp(w|α) ]+c o n s t\n=( a0 − 1) lnα − b0α + M\n2 lnα − α\n2 E[wTw]+c o n s t. (10.92)\nWe recognize this as the log of a gamma distribution, and so identifying the coefﬁ-\ncients of α and lnα we obtain\nq⋆ (α)=G a m (α|aN ,b N ) (10.93)\nwhere\naN = a0 + M\n2 (10.94)\nbN = b0 + 1\n2E[wTw]. (10.95)\nSimilarly, we can ﬁnd the variational re-estimation equation for the posterior\ndistribution over w. Again, using the general result (10.9), and keeping only those\nterms that have a functional dependence on w,w eh a v e\nlnq⋆ (w)=l n p(t|w)+ Eα [lnp(w|α)] + const (10.96)\n= −β\n2\nN∑\nn=1\n{wTφn − tn}2 − 1\n2E[α]wTw +c o n s t (10.97)\n= −1\n2wT (\nE[α]I + βΦTΦ\n)\nw + βwTΦTt +c o n s t. (10.98)\nBecause this is a quadratic form, the distribution q⋆ (w) is Gaussian, and so we can",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 506,
      "page_label": "487"
    }
  },
  {
    "page_content": "= −1\n2wT (\nE[α]I + βΦTΦ\n)\nw + βwTΦTt +c o n s t. (10.98)\nBecause this is a quadratic form, the distribution q⋆ (w) is Gaussian, and so we can\ncomplete the square in the usual way to identify the mean and covariance, giving\nq⋆ (w)= N(w|mN , SN ) (10.99)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 506,
      "page_label": "487"
    }
  },
  {
    "page_content": "488 10. APPROXIMATE INFERENCE\nwhere\nmN = βSN ΦTt (10.100)\nSN =\n(\nE[α]I + βΦTΦ\n)−1\n. (10.101)\nNote the close similarity to the posterior distribution (3.52) obtained when α was\ntreated as a ﬁxed parameter. The difference is that here α is replaced by its expecta-\ntion E[α] under the variational distribution. Indeed, we have chosen to use the same\nnotation for the covariance matrix SN in both cases.\nUsing the standard results (B.27), (B.38), and (B.39), we can obtain the required\nmoments as follows\nE[α]= aN /bN (10.102)\nE[wwT]= mN mT\nN + SN . (10.103)\nThe evaluation of the variational posterior distribution begins by initializing the pa-\nrameters of one of the distributions q(w) or q(α), and then alternately re-estimates\nthese factors in turn until a suitable convergence criterion is satisﬁed (usually speci-\nﬁed in terms of the lower bound to be discussed shortly).\nIt is instructive to relate the variational solution to that found using the evidence\nframework in Section 3.5. To do this consider the case a0 = b0 =0 , corresponding\nto the limit of an inﬁnitely broad prior over α. The mean of the variational posterior\ndistribution q(α) is then given by\nE[α]= aN\nbN\n= M/2\nE[wTw]/2 = M\nmT\nN mN + Tr(SN ). (10.104)\nComparison with (9.63) shows that in the case of this particularly simple model,\nthe variational approach gives precisely the same expression as that obtained by\nmaximizing the evidence function using EM except that the point estimate for α",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 507,
      "page_label": "488"
    }
  },
  {
    "page_content": "maximizing the evidence function using EM except that the point estimate for α\nis replaced by its expected value. Because the distribution q(w) depends on q(α)\nonly through the expectationE[α], we see that the two approaches will give identical\nresults for the case of an inﬁnitely broad prior.\n10.3.2 Predictive distribution\nThe predictive distribution over t, given a new input x, is easily evaluated for\nthis model using the Gaussian variational posterior for the parameters\np(t|x, t)=\n∫\np(t|x, w)p(w|t)d w\n≃\n∫\np(t|x, w)q(w)d w\n=\n∫\nN(t|wTφ(x),β −1)N(w|mN , SN )d w\n= N(t|mT\nN φ(x),σ 2(x)) (10.105)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 507,
      "page_label": "488"
    }
  },
  {
    "page_content": "10.3. Variational Linear Regression 489\nwhere we have evaluated the integral by making use of the result (2.115) for the\nlinear-Gaussian model. Here the input-dependent variance is given by\nσ2(x)= 1\nβ + φ(x)TSN φ(x). (10.106)\nNote that this takes the same form as the result (3.59) obtained with ﬁxed α except\nthat now the expected value E[α] appears in the deﬁnition of SN .\n10.3.3 Lower bound\nAnother quantity of importance is the lower bound L deﬁned by\nL(q)= E[lnp(w,α ,t)] − E[lnq(w,α )]\n= Ew[lnp(t|w)] +Ew,α[lnp(w|α)] +Eα[lnp(α)]\n−Eα[ln q(w)]w − E[lnq(α)]. (10.107)\nEvaluation of the various terms is straightforward, making use of results obtained inExercise 10.27\nprevious chapters, and gives\nE[lnp(t|w)]w = N\n2 ln\n( β\n2π\n)\n− β\n2 tTt + βmT\nN ΦTt\n−β\n2 Tr\n[\nΦTΦ(mN mT\nN + SN )\n]\n(10.108)\nE[lnp(w|α)]w,α = −M\n2 ln(2π)+ M\n2 (ψ(aN ) − lnbN )\n− aN\n2bN\n[\nmT\nN mN + Tr(SN )\n]\n(10.109)\nE[lnp(α)]α = a0 ln b0 +( a0 − 1) [ψ(aN ) − lnbN ]\n−b0\naN\nbN\n− ln Γ(aN ) (10.110)\n−E[lnq(w)]w = 1\n2 ln |SN | + M\n2 [1 + ln(2π)] (10.111)\n−E[lnq(α)]α =l n Γ (aN ) − (aN − 1)ψ(aN ) − lnbN + aN . (10.112)\nFigure 10.9 shows a plot of the lower bound L(q) versus the degree of a polynomial\nmodel for a synthetic data set generated from a degree three polynomial. Here the\nprior parameters have been set toa0 = b0 =0 , corresponding to the noninformative\nprior p(α) ∝ 1/α, which is uniform over lnα as discussed in Section 2.3.6. As\nwe saw in Section 10.1, the quantity L represents lower bound on the log marginal",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 508,
      "page_label": "489"
    }
  },
  {
    "page_content": "we saw in Section 10.1, the quantity L represents lower bound on the log marginal\nlikelihood p(t|M) for the model. If we assign equal prior probabilities p(M) to the\ndifferent values of M, then we can interpret L as an approximation to the poste-\nrior model probability p(M|t). Thus the variational framework assigns the highest\nprobability to the model with M =3 . This should be contrasted with the maximum\nlikelihood result, which assigns ever smaller residual error to models of increasing\ncomplexity until the residual error is driven to zero, causing maximum likelihood to\nfavour severely over-ﬁtted models.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 508,
      "page_label": "489"
    }
  },
  {
    "page_content": "490 10. APPROXIMATE INFERENCE\nFigure 10.9 Plot of the lower bound L ver-\nsus the order M of the polyno-\nmial, for a polynomial model, in\nwhich a set of 10 data points is\ngenerated from a polynomial with\nM =3 sampled over the inter-\nval (−5, 5) with additive Gaussian\nnoise of variance 0.09. The value\nof the bound gives the log prob-\nability of the model, and we see\nthat the value of the bound peaks\nat M =3 , corresponding to the\ntrue model from which the data\nset was generated.\n1 3 5 7 9\n10.4. Exponential Family Distributions\nIn Chapter 2, we discussed the important role played by the exponential family of\ndistributions and their conjugate priors. For many of the models discussed in this\nbook, the complete-data likelihood is drawn from the exponential family. However,\nin general this will not be the case for the marginal likelihood function for the ob-\nserved data. For example, in a mixture of Gaussians, the joint distribution of obser-\nvations xn and corresponding hidden variables zn is a member of the exponential\nfamily, whereas the marginal distribution of xn is a mixture of Gaussians and hence\nis not.\nUp to now we have grouped the variables in the model into observed variables\nand hidden variables. We now make a further distinction between latent variables,\ndenoted Z, and parameters, denotedθ, where parameters areintensive (ﬁxed in num-\nber independent of the size of the data set), whereas latent variables are extensive",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 509,
      "page_label": "490"
    }
  },
  {
    "page_content": "ber independent of the size of the data set), whereas latent variables are extensive\n(scale in number with the size of the data set). For example, in a Gaussian mixture\nmodel, the indicator variables zkn (which specify which component k is responsible\nfor generating data point xn) represent the latent variables, whereas the means µk,\nprecisions Λk and mixing proportions πk represent the parameters.\nConsider the case of independent identically distributed data. We denote the\ndata values by X = {xn}, where n =1 ,...N , with corresponding latent variables\nZ = {zn}. Now suppose that the joint distribution of observed and latent variables\nis a member of the exponential family, parameterized by natural parametersη so that\np(X, Z|η)=\nN∏\nn=1\nh(xn, zn)g(η)e x p\n{\nηTu(xn, zn)\n}\n. (10.113)\nWe shall also use a conjugate prior for η, which can be written as\np(η|ν0, v0)= f(ν0, χ0)g(η)ν0 exp\n{\nνoηTχ0\n}\n. (10.114)\nRecall that the conjugate prior distribution can be interpreted as a prior number ν0\nof observations all having the value χ0 for the u vector. Now consider a variational",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 509,
      "page_label": "490"
    }
  },
  {
    "page_content": "10.4. Exponential Family Distributions 491\ndistribution that factorizes between the latent variables and the parameters, so that\nq(Z, η)= q(Z)q(η). Using the general result (10.9), we can solve for the two\nfactors as follows\nlnq⋆ (Z)= Eη[lnp(X, Z|η) ]+c o n s t\n=\nN∑\nn=1\n{\nln h(xn, zn)+ E[ηT]u(xn, zn)\n}\n+c o n s t. (10.115)\nThus we see that this decomposes into a sum of independent terms, one for each\nvalue of n, and hence the solution for q⋆ (Z) will factorize over n so that q⋆ (Z)=∏\nn q⋆ (zn). This is an example of an induced factorization. Taking the exponentialSection 10.2.5\nof both sides, we have\nq⋆ (zn)= h(xn, zn)g (E[η]) exp\n{\nE[ηT]u(xn, zn)\n}\n(10.116)\nwhere the normalization coefﬁcient has been re-instated by comparison with the\nstandard form for the exponential family.\nSimilarly, for the variational distribution over the parameters, we have\nln q⋆ (η)=l n p(η|ν0, χ0)+ EZ[lnp(X, Z|η) ]+c o n s t (10.117)\n= ν0 ln g(η)+ ηTχ0 +\nN∑\nn=1\n{\nlng(η)+ ηTEzn [u(xn, zn)]\n}\n+c o n s t. (10.118)\nAgain, taking the exponential of both sides, and re-instating the normalization coef-\nﬁcient by inspection, we have\nq⋆ (η)= f(νN , χN )g(η)νN exp\n{\nηTχN\n}\n(10.119)\nwhere we have deﬁned\nνN = ν0 + N (10.120)\nχN = χ0 +\nN∑\nn=1\nEzn [u(xn, zn)]. (10.121)\nNote that the solutions for q⋆ (zn) and q⋆ (η) are coupled, and so we solve them iter-\natively in a two-stage procedure. In the variational E step, we evaluate the expected",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 510,
      "page_label": "491"
    }
  },
  {
    "page_content": "atively in a two-stage procedure. In the variational E step, we evaluate the expected\nsufﬁcient statistics E[u(xn, zn)] using the current posterior distribution q(zn) over\nthe latent variables and use this to compute a revised posterior distributionq(η) over\nthe parameters. Then in the subsequent variational M step, we use this revised pa-\nrameter posterior distribution to ﬁnd the expected natural parameters E[ηT], which\ngives rise to a revised variational distribution over the latent variables.\n10.4.1 Variational message passing\nWe have illustrated the application of variational methods by considering a spe-\nciﬁc model, the Bayesian mixture of Gaussians, in some detail. This model can be",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 510,
      "page_label": "491"
    }
  },
  {
    "page_content": "492 10. APPROXIMATE INFERENCE\ndescribed by the directed graph shown in Figure 10.5. Here we consider more gen-\nerally the use of variational methods for models described by directed graphs and\nderive a number of widely applicable results.\nThe joint distribution corresponding to a directed graph can be written using the\ndecomposition\np(x)=\n∏\ni\np(xi|pai) (10.122)\nwhere xi denotes the variable(s) associated with node i, and pai denotes the parent\nset corresponding to node i. Note that xi may be a latent variable or it may belong\nto the set of observed variables. Now consider a variational approximation in which\nthe distribution q(x) is assumed to factorize with respect to the xi so that\nq(x)=\n∏\ni\nqi(xi). (10.123)\nNote that for observed nodes, there is no factor q(xi) in the variational distribution.\nWe now substitute (10.122) into our general result (10.9) to give\nlnq⋆\nj (xj)= Ei̸=j\n[∑\ni\nln p(xi|pai)\n]\n+c o n s t. (10.124)\nAny terms on the right-hand side that do not depend on xj can be absorbed into\nthe additive constant. In fact, the only terms that do depend on xj are the con-\nditional distribution for xj given by p(xj|paj) together with any other conditional\ndistributions that have xj in the conditioning set. By deﬁnition, these conditional\ndistributions correspond to the children of node j, and they therefore also depend on\nthe co-parents of the child nodes, i.e., the other parents of the child nodes besides",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 511,
      "page_label": "492"
    }
  },
  {
    "page_content": "the co-parents of the child nodes, i.e., the other parents of the child nodes besides\nnode xj itself. We see that the set of all nodes on whichq⋆ (xj) depends corresponds\nto the Markov blanket of node xj, as illustrated in Figure 8.26. Thus the update\nof the factors in the variational posterior distribution represents a local calculation\non the graph. This makes possible the construction of general purpose software for\nvariational inference in which the form of the model does not need to be speciﬁed in\nadvance (Bishop et al., 2003).\nIf we now specialize to the case of a model in which all of the conditional dis-\ntributions have a conjugate-exponential structure, then the variational update proce-\ndure can be cast in terms of a local message passing algorithm (Winn and Bishop,\n2005). In particular, the distribution associated with a particular node can be updated\nonce that node has received messages from all of its parents and all of its children.\nThis in turn requires that the children have already received messages from their co-\nparents. The evaluation of the lower bound can also be simpliﬁed because many of\nthe required quantities are already evaluated as part of the message passing scheme.\nThis distributed message passing formulation has good scaling properties and is well\nsuited to large networks.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 511,
      "page_label": "492"
    }
  },
  {
    "page_content": "10.5. Local Variational Methods 493\n10.5. Local Variational Methods\nThe variational framework discussed in Sections 10.1 and 10.2 can be considered a\n‘global’ method in the sense that it directly seeks an approximation to the full poste-\nrior distribution over all random variables. An alternative ‘local’ approach involves\nﬁnding bounds on functions over individual variables or groups of variables within\na model. For instance, we might seek a bound on a conditional distribution p(y|x),\nwhich is itself just one factor in a much larger probabilistic model speciﬁed by a\ndirected graph. The purpose of introducing the bound of course is to simplify the\nresulting distribution. This local approximation can be applied to multiple variables\nin turn until a tractable approximation is obtained, and in Section 10.6.1 we shall\ngive a practical example of this approach in the context of logistic regression. Here\nwe focus on developing the bounds themselves.\nWe have already seen in our discussion of the Kullback-Leibler divergence that\nthe convexity of the logarithm function played a key role in developing the lower\nbound in the global variational approach. We have deﬁned a (strictly) convex func-\ntion as one for which every chord lies above the function. Convexity also plays aSection 1.6.1\ncentral role in the local variational framework. Note that our discussion will ap-\nply equally to concave functions with ‘min’ and ‘max’ interchanged and with lower\nbounds replaced by upper bounds.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 512,
      "page_label": "493"
    }
  },
  {
    "page_content": "ply equally to concave functions with ‘min’ and ‘max’ interchanged and with lower\nbounds replaced by upper bounds.\nLet us begin by considering a simple example, namely the function f(x)=\nexp(−x), which is a convex function of x, and which is shown in the left-hand plot\nof Figure 10.10. Our goal is to approximate f(x) by a simpler function, in particular\na linear function of x. From Figure 10.10, we see that this linear function will be a\nlower bound on f(x) if it corresponds to a tangent. We can obtain the tangent line\ny(x) at a speciﬁc value of x, say x = ξ, by making a ﬁrst order Taylor expansion\ny(x)= f(ξ)+ f′(ξ)(x − ξ) (10.125)\nso that y(x) ⩽ f(x) with equality when x = ξ. For our example function f(x)=\nFigure 10.10 In the left-hand ﬁg-\nure the red curve shows the function\nexp(−x), and the blue line shows\nthe tangent at x = ξ deﬁned by\n(10.125) with ξ =1 . This line has\nslope λ = f′(ξ)= −exp(−ξ). Note\nthat any other tangent line, for ex-\nample the ones shown in green, will\nhave a smaller value of y at x =\nξ. The right-hand ﬁgure shows the\ncorresponding plot of the function\nλξ − g(λ), where g(λ) is given by\n(10.131), versus λ for ξ =1 ,i n\nwhich the maximum corresponds to\nλ = −exp(−ξ)= −1/e.\nxξ0 1.5 3\n0\n0.5\n1\nλ\nλξ − g(λ)\n−1 −0.5 0\n0\n0.2\n0.4",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 512,
      "page_label": "493"
    }
  },
  {
    "page_content": "494 10. APPROXIMATE INFERENCE\nx\ny\nf(x)\nλx\nx\ny\nf(x)\nλx − g(λ)\n−g(λ)\nFigure 10.11 In the left-hand plot the red curve shows a convex functionf(x), and the blue line represents the\nlinear function λx, which is a lower bound onf(x) because f(x) >λ x for all x. For the given value of slopeλ the\ncontact point of the tangent line having the same slope is found by minimizing with respect to x the discrepancy\n(shown by the green dashed lines) given by f(x) − λx. This deﬁnes the dual function g(λ), which corresponds\nto the (negative of the) intercept of the tangent line having slope λ.\nexp(−x), we therefore obtain the tangent line in the form\ny(x)=e x p (−ξ) − exp(−ξ)(x − ξ) (10.126)\nwhich is a linear function parameterized by ξ. For consistency with subsequent\ndiscussion, let us deﬁne λ = −exp(−ξ) so that\ny(x, λ)= λx − λ + λ ln(−λ). (10.127)\nDifferent values ofλ correspond to different tangent lines, and because all such lines\nare lower bounds on the function, we have f(x) ⩾ y(x, λ). Thus we can write the\nfunction in the form\nf(x)=m a x\nλ\n{λx − λ + λ ln(−λ)}. (10.128)\nWe have succeeded in approximating the convex functionf(x) by a simpler, lin-\near function y(x, λ). The price we have paid is that we have introduced a variational\nparameter λ, and to obtain the tightest bound we must optimize with respect to λ.\nWe can formulate this approach more generally using the framework of convex\nduality (Rockafellar, 1972; Jordan et al., 1999). Consider the illustration of a convex",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 513,
      "page_label": "494"
    }
  },
  {
    "page_content": "duality (Rockafellar, 1972; Jordan et al., 1999). Consider the illustration of a convex\nfunction f(x) shown in the left-hand plot in Figure 10.11. In this example, the\nfunction λx is a lower bound on f(x) but it is not the best lower bound that can\nbe achieved by a linear function having slope λ, because the tightest bound is given\nby the tangent line. Let us write the equation of the tangent line, having slope λ as\nλx − g(λ) where the (negative) intercept g(λ) clearly depends on the slope λ of the\ntangent. To determine the intercept, we note that the line must be moved vertically by\nan amount equal to the smallest vertical distance between the line and the function,\nas shown in Figure 10.11. Thus\ng(λ)= −min\nx\n{f(x) − λx}\n=m a x\nx\n{λx − f(x)}. (10.129)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 513,
      "page_label": "494"
    }
  },
  {
    "page_content": "10.5. Local Variational Methods 495\nNow, instead of ﬁxing λ and varying x, we can consider a particular x and then\nadjust λ until the tangent plane is tangent at that particular x. Because the y value\nof the tangent line at a particular x is maximized when that value coincides with its\ncontact point, we have\nf(x) = max\nλ\n{λx − g(λ)}. (10.130)\nWe see that the functions f(x) and g(λ) play a dual role, and are related through\n(10.129) and (10.130).\nLet us apply these duality relations to our simple example f(x)=e x p (−x).\nFrom (10.129) we see that the maximizing value ofx is given by ξ = −ln(−λ), and\nback-substituting we obtain the conjugate function g(λ) in the form\ng(λ)= λ − λ ln(−λ) (10.131)\nas obtained previously. The function λξ −g(λ) is shown, for ξ =1 in the right-hand\nplot in Figure 10.10. As a check, we can substitute (10.131) into (10.130), which\ngives the maximizing value of λ = −exp(−x), and back-substituting then recovers\nthe original function f(x)=e x p (−x).\nFor concave functions, we can follow a similar argument to obtain upper bounds,\nin which max’ is replaced with ‘min’, so that\nf(x) = min\nλ\n{λx − g(λ)} (10.132)\ng(λ) = min\nx\n{λx − f(x)}. (10.133)\nIf the function of interest is not convex (or concave), then we cannot directly\napply the method above to obtain a bound. However, we can ﬁrst seek invertible\ntransformations either of the function or of its argument which change it into a con-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 514,
      "page_label": "495"
    }
  },
  {
    "page_content": "transformations either of the function or of its argument which change it into a con-\nvex form. We then calculate the conjugate function and then transform back to the\noriginal variables.\nAn important example, which arises frequently in pattern recognition, is the\nlogistic sigmoid function deﬁned by\nσ(x)= 1\n1+ e−x . (10.134)\nAs it stands this function is neither convex nor concave. However, if we take the\nlogarithm we obtain a function which is concave, as is easily veriﬁed by ﬁnding the\nsecond derivative. From (10.133) the corresponding conjugate function then takesExercise 10.30\nthe form\ng(λ)=m i n\nx\n{λx − f(x)} = −λ ln λ − (1 − λ)l n ( 1− λ) (10.135)\nwhich we recognize as the binary entropy function for a variable whose probability\nof having the value 1 is λ. Using (10.132), we then obtain an upper bound on the logAppendix B\nsigmoid\nln σ(x) ⩽ λx − g(λ) (10.136)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 514,
      "page_label": "495"
    }
  },
  {
    "page_content": "496 10. APPROXIMATE INFERENCE\nλ =0 .2\nλ =0 .7\n−6 0 6\n0\n0.5\n1\nξ =2 .5\n−ξξ−6 0 6\n0\n0.5\n1\nFigure 10.12 The left-hand plot shows the logistic sigmoid function σ(x) deﬁned by (10.134) in red, together\nwith two examples of the exponential upper bound (10.137) shown in blue. The right-hand plot shows the logistic\nsigmoid again in red together with the Gaussian lower bound (10.144) shown in blue. Here the parameter\nξ =2 .5, and the bound is exact at x = ξ and x = −ξ, denoted by the dashed green lines.\nand taking the exponential, we obtain an upper bound on the logistic sigmoid itself\nof the form\nσ(x) ⩽ exp(λx − g(λ)) (10.137)\nwhich is plotted for two values of λ on the left-hand plot in Figure 10.12.\nWe can also obtain a lower bound on the sigmoid having the functional form of\na Gaussian. To do this, we follow Jaakkola and Jordan (2000) and make transforma-\ntions both of the input variable and of the function itself. First we take the log of the\nlogistic function and then decompose it so that\nln σ(x)= −ln(1 +e−x)= −ln\n{\ne−x/2(ex/2 + e−x/2)\n}\n= x/2 − ln(ex/2 + e−x/2). (10.138)\nWe now note that the function f(x)= −ln(ex/2 + e−x/2) is a convex function of\nthe variable x2, as can again be veriﬁed by ﬁnding the second derivative. This leadsExercise 10.31\nto a lower bound on f(x), which is a linear function of x2 whose conjugate function\nis given by\ng(λ)=m a x\nx2\n{\nλx2 − f\n(√\nx2\n)}\n. (10.139)\nThe stationarity condition leads to\n0= λ − dx\ndx2\nd\ndxf(x)= λ + 1\n4x tanh\n(x\n2\n)\n. (10.140)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 515,
      "page_label": "496"
    }
  },
  {
    "page_content": "is given by\ng(λ)=m a x\nx2\n{\nλx2 − f\n(√\nx2\n)}\n. (10.139)\nThe stationarity condition leads to\n0= λ − dx\ndx2\nd\ndxf(x)= λ + 1\n4x tanh\n(x\n2\n)\n. (10.140)\nIf we denote this value of x, corresponding to the contact point of the tangent line\nfor this particular value of λ,b y ξ, then we have\nλ(ξ)= − 1\n4ξ tanh\n( ξ\n2\n)\n= − 1\n2ξ\n[\nσ(ξ) − 1\n2\n]\n. (10.141)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 515,
      "page_label": "496"
    }
  },
  {
    "page_content": "10.5. Local Variational Methods 497\nInstead of thinking of λ as the variational parameter, we can let ξ play this role as\nthis leads to simpler expressions for the conjugate function, which is then given by\ng(λ)= λ(ξ)ξ2 − f(ξ)= λ(ξ)ξ2 +l n (eξ/2 + e−ξ/2). (10.142)\nHence the bound on f(x) can be written as\nf(x) ⩾ λx2 − g(λ)= λx2 − λξ2 − ln(eξ/2 + e−ξ/2). (10.143)\nThe bound on the sigmoid then becomes\nσ(x) ⩾ σ(ξ)e x p\n{\n(x − ξ)/2 − λ(ξ)(x2 − ξ2)\n}\n(10.144)\nwhere λ(ξ) is deﬁned by (10.141). This bound is illustrated in the right-hand plot of\nFigure 10.12. We see that the bound has the form of the exponential of a quadratic\nfunction of x, which will prove useful when we seek Gaussian representations of\nposterior distributions deﬁned through logistic sigmoid functions.Section 4.5\nThe logistic sigmoid arises frequently in probabilistic models over binary vari-\nables because it is the function that transforms a log odds ratio into a posterior prob-\nability. The corresponding transformation for a multiclass distribution is given by\nthe softmax function. Unfortunately, the lower bound derived here for the logisticSection 4.3\nsigmoid does not directly extend to the softmax. Gibbs (1997) proposes a method\nfor constructing a Gaussian distribution that is conjectured to be a bound (although\nno rigorous proof is given), which may be used to apply local variational methods to\nmulticlass problems.\nWe shall see an example of the use of local variational bounds in Sections 10.6.1.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 516,
      "page_label": "497"
    }
  },
  {
    "page_content": "multiclass problems.\nWe shall see an example of the use of local variational bounds in Sections 10.6.1.\nFor the moment, however, it is instructive to consider in general terms how these\nbounds can be used. Suppose we wish to evaluate an integral of the form\nI =\n∫\nσ(a)p(a)d a (10.145)\nwhere σ(a) is the logistic sigmoid, and p(a) is a Gaussian probability density. Such\nintegrals arise in Bayesian models when, for instance, we wish to evaluate the pre-\ndictive distribution, in which case p(a) represents a posterior parameter distribution.\nBecause the integral is intractable, we employ the variational bound (10.144), which\nwe write in the form σ(a) ⩾ f(a, ξ) where ξ is a variational parameter. The inte-\ngral now becomes the product of two exponential-quadratic functions and so can be\nintegrated analytically to give a bound onI\nI ⩾\n∫\nf(a, ξ)p(a)d a = F(ξ). (10.146)\nWe now have the freedom to choose the variational parameter ξ, which we do by\nﬁnding the value ξ⋆ that maximizes the function F(ξ). The resulting value F(ξ⋆ )\nrepresents the tightest bound within this family of bounds and can be used as an\napproximation to I. This optimized bound, however, will in general not be exact.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 516,
      "page_label": "497"
    }
  },
  {
    "page_content": "498 10. APPROXIMATE INFERENCE\nAlthough the boundσ(a) ⩾ f(a, ξ) on the logistic sigmoid can be optimized exactly,\nthe required choice for ξ depends on the value ofa, so that the bound is exact for one\nvalue of a only. Because the quantity F(ξ) is obtained by integrating over all values\nof a, the value of ξ⋆ represents a compromise, weighted by the distribution p(a).\n10.6. Variational Logistic Regression\nWe now illustrate the use of local variational methods by returning to the Bayesian\nlogistic regression model studied in Section 4.5. There we focussed on the use of\nthe Laplace approximation, while here we consider a variational treatment based on\nthe approach of Jaakkola and Jordan (2000). Like the Laplace method, this also\nleads to a Gaussian approximation to the posterior distribution. However, the greater\nﬂexibility of the variational approximation leads to improved accuracy compared\nto the Laplace method. Furthermore (unlike the Laplace method), the variational\napproach is optimizing a well deﬁned objective function given by a rigourous bound\non the model evidence. Logistic regression has also been treated by Dybowski and\nRoberts (2005) from a Bayesian perspective using Monte Carlo sampling techniques.\n10.6.1 Variational posterior distribution\nHere we shall make use of a variational approximation based on the local bounds\nintroduced in Section 10.5. This allows the likelihood function for logistic regres-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 517,
      "page_label": "498"
    }
  },
  {
    "page_content": "introduced in Section 10.5. This allows the likelihood function for logistic regres-\nsion, which is governed by the logistic sigmoid, to be approximated by the expo-\nnential of a quadratic form. It is therefore again convenient to choose a conjugate\nGaussian prior of the form (4.140). For the moment, we shall treat the hyperparam-\neters m0 and S0 as ﬁxed constants. In Section 10.6.3, we shall demonstrate how the\nvariational formalism can be extended to the case where there are unknown hyper-\nparameters whose values are to be inferred from the data.\nIn the variational framework, we seek to maximize a lower bound on the marginal\nlikelihood. For the Bayesian logistic regression model, the marginal likelihood takes\nthe form\np(t)=\n∫\np(t|w)p(w)d w =\n∫ [N∏\nn=1\np(tn|w)\n]\np(w)d w. (10.147)\nWe ﬁrst note that the conditional distribution for t can be written as\np(t|w)= σ(a)t {1 − σ(a)}1−t\n=\n( 1\n1+ e−a\n)t (\n1 − 1\n1+ e−a\n)1−t\n= eat e−a\n1+ e−a = eatσ(−a) (10.148)\nwhere a = wTφ. In order to obtain a lower bound on p(t), we make use of the\nvariational lower bound on the logistic sigmoid function given by (10.144), which",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 517,
      "page_label": "498"
    }
  },
  {
    "page_content": "10.6. Variational Logistic Regression 499\nwe reproduce here for convenience\nσ(z) ⩾ σ(ξ)e x p\n{\n(z − ξ)/2 − λ(ξ)(z2 − ξ2)\n}\n(10.149)\nwhere\nλ(ξ)= 1\n2ξ\n[\nσ(ξ) − 1\n2\n]\n. (10.150)\nWe can therefore write\np(t|w)= eatσ(−a) ⩾ eatσ(ξ)e x p\n{\n−(a + ξ)/2 − λ(ξ)(a2 − ξ2)\n}\n. (10.151)\nNote that because this bound is applied to each of the terms in the likelihood function\nseparately, there is a variational parameter ξn corresponding to each training set\nobservation (φn,t n). Using a = wTφ, and multiplying by the prior distribution, we\nobtain the following bound on the joint distribution of t and w\np(t, w)= p(t|w)p(w) ⩾ h(w, ξ)p(w) (10.152)\nwhere ξ denotes the set {ξn} of variational parameters, and\nh(w, ξ)=\nN∏\nn=1\nσ(ξn)e x p\n{\nwTφntn − (wTφn + ξn)/2\n− λ(ξn)([wTφn]2 − ξ2\nn)\n}\n. (10.153)\nEvaluation of the exact posterior distribution would require normalization of the left-\nhand side of this inequality. Because this is intractable, we work instead with the\nright-hand side. Note that the function on the right-hand side cannot be interpreted\nas a probability density because it is not normalized. Once it is normalized to give a\nvariational posterior distributionq(w), however, it no longer represents a bound.\nBecause the logarithm function is monotonically increasing, the inequality A ⩾\nB implies ln A ⩾ lnB. This gives a lower bound on the log of the joint distribution\nof t and w of the form\nln{p(t|w)p(w)} ⩾ ln p(w)+\nN∑\nn=1\n{\nlnσ(ξn)+ wTφntn\n− (wTφn + ξn)/2 − λ(ξn)([wTφn]2 − ξ2\nn)\n}\n. (10.154)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 518,
      "page_label": "499"
    }
  },
  {
    "page_content": "of t and w of the form\nln{p(t|w)p(w)} ⩾ ln p(w)+\nN∑\nn=1\n{\nlnσ(ξn)+ wTφntn\n− (wTφn + ξn)/2 − λ(ξn)([wTφn]2 − ξ2\nn)\n}\n. (10.154)\nSubstituting for the prior p(w), the right-hand side of this inequality becomes, as a\nfunction of w\n−1\n2(w − m0)TS−1\n0 (w − m0)\n+\nN∑\nn=1\n{\nwTφn(tn − 1/2) − λ(ξn)wT(φnφT\nn)w\n}\n+c o n s t. (10.155)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 518,
      "page_label": "499"
    }
  },
  {
    "page_content": "500 10. APPROXIMATE INFERENCE\nThis is a quadratic function ofw, and so we can obtain the corresponding variational\napproximation to the posterior distribution by identifying the linear and quadratic\nterms inw, giving a Gaussian variational posterior of the form\nq(w)= N(w|mN , SN ) (10.156)\nwhere\nmN = SN\n(\nS−1\n0 m0 +\nN∑\nn=1\n(tn − 1/2)φn\n)\n(10.157)\nS−1\nN = S−1\n0 +2\nN∑\nn=1\nλ(ξn)φnφT\nn. (10.158)\nAs with the Laplace framework, we have again obtained a Gaussian approximation\nto the posterior distribution. However, the additional ﬂexibility provided by the vari-\national parameters{ξn} leads to improved accuracy in the approximation (Jaakkola\nand Jordan, 2000).\nHere we have considered a batch learning context in which all of the training\ndata is available at once. However, Bayesian methods are intrinsically well suited\nto sequential learning in which the data points are processed one at a time and then\ndiscarded. The formulation of this variational approach for the sequential case is\nstraightforward.Exercise 10.32\nNote that the bound given by (10.149) applies only to the two-class problem and\nso this approach does not directly generalize to classiﬁcation problems with K> 2\nclasses. An alternative bound for the multiclass case has been explored by Gibbs\n(1997).\n10.6.2 Optimizing the variational parameters\nWe now have a normalized Gaussian approximation to the posterior distribution,\nwhich we shall use shortly to evaluate the predictive distribution for new data points.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 519,
      "page_label": "500"
    }
  },
  {
    "page_content": "which we shall use shortly to evaluate the predictive distribution for new data points.\nFirst, however, we need to determine the variational parameters{ξn} by maximizing\nthe lower bound on the marginal likelihood.\nTo do this, we substitute the inequality (10.152) back into the marginal likeli-\nhood to give\nlnp(t)=l n\n∫\np(t|w)p(w)d w ⩾ ln\n∫\nh(w, ξ)p(w)d w = L(ξ). (10.159)\nAs with the optimization of the hyperparameter α in the linear regression model of\nSection 3.5, there are two approaches to determining theξn. In the ﬁrst approach, we\nrecognize that the function L(ξ) is deﬁned by an integration over w and so we can\nview w as a latent variable and invoke the EM algorithm. In the second approach,\nwe integrate over w analytically and then perform a direct maximization over ξ. Let\nus begin by considering the EM approach.\nThe EM algorithm starts by choosing some initial values for the parameters\n{ξn}, which we denote collectively by ξold. In the E step of the EM algorithm,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 519,
      "page_label": "500"
    }
  },
  {
    "page_content": "10.6. Variational Logistic Regression 501\nwe then use these parameter values to ﬁnd the posterior distribution over w, which\nis given by (10.156). In the M step, we then maximize the expected complete-data\nlog likelihood which is given by\nQ(ξ, ξold)= E [lnh(w, ξ)p(w)] (10.160)\nwhere the expectation is taken with respect to the posterior distribution q(w) evalu-\nated using ξold. Noting that p(w) does not depend on ξ, and substituting for h(w, ξ)\nwe obtain\nQ(ξ, ξold)=\nN∑\nn=1\n{\nln σ(ξn) − ξn/2 − λ(ξn)(φT\nnE[wwT]φn − ξ2\nn)\n}\n+c o n s t\n(10.161)\nwhere ‘const’ denotes terms that are independent ofξ. We now set the derivative with\nrespect to ξn equal to zero. A few lines of algebra, making use of the deﬁnitions of\nσ(ξ) and λ(ξ), then gives\n0= λ′(ξn)(φT\nnE[wwT]φn − ξ2\nn). (10.162)\nWe now note that λ′(ξ) is a monotonic function of ξ for ξ ⩾ 0, and that we can\nrestrict attention to nonnegative values of ξ without loss of generality due to the\nsymmetry of the bound around ξ =0 . Thus λ′(ξ) ̸=0, and hence we obtain the\nfollowing re-estimation equationsExercise 10.33\n(ξnew\nn )2 = φT\nnE[wwT]φn = φT\nn\n(\nSN + mN mT\nN\n)\nφn (10.163)\nwhere we have used (10.156).\nLet us summarize the EM algorithm for ﬁnding the variational posterior distri-\nbution. We ﬁrst initialize the variational parameters ξold. In the E step, we evaluate\nthe posterior distribution over w given by (10.156), in which the mean and covari-\nance are deﬁned by (10.157) and (10.158). In the M step, we then use this variational",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 520,
      "page_label": "501"
    }
  },
  {
    "page_content": "ance are deﬁned by (10.157) and (10.158). In the M step, we then use this variational\nposterior to compute a new value for ξ given by (10.163). The E and M steps are\nrepeated until a suitable convergence criterion is satisﬁed, which in practice typically\nrequires only a few iterations.\nAn alternative approach to obtaining re-estimation equations for ξ is to note\nthat in the integral over w in the deﬁnition (10.159) of the lower bound L(ξ), the\nintegrand has a Gaussian-like form and so the integral can be evaluated analytically.\nHaving evaluated the integral, we can then differentiate with respect to ξn. It turns\nout that this gives rise to exactly the same re-estimation equations as does the EM\napproach given by (10.163).Exercise 10.34\nAs we have emphasized already, in the application of variational methods it is\nuseful to be able to evaluate the lower boundL(ξ) given by (10.159). The integration\nover w can be performed analytically by noting that p(w) is Gaussian and h(w, ξ)\nis the exponential of a quadratic function of w. Thus, by completing the square\nand making use of the standard result for the normalization coefﬁcient of a Gaussian\ndistribution, we can obtain a closed form solution which takes the formExercise 10.35",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 520,
      "page_label": "501"
    }
  },
  {
    "page_content": "502 10. APPROXIMATE INFERENCE\n0.010.250.75\n0.99\n−4 −2 0 2 4\n−6\n−4\n−2\n0\n2\n4\n6\n−4 −2 0 2 4\n−6\n−4\n−2\n0\n2\n4\n6\nFigure 10.13 Illustration of the Bayesian approach to logistic regression for a simple linearly separable data\nset. The plot on the left shows the predictive distribution obtained using variational inference. We see that\nthe decision boundary lies roughly mid way between the clusters of data points, and that the contours of the\npredictive distribution splay out away from the data reﬂecting the greater uncertainty in the classiﬁcation of such\nregions. The plot on the right shows the decision boundaries corresponding to ﬁve samples of the parameter\nvector w drawn from the posterior distribution p(w|t).\nL(ξ)= 1\n2 ln |SN |\n|S0| − 1\n2mT\nN S−1\nN mN + 1\n2mT\n0 S−1\n0 m0\n+\nN∑\nn=1\n{\nlnσ(ξn) − 1\n2ξn − λ(ξn)ξ2\nn\n}\n. (10.164)\nThis variational framework can also be applied to situations in which the data\nis arriving sequentially (Jaakkola and Jordan, 2000). In this case we maintain a\nGaussian posterior distribution over w, which is initialized using the prior p(w).A s\neach data point arrives, the posterior is updated by making use of the bound (10.151)\nand then normalized to give an updated posterior distribution.\nThe predictive distribution is obtained by marginalizing over the posterior dis-\ntribution, and takes the same form as for the Laplace approximation discussed in\nSection 4.5.2. Figure 10.13 shows the variational predictive distributions for a syn-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 521,
      "page_label": "502"
    }
  },
  {
    "page_content": "Section 4.5.2. Figure 10.13 shows the variational predictive distributions for a syn-\nthetic data set. This example provides interesting insights into the concept of ‘large\nmargin’, which was discussed in Section 7.1 and which has qualitatively similar be-\nhaviour to the Bayesian solution.\n10.6.3 Inference of hyperparameters\nSo far, we have treated the hyperparameterα in the prior distribution as a known\nconstant. We now extend the Bayesian logistic regression model to allow the value of\nthis parameter to be inferred from the data set. This can be achieved by combining\nthe global and local variational approximations into a single framework, so as to\nmaintain a lower bound on the marginal likelihood at each stage. Such a combined\napproach was adopted by Bishop and Svens ´en (2003) in the context of a Bayesian\ntreatment of the hierarchical mixture of experts model.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 521,
      "page_label": "502"
    }
  },
  {
    "page_content": "10.6. Variational Logistic Regression 503\nSpeciﬁcally, we consider once again a simple isotropic Gaussian prior distribu-\ntion of the form\np(w|α)= N(w|0,α −1I). (10.165)\nOur analysis is readily extended to more general Gaussian priors, for instance if we\nwish to associate a different hyperparameter with different subsets of the parame-\nters wj. As usual, we consider a conjugate hyperprior over α given by a gamma\ndistribution\np(α)=G a m (α|a0,b 0) (10.166)\ngoverned by the constants a0 and b0.\nThe marginal likelihood for this model now takes the form\np(t)=\n∫∫\np(w,α ,t)d wdα (10.167)\nwhere the joint distribution is given by\np(w,α ,t)= p(t|w)p(w|α)p(α). (10.168)\nWe are now faced with an analytically intractable integration over w and α, which\nwe shall tackle by using both the local and global variational approaches in the same\nmodel\nTo begin with, we introduce a variational distribution q(w,α ), and then apply\nthe decomposition (10.2), which in this instance takes the form\nlnp(t)= L(q) + KL(q∥p) (10.169)\nwhere the lower bound L(q) and the Kullback-Leibler divergence KL(q∥p) are de-\nﬁned by\nL(q)=\n∫∫\nq(w,α )l n\n{p(w,α ,t)\nq(w,α )\n}\ndwdα (10.170)\nKL(q∥p)= −\n∫∫\nq(w,α )l n\n{p(w,α |t))\nq(w,α )\n}\ndwdα. (10.171)\nAt this point, the lower bound L(q) is still intractable due to the form of the\nlikelihood factor p(t|w). We therefore apply the local variational bound to each of\nthe logistic sigmoid factors as before. This allows us to use the inequality (10.152)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 522,
      "page_label": "503"
    }
  },
  {
    "page_content": "the logistic sigmoid factors as before. This allows us to use the inequality (10.152)\nand place a lower bound onL(q), which will therefore also be a lower bound on the\nlog marginal likelihood\nln p(t) ⩾ L(q) ⩾ ˜L(q, ξ)\n=\n∫∫\nq(w,α )l n\n{h(w, ξ)p(w|α)p(α)\nq(w,α )\n}\ndwdα. (10.172)\nNext we assume that the variational distribution factorizes between parameters and\nhyperparameters so that\nq(w,α )= q(w)q(α). (10.173)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 522,
      "page_label": "503"
    }
  },
  {
    "page_content": "504 10. APPROXIMATE INFERENCE\nWith this factorization we can appeal to the general result (10.9) to ﬁnd expressions\nfor the optimal factors. Consider ﬁrst the distribution q(w). Discarding terms that\nare independent of w,w eh a v e\nlnq(w)= Eα [ln{h(w, ξ)p(w|α)p(α)}]+c o n s t\n=l n h(w, ξ)+ Eα [lnp(w|α) ]+c o n s t.\nWe now substitute for lnh(w, ξ) using (10.153), and for lnp(w|α) using (10.165),\ngiving\nln q(w)= −E[α]\n2 wTw +\nN∑\nn=1\n{\n(tn − 1/2)wTφn − λ(ξn)wTφnφT\nnw\n}\n+c o n s t.\nWe see that this is a quadratic function of w and so the solution for q(w) will be\nGaussian. Completing the square in the usual way, we obtain\nq(w)= N(w|µN , ΣN ) (10.174)\nwhere we have deﬁned\nΣ−1\nN µN =\nN∑\nn=1\n(tn − 1/2)φn (10.175)\nΣ−1\nN = E[α]I +2\nN∑\nn=1\nλ(ξn)φnφT\nn. (10.176)\nSimilarly, the optimal solution for the factor q(α) is obtained from\nlnq(α)= Ew [lnp(w|α)] + lnp(α)+c o n s t.\nSubstituting for ln p(w|α) using (10.165), and for lnp(α) using (10.166), we obtain\nlnq(α)= M\n2 lnα − α\n2 E\n[\nwTw\n]\n+( a0 − 1) lnα − b0α +c o n s t.\nWe recognize this as the log of a gamma distribution, and so we obtain\nq(α)=G a m (α|aN ,b N )= 1\nΓ(a0)ab0\n0 αa0−1e−b0α (10.177)\nwhere\naN = a0 + M\n2 (10.178)\nbN = b0 + 1\n2Ew\n[\nwTw\n]\n. (10.179)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 523,
      "page_label": "504"
    }
  },
  {
    "page_content": "10.7. Expectation Propagation 505\nWe also need to optimize the variational parameters ξn, and this is also done by\nmaximizing the lower bound ˜L(q, ξ). Omitting terms that are independent of ξ, and\nintegrating over α,w eh a v e\n˜L(q, ξ)=\n∫\nq(w)l nh(w, ξ)d w +c o n s t. (10.180)\nNote that this has precisely the same form as (10.159), and so we can again appeal\nto our earlier result (10.163), which can be obtained by direct optimization of the\nmarginal likelihood function, leading to re-estimation equations of the form\n(ξnew\nn )2 = φT\nn\n(\nΣN + µN µT\nN\n)\nφn. (10.181)\nWe have obtained re-estimation equations for the three quantities q(w), q(α),\nand ξ, and so after making suitable initializations, we can cycle through these quan-\ntities, updating each in turn. The required moments are given byAppendix B\nE [α]= aN\nbN\n(10.182)\nE\n[\nwTw\n]\n= ΣN + µT\nN µN . (10.183)\n10.7. Expectation Propagation\nWe conclude this chapter by discussing an alternative form of deterministic approx-\nimate inference, known as expectation propagation or EP (Minka, 2001a; Minka,\n2001b). As with the variational Bayes methods discussed so far, this too is based\non the minimization of a Kullback-Leibler divergence but now of the reverse form,\nwhich gives the approximation rather different properties.\nConsider for a moment the problem of minimizingKL(p∥q) with respect to q(z)\nwhen p(z) is a ﬁxed distribution and q(z) is a member of the exponential family and\nso, from (2.194), can be written in the form",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 524,
      "page_label": "505"
    }
  },
  {
    "page_content": "when p(z) is a ﬁxed distribution and q(z) is a member of the exponential family and\nso, from (2.194), can be written in the form\nq(z)= h(z)g(η)e x p\n{\nηTu(z)\n}\n. (10.184)\nAs a function of η, the Kullback-Leibler divergence then becomes\nKL(p∥q)= −lng(η) − ηTEp(z)[u(z) ]+c o n s t (10.185)\nwhere the constant terms are independent of the natural parameters η. We can mini-\nmize KL(p∥q) within this family of distributions by setting the gradient with respect\nto η to zero, giving\n−∇lng(η)= Ep(z)[u(z)]. (10.186)\nHowever, we have already seen in (2.226) that the negative gradient of lng(η) is\ngiven by the expectation of u(z) under the distribution q(z). Equating these two\nresults, we obtain\nEq(z)[u(z)] = Ep(z)[u(z)]. (10.187)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 524,
      "page_label": "505"
    }
  },
  {
    "page_content": "506 10. APPROXIMATE INFERENCE\nWe see that the optimum solution simply corresponds to matching the expected suf-\nﬁcient statistics. So, for instance, if q(z) is a Gaussian N(z|µ, Σ) then we minimize\nthe Kullback-Leibler divergence by setting the mean µ of q(z) equal to the mean of\nthe distribution p(z) and the covariance Σ equal to the covariance of p(z). This is\nsometimes called moment matching. An example of this was seen in Figure 10.3(a).\nNow let us exploit this result to obtain a practical algorithm for approximate\ninference. For many probabilistic models, the joint distribution of dataD and hidden\nvariables (including parameters) θ comprises a product of factors in the form\np(D, θ)=\n∏\ni\nfi(θ). (10.188)\nThis would arise, for example, in a model for independent, identically distributed\ndata in which there is one factor fn(θ)= p(xn|θ) for each data point xn, along\nwith a factor f0(θ)= p(θ) corresponding to the prior. More generally, it would also\napply to any model deﬁned by a directed probabilistic graph in which each factor is a\nconditional distribution corresponding to one of the nodes, or an undirected graph in\nwhich each factor is a clique potential. We are interested in evaluating the posterior\ndistribution p(θ|D) for the purpose of making predictions, as well as the model\nevidence p(D) for the purpose of model comparison. From (10.188) the posterior is\ngiven by\np(θ|D)= 1\np(D)\n∏\ni\nfi(θ) (10.189)\nand the model evidence is given by\np(D)=\n∫ ∏\ni\nfi(θ)d θ. (10.190)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 525,
      "page_label": "506"
    }
  },
  {
    "page_content": "given by\np(θ|D)= 1\np(D)\n∏\ni\nfi(θ) (10.189)\nand the model evidence is given by\np(D)=\n∫ ∏\ni\nfi(θ)d θ. (10.190)\nHere we are considering continuous variables, but the following discussion applies\nequally to discrete variables with integrals replaced by summations. We shall sup-\npose that the marginalization overθ, along with the marginalizations with respect to\nthe posterior distribution required to make predictions, are intractable so that some\nform of approximation is required.\nExpectation propagation is based on an approximation to the posterior distribu-\ntion which is also given by a product of factors\nq(θ)= 1\nZ\n∏\ni\n˜fi(θ) (10.191)\nin which each factor ˜fi(θ) in the approximation corresponds to one of the factors\nfi(θ) in the true posterior (10.189), and the factor 1/Z is the normalizing constant\nneeded to ensure that the left-hand side of (10.191) integrates to unity. In order to\nobtain a practical algorithm, we need to constrain the factors ˜fi(θ) in some way,\nand in particular we shall assume that they come from the exponential family. The\nproduct of the factors will therefore also be from the exponential family and so can",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 525,
      "page_label": "506"
    }
  },
  {
    "page_content": "10.7. Expectation Propagation 507\nbe described by a ﬁnite set of sufﬁcient statistics. For example, if each of the ˜fi(θ)\nis a Gaussian, then the overall approximation q(θ) will also be Gaussian.\nIdeally we would like to determine the˜fi(θ) by minimizing the Kullback-Leibler\ndivergence between the true posterior and the approximation given by\nKL (p∥q)=K L\n(\n1\np(D)\n∏\ni\nfi(θ)\n\n1\nZ\n∏\ni\n˜fi(θ)\n)\n. (10.192)\nNote that this is the reverse form of KL divergence compared with that used in varia-\ntional inference. In general, this minimization will be intractable because the KL di-\nvergence involves averaging with respect to the true distribution. As a rough approx-\nimation, we could instead minimize the KL divergences between the corresponding\npairs fi(θ) and ˜fi(θ) of factors. This represents a much simpler problem to solve,\nand has the advantage that the algorithm is noniterative. However, because each fac-\ntor is individually approximated, the product of the factors could well give a poor\napproximation.\nExpectation propagation makes a much better approximation by optimizing each\nfactor in turn in the context of all of the remaining factors. It starts by initializing\nthe factors ˜fi(θ), and then cycles through the factors reﬁning them one at a time.\nThis is similar in spirit to the update of factors in the variational Bayes framework\nconsidered earlier. Suppose we wish to reﬁne factor ˜fj(θ). We ﬁrst remove this\nfactor from the product to give ∏\ni̸=j",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 526,
      "page_label": "507"
    }
  },
  {
    "page_content": "considered earlier. Suppose we wish to reﬁne factor ˜fj(θ). We ﬁrst remove this\nfactor from the product to give ∏\ni̸=j\n˜fi(θ). Conceptually, we will now determine a\nrevised form of the factor ˜fj(θ) by ensuring that the product\nqnew(θ) ∝ ˜fj(θ)\n∏\ni̸=j\n˜fi(θ) (10.193)\nis as close as possible to\nfj(θ)\n∏\ni̸=j\n˜fi(θ) (10.194)\nin which we keep ﬁxed all of the factors ˜fi(θ) for i ̸=j. This ensures that the\napproximation is most accurate in the regions of high posterior probability as deﬁned\nby the remaining factors. We shall see an example of this effect when we apply EP\nto the ‘clutter problem’. To achieve this, we ﬁrst remove the factor ˜fj(θ) from theSection 10.7.1\ncurrent approximation to the posterior by deﬁning the unnormalized distribution\nq\\j(θ)= q(θ)\n˜fj(θ)\n. (10.195)\nNote that we could instead ﬁnd q\\j(θ) from the product of factors i ̸=j, although\nin practice division is usually easier. This is now combined with the factor fj(θ) to\ngive a distribution\n1\nZj\nfj(θ)q\\j(θ) (10.196)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 526,
      "page_label": "507"
    }
  },
  {
    "page_content": "508 10. APPROXIMATE INFERENCE\n−2 −1 0 1 2 3 4\n0\n0.2\n0.4\n0.6\n0.8\n1\n−2 −1 0 1 2 3 4\n0\n10\n20\n30\n40\nFigure 10.14 Illustration of the expectation propagation approximation using a Gaussian distribution for the\nexample considered earlier in Figures 4.14 and 10.1. The left-hand plot shows the original distribution (yellow)\nalong with the Laplace (red), global variational (green), and EP (blue) approximations, and the right-hand plot\nshows the corresponding negative logarithms of the distributions. Note that the EP distribution is broader than\nthat variational inference, as a consequence of the different form of KL divergence.\nwhere Zj is the normalization constant given by\nZj =\n∫\nfj(θ)q\\j(θ)d θ. (10.197)\nWe now determine a revised factor˜fj(θ) by minimizing the Kullback-Leibler diver-\ngence\nKL\n( fj(θ)q\\j(θ)\nZj\nqnew(θ)\n)\n. (10.198)\nThis is easily solved because the approximating distribution qnew(θ) is from the ex-\nponential family, and so we can appeal to the result (10.187), which tells us that the\nparameters of qnew(θ) are obtained by matching its expected sufﬁcient statistics to\nthe corresponding moments of (10.196). We shall assume that this is a tractable oper-\nation. For example, if we chooseq(θ) to be a Gaussian distributionN(θ|µ, Σ), then\nµ is set equal to the mean of the (unnormalized) distribution fj(θ)q\\j(θ), and Σ is\nset to its covariance. More generally, it is straightforward to obtain the required ex-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 527,
      "page_label": "508"
    }
  },
  {
    "page_content": "set to its covariance. More generally, it is straightforward to obtain the required ex-\npectations for any member of the exponential family, provided it can be normalized,\nbecause the expected statistics can be related to the derivatives of the normalization\ncoefﬁcient, as given by (2.226). The EP approximation is illustrated in Figure 10.14.\nFrom (10.193), we see that the revised factor ˜fj(θ) can be found by taking\nqnew(θ) and dividing out the remaining factors so that\n˜fj(θ)= K qnew(θ)\nq\\j(θ) (10.199)\nwhere we have used (10.195). The coefﬁcient K is determined by multiplying both",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 527,
      "page_label": "508"
    }
  },
  {
    "page_content": "10.7. Expectation Propagation 509\nsides of (10.199) by q\\i(θ) and integrating to give\nK =\n∫\n˜fj(θ)q\\j(θ)d θ (10.200)\nwhere we have used the fact thatqnew(θ) is normalized. The value ofK can therefore\nbe found by matching zeroth-order moments\n∫\n˜fj(θ)q\\j(θ)d θ =\n∫\nfj(θ)q\\j(θ)d θ. (10.201)\nCombining this with (10.197), we then see that K = Zj and so can be found by\nevaluating the integral in (10.197).\nIn practice, several passes are made through the set of factors, revising each\nfactor in turn. The posterior distributionp(θ|D) is then approximated using (10.191),\nand the model evidencep(D) can be approximated by using (10.190) with the factors\nfi(θ) replaced by their approximations ˜fi(θ).\nExpectation Propagation\nWe are given a joint distribution over observed data D and stochastic variables\nθ in the form of a product of factors\np(D, θ)=\n∏\ni\nfi(θ) (10.202)\nand we wish to approximate the posterior distribution p(θ|D) by a distribution\nof the form\nq(θ)= 1\nZ\n∏\ni\n˜fi(θ). (10.203)\nWe also wish to approximate the model evidence p(D).\n1. Initialize all of the approximating factors ˜fi(θ).\n2. Initialize the posterior approximation by setting\nq(θ) ∝\n∏\ni\n˜fi(θ). (10.204)\n3. Until convergence:\n(a) Choose a factor ˜fj(θ) to reﬁne.\n(b) Remove ˜fj(θ) from the posterior by division\nq\\j(θ)= q(θ)\n˜fj(θ)\n. (10.205)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 528,
      "page_label": "509"
    }
  },
  {
    "page_content": "510 10. APPROXIMATE INFERENCE\n(c) Evaluate the new posterior by setting the sufﬁcient statistics (moments)\nof qnew(θ) equal to those of q\\j(θ)fj(θ), including evaluation of the\nnormalization constant\nZj =\n∫\nq\\j(θ)fj(θ)d θ. (10.206)\n(d) Evaluate and store the new factor\n˜fj(θ)= Zj\nqnew(θ)\nq\\j(θ) . (10.207)\n4. Evaluate the approximation to the model evidence\np(D) ≃\n∫ ∏\ni\n˜fi(θ)d θ. (10.208)\nA special case of EP, known as assumed density ﬁltering (ADF) or moment\nmatching (Maybeck, 1982; Lauritzen, 1992; Boyen and Koller, 1998; Opper and\nWinther, 1999), is obtained by initializing all of the approximating factors except\nthe ﬁrst to unity and then making one pass through the factors updating each of them\nonce. Assumed density ﬁltering can be appropriate for on-line learning in which data\npoints are arriving in a sequence and we need to learn from each data point and then\ndiscard it before considering the next point. However, in a batch setting we have the\nopportunity to re-use the data points many times in order to achieve improved ac-\ncuracy, and it is this idea that is exploited in expectation propagation. Furthermore,\nif we apply ADF to batch data, the results will have an undesirable dependence on\nthe (arbitrary) order in which the data points are considered, which again EP can\novercome.\nOne disadvantage of expectation propagation is that there is no guarantee that\nthe iterations will converge. However, for approximations q(θ) in the exponential",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 529,
      "page_label": "510"
    }
  },
  {
    "page_content": "the iterations will converge. However, for approximations q(θ) in the exponential\nfamily, if the iterations do converge, the resulting solution will be a stationary point\nof a particular energy function (Minka, 2001a), although each iteration of EP does\nnot necessarily decrease the value of this energy function. This is in contrast to\nvariational Bayes, which iteratively maximizes a lower bound on the log marginal\nlikelihood, in which each iteration is guaranteed not to decrease the bound. It is\npossible to optimize the EP cost function directly, in which case it is guaranteed\nto converge, although the resulting algorithms can be slower and more complex to\nimplement.\nAnother difference between variational Bayes and EP arises from the form of\nKL divergence that is minimized by the two algorithms, because the former mini-\nmizes KL(q∥p) whereas the latter minimizes KL(p∥q). As we saw in Figure 10.3,\nfor distributions p(θ) which are multimodal, minimizing KL(p∥q) can lead to poor\napproximations. In particular, if EP is applied to mixtures the results are not sen-\nsible because the approximation tries to capture all of the modes of the posterior\ndistribution. Conversely, in logistic-type models, EP often out-performs both local\nvariational methods and the Laplace approximation (Kuss and Rasmussen, 2006).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 529,
      "page_label": "510"
    }
  },
  {
    "page_content": "10.7. Expectation Propagation 511\nFigure 10.15 Illustration of the clutter problem\nfor a data space dimensionality of\nD =1 . Training data points, de-\nnoted by the crosses, are drawn\nfrom a mixture of two Gaussians\nwith components shown in red\nand green. The goal is to infer the\nmean of the green Gaussian from\nthe observed data.\nθ x−5 0 5 10\n10.7.1 Example: The clutter problem\nFollowing Minka (2001b), we illustrate the EP algorithm using a simple exam-\nple in which the goal is to infer the mean θ of a multivariate Gaussian distribution\nover a variable x given a set of observations drawn from that distribution. To make\nthe problem more interesting, the observations are embedded in background clutter,\nwhich itself is also Gaussian distributed, as illustrated in Figure 10.15. The distribu-\ntion of observed values x is therefore a mixture of Gaussians, which we take to be\nof the form\np(x|θ)=( 1 − w)N(x|θ, I)+ wN(x|0,aI) (10.209)\nwhere w is the proportion of background clutter and is assumed to be known. The\nprior over θ is taken to be Gaussian\np(θ)= N(θ|0,b I) (10.210)\nand Minka (2001a) chooses the parameter values a =1 0, b = 100 and w =0 .5.\nThe joint distribution of N observations D = {x1,..., xN } and θ is given by\np(D, θ)= p(θ)\nN∏\nn=1\np(xn|θ) (10.211)\nand so the posterior distribution comprises a mixture of 2N Gaussians. Thus the\ncomputational cost of solving this problem exactly would grow exponentially with",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 530,
      "page_label": "511"
    }
  },
  {
    "page_content": "computational cost of solving this problem exactly would grow exponentially with\nthe size of the data set, and so an exact solution is intractable for moderately large\nN.\nTo apply EP to the clutter problem, we ﬁrst identify the factors f0(θ)= p(θ)\nand fn(θ)= p(xn|θ). Next we select an approximating distribution from the expo-\nnential family, and for this example it is convenient to choose a spherical Gaussian\nq(θ)= N(θ|m,v I). (10.212)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 530,
      "page_label": "511"
    }
  },
  {
    "page_content": "512 10. APPROXIMATE INFERENCE\nThe factor approximations will therefore take the form of exponential-quadratic\nfunctions of the form\n˜fn(θ)= snN(θ|mn,vnI) (10.213)\nwhere n =1 ,...,N , and we set ˜f0(θ) equal to the prior p(θ). Note that the use of\nN(θ|·,·) does not imply that the right-hand side is a well-deﬁned Gaussian density\n(in fact, as we shall see, the variance parameter vn can be negative) but is simply a\nconvenient shorthand notation. The approximations ˜fn(θ), for n =1 ,...,N , can\nbe initialized to unity, corresponding to sn =( 2πvn)D/2, vn →∞ and mn = 0,\nwhere D is the dimensionality of x and hence of θ. The initial q(θ), deﬁned by\n(10.191), is therefore equal to the prior.\nWe then iteratively reﬁne the factors by taking one factor fn(θ) at a time and\napplying (10.205), (10.206), and (10.207). Note that we do not need to revise the\nterm f0(θ) because an EP update will leave this term unchanged. Here we state theExercise 10.37\nresults and leave the reader to ﬁll in the details.\nFirst we remove the current estimate˜fn(θ) from q(θ) by division using (10.205)\nto give q\\n(θ), which has mean and inverse variance given byExercise 10.38\nm\\n = m + v\\nv−1\nn (m − mn) (10.214)\n(v\\n)−1 = v−1 − v−1\nn . (10.215)\nNext we evaluate the normalization constant Zn using (10.206) to give\nZn =( 1− w)N(xn|m\\n, (v\\n +1 )I)+ wN(xn|0,aI). (10.216)\nSimilarly, we compute the mean and variance of qnew(θ) by ﬁnding the mean and\nvariance of q\\n(θ)fn(θ) to giveExercise 10.39\nm = m\\n + ρn\nv\\n",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 531,
      "page_label": "512"
    }
  },
  {
    "page_content": "Similarly, we compute the mean and variance of qnew(θ) by ﬁnding the mean and\nvariance of q\\n(θ)fn(θ) to giveExercise 10.39\nm = m\\n + ρn\nv\\n\nv\\n +1 (xn − m\\n) (10.217)\nv = v\\n − ρn\n(v\\n)2\nv\\n +1 + ρn(1 − ρn)(v\\n)2∥xn − m\\n∥2\nD(v\\n +1 )2 (10.218)\nwhere the quantity\nρn =1 − w\nZn\nN(xn|0,aI) (10.219)\nhas a simple interpretation as the probability of the point xn not being clutter. Then\nwe use (10.207) to compute the reﬁned factor ˜fn(θ) whose parameters are given by\nv−1\nn =( vnew)−1 − (v\\n)−1 (10.220)\nmn = m\\n +( vn + v\\n)(v\\n)−1(mnew − m\\n) (10.221)\nsn = Zn\n(2πvn)D/2N(mn|m\\n, (vn + v\\n)I). (10.222)\nThis reﬁnement process is repeated until a suitable termination criterion is satisﬁed,\nfor instance that the maximum change in parameter values resulting from a complete",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 531,
      "page_label": "512"
    }
  },
  {
    "page_content": "10.7. Expectation Propagation 513\nθ−5 0 5 10 θ−5 0 5 10\nFigure 10.16 Examples of the approximation of speciﬁc factors for a one-dimensional version of the clutter\nproblem, showing fn(θ) in blue, efn(θ) in red, and q\\n(θ) in green. Notice that the current form for q\\n(θ) controls\nthe range of θ over which efn(θ) will be a good approximation to fn(θ).\npass through all factors is less than some threshold. Finally, we use (10.208) to\nevaluate the approximation to the model evidence, given by\np(D) ≃ (2πvnew)D/2 exp(B/2)\nN∏\nn=1\n{\nsn(2πvn)−D/2}\n(10.223)\nwhere\nB = (mnew)Tmnew\nv −\nN∑\nn=1\nmT\nnmn\nvn\n. (10.224)\nExamples factor approximations for the clutter problem with a one-dimensional pa-\nrameter space θ are shown in Figure 10.16. Note that the factor approximations can\nhave inﬁnite or even negative values for the ‘variance’ parameter vn. This simply\ncorresponds to approximations that curve upwards instead of downwards and are not\nnecessarily problematic provided the overall approximate posterior q(θ) has posi-\ntive variance. Figure 10.17 compares the performance of EP with variational Bayes\n(mean ﬁeld theory) and the Laplace approximation on the clutter problem.\n10.7.2 Expectation propagation on graphs\nSo far in our general discussion of EP, we have allowed the factors fi(θ) in the\ndistribution p(θ) to be functions of all of the components of θ, and similarly for the\napproximating factors ˜f(θ) in the approximating distributionq(θ). We now consider",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 532,
      "page_label": "513"
    }
  },
  {
    "page_content": "approximating factors ˜f(θ) in the approximating distributionq(θ). We now consider\nsituations in which the factors depend only on subsets of the variables. Such restric-\ntions can be conveniently expressed using the framework of probabilistic graphical\nmodels, as discussed in Chapter 8. Here we use a factor graph representation because\nthis encompasses both directed and undirected graphs.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 532,
      "page_label": "513"
    }
  },
  {
    "page_content": "514 10. APPROXIMATE INFERENCE\nep\nvblaplace\nPosterior mean\nFLOPS\nError\n10\n4\n10\n6\n10\n−5\n10\n0\nep\nvb\nlaplace\nEvidence\nFLOPS\nError\n10\n4\n10\n6\n10\n−204\n10\n−202\n10\n−200\nFigure 10.17 Comparison of expectation propagation, variational inference, and the Laplace approximation on\nthe clutter problem. The left-hand plot shows the error in the predicted posterior mean versus the number of\nﬂoating point operations, and the right-hand plot shows the corresponding results for the model evidence.\nWe shall focus on the case in which the approximating distribution is fully fac-\ntorized, and we shall show that in this case expectation propagation reduces to loopy\nbelief propagation (Minka, 2001a). To start with, we show this in the context of a\nsimple example, and then we shall explore the general case.\nFirst of all, recall from (10.17) that if we minimize the Kullback-Leibler diver-\ngence KL(p∥q) with respect to a factorized distribution q, then the optimal solution\nfor each factor is simply the corresponding marginal of p.\nNow consider the factor graph shown on the left in Figure 10.18, which was\nintroduced earlier in the context of the sum-product algorithm. The joint distributionSection 8.4.4\nis given by\np(x)= fa(x1,x2)fb(x2,x3)fc(x2,x4). (10.225)\nWe seek an approximation q(x) that has the same factorization, so that\nq(x) ∝ ˜fa(x1,x2)˜fb(x2,x3)˜fc(x2,x4). (10.226)\nNote that normalization constants have been omitted, and these can be re-instated at",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 533,
      "page_label": "514"
    }
  },
  {
    "page_content": "q(x) ∝ ˜fa(x1,x2)˜fb(x2,x3)˜fc(x2,x4). (10.226)\nNote that normalization constants have been omitted, and these can be re-instated at\nthe end by local normalization, as is generally done in belief propagation. Now sup-\npose we restrict attention to approximations in which the factors themselves factorize\nwith respect to the individual variables so that\nq(x) ∝ ˜fa1(x1)˜fa2(x2)˜fb2(x2)˜fb3(x3)˜fc2(x2)˜fc4(x4) (10.227)\nwhich corresponds to the factor graph shown on the right in Figure 10.18. Because\nthe individual factors are factorized, the overall distribution q(x) is itself fully fac-\ntorized.\nNow we apply the EP algorithm using the fully factorized approximation. Sup-\npose that we have initialized all of the factors and that we choose to reﬁne factor",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 533,
      "page_label": "514"
    }
  },
  {
    "page_content": "10.7. Expectation Propagation 515\nx1 x2 x3\nx4\nfa fb\nfc\nx1 x2 x3\nx4\n˜fa1 ˜fa2 ˜fb2 ˜fb3\n˜fc2\n˜fc4\nFigure 10.18 On the left is a simple factor graph from Figure 8.51 and reproduced here for convenience. On\nthe right is the corresponding factorized approximation.\n˜fb(x2,x3)= ˜fb2(x2)˜fb3(x3). We ﬁrst remove this factor from the approximating\ndistribution to give\nq\\b(x)= ˜fa1(x1)˜fa2(x2)˜fc2(x2)˜fc4(x4) (10.228)\nand we then multiply this by the exact factor fb(x2,x3) to give\nˆp(x)= q\\b(x)fb(x2,x3)= ˜fa1(x1)˜fa2(x2)˜fc2(x2)˜fc4(x4)fb(x2,x3). (10.229)\nWe now ﬁnd qnew(x) by minimizing the Kullback-Leibler divergence KL(ˆp∥qnew).\nThe result, as noted above, is that qnew(z) comprises the product of factors, one for\neach variable xi, in which each factor is given by the corresponding marginal of\nˆp(x). These four marginals are given by\nˆp(x1) ∝ ˜fa1(x1) (10.230)\nˆp(x2) ∝ ˜fa2(x2)˜fc2(x2)\n∑\nx3\nfb(x2,x3) (10.231)\nˆp(x3) ∝\n∑\nx2\n{\nfb(x2,x3)˜fa2(x2)˜fc2(x2)\n}\n(10.232)\nˆp(x4) ∝ ˜fc4(x4) (10.233)\nand qnew(x) is obtained by multiplying these marginals together. We see that the\nonly factors in q(x) that change when we update ˜fb(x2,x3) are those that involve\nthe variables in fb namely x2 and x3. To obtain the reﬁned factor ˜fb(x2,x3)=\n˜fb2(x2)˜fb3(x3) we simply divide qnew(x) by q\\b(x), which gives\n˜fb2(x2) ∝\n∑\nx3\nfb(x2,x3) (10.234)\n˜fb3(x3) ∝\n∑\nx2\n{\nfb(x2,x3)˜fa2(x2)˜fc2(x2)\n}\n. (10.235)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 534,
      "page_label": "515"
    }
  },
  {
    "page_content": "516 10. APPROXIMATE INFERENCE\nThese are precisely the messages obtained using belief propagation in which mes-Section 8.4.4\nsages from variable nodes to factor nodes have been folded into the messages from\nfactor nodes to variable nodes. In particular, ˜fb2(x2) corresponds to the message\nµfb→x2 (x2) sent by factor node fb to variable node x2 and is given by (8.81). Simi-\nlarly, if we substitute (8.78) into (8.79), we obtain (10.235) in which ˜fa2(x2) corre-\nsponds to µfa→x2 (x2) and ˜fc2(x2) corresponds to µfc→x2 (x2), giving the message\n˜fb3(x3) which corresponds to µfb→x3 (x3).\nThis result differs slightly from standard belief propagation in that messages are\npassed in both directions at the same time. We can easily modify the EP procedure\nto give the standard form of the sum-product algorithm by updating just one of the\nfactors at a time, for instance if we reﬁne only ˜fb3(x3), then ˜fb2(x2) is unchanged\nby deﬁnition, while the reﬁned version of ˜fb3(x3) is again given by (10.235). If\nwe are reﬁning only one term at a time, then we can choose the order in which the\nreﬁnements are done as we wish. In particular, for a tree-structured graph we can\nfollow a two-pass update scheme, corresponding to the standard belief propagation\nschedule, which will result in exact inference of the variable and factor marginals.\nThe initialization of the approximation factors in this case is unimportant.\nNow let us consider a general factor graph corresponding to the distribution\np(θ)=\n∏\ni",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 535,
      "page_label": "516"
    }
  },
  {
    "page_content": "Now let us consider a general factor graph corresponding to the distribution\np(θ)=\n∏\ni\nfi(θi) (10.236)\nwhere θi represents the subset of variables associated with factorfi. We approximate\nthis using a fully factorized distribution of the form\nq(θ) ∝\n∏\ni\n∏\nk\n˜fik(θk) (10.237)\nwhere θk corresponds to an individual variable node. Suppose that we wish to reﬁne\nthe particular term ˜fjl(θl) keeping all other terms ﬁxed. We ﬁrst remove the term\n˜fj(θj) from q(θ) to give\nq\\j(θ) ∝\n∏\ni̸=j\n∏\nk\n˜fik(θk) (10.238)\nand then multiply by the exact factor fj(θj). To determine the reﬁned term ˜fjl(θl),\nwe need only consider the functional dependence on θl, and so we simply ﬁnd the\ncorresponding marginal of\nq\\j(θ)fj(θj). (10.239)\nUp to a multiplicative constant, this involves taking the marginal offj(θj) multiplied\nby any terms from q\\j(θ) that are functions of any of the variables inθj. Terms that\ncorrespond to other factors ˜fi(θi) for i ̸=j will cancel between numerator and\ndenominator when we subsequently divide by q\\j(θ). We therefore obtain\n˜fjl(θl) ∝\n∑\nθm̸=l∈θj\nfj(θj)\n∏\nk\n∏\nm̸=l\n˜fkm(θm). (10.240)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 535,
      "page_label": "516"
    }
  },
  {
    "page_content": "Exercises 517\nWe recognize this as the sum-product rule in the form in which messages from vari-\nable nodes to factor nodes have been eliminated, as illustrated by the example shown\nin Figure 8.50. The quantity ˜fjm(θm) corresponds to the message µfj→θm (θm),\nwhich factor node j sends to variable node m, and the product over k in (10.240)\nis over all factors that depend on the variables θm that have variables (other than\nvariable θl) in common with factor fj(θj). In other words, to compute the outgoing\nmessage from a factor node, we take the product of all the incoming messages from\nother factor nodes, multiply by the local factor, and then marginalize.\nThus, the sum-product algorithm arises as a special case of expectation propa-\ngation if we use an approximating distribution that is fully factorized. This suggests\nthat more ﬂexible approximating distributions, corresponding to partially discon-\nnected graphs, could be used to achieve higher accuracy. Another generalization is\nto group factors fi(θi) together into sets and to reﬁne all the factors in a set together\nat each iteration. Both of these approaches can lead to improvements in accuracy\n(Minka, 2001b). In general, the problem of choosing the best combination of group-\ning and disconnection is an open research issue.\nWe have seen that variational message passing and expectation propagation op-\ntimize two different forms of the Kullback-Leibler divergence. Minka (2005) has",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 536,
      "page_label": "517"
    }
  },
  {
    "page_content": "timize two different forms of the Kullback-Leibler divergence. Minka (2005) has\nshown that a broad range of message passing algorithms can be derived from a com-\nmon framework involving minimization of members of the alpha family of diver-\ngences, given by (10.19). These include variational message passing, loopy belief\npropagation, and expectation propagation, as well as a range of other algorithms,\nwhich we do not have space to discuss here, such as tree-reweighted message pass-\ning (Wainwright et al., 2005), fractional belief propagation(Wiegerinck and Heskes,\n2003), and power EP(Minka, 2004).\nExercises\n10.1 (⋆) www Verify that the log marginal distribution of the observed data ln p(X)\ncan be decomposed into two terms in the form (10.2) where L(q) is given by (10.3)\nand KL(q∥p) is given by (10.4).\n10.2 (⋆) Use the properties E[z1]= m1 and E[z2]= m2 to solve the simultaneous equa-\ntions (10.13) and (10.15), and hence show that, provided the original distribution\np(z) is nonsingular, the unique solution for the means of the factors in the approxi-\nmation distribution is given by E[z1]= µ1 and E[z2]= µ2.\n10.3 (⋆⋆ ) www Consider a factorized variational distribution q(Z) of the form (10.5).\nBy using the technique of Lagrange multipliers, verify that minimization of the\nKullback-Leibler divergenceKL(p∥q) with respect to one of the factors qi(Zi),\nkeeping all other factors ﬁxed, leads to the solution (10.17).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 536,
      "page_label": "517"
    }
  },
  {
    "page_content": "Kullback-Leibler divergenceKL(p∥q) with respect to one of the factors qi(Zi),\nkeeping all other factors ﬁxed, leads to the solution (10.17).\n10.4 (⋆⋆ ) Suppose that p(x) is some ﬁxed distribution and that we wish to approximate\nit using a Gaussian distribution q(x)= N(x|µ, Σ). By writing down the form of\nthe KL divergence KL(p∥q) for a Gaussian q(x) and then differentiating, show that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 536,
      "page_label": "517"
    }
  },
  {
    "page_content": "518 10. APPROXIMATE INFERENCE\nminimization of KL(p∥q) with respect to µ and Σ leads to the result that µ is given\nby the expectation of x under p(x) and that Σ is given by the covariance.\n10.5 (⋆⋆ ) www Consider a model in which the set of all hidden stochastic variables, de-\nnoted collectively byZ, comprises some latent variablesz together with some model\nparameters θ. Suppose we use a variational distribution that factorizes between la-\ntent variables and parameters so that q(z, θ)= qz(z)qθ(θ), in which the distribution\nqθ(θ) is approximated by a point estimate of the form qθ(θ)= δ(θ − θ0) where θ0\nis a vector of free parameters. Show that variational optimization of this factorized\ndistribution is equivalent to an EM algorithm, in which the E step optimizes qz(z),\nand the M step maximizes the expected complete-data log posterior distribution ofθ\nwith respect to θ0.\n10.6 (⋆⋆ ) The alpha family of divergences is deﬁned by (10.19). Show that the Kullback-\nLeibler divergence KL(p∥q) corresponds to α → 1. This can be done by writing\npϵ = exp(ϵlnp)=1+ ϵlnp + O(ϵ2) and then taking ϵ → 0. Similarly show that\nKL(q∥p) corresponds to α →− 1.\n10.7 (⋆⋆ ) Consider the problem of inferring the mean and precision of a univariate Gaus-\nsian using a factorized variational approximation, as considered in Section 10.1.3.\nShow that the factorqµ(µ) is a Gaussian of the form N(µ|µN ,λ −1\nN ) with mean and\nprecision given by (10.26) and (10.27), respectively. Similarly show that the factor",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 537,
      "page_label": "518"
    }
  },
  {
    "page_content": "N ) with mean and\nprecision given by (10.26) and (10.27), respectively. Similarly show that the factor\nqτ (τ) is a gamma distribution of the formGam(τ|aN ,b N ) with parameters given by\n(10.29) and (10.30).\n10.8 (⋆) Consider the variational posterior distribution for the precision of a univariate\nGaussian whose parameters are given by (10.29) and (10.30). By using the standard\nresults for the mean and variance of the gamma distribution given by (B.27) and\n(B.28), show that if we letN →∞ , this variational posterior distribution has a\nmean given by the inverse of the maximum likelihood estimator for the variance of\nthe data, and a variance that goes to zero.\n10.9 (⋆⋆ ) By making use of the standard resultE[τ]= aN /bN for the mean of a gamma\ndistribution, together with (10.26), (10.27), (10.29), and (10.30), derive the result\n(10.33) for the reciprocal of the expected precision in the factorized variational treat-\nment of a univariate Gaussian.\n10.10 (⋆) www Derive the decomposition given by (10.34) that is used to ﬁnd approxi-\nmate posterior distributions over models using variational inference.\n10.11 (⋆⋆ ) www By using a Lagrange multiplier to enforce the normalization constraint\non the distributionq(m), show that the maximum of the lower bound (10.35) is given\nby (10.36).\n10.12 (⋆⋆ ) Starting from the joint distribution (10.41), and applying the general result\n(10.9), show that the optimal variational distribution q⋆ (Z) over the latent variables",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 537,
      "page_label": "518"
    }
  },
  {
    "page_content": "(10.9), show that the optimal variational distribution q⋆ (Z) over the latent variables\nfor the Bayesian mixture of Gaussians is given by (10.48) by verifying the steps\ngiven in the text.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 537,
      "page_label": "518"
    }
  },
  {
    "page_content": "Exercises 519\n10.13 (⋆⋆ ) www Starting from (10.54), derive the result (10.59) for the optimum vari-\national posterior distribution over µk and Λk in the Bayesian mixture of Gaussians,\nand hence verify the expressions for the parameters of this distribution given by\n(10.60)–(10.63).\n10.14 (⋆⋆ ) Using the distribution (10.59), verify the result (10.64).\n10.15 (⋆) Using the result (B.17), show that the expected value of the mixing coefﬁcients\nin the variational mixture of Gaussians is given by (10.69).\n10.16 (⋆⋆ ) www Verify the results (10.71) and (10.72) for the ﬁrst two terms in the\nlower bound for the variational Gaussian mixture model given by (10.70).\n10.17 (⋆⋆⋆ ) Verify the results (10.73)–(10.77) for the remaining terms in the lower bound\nfor the variational Gaussian mixture model given by (10.70).\n10.18 (⋆⋆⋆ ) In this exercise, we shall derive the variational re-estimation equations for\nthe Gaussian mixture model by direct differentiation of the lower bound. To do this\nwe assume that the variational distribution has the factorization deﬁned by (10.42)\nand (10.55) with factors given by (10.48), (10.57), and (10.59). Substitute these into\n(10.70) and hence obtain the lower bound as a function of the parameters of the varia-\ntional distribution. Then, by maximizing the bound with respect to these parameters,\nderive the re-estimation equations for the factors in the variational distribution, and\nshow that these are the same as those obtained in Section 10.2.1.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 538,
      "page_label": "519"
    }
  },
  {
    "page_content": "show that these are the same as those obtained in Section 10.2.1.\n10.19 (⋆⋆ ) Derive the result (10.81) for the predictive distribution in the variational treat-\nment of the Bayesian mixture of Gaussians model.\n10.20 (⋆⋆ ) www This exercise explores the variational Bayes solution for the mixture of\nGaussians model when the sizeN of the data set is large and shows that it reduces (as\nwe would expect) to the maximum likelihood solution based on EM derived in Chap-\nter 9. Note that results from Appendix B may be used to help answer this exercise.\nFirst show that the posterior distributionq⋆ (Λk) of the precisions becomes sharply\npeaked around the maximum likelihood solution. Do the same for the posterior dis-\ntribution of the means q⋆ (µk|Λk). Next consider the posterior distribution q⋆ (π)\nfor the mixing coefﬁcients and show that this too becomes sharply peaked around\nthe maximum likelihood solution. Similarly, show that the responsibilities become\nequal to the corresponding maximum likelihood values for large N, by making use\nof the following asymptotic result for the digamma function for large x\nψ(x)=l n x + O (1/x) . (10.241)\nFinally, by making use of (10.80), show that for large N, the predictive distribution\nbecomes a mixture of Gaussians.\n10.21 (⋆) Show that the number of equivalent parameter settings due to interchange sym-\nmetries in a mixture model with K components is K!.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 538,
      "page_label": "519"
    }
  },
  {
    "page_content": "520 10. APPROXIMATE INFERENCE\n10.22 (⋆⋆ ) We have seen that each mode of the posterior distribution in a Gaussian mix-\nture model is a member of a family of K! equivalent modes. Suppose that the result\nof running the variational inference algorithm is an approximate posterior distribu-\ntion q that is localized in the neighbourhood of one of the modes. We can then\napproximate the full posterior distribution as a mixture of K! such q distributions,\nonce centred on each mode and having equal mixing coefﬁcients. Show that if we\nassume negligible overlap between the components of the q mixture, the resulting\nlower bound differs from that for a single component q distribution through the ad-\ndition of an extra term ln K!.\n10.23 (⋆⋆ ) www Consider a variational Gaussian mixture model in which there is no\nprior distribution over mixing coefﬁcients {πk}. Instead, the mixing coefﬁcients are\ntreated as parameters, whose values are to be found by maximizing the variational\nlower bound on the log marginal likelihood. Show that maximizing this lower bound\nwith respect to the mixing coefﬁcients, using a Lagrange multiplier to enforce the\nconstraint that the mixing coefﬁcients sum to one, leads to the re-estimation result\n(10.83). Note that there is no need to consider all of the terms in the lower bound but\nonly the dependence of the bound on the {πk}.\n10.24 (⋆⋆ ) www We have seen in Section 10.2 that the singularities arising in the max-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 539,
      "page_label": "520"
    }
  },
  {
    "page_content": "only the dependence of the bound on the {πk}.\n10.24 (⋆⋆ ) www We have seen in Section 10.2 that the singularities arising in the max-\nimum likelihood treatment of Gaussian mixture models do not arise in a Bayesian\ntreatment. Discuss whether such singularities would arise if the Bayesian model\nwere solved using maximum posterior (MAP) estimation.\n10.25 (⋆⋆ ) The variational treatment of the Bayesian mixture of Gaussians, discussed in\nSection 10.2, made use of a factorized approximation (10.5) to the posterior distribu-\ntion. As we saw in Figure 10.2, the factorized assumption causes the variance of the\nposterior distribution to be under-estimated for certain directions in parameter space.\nDiscuss qualitatively the effect this will have on the variational approximation to the\nmodel evidence, and how this effect will vary with the number of components in\nthe mixture. Hence explain whether the variational Gaussian mixture will tend to\nunder-estimate or over-estimate the optimal number of components.\n10.26 (⋆⋆⋆ ) Extend the variational treatment of Bayesian linear regression to include\na gamma hyperprior Gam(β|c0,d 0) over β and solve variationally, by assuming a\nfactorized variational distribution of the form q(w)q(α)q(β). Derive the variational\nupdate equations for the three factors in the variational distribution and also obtain\nan expression for the lower bound and for the predictive distribution.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 539,
      "page_label": "520"
    }
  },
  {
    "page_content": "an expression for the lower bound and for the predictive distribution.\n10.27 (⋆⋆ ) By making use of the formulae given in Appendix B show that the variational\nlower bound for the linear basis function regression model, deﬁned by (10.107), can\nbe written in the form (10.107) with the various terms deﬁned by (10.108)–(10.112).\n10.28 (⋆⋆⋆ ) Rewrite the model for the Bayesian mixture of Gaussians, introduced in\nSection 10.2, as a conjugate model from the exponential family, as discussed in\nSection 10.4. Hence use the general results (10.115) and (10.119) to derive the\nspeciﬁc results (10.48), (10.57), and (10.59).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 539,
      "page_label": "520"
    }
  },
  {
    "page_content": "Exercises 521\n10.29 (⋆) www Show that the function f(x)=l n ( x) is concave for 0 <x< ∞\nby computing its second derivative. Determine the form of the dual function g(λ)\ndeﬁned by (10.133), and verify that minimization of λx − g(λ) with respect to λ\naccording to (10.132) indeed recovers the function ln(x).\n10.30 (⋆) By evaluating the second derivative, show that the log logistic function f(x)=\n−ln(1 + e−x) is concave. Derive the variational upper bound (10.137) directly by\nmaking a second order Taylor expansion of the log logistic function around a point\nx = ξ.\n10.31 (⋆⋆ ) By ﬁnding the second derivative with respect to x, show that the function\nf(x)= −ln(ex/2 + e−x/2) is a concave function of x. Now consider the second\nderivatives with respect to the variablex2 and hence show that it is a convex function\nof x2. Plot graphs of f(x) against x and against x2. Derive the lower bound (10.144)\non the logistic sigmoid function directly by making a ﬁrst order Taylor series expan-\nsion of the functionf(x) in the variable x2 centred on the value ξ2.\n10.32 (⋆⋆ ) www Consider the variational treatment of logistic regression with sequen-\ntial learning in which data points are arriving one at a time and each must be pro-\ncessed and discarded before the next data point arrives. Show that a Gaussian ap-\nproximation to the posterior distribution can be maintained through the use of the\nlower bound (10.151), in which the distribution is initialized using the prior, and as",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 540,
      "page_label": "521"
    }
  },
  {
    "page_content": "lower bound (10.151), in which the distribution is initialized using the prior, and as\neach data point is absorbed its corresponding variational parameter ξn is optimized.\n10.33 (⋆) By differentiating the quantity Q(ξ, ξold) deﬁned by (10.161) with respect to\nthe variational parameter ξn show that the update equation for ξn for the Bayesian\nlogistic regression model is given by (10.163).\n10.34 (⋆⋆ ) In this exercise we derive re-estimation equations for the variational parame-\nters ξ in the Bayesian logistic regression model of Section 4.5 by direct maximization\nof the lower bound given by (10.164). To do this set the derivative of L(ξ) with re-\nspect to ξn equal to zero, making use of the result (3.117) for the derivative of the log\nof a determinant, together with the expressions (10.157) and (10.158) which deﬁne\nthe mean and covariance of the variational posterior distributionq(w).\n10.35 (⋆⋆ ) Derive the result (10.164) for the lower bound L(ξ) in the variational logistic\nregression model. This is most easily done by substituting the expressions for the\nGaussian prior q(w)= N(w|m0, S0), together with the lower bound h(w, ξ) on\nthe likelihood function, into the integral (10.159) which deﬁnes L(ξ). Next gather\ntogether the terms which depend on w in the exponential and complete the square\nto give a Gaussian integral, which can then be evaluated by invoking the standard\nresult for the normalization coefﬁcient of a multivariate Gaussian. Finally take the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 540,
      "page_label": "521"
    }
  },
  {
    "page_content": "result for the normalization coefﬁcient of a multivariate Gaussian. Finally take the\nlogarithm to obtain (10.164).\n10.36 (⋆⋆ ) Consider the ADF approximation scheme discussed in Section 10.7, and show\nthat inclusion of the factor fj(θ) leads to an update of the model evidence of the\nform\npj(D) ≃ pj−1(D)Zj (10.242)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 540,
      "page_label": "521"
    }
  },
  {
    "page_content": "522 10. APPROXIMATE INFERENCE\nwhere Zj is the normalization constant deﬁned by (10.197). By applying this result\nrecursively, and initializing with p0(D)=1 , derive the result\np(D) ≃\n∏\nj\nZj. (10.243)\n10.37 (⋆) www Consider the expectation propagation algorithm from Section 10.7, and\nsuppose that one of the factors f0(θ) in the deﬁnition (10.188) has the same expo-\nnential family functional form as the approximating distribution q(θ). Show that if\nthe factor ˜f0(θ) is initialized to be f0(θ), then an EP update to reﬁne ˜f0(θ) leaves\n˜f0(θ) unchanged. This situation typically arises when one of the factors is the prior\np(θ), and so we see that the prior factor can be incorporated once exactly and does\nnot need to be reﬁned.\n10.38 (⋆⋆⋆ ) In this exercise and the next, we shall verify the results (10.214)–(10.224)\nfor the expectation propagation algorithm applied to the clutter problem. Begin by\nusing the division formula (10.205) to derive the expressions (10.214) and (10.215)\nby completing the square inside the exponential to identify the mean and variance.\nAlso, show that the normalization constant Zn, deﬁned by (10.206), is given for the\nclutter problem by (10.216). This can be done by making use of the general result\n(2.115).\n10.39 (⋆⋆⋆ ) Show that the mean and variance of qnew(θ) for EP applied to the clutter\nproblem are given by (10.217) and (10.218). To do this, ﬁrst prove the following\nresults for the expectations ofθ and θθT under qnew(θ)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 541,
      "page_label": "522"
    }
  },
  {
    "page_content": "problem are given by (10.217) and (10.218). To do this, ﬁrst prove the following\nresults for the expectations ofθ and θθT under qnew(θ)\nE[θ]= m\\n + v\\n∇m\\n lnZn (10.244)\nE[θTθ]=2 ( v\\n)2∇v\\n ln Zn +2 E[θ]Tm\\n −∥m\\n∥2 (10.245)\nand then make use of the result (10.216) for Zn. Next, prove the results (10.220)–\n(10.222) by using (10.207) and completing the square in the exponential. Finally,\nuse (10.208) to derive the result (10.223).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 541,
      "page_label": "522"
    }
  },
  {
    "page_content": "11\nSampling\nMethods\nFor most probabilistic models of practical interest, exact inference is intractable, and\nso we have to resort to some form of approximation. In Chapter 10, we discussed\ninference algorithms based on deterministic approximations, which include methods\nsuch as variational Bayes and expectation propagation. Here we consider approxi-\nmate inference methods based on numerical sampling, also known as Monte Carlo\ntechniques.\nAlthough for some applications the posterior distribution over unobserved vari-\nables will be of direct interest in itself, for most situations the posterior distribution\nis required primarily for the purpose of evaluating expectations, for example in order\nto make predictions. The fundamental problem that we therefore wish to address in\nthis chapter involves ﬁnding the expectation of some function f(z) with respect to a\nprobability distribution p(z). Here, the components of z might comprise discrete or\ncontinuous variables or some combination of the two. Thus in the case of continuous\n523",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 542,
      "page_label": "523"
    }
  },
  {
    "page_content": "524 11. SAMPLING METHODS\nFigure 11.1 Schematic illustration of a function f(z)\nwhose expectation is to be evaluated with\nrespect to a distribution p(z).\np(z) f(z)\nz\nvariables, we wish to evaluate the expectation\nE[f]=\n∫\nf(z)p(z)d z (11.1)\nwhere the integral is replaced by summation in the case of discrete variables. This\nis illustrated schematically for a single continuous variable in Figure 11.1. We shall\nsuppose that such expectations are too complex to be evaluated exactly using analyt-\nical techniques.\nThe general idea behind sampling methods is to obtain a set of samples z(l)\n(where l =1 ,...,L ) drawn independently from the distribution p(z). This allows\nthe expectation (11.1) to be approximated by a ﬁnite sum\nˆf = 1\nL\nL∑\nl=1\nf(z(l)). (11.2)\nAs long as the samples z(l) are drawn from the distribution p(z), then E[ˆf]= E[f]\nand so the estimator ˆf has the correct mean. The variance of the estimator is given\nbyExercise 11.1\nvar[ˆf]= 1\nLE\n[\n(f − E[f])2]\n(11.3)\nis the variance of the functionf(z) under the distribution p(z). It is worth emphasiz-\ning that the accuracy of the estimator therefore does not depend on the dimension-\nality of z, and that, in principle, high accuracy may be achievable with a relatively\nsmall number of samples z(l). In practice, ten or twenty independent samples may\nsufﬁce to estimate an expectation to sufﬁcient accuracy.\nThe problem, however, is that the samples{z(l)} might not be independent, and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 543,
      "page_label": "524"
    }
  },
  {
    "page_content": "sufﬁce to estimate an expectation to sufﬁcient accuracy.\nThe problem, however, is that the samples{z(l)} might not be independent, and\nso the effective sample size might be much smaller than the apparent sample size.\nAlso, referring back to Figure 11.1, we note that if f(z) is small in regions where\np(z) is large, and vice versa, then the expectation may be dominated by regions\nof small probability, implying that relatively large sample sizes will be required to\nachieve sufﬁcient accuracy.\nFor many models, the joint distribution p(z) is conveniently speciﬁed in terms\nof a graphical model. In the case of a directed graph with no observed variables, it is",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 543,
      "page_label": "524"
    }
  },
  {
    "page_content": "11. SAMPLING METHODS 525\nstraightforward to sample from the joint distribution (assuming that it is possible to\nsample from the conditional distributions at each node) using the following ances-\ntral sampling approach, discussed brieﬂy in Section 8.1.2. The joint distribution is\nspeciﬁed by\np(z)=\nM∏\ni=1\np(zi|pai) (11.4)\nwhere zi are the set of variables associated with node i, and pai denotes the set of\nvariables associated with the parents of node i. To obtain a sample from the joint\ndistribution, we make one pass through the set of variables in the order z1,..., zM\nsampling from the conditional distributions p(zi|pai). This is always possible be-\ncause at each step all of the parent values will have been instantiated. After one pass\nthrough the graph, we will have obtained a sample from the joint distribution.\nNow consider the case of a directed graph in which some of the nodes are in-\nstantiated with observed values. We can in principle extend the above procedure, at\nleast in the case of nodes representing discrete variables, to give the following logic\nsampling approach (Henrion, 1988), which can be seen as a special case of impor-\ntance sampling discussed in Section 11.1.4. At each step, when a sampled value is\nobtained for a variable zi whose value is observed, the sampled value is compared\nto the observed value, and if they agree then the sample value is retained and the al-\ngorithm proceeds to the next variable in turn. However, if the sampled value and the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 544,
      "page_label": "525"
    }
  },
  {
    "page_content": "gorithm proceeds to the next variable in turn. However, if the sampled value and the\nobserved value disagree, then the whole sample so far is discarded and the algorithm\nstarts again with the ﬁrst node in the graph. This algorithm samples correctly from\nthe posterior distribution because it corresponds simply to drawing samples from the\njoint distribution of hidden variables and data variables and then discarding those\nsamples that disagree with the observed data (with the slight saving of not continu-\ning with the sampling from the joint distribution as soon as one contradictory value is\nobserved). However, the overall probability of accepting a sample from the posterior\ndecreases rapidly as the number of observed variables increases and as the number\nof states that those variables can take increases, and so this approach is rarely used\nin practice.\nIn the case of probability distributions deﬁned by an undirected graph, there is\nno one-pass sampling strategy that will sample even from the prior distribution with\nno observed variables. Instead, computationally more expensive techniques must be\nemployed, such as Gibbs sampling, which is discussed in Section 11.3.\nAs well as sampling from conditional distributions, we may also require samples\nfrom a marginal distribution. If we already have a strategy for sampling from a joint\ndistribution p(u, v), then it is straightforward to obtain samples from the marginal",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 544,
      "page_label": "525"
    }
  },
  {
    "page_content": "distribution p(u, v), then it is straightforward to obtain samples from the marginal\ndistribution p(u) simply by ignoring the values for v in each sample.\nThere are numerous texts dealing with Monte Carlo methods. Those of partic-\nular interest from the statistical inference perspective include Chen et al. (2001),\nGamerman (1997), Gilks et al. (1996), Liu (2001), Neal (1996), and Robert and\nCasella (1999). Also there are review articles by Besaget al. (1995), Brooks (1998),\nDiaconis and Saloff-Coste (1998), Jerrum and Sinclair (1996), Neal (1993), Tierney\n(1994), and Andrieuet al. (2003) that provide additional information on sampling",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 544,
      "page_label": "525"
    }
  },
  {
    "page_content": "526 11. SAMPLING METHODS\nmethods for statistical inference.\nDiagnostic tests for convergence of Markov chain Monte Carlo algorithms are\nsummarized in Robert and Casella (1999), and some practical guidance on the use of\nsampling methods in the context of machine learning is given in Bishop and Nabney\n(2008).\n11.1. Basic Sampling Algorithms\nIn this section, we consider some simple strategies for generating random samples\nfrom a given distribution. Because the samples will be generated by a computer\nalgorithm they will in fact be pseudo-random numbers, that is, they will be deter-\nministically calculated, but must nevertheless pass appropriate tests for randomness.\nGenerating such numbers raises several subtleties (Presset al., 1992) that lie outside\nthe scope of this book. Here we shall assume that an algorithm has been provided\nthat generates pseudo-random numbers distributed uniformly over(0,1), and indeed\nmost software environments have such a facility built in.\n11.1.1 Standard distributions\nWe ﬁrst consider how to generate random numbers from simple nonuniform dis-\ntributions, assuming that we already have available a source of uniformly distributed\nrandom numbers. Suppose that z is uniformly distributed over the interval (0, 1),\nand that we transform the values of z using some function f(·) so that y = f(z).\nThe distribution of y will be governed by\np(y)= p(z)\n⏐⏐⏐⏐\ndz\ndy\n⏐⏐⏐⏐ (11.5)\nwhere, in this case, p(z)=1 . Our goal is to choose the function f(z) such that the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 545,
      "page_label": "526"
    }
  },
  {
    "page_content": "p(y)= p(z)\n⏐⏐⏐⏐\ndz\ndy\n⏐⏐⏐⏐ (11.5)\nwhere, in this case, p(z)=1 . Our goal is to choose the function f(z) such that the\nresulting values of y have some speciﬁc desired distribution p(y). Integrating (11.5)\nwe obtain\nz = h(y) ≡\n∫ y\n−∞\np(ˆy)dˆy (11.6)\nwhich is the indeﬁnite integral of p(y). Thus, y = h−1(z), and so we have toExercise 11.2\ntransform the uniformly distributed random numbers using a function which is the\ninverse of the indeﬁnite integral of the desired distribution. This is illustrated in\nFigure 11.2.\nConsider for example the exponential distribution\np(y)= λ exp(−λy) (11.7)\nwhere 0 ⩽ y< ∞. In this case the lower limit of the integral in (11.6) is 0, and so\nh(y)=1 − exp(−λy). Thus, if we transform our uniformly distributed variable z\nusing y = −λ−1 ln(1 − z), then y will have an exponential distribution.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 545,
      "page_label": "526"
    }
  },
  {
    "page_content": "11.1. Basic Sampling Algorithms 527\nFigure 11.2 Geometrical interpretation of the trans-\nformation method for generating nonuni-\nformly distributed random numbers. h(y)\nis the indeﬁnite integral of the desired dis-\ntribution p(y). If a uniformly distributed\nrandom variable z is transformed using\ny = h−1(z), then y will be distributed ac-\ncording to p(y). p(y)\nh(y)\ny0\n1\nAnother example of a distribution to which the transformation method can be\napplied is given by the Cauchy distribution\np(y)= 1\nπ\n1\n1+ y2 . (11.8)\nIn this case, the inverse of the indeﬁnite integral can be expressed in terms of the\n‘tan’ function.Exercise 11.3\nThe generalization to multiple variables is straightforward and involves the Ja-\ncobian of the change of variables, so that\np(y1,...,y M )= p(z1,...,z M )\n⏐⏐⏐⏐\n∂(z1,...,z M )\n∂(y1,...,y M )\n⏐⏐⏐⏐. (11.9)\nAs a ﬁnal example of the transformation method we consider the Box-Muller\nmethod for generating samples from a Gaussian distribution. First, suppose we gen-\nerate pairs of uniformly distributed random numbers z1,z 2 ∈ (−1,1), which we can\ndo by transforming a variable distributed uniformly over (0, 1) using z → 2z − 1.\nNext we discard each pair unless it satisﬁes z2\n1 + z2\n2 ⩽ 1. This leads to a uniform\ndistribution of points inside the unit circle with p(z1,z 2)=1 /π, as illustrated in\nFigure 11.3. Then, for each pair z1,z 2 we evaluate the quantities\nFigure 11.3 The Box-Muller method for generating Gaussian dis-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 546,
      "page_label": "527"
    }
  },
  {
    "page_content": "Figure 11.3. Then, for each pair z1,z 2 we evaluate the quantities\nFigure 11.3 The Box-Muller method for generating Gaussian dis-\ntributed random numbers starts by generating samples\nfrom a uniform distribution inside the unit circle.\n−1−1\n1\n1z1\nz2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 546,
      "page_label": "527"
    }
  },
  {
    "page_content": "528 11. SAMPLING METHODS\ny1 = z1\n( −2l nz1\nr2\n)1/2\n(11.10)\ny2 = z2\n( −2l nz2\nr2\n)1/2\n(11.11)\nwhere r2 = z2\n1 + z2\n2. Then the joint distribution of y1 and y2 is given byExercise 11.4\np(y1,y 2)= p(z1,z 2)\n⏐⏐⏐⏐\n∂(z1,z 2)\n∂(y1,y 2)\n⏐⏐⏐⏐\n=\n[ 1√\n2π\nexp(−y2\n1/2)\n][ 1√\n2π\nexp(−y2\n2/2)\n]\n(11.12)\nand so y1 and y2 are independent and each has a Gaussian distribution with zero\nmean and unit variance.\nIf y has a Gaussian distribution with zero mean and unit variance, then σy + µ\nwill have a Gaussian distribution with mean µ and variance σ2. To generate vector-\nvalued variables having a multivariate Gaussian distribution with mean µ and co-\nvariance Σ, we can make use of the Cholesky decomposition, which takes the form\nΣ = LLT (Press et al., 1992). Then, if z is a vector valued random variable whose\ncomponents are independent and Gaussian distributed with zero mean and unit vari-\nance, then y = µ + Lz will have mean µ and covariance Σ.Exercise 11.5\nObviously, the transformation technique depends for its success on the ability\nto calculate and then invert the indeﬁnite integral of the required distribution. Such\noperations will only be feasible for a limited number of simple distributions, and so\nwe must turn to alternative approaches in search of a more general strategy. Here\nwe consider two techniques called rejection samplingand importance sampling.A l -\nthough mainly limited to univariate distributions and thus not directly applicable to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 547,
      "page_label": "528"
    }
  },
  {
    "page_content": "though mainly limited to univariate distributions and thus not directly applicable to\ncomplex problems in many dimensions, they do form important components in more\ngeneral strategies.\n11.1.2 Rejection sampling\nThe rejection sampling framework allows us to sample from relatively complex\ndistributions, subject to certain constraints. We begin by considering univariate dis-\ntributions and discuss the extension to multiple dimensions subsequently.\nSuppose we wish to sample from a distributionp(z) that is not one of the simple,\nstandard distributions considered so far, and that sampling directly from p(z) is dif-\nﬁcult. Furthermore suppose, as is often the case, that we are easily able to evaluate\np(z) for any given value of z, up to some normalizing constant Z, so that\np(z)= 1\nZp\n˜p(z) (11.13)\nwhere ˜p(z) can readily be evaluated, but Zp is unknown.\nIn order to apply rejection sampling, we need some simpler distribution q(z),\nsometimes called a proposal distribution, from which we can readily draw samples.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 547,
      "page_label": "528"
    }
  },
  {
    "page_content": "11.1. Basic Sampling Algorithms 529\nFigure 11.4 In the rejection sampling method,\nsamples are drawn from a sim-\nple distribution q(z) and rejected\nif they fall in the grey area be-\ntween the unnormalized distribu-\ntion ep(z) and the scaled distribu-\ntion kq(z). The resulting samples\nare distributed according to p(z),\nwhich is the normalized version of\nep(z). z0 z\nu0\nkq(z0) kq(z)\n˜p(z)\nWe next introduce a constant k whose value is chosen such that kq(z) ⩾ ˜p(z) for\nall values of z. The function kq(z) is called the comparison function and is illus-\ntrated for a univariate distribution in Figure 11.4. Each step of the rejection sampler\ninvolves generating two random numbers. First, we generate a number z0 from the\ndistribution q(z). Next, we generate a number u0 from the uniform distribution over\n[0,kq (z0)]. This pair of random numbers has uniform distribution under the curve\nof the function kq(z). Finally, if u0 > ˜p(z0) then the sample is rejected, otherwise\nu0 is retained. Thus the pair is rejected if it lies in the grey shaded region in Fig-\nure 11.4. The remaining pairs then have uniform distribution under the curve of˜p(z),\nand hence the corresponding z values are distributed according to p(z), as desired.Exercise 11.6\nThe original values of z are generated from the distributionq(z), and these sam-\nples are then accepted with probability ˜p(z)/kq(z), and so the probability that a\nsample will be accepted is given by\np(accept) =\n∫\n{˜p(z)/kq(z)}q(z)d z\n= 1\nk\n∫",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 548,
      "page_label": "529"
    }
  },
  {
    "page_content": "sample will be accepted is given by\np(accept) =\n∫\n{˜p(z)/kq(z)}q(z)d z\n= 1\nk\n∫\n˜p(z)d z. (11.14)\nThus the fraction of points that are rejected by this method depends on the ratio of\nthe area under the unnormalized distribution ˜p(z) to the area under the curve kq(z).\nWe therefore see that the constant k should be as small as possible subject to the\nlimitation that kq(z) must be nowhere less than ˜p(z).\nAs an illustration of the use of rejection sampling, consider the task of sampling\nfrom the gamma distribution\nGam(z|a, b)= baza−1 exp(−bz)\nΓ(a) (11.15)\nwhich, for a> 1, has a bell-shaped form, as shown in Figure 11.5. A suitable\nproposal distribution is therefore the Cauchy (11.8) because this too is bell-shaped\nand because we can use the transformation method, discussed earlier, to sample from\nit. We need to generalize the Cauchy slightly to ensure that it nowhere has a smaller\nvalue than the gamma distribution. This can be achieved by transforming a uniform\nrandom variable y using z = b tany + c, which gives random numbers distributed\naccording to.Exercise 11.7",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 548,
      "page_label": "529"
    }
  },
  {
    "page_content": "530 11. SAMPLING METHODS\nFigure 11.5 Plot showing the gamma distribu-\ntion given by (11.15) as the green\ncurve, with a scaled Cauchy pro-\nposal distribution shown by the red\ncurve. Samples from the gamma\ndistribution can be obtained by\nsampling from the Cauchy and\nthen applying the rejection sam-\npling criterion.\nz\np(z)\n0 10 20 30\n0\n0.05\n0.1\n0.15\nq(z)= k\n1+( z − c)2/b2 . (11.16)\nThe minimum reject rate is obtained by setting c = a − 1, b2 =2 a − 1 and choos-\ning the constant k to be as small as possible while still satisfying the requirement\nkq(z) ⩾ ˜p(z). The resulting comparison function is also illustrated in Figure 11.5.\n11.1.3 Adaptive rejection sampling\nIn many instances where we might wish to apply rejection sampling, it proves\ndifﬁcult to determine a suitable analytic form for the envelope distribution q(z).A n\nalternative approach is to construct the envelope function on the ﬂy based on mea-\nsured values of the distribution p(z) (Gilks and Wild, 1992). Construction of an\nenvelope function is particularly straightforward for cases in which p(z) is log con-\ncave, in other words when lnp(z) has derivatives that are nonincreasing functions\nof z. The construction of a suitable envelope function is illustrated graphically in\nFigure 11.6.\nThe function lnp(z) and its gradient are evaluated at some initial set of grid\npoints, and the intersections of the resulting tangent lines are used to construct the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 549,
      "page_label": "530"
    }
  },
  {
    "page_content": "points, and the intersections of the resulting tangent lines are used to construct the\nenvelope function. Next a sample value is drawn from the envelope distribution.\nThis is straightforward because the log of the envelope distribution is a successionExercise 11.9\nFigure 11.6 In the case of distributions that are\nlog concave, an envelope function\nfor use in rejection sampling can be\nconstructed using the tangent lines\ncomputed at a set of grid points. If a\nsample point is rejected, it is added\nto the set of grid points and used to\nreﬁne the envelope distribution.\nz1 z2 z3 z\nlnp(z)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 549,
      "page_label": "530"
    }
  },
  {
    "page_content": "11.1. Basic Sampling Algorithms 531\nFigure 11.7 Illustrative example of rejection\nsampling involving sampling from a\nGaussian distribution p(z) shown by\nthe green curve, by using rejection\nsampling from a proposal distri-\nbution q(z) that is also Gaussian\nand whose scaled version kq(z) is\nshown by the red curve.\nz\np(z)\n−5 0 5\n0\n0.25\n0.5\nof linear functions, and hence the envelope distribution itself comprises a piecewise\nexponential distribution of the form\nq(z)= kiλi exp {−λi(z − zi−1)} zi−1 <z ⩽ zi. (11.17)\nOnce a sample has been drawn, the usual rejection criterion can be applied. If the\nsample is accepted, then it will be a draw from the desired distribution. If, however,\nthe sample is rejected, then it is incorporated into the set of grid points, a new tangent\nline is computed, and the envelope function is thereby reﬁned. As the number of\ngrid points increases, so the envelope function becomes a better approximation of\nthe desired distribution p(z) and the probability of rejection decreases.\nA variant of the algorithm exists that avoids the evaluation of derivatives (Gilks,\n1992). The adaptive rejection sampling framework can also be extended to distri-\nbutions that are not log concave, simply by following each rejection sampling step\nwith a Metropolis-Hastings step (to be discussed in Section 11.2.2), giving rise to\nadaptive rejection Metropolissampling (Gilks et al., 1995).\nClearly for rejection sampling to be of practical value, we require that the com-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 550,
      "page_label": "531"
    }
  },
  {
    "page_content": "adaptive rejection Metropolissampling (Gilks et al., 1995).\nClearly for rejection sampling to be of practical value, we require that the com-\nparison function be close to the required distribution so that the rate of rejection is\nkept to a minimum. Now let us examine what happens when we try to use rejection\nsampling in spaces of high dimensionality. Consider, for the sake of illustration,\na somewhat artiﬁcial problem in which we wish to sample from a zero-mean mul-\ntivariate Gaussian distribution with covariance σ2\npI, where I is the unit matrix, by\nrejection sampling from a proposal distribution that is itself a zero-mean Gaussian\ndistribution having covariance σ2\nqI. Obviously, we must have σ2\nq ⩾ σ2\np in order that\nthere exists a k such that kq(z) ⩾ p(z).I n D-dimensions the optimum value of k\nis given by k =( σq/σp)D, as illustrated for D =1 in Figure 11.7. The acceptance\nrate will be the ratio of volumes underp(z) and kq(z), which, because both distribu-\ntions are normalized, is just 1/k. Thus the acceptance rate diminishes exponentially\nwith dimensionality. Even if σq exceeds σp by just one percent, for D =1 , 000 the\nacceptance ratio will be approximately 1/20,000. In this illustrative example the\ncomparison function is close to the required distribution. For more practical exam-\nples, where the desired distribution may be multimodal and sharply peaked, it will\nbe extremely difﬁcult to ﬁnd a good proposal distribution and comparison function.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 550,
      "page_label": "531"
    }
  },
  {
    "page_content": "532 11. SAMPLING METHODS\nFigure 11.8 Importance sampling addresses the prob-\nlem of evaluating the expectation of a func-\ntion f(z) with respect to a distribution p(z)\nfrom which it is difﬁcult to draw samples di-\nrectly. Instead, samples {z(l)} are drawn\nfrom a simpler distribution q(z), and the\ncorresponding terms in the summation are\nweighted by the ratios p(z(l))/q(z(l)).\np(z) f(z)\nz\nq(z)\nFurthermore, the exponential decrease of acceptance rate with dimensionality is a\ngeneric feature of rejection sampling. Although rejection can be a useful technique\nin one or two dimensions it is unsuited to problems of high dimensionality. It can,\nhowever, play a role as a subroutine in more sophisticated algorithms for sampling\nin high dimensional spaces.\n11.1.4 Importance sampling\nOne of the principal reasons for wishing to sample from complicated probability\ndistributions is to be able to evaluate expectations of the form (11.1). The technique\nof importance sampling provides a framework for approximating expectations di-\nrectly but does not itself provide a mechanism for drawing samples from distribution\np(z).\nThe ﬁnite sum approximation to the expectation, given by (11.2), depends on\nbeing able to draw samples from the distribution p(z). Suppose, however, that it is\nimpractical to sample directly from p(z) but that we can evaluatep(z) easily for any\ngiven value of z. One simplistic strategy for evaluating expectations would be to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 551,
      "page_label": "532"
    }
  },
  {
    "page_content": "given value of z. One simplistic strategy for evaluating expectations would be to\ndiscretize z-space into a uniform grid and to evaluate the integrand as a sum of the\nform\nE[f] ≃\nL∑\nl=1\np(z(l))f(z(l)). (11.18)\nAn obvious problem with this approach is that the number of terms in the summation\ngrows exponentially with the dimensionality of z. Furthermore, as we have already\nnoted, the kinds of probability distributions of interest will often have much of their\nmass conﬁned to relatively small regions ofz space and so uniform sampling will be\nvery inefﬁcient because in high-dimensional problems, only a very small proportion\nof the samples will make a signiﬁcant contribution to the sum. We would really like\nto choose the sample points to fall in regions where p(z) is large, or ideally where\nthe product p(z)f(z) is large.\nAs in the case of rejection sampling, importance sampling is based on the use\nof a proposal distribution q(z) from which it is easy to draw samples, as illustrated\nin Figure 11.8. We can then express the expectation in the form of a ﬁnite sum over",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 551,
      "page_label": "532"
    }
  },
  {
    "page_content": "11.1. Basic Sampling Algorithms 533\nsamples {z(l)} drawn from q(z)\nE[f]=\n∫\nf(z)p(z)d z\n=\n∫\nf(z)p(z)\nq(z)q(z)d z\n≃ 1\nL\nL∑\nl=1\np(z(l))\nq(z(l))f(z(l)). (11.19)\nThe quantities rl = p(z(l))/q(z(l)) are known as importance weights, and they cor-\nrect the bias introduced by sampling from the wrong distribution. Note that, unlike\nrejection sampling, all of the samples generated are retained.\nIt will often be the case that the distribution p(z) can only be evaluated up to a\nnormalization constant, so that p(z)= ˜p(z)/Zp where ˜p(z) can be evaluated easily,\nwhereas Zp is unknown. Similarly, we may wish to use an importance sampling\ndistribution q(z)= ˜q(z)/Zq, which has the same property. We then have\nE[f]=\n∫\nf(z)p(z)d z\n= Zq\nZp\n∫\nf(z)˜p(z)\n˜q(z)q(z)d z\n≃ Zq\nZp\n1\nL\nL∑\nl=1\n˜rlf(z(l)). (11.20)\nwhere ˜rl = ˜p(z(l))/˜q(z(l)). We can use the same sample set to evaluate the ratio\nZp/Zq with the result\nZp\nZq\n= 1\nZq\n∫\n˜p(z)d z =\n∫ ˜p(z)\n˜q(z)q(z)d z\n≃ 1\nL\nL∑\nl=1\n˜rl (11.21)\nand hence\nE[f] ≃\nL∑\nl=1\nwlf(z(l)) (11.22)\nwhere we have deﬁned\nwl = ˜rl∑\nm ˜rm\n= ˜p(z(l))/q(z(l))∑\nm ˜p(z(m))/q(z(m)). (11.23)\nAs with rejection sampling, the success of the importance sampling approach\ndepends crucially on how well the sampling distribution q(z) matches the desired",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 552,
      "page_label": "533"
    }
  },
  {
    "page_content": "534 11. SAMPLING METHODS\ndistribution p(z). If, as is often the case, p(z)f(z) is strongly varying and has a sig-\nniﬁcant proportion of its mass concentrated over relatively small regions of z space,\nthen the set of importance weights {rl} may be dominated by a few weights hav-\ning large values, with the remaining weights being relatively insigniﬁcant. Thus the\neffective sample size can be much smaller than the apparent sample sizeL. The prob-\nlem is even more severe if none of the samples falls in the regions where p(z)f(z)\nis large. In that case, the apparent variances of rl and rlf(z(l)) may be small even\nthough the estimate of the expectation may be severely wrong. Hence a major draw-\nback of the importance sampling method is the potential to produce results that are\narbitrarily in error and with no diagnostic indication. This also highlights a key re-\nquirement for the sampling distribution q(z), namely that it should not be small or\nzero in regions where p(z) may be signiﬁcant.\nFor distributions deﬁned in terms of a graphical model, we can apply the impor-\ntance sampling technique in various ways. For discrete variables, a simple approach\nis called uniform sampling. The joint distribution for a directed graph is deﬁned\nby (11.4). Each sample from the joint distribution is obtained by ﬁrst setting those\nvariableszi that are in the evidence set equal to their observed values. Each of the\nremaining variables is then sampled independently from a uniform distribution over",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 553,
      "page_label": "534"
    }
  },
  {
    "page_content": "remaining variables is then sampled independently from a uniform distribution over\nthe space of possible instantiations. To determine the corresponding weight associ-\nated with a samplez(l), we note that the sampling distribution ˜q(z) is uniform over\nthe possible choices for z, and that ˜p(z|x)= ˜p(z), where x denotes the subset of\nvariables that are observed, and the equality follows from the fact that every sample\nzthat is generated is necessarily consistent with the evidence. Thus the weights rl\nare simply proportional to p(z). Note that the variables can be sampled in any order.\nThis approach can yield poor results if the posterior distribution is far from uniform,\nas is often the case in practice.\nAn improvement on this approach is called likelihood weighted sampling(Fung\nand Chang, 1990; Shachter and Peot, 1990) and is based on ancestral sampling of\nthe variables. For each variable in turn, if that variable is in the evidence set, then it\nis just set to its instantiated value. If it is not in the evidence set, then it is sampled\nfrom the conditional distributionp(zi|pai) in which the conditioning variables are\nset to their currently sampled values. The weighting associated with the resulting\nsample z is then given by\nr(z)=\n∏\nzi̸∈e\np(zi|pai)\np(zi|pai)\n∏\nzi∈e\np(zi|pai)\n1 =\n∏\nzi∈e\np(zi|pai). (11.24)\nThis method can be further extended using self-importance sampling (Shachter and\nPeot, 1990) in which the importance sampling distribution is continually updated to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 553,
      "page_label": "534"
    }
  },
  {
    "page_content": "Peot, 1990) in which the importance sampling distribution is continually updated to\nreﬂect the current estimated posterior distribution.\n11.1.5 Sampling-importance-resampling\nThe rejection sampling method discussed in Section 11.1.2 depends in part for\nits success on the determination of a suitable value for the constant k. For many\npairs of distributions p(z) and q(z), it will be impractical to determine a suitable",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 553,
      "page_label": "534"
    }
  },
  {
    "page_content": "11.1. Basic Sampling Algorithms 535\nvalue for k in that any value that is sufﬁciently large to guarantee a bound on the\ndesired distribution will lead to impractically small acceptance rates.\nAs in the case of rejection sampling, thesampling-importance-resampling (SIR)\napproach also makes use of a sampling distribution q(z) but avoids having to de-\ntermine the constant k. There are two stages to the scheme. In the ﬁrst stage,\nL samples z(1),..., z(L) are drawn from q(z). Then in the second stage, weights\nw1,...,w L are constructed using (11.23). Finally, a second set of L samples is\ndrawn from the discrete distribution (z(1),..., z(L)) with probabilities given by the\nweights (w1,...,w L).\nThe resulting L samples are only approximately distributed according to p(z),\nbut the distribution becomes correct in the limit L →∞ . To see this, consider the\nunivariate case, and note that the cumulative distribution of the resampled values is\ngiven by\np(z ⩽ a)=\n∑\nl:z(l)⩽a\nwl\n=\n∑\nl I(z(l) ⩽ a)˜p(z(l))/q(z(l))∑\nl ˜p(z(l))/q(z(l)) (11.25)\nwhere I(.) is the indicator function (which equals 1 if its argument is true and 0\notherwise). Taking the limit L →∞ , and assuming suitable regularity of the dis-\ntributions, we can replace the sums by integrals weighted according to the original\nsampling distribution q(z)\np(z ⩽ a)=\n∫\nI(z ⩽ a) {˜p(z)/q(z)}q(z)d z\n∫\n{˜p(z)/q(z)}q(z)d z\n=\n∫\nI(z ⩽ a)˜p(z)d z\n∫\n˜p(z)d z\n=\n∫\nI(z ⩽ a)p(z)d z (11.26)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 554,
      "page_label": "535"
    }
  },
  {
    "page_content": "sampling distribution q(z)\np(z ⩽ a)=\n∫\nI(z ⩽ a) {˜p(z)/q(z)}q(z)d z\n∫\n{˜p(z)/q(z)}q(z)d z\n=\n∫\nI(z ⩽ a)˜p(z)d z\n∫\n˜p(z)d z\n=\n∫\nI(z ⩽ a)p(z)d z (11.26)\nwhich is the cumulative distribution function ofp(z). Again, we see that the normal-\nization of p(z) is not required.\nFor a ﬁnite value of L, and a given initial sample set, the resampled values will\nonly approximately be drawn from the desired distribution. As with rejection sam-\npling, the approximation improves as the sampling distribution q(z) gets closer to\nthe desired distribution p(z). When q(z)= p(z), the initial samples (z(1),..., z(L))\nhave the desired distribution, and the weightswn =1 /L so that the resampled values\nalso have the desired distribution.\nIf moments with respect to the distribution p(z) are required, then they can be",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 554,
      "page_label": "535"
    }
  },
  {
    "page_content": "536 11. SAMPLING METHODS\nevaluated directly using the original samples together with the weights, because\nE[f(z)] =\n∫\nf(z)p(z)d z\n=\n∫\nf(z)[˜p(z)/q(z)]q(z)d z\n∫\n[˜p(z)/q(z)]q(z)d z\n≃\nL∑\nl=1\nwlf(zl). (11.27)\n11.1.6 Sampling and the EM algorithm\nIn addition to providing a mechanism for direct implementation of the Bayesian\nframework, Monte Carlo methods can also play a role in the frequentist paradigm,\nfor example to ﬁnd maximum likelihood solutions. In particular, sampling methods\ncan be used to approximate the E step of the EM algorithm for models in which the\nE step cannot be performed analytically. Consider a model with hidden variables\nZ, visible (observed) variables X, and parameters θ. The function that is optimized\nwith respect to θ in the M step is the expected complete-data log likelihood, given\nby\nQ(θ, θold)=\n∫\np(Z|X, θold)l np(Z, X|θ)d Z. (11.28)\nWe can use sampling methods to approximate this integral by a ﬁnite sum over sam-\nples {Z(l)}, which are drawn from the current estimate for the posterior distribution\np(Z|X, θold), so that\nQ(θ, θold) ≃ 1\nL\nL∑\nl=1\nlnp(Z(l), X|θ). (11.29)\nThe Q function is then optimized in the usual way in the M step. This procedure is\ncalled the Monte Carlo EM algorithm.\nIt is straightforward to extend this to the problem of ﬁnding the mode of the\nposterior distribution over θ (the MAP estimate) when a prior distribution p(θ) has\nbeen deﬁned, simply by adding lnp(θ) to the function Q(θ, θold) before performing\nthe M step.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 555,
      "page_label": "536"
    }
  },
  {
    "page_content": "been deﬁned, simply by adding lnp(θ) to the function Q(θ, θold) before performing\nthe M step.\nA particular instance of the Monte Carlo EM algorithm, called stochastic EM,\narises if we consider a ﬁnite mixture model, and draw just one sample at each E step.\nHere the latent variable Z characterizes which of the K components of the mixture\nis responsible for generating each data point. In the E step, a sample of Z is taken\nfrom the posterior distribution p(Z|X, θold) where X is the data set. This effectively\nmakes a hard assignment of each data point to one of the components in the mixture.\nIn the M step, this sampled approximation to the posterior distribution is used to\nupdate the model parameters in the usual way.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 555,
      "page_label": "536"
    }
  },
  {
    "page_content": "11.2. Markov Chain Monte Carlo 537\nNow suppose we move from a maximum likelihood approach to a full Bayesian\ntreatment in which we wish to sample from the posterior distribution over the param-\neter vectorθ. In principle, we would like to draw samples from the joint posterior\np(θ, Z|X), but we shall suppose that this is computationally difﬁcult. Suppose fur-\nther that it is relatively straightforward to sample from the complete-data parameter\nposteriorp(θ|Z, X). This inspires the data augmentation algorithm, which alter-\nnates between two steps known as the I-step (imputation step, analogous to an E\nstep) and the P-step (posterior step, analogous to an M step).\nIP Algorithm\nI-step. We wish to sample from p(Z|X) but we cannot do this directly. We\ntherefore note the relation\np(Z|X)=\n∫\np(Z|θ, X)p(θ|X)d θ (11.30)\nand hence forl =1 ,...,L we ﬁrst draw a sampleθ(l) from the current esti-\nmate for p(θ|X), and then use this to draw a sampleZ(l) from p(Z|θ(l), X).\nP-step. Given the relation\np(θ|X)=\n∫\np(θ|Z, X)p(Z|X)d Z (11.31)\nwe use the samples {Z(l)} obtained from the I-step to compute a revised\nestimate of the posterior distribution over θ given by\np(θ|X) ≃ 1\nL\nL∑\nl=1\np(θ|Z(l), X). (11.32)\nBy assumption, it will be feasible to sample from this approximation in the\nI-step.\nNote that we are making a (somewhat artiﬁcial) distinction between parameters θ\nand hidden variables Z. From now on, we blur this distinction and focus simply on",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 556,
      "page_label": "537"
    }
  },
  {
    "page_content": "and hidden variables Z. From now on, we blur this distinction and focus simply on\nthe problem of drawing samples from a given posterior distribution.\n11.2. Markov Chain Monte Carlo\nIn the previous section, we discussed the rejection sampling and importance sam-\npling strategies for evaluating expectations of functions, and we saw that they suffer\nfrom severe limitations particularly in spaces of high dimensionality. We therefore\nturn in this section to a very general and powerful framework called Markov chain\nMonte Carlo (MCMC), which allows sampling from a large class of distributions,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 556,
      "page_label": "537"
    }
  },
  {
    "page_content": "538 11. SAMPLING METHODS\nand which scales well with the dimensionality of the sample space. Markov chain\nMonte Carlo methods have their origins in physics (Metropolis and Ulam, 1949),\nand it was only towards the end of the 1980s that they started to have a signiﬁcant\nimpact in the ﬁeld of statistics.\nAs with rejection and importance sampling, we again sample from a proposal\ndistribution. This time, however, we maintain a record of the current state z(τ), and\nthe proposal distribution q(z|z(τ)) depends on this current state, and so the sequence\nof samples z(1), z(2),... forms a Markov chain. Again, if we write p(z)= ˜p(z)/Zp,Section 11.2.1\nwe will assume that ˜p(z) can readily be evaluated for any given value ofz, although\nthe value of Zp may be unknown. The proposal distribution itself is chosen to be\nsufﬁciently simple that it is straightforward to draw samples from it directly. At\neach cycle of the algorithm, we generate a candidate sample z⋆ from the proposal\ndistribution and then accept the sample according to an appropriate criterion.\nIn the basic Metropolis algorithm (Metropolis et al., 1953), we assume that the\nproposal distribution is symmetric, that is q(zA|zB)= q(zB|zA) for all values of\nzA and zB. The candidate sample is then accepted with probability\nA(z⋆ , z(τ))=m i n\n(\n1, ˜p(z⋆ )\n˜p(z(τ))\n)\n. (11.33)\nThis can be achieved by choosing a random numberu with uniform distribution over",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 557,
      "page_label": "538"
    }
  },
  {
    "page_content": "A(z⋆ , z(τ))=m i n\n(\n1, ˜p(z⋆ )\n˜p(z(τ))\n)\n. (11.33)\nThis can be achieved by choosing a random numberu with uniform distribution over\nthe unit interval (0, 1) and then accepting the sample if A(z⋆ , z(τ)) >u . Note that\nif the step from z(τ) to z⋆ causes an increase in the value of p(z), then the candidate\npoint is certain to be kept.\nIf the candidate sample is accepted, then z(τ+1) = z⋆ , otherwise the candidate\npoint z⋆ is discarded, z(τ+1) is set to z(τ) and another candidate sample is drawn\nfrom the distribution q(z|z(τ+1)). This is in contrast to rejection sampling, where re-\njected samples are simply discarded. In the Metropolis algorithm when a candidate\npoint is rejected, the previous sample is included instead in the ﬁnal list of samples,\nleading to multiple copies of samples. Of course, in a practical implementation,\nonly a single copy of each retained sample would be kept, along with an integer\nweighting factor recording how many times that state appears. As we shall see, as\nlong as q(zA|zB) is positive for any values of zA and zB (this is a sufﬁcient but\nnot necessary condition), the distribution of z(τ) tends to p(z) as τ →∞ . It should\nbe emphasized, however, that the sequence z(1), z(2),... is not a set of independent\nsamples from p(z) because successive samples are highly correlated. If we wish to\nobtain independent samples, then we can discard most of the sequence and just re-\ntain every Mth sample. For M sufﬁciently large, the retained samples will for all",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 557,
      "page_label": "538"
    }
  },
  {
    "page_content": "tain every Mth sample. For M sufﬁciently large, the retained samples will for all\npractical purposes be independent. Figure 11.9 shows a simple illustrative exam-\nple of sampling from a two-dimensional Gaussian distribution using the Metropolis\nalgorithm in which the proposal distribution is an isotropic Gaussian.\nFurther insight into the nature of Markov chain Monte Carlo algorithms can be\ngleaned by looking at the properties of a speciﬁc example, namely a simple random",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 557,
      "page_label": "538"
    }
  },
  {
    "page_content": "11.2. Markov Chain Monte Carlo 539\nFigure 11.9 A simple illustration using Metropo-\nlis algorithm to sample from a\nGaussian distribution whose one\nstandard-deviation contour is shown\nby the ellipse. The proposal distribu-\ntion is an isotropic Gaussian distri-\nbution whose standard deviation is\n0.2. Steps that are accepted are\nshown as green lines, and rejected\nsteps are shown in red. A total of\n150 candidate samples are gener-\nated, of which 43 are rejected.\n0 0.5 1 1.5 2 2.5 3\n0\n0.5\n1\n1.5\n2\n2.5\n3\nwalk. Consider a state space z consisting of the integers, with probabilities\np(z(τ+1) = z(τ))=0 .5 (11.34)\np(z(τ+1) = z(τ) +1 ) = 0 .25 (11.35)\np(z(τ+1) = z(τ) − 1) = 0 .25 (11.36)\nwhere z(τ) denotes the state at step τ. If the initial state is z(1) =0 , then by sym-\nmetry the expected state at time τ will also be zero E[z(τ)]=0 , and similarly it is\neasily seen that E[(z(τ))2]= τ/2. Thus after τ steps, the random walk has only trav-Exercise 11.10\nelled a distance that on average is proportional to the square root of τ. This square\nroot dependence is typical of random walk behaviour and shows that random walks\nare very inefﬁcient in exploring the state space. As we shall see, a central goal in\ndesigning Markov chain Monte Carlo methods is to avoid random walk behaviour.\n11.2.1 Markov chains\nBefore discussing Markov chain Monte Carlo methods in more detail, it is use-\nful to study some general properties of Markov chains in more detail. In particular,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 558,
      "page_label": "539"
    }
  },
  {
    "page_content": "ful to study some general properties of Markov chains in more detail. In particular,\nwe ask under what circumstances will a Markov chain converge to the desired dis-\ntribution. A ﬁrst-order Markov chain is deﬁned to be a series of random variables\nz(1),..., z(M) such that the following conditional independence property holds for\nm ∈{ 1,...,M − 1}\np(z(m+1)|z(1),..., z(m))= p(z(m+1)|z(m)). (11.37)\nThis of course can be represented as a directed graph in the form of a chain, an ex-\nample of which is shown in Figure 8.38. We can then specify the Markov chain by\ngiving the probability distribution for the initial variable p(z(0)) together with the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 558,
      "page_label": "539"
    }
  },
  {
    "page_content": "540 11. SAMPLING METHODS\nconditional probabilities for subsequent variables in the form of transition probabil-\nities Tm(z(m), z(m+1)) ≡ p(z(m+1)|z(m)). A Markov chain is called homogeneous\nif the transition probabilities are the same for all m.\nThe marginal probability for a particular variable can be expressed in terms of\nthe marginal probability for the previous variable in the chain in the form\np(z(m+1))=\n∑\nz(m)\np(z(m+1)|z(m))p(z(m)). (11.38)\nA distribution is said to be invariant, or stationary, with respect to a Markov chain\nif each step in the chain leaves that distribution invariant. Thus, for a homogeneous\nMarkov chain with transition probabilitiesT(z′, z), the distributionp⋆ (z) is invariant\nif\np⋆ (z)=\n∑\nz′\nT(z′, z)p⋆ (z′). (11.39)\nNote that a given Markov chain may have more than one invariant distribution. For\ninstance, if the transition probabilities are given by the identity transformation, then\nany distribution will be invariant.\nA sufﬁcient (but not necessary) condition for ensuring that the required distribu-\ntion p(z) is invariant is to choose the transition probabilities to satisfy the property\nof detailed balance, deﬁned by\np⋆ (z)T(z, z′)= p⋆ (z′)T(z′, z) (11.40)\nfor the particular distribution p⋆ (z). It is easily seen that a transition probability\nthat satisﬁes detailed balance with respect to a particular distribution will leave that\ndistribution invariant, because\n∑\nz′\np⋆ (z′)T(z′, z)=\n∑\nz′\np⋆ (z)T(z, z′)= p⋆ (z)\n∑\nz′\np(z′|z)= p⋆ (z). (11.41)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 559,
      "page_label": "540"
    }
  },
  {
    "page_content": "distribution invariant, because\n∑\nz′\np⋆ (z′)T(z′, z)=\n∑\nz′\np⋆ (z)T(z, z′)= p⋆ (z)\n∑\nz′\np(z′|z)= p⋆ (z). (11.41)\nA Markov chain that respects detailed balance is said to be reversible.\nOur goal is to use Markov chains to sample from a given distribution. We can\nachieve this if we set up a Markov chain such that the desired distribution is invariant.\nHowever, we must also require that for m →∞ , the distribution p(z(m)) converges\nto the required invariant distribution p⋆ (z), irrespective of the choice of initial dis-\ntribution p(z(0)). This property is called ergodicity, and the invariant distribution\nis then called the equilibrium distribution. Clearly, an ergodic Markov chain can\nhave only one equilibrium distribution. It can be shown that a homogeneous Markov\nchain will be ergodic, subject only to weak restrictions on the invariant distribution\nand the transition probabilities (Neal, 1993).\nIn practice we often construct the transition probabilities from a set of ‘base’\ntransitions B1,...,B K. This can be achieved through a mixture distribution of the\nform\nT(z′, z)=\nK∑\nk=1\nαkBk(z′, z) (11.42)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 559,
      "page_label": "540"
    }
  },
  {
    "page_content": "11.2. Markov Chain Monte Carlo 541\nfor some set of mixing coefﬁcients α1,...,α K satisfying αk ⩾ 0 and ∑\nk αk =1 .\nAlternatively, the base transitions may be combined through successive application,\nso that\nT(z′, z)=\n∑\nz1\n...\n∑\nzn−1\nB1(z′, z1) ...B K−1(zK−2, zK−1)BK(zK−1, z). (11.43)\nIf a distribution is invariant with respect to each of the base transitions, then obvi-\nously it will also be invariant with respect to either of the T(z′, z) given by (11.42)\nor (11.43). For the case of the mixture (11.42), if each of the base transitions sat-\nisﬁes detailed balance, then the mixture transition T will also satisfy detailed bal-\nance. This does not hold for the transition probability constructed using (11.43), al-\nthough by symmetrizing the order of application of the base transitions, in the form\nB1,B 2,...,B K,B K,...,B 2,B 1, detailed balance can be restored. A common ex-\nample of the use of composite transition probabilities is where each base transition\nchanges only a subset of the variables.\n11.2.2 The Metropolis-Hastings algorithm\nEarlier we introduced the basic Metropolis algorithm, without actually demon-\nstrating that it samples from the required distribution. Before giving a proof, we\nﬁrst discuss a generalization, known as the Metropolis-Hastings algorithm (Hast-\nings, 1970), to the case where the proposal distribution is no longer a symmetric\nfunction of its arguments. In particular at step τ of the algorithm, in which the cur-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 560,
      "page_label": "541"
    }
  },
  {
    "page_content": "function of its arguments. In particular at step τ of the algorithm, in which the cur-\nrent state is z(τ), we draw a sample z⋆ from the distribution qk(z|z(τ)) and then\naccept it with probability Ak(z⋆ , zτ ) where\nAk(z⋆ , z(τ))=m i n\n(\n1, ˜p(z⋆ )qk(z(τ)|z⋆ )\n˜p(z(τ))qk(z⋆ |z(τ))\n)\n. (11.44)\nHere k labels the members of the set of possible transitions being considered. Again,\nthe evaluation of the acceptance criterion does not require knowledge of the normal-\nizing constant Zp in the probability distribution p(z)= ˜p(z)/Zp. For a symmetric\nproposal distribution the Metropolis-Hastings criterion (11.44) reduces to the stan-\ndard Metropolis criterion given by (11.33).\nWe can show that p(z) is an invariant distribution of the Markov chain deﬁned\nby the Metropolis-Hastings algorithm by showing that detailed balance, deﬁned by\n(11.40), is satisﬁed. Using (11.44) we have\np(z)qk(z|z′)Ak(z′, z)=m i n ( p(z)qk(z|z′),p (z′)qk(z′|z))\n=m i n (p(z′)qk(z′|z),p (z)qk(z|z′))\n= p(z′)qk(z′|z)Ak(z, z′) (11.45)\nas required.\nThe speciﬁc choice of proposal distribution can have a marked effect on the\nperformance of the algorithm. For continuous state spaces, a common choice is a\nGaussian centred on the current state, leading to an important trade-off in determin-\ning the variance parameter of this distribution. If the variance is small, then the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 560,
      "page_label": "541"
    }
  },
  {
    "page_content": "542 11. SAMPLING METHODS\nFigure 11.10 Schematic illustration of the use of an isotropic\nGaussian proposal distribution (blue circle) to\nsample from a correlated multivariate Gaussian\ndistribution (red ellipse) having very different stan-\ndard deviations in different directions, using the\nMetropolis-Hastings algorithm. In order to keep\nthe rejection rate low, the scale ρ of the proposal\ndistribution should be on the order of the smallest\nstandard deviation σmin, which leads to random\nwalk behaviour in which the number of steps sep-\narating states that are approximately independent\nis of order (σmax/σmin)2 where σmax is the largest\nstandard deviation.\nσmax\nσmin\nρ\nproportion of accepted transitions will be high, but progress through the state space\ntakes the form of a slow random walk leading to long correlation times. However,\nif the variance parameter is large, then the rejection rate will be high because, in the\nkind of complex problems we are considering, many of the proposed steps will be\nto states for which the probability p(z) is low. Consider a multivariate distribution\np(z) having strong correlations between the components of z, as illustrated in Fig-\nure 11.10. The scale ρ of the proposal distribution should be as large as possible\nwithout incurring high rejection rates. This suggests that ρ should be of the same\norder as the smallest length scale σmin. The system then explores the distribution",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 561,
      "page_label": "542"
    }
  },
  {
    "page_content": "order as the smallest length scale σmin. The system then explores the distribution\nalong the more extended direction by means of a random walk, and so the number\nof steps to arrive at a state that is more or less independent of the original state is\nof order (σmax/σmin)2. In fact in two dimensions, the increase in rejection rate as ρ\nincreases is offset by the larger steps sizes of those transitions that are accepted, and\nmore generally for a multivariate Gaussian the number of steps required to obtain\nindependent samples scales like (σmax/σ2)2 where σ2 is the second-smallest stan-\ndard deviation (Neal, 1993). These details aside, it remains the case that if the length\nscales over which the distributions vary are very different in different directions, then\nthe Metropolis Hastings algorithm can have very slow convergence.\n11.3. Gibbs Sampling\nGibbs sampling (Geman and Geman, 1984) is a simple and widely applicable Markov\nchain Monte Carlo algorithm and can be seen as a special case of the Metropolis-\nHastings algorithm.\nConsider the distribution p(z)= p(z1,...,z M ) from which we wish to sample,\nand suppose that we have chosen some initial state for the Markov chain. Each step\nof the Gibbs sampling procedure involves replacing the value of one of the variables\nby a value drawn from the distribution of that variable conditioned on the values of\nthe remaining variables. Thus we replace zi by a value drawn from the distribution",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 561,
      "page_label": "542"
    }
  },
  {
    "page_content": "the remaining variables. Thus we replace zi by a value drawn from the distribution\np(zi|z\\i), where zi denotes the ith component of z, and z\\i denotes z1,...,z M but\nwith zi omitted. This procedure is repeated either by cycling through the variables",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 561,
      "page_label": "542"
    }
  },
  {
    "page_content": "11.3. Gibbs Sampling 543\nin some particular order or by choosing the variable to be updated at each step at\nrandom from some distribution.\nFor example, suppose we have a distribution p(z1,z 2,z 3) over three variables,\nand at step τ of the algorithm we have selected values z(τ)\n1 ,z (τ)\n2 and z(τ)\n3 . We ﬁrst\nreplace z(τ)\n1 by a new value z(τ+1)\n1 obtained by sampling from the conditional distri-\nbution\np(z1|z(τ)\n2 ,z (τ)\n3 ). (11.46)\nNext we replace z(τ)\n2 by a value z(τ+1)\n2 obtained by sampling from the conditional\ndistribution\np(z2|z(τ+1)\n1 ,z (τ)\n3 ) (11.47)\nso that the new value forz1 is used straight away in subsequent sampling steps. Then\nwe update z3 with a sample z(τ+1)\n3 drawn from\np(z3|z(τ+1)\n1 ,z (τ+1)\n2 ) (11.48)\nand so on, cycling through the three variables in turn.\nGibbs Sampling\n1. Initialize {zi : i =1 ,...,M }\n2. For τ =1 ,...,T :\n– Sample z(τ+1)\n1 ∼ p(z1|z(τ)\n2 ,z (τ)\n3 ,...,z (τ)\nM ).\n– Sample z(τ+1)\n2 ∼ p(z2|z(τ+1)\n1 ,z (τ)\n3 ,...,z (τ)\nM ).\n...\n– Sample z(τ+1)\nj ∼ p(zj|z(τ+1)\n1 ,...,z (τ+1)\nj−1 ,z (τ)\nj+1,...,z (τ)\nM ).\n...\n– Sample z(τ+1)\nM ∼ p(zM |z(τ+1)\n1 ,z (τ+1)\n2 ,...,z (τ+1)\nM−1 ).\nJosiah Willard Gibbs\n1839–1903\nGibbs spent almost his entire life liv-\ning in a house built by his father in\nNew Haven, Connecticut. In 1863,\nGibbs was granted the ﬁrst PhD in\nengineering in the United States,\nand in 1871 he was appointed to\nthe ﬁrst chair of mathematical physics in the United\nStates at Y ale, a post for which he received no salary",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 562,
      "page_label": "543"
    }
  },
  {
    "page_content": "and in 1871 he was appointed to\nthe ﬁrst chair of mathematical physics in the United\nStates at Y ale, a post for which he received no salary\nbecause at the time he had no publications. He de-\nveloped the ﬁeld of vector analysis and made contri-\nbutions to crystallography and planetary orbits. His\nmost famous work, entitled On the Equilibrium of Het-\nerogeneous Substances, laid the foundations for the\nscience of physical chemistry.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 562,
      "page_label": "543"
    }
  },
  {
    "page_content": "544 11. SAMPLING METHODS\nTo show that this procedure samples from the required distribution, we ﬁrst of\nall note that the distribution p(z) is an invariant of each of the Gibbs sampling steps\nindividually and hence of the whole Markov chain. This follows from the fact that\nwhen we sample fromp(zi|{z\\i), the marginal distributionp(z\\i) is clearly invariant\nbecause the value ofz\\i is unchanged. Also, each step by deﬁnition samples from the\ncorrect conditional distribution p(zi|z\\i). Because these conditional and marginal\ndistributions together specify the joint distribution, we see that the joint distribution\nis itself invariant.\nThe second requirement to be satisﬁed in order that the Gibbs sampling proce-\ndure samples from the correct distribution is that it be ergodic. A sufﬁcient condition\nfor ergodicity is that none of the conditional distributions be anywhere zero. If this\nis the case, then any point in z space can be reached from any other point in a ﬁnite\nnumber of steps involving one update of each of the component variables. If this\nrequirement is not satisﬁed, so that some of the conditional distributions have zeros,\nthen ergodicity, if it applies, must be proven explicitly.\nThe distribution of initial states must also be speciﬁed in order to complete the\nalgorithm, although samples drawn after many iterations will effectively become\nindependent of this distribution. Of course, successive samples from the Markov",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 563,
      "page_label": "544"
    }
  },
  {
    "page_content": "independent of this distribution. Of course, successive samples from the Markov\nchain will be highly correlated, and so to obtain samples that are nearly independent\nit will be necessary to subsample the sequence.\nWe can obtain the Gibbs sampling procedure as a particular instance of the\nMetropolis-Hastings algorithm as follows. Consider a Metropolis-Hastings sampling\nstep involving the variablezk in which the remaining variablesz\\k remain ﬁxed, and\nfor which the transition probability from z to z⋆ is given by qk(z⋆ |z)= p(z⋆\nk|z\\k).\nWe note that z⋆\n\\k = z\\k because these components are unchanged by the sampling\nstep. Also, p(z)= p(zk|z\\k)p(z\\k). Thus the factor that determines the acceptance\nprobability in the Metropolis-Hastings (11.44) is given by\nA(z⋆ , z)= p(z⋆ )qk(z|z⋆ )\np(z)qk(z⋆ |z) =\np(z⋆\nk|z⋆\n\\k)p(z⋆\n\\k)p(zk|z⋆\n\\k)\np(zk|z\\k)p(z\\k)p(z⋆\nk|z\\k) =1 (11.49)\nwhere we have used z⋆\n\\k = z\\k. Thus the Metropolis-Hastings steps are always\naccepted.\nAs with the Metropolis algorithm, we can gain some insight into the behaviour of\nGibbs sampling by investigating its application to a Gaussian distribution. Consider\na correlated Gaussian in two variables, as illustrated in Figure 11.11, having con-\nditional distributions of widthl and marginal distributions of width L. The typical\nstep size is governed by the conditional distributions and will be of order l. Because\nthe state evolves according to a random walk, the number of steps needed to obtain",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 563,
      "page_label": "544"
    }
  },
  {
    "page_content": "the state evolves according to a random walk, the number of steps needed to obtain\nindependent samples from the distribution will be of order (L/l)2. Of course if the\nGaussian distribution were uncorrelated, then the Gibbs sampling procedure would\nbe optimally efﬁcient. For this simple problem, we could rotate the coordinate sys-\ntem in order to decorrelate the variables. However, in practical applications it will\ngenerally be infeasible to ﬁnd such transformations.\nOne approach to reducing random walk behaviour in Gibbs sampling is called\nover-relaxation (Adler, 1981). In its original form, this applies to problems for which",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 563,
      "page_label": "544"
    }
  },
  {
    "page_content": "11.3. Gibbs Sampling 545\nFigure 11.11 Illustration of Gibbs sampling by alter-\nnate updates of two variables whose\ndistribution is a correlated Gaussian.\nThe step size is governed by the stan-\ndard deviation of the conditional distri-\nbution (green curve), and is O(l), lead-\ning to slow progress in the direction of\nelongation of the joint distribution (red\nellipse). The number of steps needed\nto obtain an independent sample from\nthe distribution is O((L/l)2).\nz1\nz2\nL\nl\nthe conditional distributions are Gaussian, which represents a more general class of\ndistributions than the multivariate Gaussian because, for example, the non-Gaussian\ndistribution p(z,y ) ∝ exp(−z2y2) has Gaussian conditional distributions. At each\nstep of the Gibbs sampling algorithm, the conditional distribution for a particular\ncomponent zi has some meanµi and some varianceσ2\ni . In the over-relaxation frame-\nwork, the value of zi is replaced with\nz′\ni = µi + α(zi − µi)+ σi(1 − α2\ni )1/2ν (11.50)\nwhere ν is a Gaussian random variable with zero mean and unit variance, and α\nis a parameter such that −1 <α< 1.F o r α =0 , the method is equivalent to\nstandard Gibbs sampling, and for α< 0 the step is biased to the opposite side of the\nmean. This step leaves the desired distribution invariant because if zi has mean µi\nand variance σ2\ni , then so too does z′\ni. The effect of over-relaxation is to encourage\ndirected motion through state space when the variables are highly correlated. The",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 564,
      "page_label": "545"
    }
  },
  {
    "page_content": "i. The effect of over-relaxation is to encourage\ndirected motion through state space when the variables are highly correlated. The\nframework of ordered over-relaxation(Neal, 1999) generalizes this approach to non-\nGaussian distributions.\nThe practical applicability of Gibbs sampling depends on the ease with which\nsamples can be drawn from the conditional distributions p(zk|z\\k). In the case of\nprobability distributions speciﬁed using graphical models, the conditional distribu-\ntions for individual nodes depend only on the variables in the corresponding Markov\nblankets, as illustrated in Figure 11.12. For directed graphs, a wide choice of condi-\ntional distributions for the individual nodes conditioned on their parents will lead to\nconditional distributions for Gibbs sampling that are log concave. The adaptive re-\njection sampling methods discussed in Section 11.1.3 therefore provide a framework\nfor Monte Carlo sampling from directed graphs with broad applicability.\nIf the graph is constructed using distributions from the exponential family, and\nif the parent-child relationships preserve conjugacy, then the full conditional distri-\nbutions arising in Gibbs sampling will have the same functional form as the orig-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 564,
      "page_label": "545"
    }
  },
  {
    "page_content": "546 11. SAMPLING METHODS\nFigure 11.12 The Gibbs sampling method requires samples\nto be drawn from the conditional distribution of a variable condi-\ntioned on the remaining variables. For graphical models, this\nconditional distribution is a function only of the states of the\nnodes in the Markov blanket. For an undirected graph this com-\nprises the set of neighbours, as shown on the left, while for a\ndirected graph the Markov blanket comprises the parents, the\nchildren, and the co-parents, as shown on the right.\ninal conditional distributions (conditioned on the parents) deﬁning each node, and\nso standard sampling techniques can be employed. In general, the full conditional\ndistributions will be of a complex form that does not permit the use of standard sam-\npling algorithms. However, if these conditionals are log concave, then sampling can\nbe done efﬁciently using adaptive rejection sampling (assuming the corresponding\nvariable is a scalar).\nIf, at each stage of the Gibbs sampling algorithm, instead of drawing a sample\nfrom the corresponding conditional distribution, we make a point estimate of the\nvariable given by the maximum of the conditional distribution, then we obtain the\niterated conditional modes (ICM) algorithm discussed in Section 8.3.3. Thus ICM\ncan be seen as a greedy approximation to Gibbs sampling.\nBecause the basic Gibbs sampling technique considers one variable at a time,\nthere are strong dependencies between successive samples. At the opposite extreme,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 565,
      "page_label": "546"
    }
  },
  {
    "page_content": "there are strong dependencies between successive samples. At the opposite extreme,\nif we could draw samples directly from the joint distribution (an operation that we\nare supposing is intractable), then successive samples would be independent. We can\nhope to improve on the simple Gibbs sampler by adopting an intermediate strategy in\nwhich we sample successively from groups of variables rather than individual vari-\nables. This is achieved in theblocking Gibbssampling algorithm by choosing blocks\nof variables, not necessarily disjoint, and then sampling jointly from the variables in\neach block in turn, conditioned on the remaining variables (Jensen et al., 1995).\n11.4. Slice Sampling\nWe have seen that one of the difﬁculties with the Metropolis algorithm is the sensi-\ntivity to step size. If this is too small, the result is slow decorrelation due to random\nwalk behaviour, whereas if it is too large the result is inefﬁciency due to a high rejec-\ntion rate. The technique ofslice sampling(Neal, 2003) provides an adaptive step size\nthat is automatically adjusted to match the characteristics of the distribution. Again\nit requires that we are able to evaluate the unnormalized distribution ˜p(z).\nConsider ﬁrst the univariate case. Slice sampling involves augmenting z with\nan additional variable u and then drawing samples from the joint (z,u ) space. We\nshall see another example of this approach when we discuss hybrid Monte Carlo in",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 565,
      "page_label": "546"
    }
  },
  {
    "page_content": "shall see another example of this approach when we discuss hybrid Monte Carlo in\nSection 11.5. The goal is to sample uniformly from the area under the distribution",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 565,
      "page_label": "546"
    }
  },
  {
    "page_content": "11.4. Slice Sampling 547\n˜p(z)\nz(τ) z\nu\n(a)\n˜p(z)\nz(τ) z\nuzmin zmax\n(b)\nFigure 11.13 Illustration of slice sampling. (a) For a given value z(τ), a value of u is chosen uniformly in\nthe region 0 ⩽ u ⩽ ep(z(τ)), which then deﬁnes a ‘slice’ through the distribution, shown by the solid horizontal\nlines. (b) Because it is infeasible to sample directly from a slice, a new sample of z is drawn from a region\nzmin ⩽ z ⩽ zmax, which contains the previous value z(τ).\ngiven by\nˆp(z,u )=\n{1/Zp if 0 ⩽ u ⩽ ˜p(z)\n0 otherwise (11.51)\nwhere Zp =\n∫˜p(z)d z. The marginal distribution over z is given by\n∫\nˆp(z,u )d u =\n∫ep(z)\n0\n1\nZp\ndu = ˜p(z)\nZp\n= p(z) (11.52)\nand so we can sample from p(z) by sampling from ˆp(z,u ) and then ignoring the u\nvalues. This can be achieved by alternately sampling z and u. Given the value of z\nwe evaluate ˜p(z) and then sample u uniformly in the range 0 ⩽ u ⩽ ˜p(z), which is\nstraightforward. Then we ﬁx u and sample z uniformly from the ‘slice’ through the\ndistribution deﬁned by {z : ˜p(z) >u }. This is illustrated in Figure 11.13(a).\nIn practice, it can be difﬁcult to sample directly from a slice through the distribu-\ntion and so instead we deﬁne a sampling scheme that leaves the uniform distribution\nunder ˆp(z,u ) invariant, which can be achieved by ensuring that detailed balance is\nsatisﬁed. Suppose the current value of z is denoted z(τ) and that we have obtained\na corresponding sample u. The next value of z is obtained by considering a region",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 566,
      "page_label": "547"
    }
  },
  {
    "page_content": "a corresponding sample u. The next value of z is obtained by considering a region\nzmin ⩽ z ⩽ zmax that contains z(τ). It is in the choice of this region that the adap-\ntation to the characteristic length scales of the distribution takes place. We want the\nregion to encompass as much of the slice as possible so as to allow large moves in z\nspace while having as little as possible of this region lying outside the slice, because\nthis makes the sampling less efﬁcient.\nOne approach to the choice of region involves starting with a region containing\nz(τ) having some width w and then testing each of the end points to see if they lie\nwithin the slice. If either end point does not, then the region is extended in that\ndirection by increments of value w until the end point lies outside the region. A\ncandidate value z′is then chosen uniformly from this region, and if it lies within the\nslice, then it forms z(τ+1). If it lies outside the slice, then the region is shrunk such\nthat z′forms an end point and such that the region still contains z(τ). Then another",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 566,
      "page_label": "547"
    }
  },
  {
    "page_content": "548 11. SAMPLING METHODS\ncandidate point is drawn uniformly from this reduced region and so on, until a value\nof z is found that lies within the slice.\nSlice sampling can be applied to multivariate distributions by repeatedly sam-\npling each variable in turn, in the manner of Gibbs sampling. This requires that\nwe are able to compute, for each component zi, a function that is proportional to\np(zi|z\\i).\n11.5. The Hybrid Monte Carlo Algorithm\nAs we have already noted, one of the major limitations of the Metropolis algorithm\nis that it can exhibit random walk behaviour whereby the distance traversed through\nthe state space grows only as the square root of the number of steps. The problem\ncannot be resolved simply by taking bigger steps as this leads to a high rejection rate.\nIn this section, we introduce a more sophisticated class of transitions based on an\nanalogy with physical systems and that has the property of being able to make large\nchanges to the system state while keeping the rejection probability small. It is ap-\nplicable to distributions over continuous variables for which we can readily evaluate\nthe gradient of the log probability with respect to the state variables. We will discuss\nthe dynamical systems framework in Section 11.5.1, and then in Section 11.5.2 we\nexplain how this may be combined with the Metropolis algorithm to yield the pow-\nerful hybrid Monte Carlo algorithm. A background in physics is not required as this",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 567,
      "page_label": "548"
    }
  },
  {
    "page_content": "erful hybrid Monte Carlo algorithm. A background in physics is not required as this\nsection is self-contained and the key results are all derived from ﬁrst principles.\n11.5.1 Dynamical systems\nThe dynamical approach to stochastic sampling has its origins in algorithms for\nsimulating the behaviour of physical systems evolving under Hamiltonian dynam-\nics. In a Markov chain Monte Carlo simulation, the goal is to sample from a given\nprobability distribution p(z). The framework of Hamiltonian dynamics is exploited\nby casting the probabilistic simulation in the form of a Hamiltonian system. In order\nto remain in keeping with the literature in this area, we make use of the relevant\ndynamical systems terminology where appropriate, which will be deﬁned as we go\nalong.\nThe dynamics that we consider corresponds to the evolution of the state variable\nz = {zi} under continuous time, which we denote by τ. Classical dynamics is de-\nscribed by Newton’s second law of motion in which the acceleration of an object is\nproportional to the applied force, corresponding to a second-order differential equa-\ntion over time. We can decompose a second-order equation into two coupled ﬁrst-\norder equations by introducing intermediate momentum variables r, corresponding\nto the rate of change of the state variables z, having components\nri = dzi\ndτ (11.53)\nwhere the zi can be regarded asposition variables in this dynamics perspective. Thus",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 567,
      "page_label": "548"
    }
  },
  {
    "page_content": "11.5. The Hybrid Monte Carlo Algorithm 549\nfor each position variable there is a corresponding momentum variable, and the joint\nspace of position and momentum variables is called phase space.\nWithout loss of generality, we can write the probability distribution p(z) in the\nform\np(z)= 1\nZp\nexp (−E(z)) (11.54)\nwhere E(z) is interpreted as the potential energyof the system when in state z. The\nsystem acceleration is the rate of change of momentum and is given by the applied\nforce, which itself is the negative gradient of the potential energy\ndri\ndτ = −∂E(z)\n∂zi\n. (11.55)\nIt is convenient to reformulate this dynamical system using the Hamiltonian\nframework. To do this, we ﬁrst deﬁne the kinetic energyby\nK(r)= 1\n2∥r∥2 = 1\n2\n∑\ni\nr2\ni . (11.56)\nThe total energy of the system is then the sum of its potential and kinetic energies\nH(z, r)= E(z)+ K(r) (11.57)\nwhere H is the Hamiltonian function. Using (11.53), (11.55), (11.56), and (11.57),\nwe can now express the dynamics of the system in terms of the Hamiltonian equa-\ntions given byExercise 11.15\ndzi\ndτ = ∂H\n∂ri\n(11.58)\ndri\ndτ = −∂H\n∂zi\n. (11.59)\nWilliam Hamilton\n1805–1865\nWilliam Rowan Hamilton was an\nIrish mathematician and physicist,\nand child prodigy, who was ap-\npointed Professor of Astronomy at\nTrinity College, Dublin, in 1827, be-\nfore he had even graduated. One\nof Hamilton’s most important contributions was a new\nformulation of dynamics, which played a signiﬁcant\nrole in the later development of quantum mechanics.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 568,
      "page_label": "549"
    }
  },
  {
    "page_content": "formulation of dynamics, which played a signiﬁcant\nrole in the later development of quantum mechanics.\nHis other great achievement was the development of\nquaternions, which generalize the concept of complex\nnumbers by introducing three distinct square roots of\nminus one, which satisfyi2 = j2 = k2 = ijk = −1.\nIt is said that these equations occurred to him while\nwalking along the Royal Canal in Dublin with his wife,\non 16 October 1843, and he promptly carved the\nequations into the side of Broome bridge. Although\nthere is no longer any evidence of the carving, there is\nnow a stone plaque on the bridge commemorating the\ndiscovery and displaying the quaternion equations.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 568,
      "page_label": "549"
    }
  },
  {
    "page_content": "550 11. SAMPLING METHODS\nDuring the evolution of this dynamical system, the value of the Hamiltonian H is\nconstant, as is easily seen by differentiation\ndH\ndτ =\n∑\ni\n{∂H\n∂zi\ndzi\ndτ + ∂H\n∂ri\ndri\ndτ\n}\n=\n∑\ni\n{∂H\n∂zi\n∂H\n∂ri\n− ∂H\n∂ri\n∂H\n∂zi\n}\n=0 . (11.60)\nA second important property of Hamiltonian dynamical systems, known as Li-\nouville’s Theorem, is that they preserve volume in phase space. In other words, if\nwe consider a region within the space of variables (z, r), then as this region evolves\nunder the equations of Hamiltonian dynamics, its shape may change but its volume\nwill not. This can be seen by noting that the ﬂow ﬁeld (rate of change of location in\nphase space) is given by\nV =\n( dz\ndτ , dr\ndτ\n)\n(11.61)\nand that the divergence of this ﬁeld vanishes\ndiv V =\n∑\ni\n{ ∂\n∂zi\ndzi\ndτ + ∂\n∂ri\ndri\ndτ\n}\n=\n∑\ni\n{\n− ∂\n∂zi\n∂H\n∂ri\n+ ∂\n∂ri\n∂H\n∂zi\n}\n=0 . (11.62)\nNow consider the joint distribution over phase space whose total energy is the\nHamiltonian, i.e., the distribution given by\np(z, r)= 1\nZH\nexp(−H(z, r)). (11.63)\nUsing the two results of conservation of volume and conservation of H, it follows\nthat the Hamiltonian dynamics will leave p(z, r) invariant. This can be seen by\nconsidering a small region of phase space over which H is approximately constant.\nIf we follow the evolution of the Hamiltonian equations for a ﬁnite time, then the\nvolume of this region will remain unchanged as will the value ofH in this region, and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 569,
      "page_label": "550"
    }
  },
  {
    "page_content": "volume of this region will remain unchanged as will the value ofH in this region, and\nhence the probability density, which is a function only ofH, will also be unchanged.\nAlthough H is invariant, the values of z and r will vary, and so by integrating\nthe Hamiltonian dynamics over a ﬁnite time duration it becomes possible to make\nlarge changes to z in a systematic way that avoids random walk behaviour.\nEvolution under the Hamiltonian dynamics will not, however, sample ergodi-\ncally from p(z, r) because the value of H is constant. In order to arrive at an ergodic\nsampling scheme, we can introduce additional moves in phase space that change\nthe value of H while also leaving the distribution p(z, r) invariant. The simplest\nway to achieve this is to replace the value of r with one drawn from its distribution\nconditioned on z. This can be regarded as a Gibbs sampling step, and hence from",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 569,
      "page_label": "550"
    }
  },
  {
    "page_content": "11.5. The Hybrid Monte Carlo Algorithm 551\nSection 11.3 we see that this also leaves the desired distribution invariant. Noting\nthat z and r are independent in the distribution p(z, r), we see that the conditional\ndistribution p(r|z) is a Gaussian from which it is straightforward to sample.Exercise 11.16\nIn a practical application of this approach, we have to address the problem of\nperforming a numerical integration of the Hamiltonian equations. This will neces-\nsarily introduce numerical errors and so we should devise a scheme that minimizes\nthe impact of such errors. In fact, it turns out that integration schemes can be devised\nfor which Liouville’s theorem still holds exactly. This property will be important in\nthe hybrid Monte Carlo algorithm, which is discussed in Section 11.5.2. One scheme\nfor achieving this is called theleapfrog discretization and involves alternately updat-\ning discrete-time approximations ˆz and ˆr to the position and momentum variables\nusing\nˆri(τ + ϵ/2) = ˆri(τ) − ϵ\n2\n∂E\n∂zi\n(ˆz(τ)) (11.64)\nˆzi(τ + ϵ)= ˆzi(τ)+ ϵˆri(τ + ϵ/2) (11.65)\nˆri(τ + ϵ)= ˆri(τ + ϵ/2) − ϵ\n2\n∂E\n∂zi\n(ˆz(τ + ϵ)). (11.66)\nWe see that this takes the form of a half-step update of the momentum variables with\nstep size ϵ/2, followed by a full-step update of the position variables with step sizeϵ,\nfollowed by a second half-step update of the momentum variables. If several leapfrog\nsteps are applied in succession, it can be seen that half-step updates to the momentum",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 570,
      "page_label": "551"
    }
  },
  {
    "page_content": "steps are applied in succession, it can be seen that half-step updates to the momentum\nvariables can be combined into full-step updates with step sizeϵ. The successive\nupdates to position and momentum variables then leapfrog over each other. In order\nto advance the dynamics by a time interval τ, we need to take τ/ϵ steps. The error\ninvolved in the discretized approximation to the continuous time dynamics will go to\nzero, assuming a smooth function E(z), in the limit ϵ → 0. However, for a nonzero\nϵ as used in practice, some residual error will remain. We shall see in Section 11.5.2\nhow the effects of such errors can be eliminated in the hybrid Monte Carlo algorithm.\nIn summary then, the Hamiltonian dynamical approach involves alternating be-\ntween a series of leapfrog updates and a resampling of the momentum variables from\ntheir marginal distribution.\nNote that the Hamiltonian dynamics method, unlike the basic Metropolis algo-\nrithm, is able to make use of information about the gradient of the log probability\ndistribution as well as about the distribution itself. An analogous situation is familiar\nfrom the domain of function optimization. In most cases where gradient informa-\ntion is available, it is highly advantageous to make use of it. Informally, this follows\nfrom the fact that in a space of dimension D, the additional computational cost of\nevaluating a gradient compared with evaluating the function itself will typically be a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 570,
      "page_label": "551"
    }
  },
  {
    "page_content": "evaluating a gradient compared with evaluating the function itself will typically be a\nﬁxed factor independent ofD, whereas the D-dimensional gradient vector conveys\nD pieces of information compared with the one piece of information given by the\nfunction itself.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 570,
      "page_label": "551"
    }
  },
  {
    "page_content": "552 11. SAMPLING METHODS\n11.5.2 Hybrid Monte Carlo\nAs we discussed in the previous section, for a nonzero step sizeϵ, the discretiza-\ntion of the leapfrog algorithm will introduce errors into the integration of the Hamil-\ntonian dynamical equations. Hybrid Monte Carlo(Duane et al., 1987; Neal, 1996)\ncombines Hamiltonian dynamics with the Metropolis algorithm and thereby removes\nany bias associated with the discretization.\nSpeciﬁcally, the algorithm uses a Markov chain consisting of alternate stochastic\nupdates of the momentum variable r and Hamiltonian dynamical updates using the\nleapfrog algorithm. After each application of the leapfrog algorithm, the resulting\ncandidate state is accepted or rejected according to the Metropolis criterion based\non the value of the HamiltonianH. Thus if (z, r) is the initial state and (z⋆ , r⋆ )\nis the state after the leapfrog integration, then this candidate state is accepted with\nprobability\nmin (1, exp{H(z, r) − H(z⋆ , r⋆ )}) . (11.67)\nIf the leapfrog integration were to simulate the Hamiltonian dynamics perfectly,\nthen every such candidate step would automatically be accepted because the value\nof H would be unchanged. Due to numerical errors, the value of H may sometimes\ndecrease, and we would like the Metropolis criterion to remove any bias due to this\neffect and ensure that the resulting samples are indeed drawn from the required dis-\ntribution. In order for this to be the case, we need to ensure that the update equations",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 571,
      "page_label": "552"
    }
  },
  {
    "page_content": "tribution. In order for this to be the case, we need to ensure that the update equations\ncorresponding to the leapfrog integration satisfy detailed balance (11.40). This is\neasily achieved by modifying the leapfrog scheme as follows.\nBefore the start of each leapfrog integration sequence, we choose at random,\nwith equal probability, whether to integrate forwards in time (using step size ϵ)o r\nbackwards in time (using step size −ϵ). We ﬁrst note that the leapfrog integration\nscheme (11.64), (11.65), and (11.66) is time-reversible, so that integration forL steps\nusing step size −ϵ will exactly undo the effect of integration for L steps using step\nsize ϵ. Next we show that the leapfrog integration preserves phase-space volume\nexactly. This follows from the fact that each step in the leapfrog scheme updates\neither a zi variable or an ri variable by an amount that is a function only of the other\nvariable. As shown in Figure 11.14, this has the effect of shearing a region of phase\nspace while not altering its volume.\nFinally, we use these results to show that detailed balance holds. Consider a\nsmall region R of phase space that, under a sequence of L leapfrog iterations of\nstep size ϵ, maps to a region R′. Using conservation of volume under the leapfrog\niteration, we see that if R has volume δV then so too will R′. If we choose an initial\npoint from the distribution (11.63) and then update it using L leapfrog interactions,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 571,
      "page_label": "552"
    }
  },
  {
    "page_content": "point from the distribution (11.63) and then update it using L leapfrog interactions,\nthe probability of the transition going from R to R′is given by\n1\nZH\nexp(−H(R))δV 1\n2 min{1, exp(−H(R)+ H(R′))}. (11.68)\nwhere the factor of 1/2 arises from the probability of choosing to integrate with a\npositive step size rather than a negative one. Similarly, the probability of starting in",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 571,
      "page_label": "552"
    }
  },
  {
    "page_content": "11.5. The Hybrid Monte Carlo Algorithm 553\nri\nzi\nr′\ni\nz′\ni\nFigure 11.14 Each step of the leapfrog algorithm (11.64)–(11.66) modiﬁes either a position variable zi or a\nmomentum variable ri. Because the change to one variable is a function only of the other, any region in phase\nspace will be sheared without change of volume.\nregion R′and integrating backwards in time to end up in region R is given by\n1\nZH\nexp(−H(R′))δV 1\n2 min{1, exp(−H(R′)+ H(R))}. (11.69)\nIt is easily seen that the two probabilities (11.68) and (11.69) are equal, and hence\ndetailed balance holds. Note that this proof ignores any overlap between the regionsExercise 11.17\nR and R′but is easily generalized to allow for such overlap.\nIt is not difﬁcult to construct examples for which the leapfrog algorithm returns\nto its starting position after a ﬁnite number of iterations. In such cases, the random\nreplacement of the momentum values before each leapfrog integration will not be\nsufﬁcient to ensure ergodicity because the position variables will never be updated.\nSuch phenomena are easily avoided by choosing the magnitude of the step size at\nrandom from some small interval, before each leapfrog integration.\nWe can gain some insight into the behaviour of the hybrid Monte Carlo algo-\nrithm by considering its application to a multivariate Gaussian. For convenience,\nconsider a Gaussian distribution p(z) with independent components, for which the\nHamiltonian is given by\nH(z, r)= 1\n2\n∑\ni\n1\nσ2\ni\nz2\ni + 1\n2\n∑\ni\nr2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 572,
      "page_label": "553"
    }
  },
  {
    "page_content": "consider a Gaussian distribution p(z) with independent components, for which the\nHamiltonian is given by\nH(z, r)= 1\n2\n∑\ni\n1\nσ2\ni\nz2\ni + 1\n2\n∑\ni\nr2\ni . (11.70)\nOur conclusions will be equally valid for a Gaussian distribution having correlated\ncomponents because the hybrid Monte Carlo algorithm exhibits rotational isotropy.\nDuring the leapfrog integration, each pair of phase-space variables zi,r i evolves in-\ndependently. However, the acceptance or rejection of the candidate point is based\non the value of H, which depends on the values of all of the variables. Thus, a\nsigniﬁcant integration error in any one of the variables could lead to a high prob-\nability of rejection. In order that the discrete leapfrog integration be a reasonably",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 572,
      "page_label": "553"
    }
  },
  {
    "page_content": "554 11. SAMPLING METHODS\ngood approximation to the true continuous-time dynamics, it is necessary for the\nleapfrog integration scale ϵ to be smaller than the shortest length-scale over which\nthe potential is varying signiﬁcantly. This is governed by the smallest value of σi,\nwhich we denote by σmin. Recall that the goal of the leapfrog integration in hybrid\nMonte Carlo is to move a substantial distance through phase space to a new state\nthat is relatively independent of the initial state and still achieve a high probability of\nacceptance. In order to achieve this, the leapfrog integration must be continued for a\nnumber of iterations of order σmax/σmin.\nBy contrast, consider the behaviour of a simple Metropolis algorithm with an\nisotropic Gaussian proposal distribution of variance s2, considered earlier. In order\nto avoid high rejection rates, the value ofs must be of order σmin. The exploration of\nstate space then proceeds by a random walk and takes of order (σmax/σmin)2 steps\nto arrive at a roughly independent state.\n11.6. Estimating the Partition Function\nAs we have seen, most of the sampling algorithms considered in this chapter re-\nquire only the functional form of the probability distribution up to a multiplicative\nconstant. Thus if we write\npE(z)= 1\nZE\nexp(−E(z)) (11.71)\nthen the value of the normalization constant ZE, also known as the partition func-\ntion, is not needed in order to draw samples from p(z). However, knowledge of the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 573,
      "page_label": "554"
    }
  },
  {
    "page_content": "tion, is not needed in order to draw samples from p(z). However, knowledge of the\nvalue of ZE can be useful for Bayesian model comparison since it represents the\nmodel evidence (i.e., the probability of the observed data given the model), and so\nit is of interest to consider how its value might be obtained. We assume that direct\nevaluation by summing, or integrating, the functionexp(−E(z)) over the state space\nof z is intractable.\nFor model comparison, it is actually the ratio of the partition functions for two\nmodels that is required. Multiplication of this ratio by the ratio of prior probabilities\ngives the ratio of posterior probabilities, which can then be used for model selection\nor model averaging.\nOne way to estimate a ratio of partition functions is to use importance sampling\nfrom a distribution with energy function G(z)\nZE\nZG\n=\n∑\nz exp(−E(z))∑\nz exp(−G(z))\n=\n∑\nz exp(−E(z)+ G(z)) exp(−G(z))∑\nz exp(−G(z))\n= EG(z)[exp(−E + G)]\n≃\n∑\nl\nexp(−E(z(l))+ G(z(l))) (11.72)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 573,
      "page_label": "554"
    }
  },
  {
    "page_content": "11.6. Estimating the Partition Function 555\nwhere {z(l)} are samples drawn from the distribution deﬁned by pG(z). If the dis-\ntribution pG is one for which the partition function can be evaluated analytically, for\nexample a Gaussian, then the absolute value of ZE can be obtained.\nThis approach will only yield accurate results if the importance sampling distri-\nbution pG is closely matched to the distribution pE, so that the ratio pE/pG does not\nhave wide variations. In practice, suitable analytically speciﬁed importance sampling\ndistributions cannot readily be found for the kinds of complex models considered in\nthis book.\nAn alternative approach is therefore to use the samples obtained from a Markov\nchain to deﬁne the importance-sampling distribution. If the transition probability for\nthe Markov chain is given by T(z, z′), and the sample set is given by z(1),..., z(L),\nthen the sampling distribution can be written as\n1\nZG\nexp (−G(z)) =\nL∑\nl=1\nT(z(l), z) (11.73)\nwhich can be used directly in (11.72).\nMethods for estimating the ratio of two partition functions require for their suc-\ncess that the two corresponding distributions be reasonably closely matched. This is\nespecially problematic if we wish to ﬁnd the absolute value of the partition function\nfor a complex distribution because it is only for relatively simple distributions that\nthe partition function can be evaluated directly, and so attempting to estimate the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 574,
      "page_label": "555"
    }
  },
  {
    "page_content": "the partition function can be evaluated directly, and so attempting to estimate the\nratio of partition functions directly is unlikely to be successful. This problem can be\ntackled using a technique known aschaining (Neal, 1993; Barber and Bishop, 1997),\nwhich involves introducing a succession of intermediate distributions p2,...,p M−1\nthat interpolate between a simple distribution p1(z) for which we can evaluate the\nnormalization coefﬁcient Z1 and the desired complex distribution pM (z). We then\nhave ZM\nZ1\n= Z2\nZ1\nZ3\nZ2\n··· ZM\nZM−1\n(11.74)\nin which the intermediate ratios can be determined using Monte Carlo methods as\ndiscussed above. One way to construct such a sequence of intermediate systems\nis to use an energy function containing a continuous parameter 0 ⩽ α ⩽ 1 that\ninterpolates between the two distributions\nEα(z)=( 1 − α)E1(z)+ αEM (z). (11.75)\nIf the intermediate ratios in (11.74) are to be found using Monte Carlo, it may be\nmore efﬁcient to use a single Markov chain run than to restart the Markov chain for\neach ratio. In this case, the Markov chain is run initially for the systemp1 and then\nafter some suitable number of steps moves on to the next distribution in the sequence.\nNote, however, that the system must remain close to the equilibrium distribution at\neach stage.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 574,
      "page_label": "555"
    }
  },
  {
    "page_content": "556 11. SAMPLING METHODS\nExercises\n11.1 (⋆) www Show that the ﬁnite sample estimator ˆf deﬁned by (11.2) has mean\nequal to E[f] and variance given by (11.3).\n11.2 (⋆) Suppose that z is a random variable with uniform distribution over (0,1) and\nthat we transform z using y = h−1(z) where h(y) is given by (11.6). Show that y\nhas the distribution p(y).\n11.3 (⋆) Given a random variable z that is uniformly distributed over (0, 1), ﬁnd a trans-\nformation y = f(z) such that y has a Cauchy distribution given by (11.8).\n11.4 (⋆⋆ ) Suppose that z1 and z2 are uniformly distributed over the unit circle, as\nshown in Figure 11.3, and that we make the change of variables given by (11.10)\nand (11.11). Show that (y1,y 2) will be distributed according to (11.12).\n11.5 (⋆) www Let z be a D-dimensional random variable having a Gaussian distribu-\ntion with zero mean and unit covariance matrix, and suppose that the positive deﬁnite\nsymmetric matrix Σ has the Cholesky decomposition Σ = LLT where L is a lower-\ntriangular matrix (i.e., one with zeros above the leading diagonal). Show that the\nvariabley = µ + Lz has a Gaussian distribution with mean µ and covariance Σ.\nThis provides a technique for generating samples from a general multivariate Gaus-\nsian using samples from a univariate Gaussian having zero mean and unit variance.\n11.6 (⋆⋆ ) www In this exercise, we show more carefully that rejection sampling does\nindeed draw samples from the desired distribution p(z). Suppose the proposal dis-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 575,
      "page_label": "556"
    }
  },
  {
    "page_content": "indeed draw samples from the desired distribution p(z). Suppose the proposal dis-\ntribution is q(z) and show that the probability of a sample value z being accepted is\ngiven by ˜p(z)/kq(z) where ˜p is any unnormalized distribution that is proportional to\np(z), and the constant k is set to the smallest value that ensures kq(z) ⩾ ˜p(z) for all\nvalues of z. Note that the probability of drawing a valuez is given by the probability\nof drawing that value from q(z) times the probability of accepting that value given\nthat it has been drawn. Make use of this, along with the sum and product rules of\nprobability, to write down the normalized form for the distribution overz, and show\nthat it equals p(z).\n11.7 (⋆) Suppose that z has a uniform distribution over the interval [0,1]. Show that the\nvariable y = b tanz + c has a Cauchy distribution given by (11.16).\n11.8 (⋆⋆ ) Determine expressions for the coefﬁcients ki in the envelope distribution\n(11.17) for adaptive rejection sampling using the requirements of continuity and nor-\nmalization.\n11.9 (⋆⋆ ) By making use of the technique discussed in Section 11.1.1 for sampling\nfrom a single exponential distribution, devise an algorithm for sampling from the\npiecewise exponential distribution deﬁned by (11.17).\n11.10 (⋆) Show that the simple random walk over the integers deﬁned by (11.34), (11.35),\nand (11.36) has the property that E[(z(τ))2]= E[(z(τ−1))2]+1 /2 and hence by\ninduction that E[(z(τ))2]= τ/2.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 575,
      "page_label": "556"
    }
  },
  {
    "page_content": "Exercises 557\nFigure 11.15 A probability distribution over two variables z1\nand z2 that is uniform over the shaded regions\nand that is zero everywhere else.\nz1\nz2\n11.11 (⋆⋆ ) www Show that the Gibbs sampling algorithm, discussed in Section 11.3,\nsatisﬁes detailed balance as deﬁned by (11.40).\n11.12 (⋆) Consider the distribution shown in Figure 11.15. Discuss whether the standard\nGibbs sampling procedure for this distribution is ergodic, and therefore whether it\nwould sample correctly from this distribution\n11.13 (⋆⋆ ) Consider the simple 3-node graph shown in Figure 11.16 in which the observed\nnode x is given by a Gaussian distribution N(x|µ, τ−1) with mean µ and precision\nτ. Suppose that the marginal distributions over the mean and precision are given\nby N(µ|µ0,s 0) and Gam(τ|a, b), where Gam(·|·, ·) denotes a gamma distribution.\nWrite down expressions for the conditional distributionsp(µ|x, τ) and p(τ|x, µ) that\nwould be required in order to apply Gibbs sampling to the posterior distribution\np(µ, τ|x).\n11.14 (⋆) Verify that the over-relaxation update (11.50), in which zi has mean µi and\nvariance σi, and where ν has zero mean and unit variance, gives a value z′\ni with\nmean µi and variance σ2\ni .\n11.15 (⋆) www Using (11.56) and (11.57), show that the Hamiltonian equation (11.58)\nis equivalent to (11.53). Similarly, using (11.57) show that (11.59) is equivalent to\n(11.55).\n11.16 (⋆) By making use of (11.56), (11.57), and (11.63), show that the conditional dis-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 576,
      "page_label": "557"
    }
  },
  {
    "page_content": "(11.55).\n11.16 (⋆) By making use of (11.56), (11.57), and (11.63), show that the conditional dis-\ntribution p(r|z) is a Gaussian.\nFigure 11.16 A graph involving an observed Gaussian variable x with\nprior distributions over its mean µ and precision τ.\nµ τ\nx",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 576,
      "page_label": "557"
    }
  },
  {
    "page_content": "558 11. SAMPLING METHODS\n11.17 (⋆) www Verify that the two probabilities (11.68) and (11.69) are equal, and hence\nthat detailed balance holds for the hybrid Monte Carlo algorithm.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 577,
      "page_label": "558"
    }
  },
  {
    "page_content": "AppendixA\nInChapter9,we discussedprobabilisticmodels havingdiscretelatentvariables,such\nas themixtureof Gaussians.We now exploremodels inwhich some, or all,of the\nlatentvariablesarecontinuous.An importantmotivationforsuch models isthat\nmany datasetshave thepropertythatthedatapointsallliecloseto a manifoldof\nmuch lower dimensionalitythan thatof the originaldataspace. To see why this\nmight arise,consideran artificialdatasetconstructedby takingone of theoff-line\ndigits,representedby a 64 x 64 pixelgrey-levelimage, andembedding itina larger\nimage ofsize100 x 100 by paddingwithpixelshavingthevaluezero(corresponding\ntowhite pixels)inwhich thelocationand orientationofthedigitisvariedatrandom,\nasillustratedinFigure12.1.Each oftheresultingimages isrepresentedby a pointin\nthe100 x 100 = 10,OOO-dimensionaldataspace.However, acrossa datasetofsuch\nimages,thereareonly threedegreesoffreedom ofvariability,correspondingtothe\nverticaland horizontaltranslationsand therotations.The datapointswilltherefore\nliveon a subspaceof thedataspacewhose intrinsicdimensionalityisthree.Note\n559",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 578,
      "page_label": "559"
    }
  },
  {
    "page_content": "560 12.CONTINUOUS LATENT VARIABLES\nFigure12.1 A syntheticdataselobtainedby takingone oftheoff-linedigitimages and creatingmulti­\nplecopiesineach ofwhich thedigithas undergone a random displacementand rotation\nwithinsome largerimage field.The resultingimages each have 100 )( 100 = 10.000\npixels.\nthatthe manifold willbe nonlinearbecause.for instance.ifwe translatethe digit\npasta particularpixel,thatpixelvaluewillgo from zero(white)10 one (black)and\nback to zero again.which isclearlya nonlinearfunctionof the digitposition.In\nthisexample.!.helranslationand rotationparametersarelatentvariablesbecausewe\nobserve only the image vectorsand are not toldwhich valuesof the translationor\nrotationvariableswere used tocreatethem.\nFor realdigitimage data,therewillbe a funherdegreeof freedom arisingfrom\nscaling.Moreover therewillbe multipleaddilionaldegreesof freedom associaled\nwilh more complex deformationsdue to the variabilityin an individual'swriling\n3S well as lhe differencesin writingslylesbetween individuals. evenheless.the\nnumber ofsuch degreesoffreedom willbe smallcompared tothedimensionalityof\nIhedataset.\nAppendiX A Another example isprovidedby theoilflow dataset.in which (fora givenge-\nometricalconfigurationofthegas,WOller,and oilphases)thereareonly two degrees\noffreedom ofvariabilitycorrespondingtothefractionofoilinthepipeand thefrac­\ntionof water (thefractionof gas Ihen being determined).Ahhough thedataspace",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 579,
      "page_label": "560"
    }
  },
  {
    "page_content": "tionof water (thefractionof gas Ihen being determined).Ahhough thedataspace\ncomprises 12 measuremenlS, a datasetof pointswilllieclosetoa Iwo-dimensional\nmanifold embedded withinthisspace. In thiscase,the manifoldcomprises scveral\ndistinctsegments correspondingtodifferentflow regimes.each such segment being\na (noisy)continuoustwo-dimensionalmanifold.Ifour goal isdatacompression.or\ndensitymodelling,then therecan be benefitsinexploilingthismanifoldstruclUre.\nIn praclice.the data pointswill not be confined preciselyto a smooth low­\ndimensional manifold,and we can interpretthe departuresof data pointsfrom the\nmanifold as ·noise'.This leadsnaturallyto a generativeview of such models in\nwhich we firstselecta poinlwithinthe manifold accordingto some latentvariable\ndistributionand then generatean observed data pointby :lddingnoise,drawn from\nsome conditionaldistributionof thedata varillblesgiven thelatentvarillbles.\nThc simplestcontinuouslatentvariablemodel assumes Gaussian distributions\nfor both thc latentand observed variablesand makes use of a linear,Gaussiande-\nSeCTion8.1..J pendence of the observed variableson Ihe slateof the latentvariables.This leads\nto a probabilislicfonnulationof the well-known techniqueof principalcomponent\nanalysis(PeA), as wellas 10 a relatedmodel calledfactoranalysis.\nSection 12.1 In thischapterw willbeginwilh a slandard,nonprobabilistictreatmentof PeA.\nand thcn we show how PeA arisesnaturallyas themaximum likelihoodsolution10",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 579,
      "page_label": "560"
    }
  },
  {
    "page_content": "12.1.PrincipalC01n[>OM\"1 AnalJsis 561\nFlgu,e12.2 P'if>cipalcompooont a\",,~\" seeks\" $pace\n01 !owe, dimensionality.kt\"(>WIlas !he P<lno>\npal subSpace \"nd denoted I:Jythe magenta \"1\nline.SUCh Itletthe Grthogonet [jiojectiOh01\n!he data points('eddoIslonto tP'Ns~\n\"\"\"'imizesthevaria,..,.,of!he proja<:tedpoints\n(greendoIs).An \"It\",nati\"\"....finilion01 PCA\nis based on m ..mizing the \"\"\",-<>I·squares\nof !he projectionerrors.ind'catedby the bfi.>e\nlines.\nS'crio\"12.2 a panlcula,fonn of linear-Gau\"ianlatem \"ariablemodel. This probabilisticrefor­\nmulationbring~ many ad\\'imlag~s, su~h as tl>l:useI)fEM forparametereslimalion,\nrrinciple<Jc~tensioos 10 Oli~turc, of PeA model\" and Ba)'~sian formulat;onsthat\nallowtbenumber of rrincipalcom[>Oncntsto be detennined aUlOmaticallyfrom !be\ndata.Finally'.\"cdiSl;us<briefly\"\"'eralgencrali,ation,ofthelatentYariableconcept\nthatg<l be~ood tbelinear-Gaussianassumption includingnon·Gau\"i\"n I.tcntyari­\nabies.....hichlea'\"totbe fr.me....ork of indrl\"'mJ.mcompon.nl anal,-.;.,as ....ella,\nS'di\"\"12.4 models ha\"inga nonlinearrclationshipbet....een latentand oose\",e<J,'lUiable,.\n____'c2=.~1. PrincipalComponent Analysis\nPrincipalcompooem analy,;\"or rcA.;sa techniquetha!is\"'idelyu<ed forappli.\ncationssuch asdimensionality.-eduction,lossydatacomprc\"ion, featuree>tracti\"\".\nand datav;,ualizatiOll(Jolliffe,2(02). It;salsokno....\" astileKaroan.n·I..,;\"\"tran,·\nf~.\nlbcrc an: t....o commonly used definitionsof PeA thatgiye riseto the >arne",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 580,
      "page_label": "561"
    }
  },
  {
    "page_content": "f~.\nlbcrc an: t....o commonly used definitionsof PeA thatgiye riseto the >arne\nalgorithm.PeA can be definedastheunhog<lnalprojtttionof thedataO/1tO a lo....er\ndimensionallincarspace.kno....n asthepri/lcip.al$uh.•p.aa.soch thatthe\\'arianceof\ntheprojttteddatai'ma~imi,e<J (1I\",.lIing.1933).Equi\"alemly,;tcan be definedas\ntbelinearprojectionthatminimi\"'.theaverageprojttlioncost.definedas t~ mean\nsqua.-eddistance!letweenthedata[>Oint<and tbeirp<ojtttioo,(Pearson,19(1).The\nl\"J'\"OC\"s<of onhogonal projectioni'illustraledin FiguTe 12.2.We con,idereach of\nthesedefinitionsintum.\n12,1.1 Mllximllm variancelormulation\nCon,ider a dala set <Ifobser\"\\lations{x,,} where\" = 1.....S, and x\" i,a\nEuclideanvariable\"'ilhdimen,ionalityD. Our goal isto projectIf>/::data onto a\n'paceha\"ingdimen,ionalityM < D\" hileIll3Jli\",i,illgthe\"ariallCeoftheprojttted\ndata.For the!noll..nl.we 'hallassume thattbe \"alueof M isg;\\·en.Latcrinthis",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 580,
      "page_label": "561"
    }
  },
  {
    "page_content": "562 12.CONTINUOUS LATENT VARIABLES\nchapter,we shallconsidertechniquestodeterminean appropriatevalueof IV! from\nthedata.\nTo beginwith,considertheprojectiononto a one-dimensionalspace(M = 1).\nWe can definethedirectionof thisspace using a D-dimensional vectorUl, which\nforconvenience(and withoutlossofgenerality)we shallchoose tobe a unitvector\nso thatufUl = 1 (notethatwe areonly interestedinthedirectiondefinedby Ul,\nnotinthemagnitude ofUl itself).Each datapointX n isthenprojectedontoa scalar\nvalueufX n . The mean of theprojecteddataisufx where x isthesample setmean\ngivenby\n(12.1)\nand thevarianceoftheprojecteddataisgivenby\nwhere S isthedatacovariancematrixdefinedby\n1 N\nS = - \"(xn - x)(xn - x)TNLJ .\nn=l\n(12.2)\n(12.3)\nAppendix E\nWe now maximize theprojectedvarianceUfSUl withrespecttoUl. Clearly,thishas\ntobe a constrainedmaximizationtopreventIlulll.....00. The appropriateconstraint\ncomes from the normalizationconditionufUl = 1. To enforcethisconstraint,\nwe introducea Lagrange multiplierthatwe shalldenoteby AI, and then make an\nunconstrainedmaximizationof\n(12.4)\nBy settingthederivativewith respectto Ul equalto zero,we see thatthisquantity\nwillhave a stationarypointwhen\n(12.5)\nwhich saysthatUl must be an eigenvectorofS. Ifwe left-multiplyby uf and make\nuse ofufUl = 1,we seethatthevarianceisgivenby\n(12.6)\nand so the variancewillbe a maximum when we setUl equal to the eigenvector\nhaving the largesteigenvalueAI. This eigenvectorisknown as the firstprincipal\ncomponent.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 581,
      "page_label": "562"
    }
  },
  {
    "page_content": "having the largesteigenvalueAI. This eigenvectorisknown as the firstprincipal\ncomponent.\nWe can defineadditionalprincipalcomponents in an incrementalfashionby\nchoosing each new directionto be thatwhich maximizes the projectedvariance",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 581,
      "page_label": "562"
    }
  },
  {
    "page_content": "Exercise12.1\nSection12.2.2\nAppendix C\n12.1.PrincipalComponent Analysis 563\namongst allpossibledirectionsorthogonalto those alreadyconsidered.Ifwe con­\nsiderthegeneralcase ofan M -dimensionalprojectionspace,theoptimallinearpro­\njectionforwhich the varianceofthe projecteddataismaximized isnow definedby\ntheM eigenvectorsU 1, ..., U M ofthedatacovariancematrixS correspondingtothe\nM largesteigenvalues>'1,...,AM. This iseasilyshown using proofby induction.\nTo summarize, principalcomponent analysisinvolvesevaluatingthe mean x\nand thecovariancematrixS ofthedatasetand then findingtheM eigenvectorsofS\ncorrespondingtotheM largesteigenvalues.Algorithms forfindingeigenvectorsand\neigenvalues,as well as additionaltheorems relatedto eigenvectordecomposition,\ncan be found in Golub and Van Loan (1996). Note thatthe computationalcostof\ncomputing thefulleigenvectordecomposition fora matrixofsizeD x Dis O(D3).\nIfwe plan to projectour data onto the firstM principalcomponents, then we only\nneed to findthe firstM eigenvaluesand eigenvectors.This can be done with more\nefficienttechniques,such as thepower method (Golub and Van Loan, 1996), that\nscalelikeO(M D 2 ),or alternativelywe can make use oftheEM algorithm.\n12.1.2 Minimum-error formulation\nWe now discussan alternativeformulationof peA based on projectionerror\nminimization.To do this,we introducea complete orthonormalsetofD-dimensional\nbasisvectors{Ui} where i = 1,...,D thatsatisfy\n(12.7)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 582,
      "page_label": "563"
    }
  },
  {
    "page_content": "minimization.To do this,we introducea complete orthonormalsetofD-dimensional\nbasisvectors{Ui} where i = 1,...,D thatsatisfy\n(12.7)\nBecause thisbasisiscomplete,each datapointcan be representedexactlyby a linear\ncombination ofthebasisvectors\nD\nX n = Laniui\ni=l\n(12.8)\nwhere the coefficientsani willbe differentfor differentdata points.This simply\ncorrespondsto a rotationof the coordinatesystem to a new system definedby the\n{Ui},and theoriginalD components {Xnl'...,XnD} arereplacedby an equivalent\nset{anl'...,anD}. Taking the innerproduct with Uj, and making use of the or­\nthonormalityproperty,we obtainanj = x;Uj, and so withoutlossofgeneralitywe\ncan write\nD\nX n = L (X~Ui) Ui·\ni=l\n(12.9)\n(12.10)\nOur goal,however, isto approximate thisdata pointusing a representationin­\nvolvinga restrictednumber M < D of variablescorrespondingtoa projectiononto\na lower-dimensionalsubspace. The M -dimensionallinearsubspace can be repre­\nsented,without lossof generality,by the firstM of the basisvectors,and so we\napproximate each datapointX n by\nM D\nxn = L ZniUi + L biUi\ni=l i=M+l",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 582,
      "page_label": "563"
    }
  },
  {
    "page_content": "564 12.CONTINUOUS LATENT VARIABLES\nwhere the{Zni} depend on theparticulardatapoint,whereas the{bd areconstants\nthatarethesame foralldatapoints.We arefreetochoose the{Ui},the{Zni},and\nthe{bd so astominimize thedistortion introducedby thereductionindimensional­\nity.As ourdistortionmeasure,we shallusethesquareddistancebetween theoriginal\ndatapointX n and itsapproximationXn , averagedoverthedataset,so thatour goal\nistominimize\nN\nJ = ~ L Ilxn - xn 11\n2\n.\nn=l\n(12.11)\nConsiderfirstofalltheminimizationwith respecttothequantities{Zni}. Sub­\nstitutingforXn , settingthederivativewithrespecttoZnj tozero,and making useof\ntheorthonormalityconditions,we obtain\n(12.12)\nwhere j = 1,...,M. Similarly,settingthederivativeofJ withrespecttobi tozero,\nand againmaking use oftheorthonormalityrelations,gives\nb -Tj = X Uj (12.13)\nwhere j = M +1,...,D.Ifwe substituteforZni and bi,and make useofthegeneral\nexpansion(12.9),we obtain\nD\nX n - X n = L {(X n - x)Tud Ui\ni=M+l\n(12.14)\nfrom which we see thatthe displacementvectorfrom X n to xn liesin the space\northogonaltotheprincipalsubspace,becauseitisa linearcombinationof {ud for\ni = M + 1,...,D, as illustratedinFigure12.2.This istobe expectedbecausethe\nprojectedpointsxn must liewithintheprincipalsubspace,but we can move them\nfreelywithinthatsubspace,and so the minimum errorisgiven by the orthogonal\nprojection.\nWe thereforeobtainan expression forthe distortionmeasure J as a function\npurelyofthe{ud intheform\n1 ~ ~ (T _T)2 D TJ = N L L X n Ui - X Ui = L U i SUi.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 583,
      "page_label": "564"
    }
  },
  {
    "page_content": "purelyofthe{ud intheform\n1 ~ ~ (T _T)2 D TJ = N L L X n Ui - X Ui = L U i SUi.\nn=l i=M+l i=M+l\n(12.15)\nThere remains the taskof minimizing J with respectto the {Ui},which must\nbe a constrainedminimizationotherwisewe willobtainthevacuous resultUi = O.\nThe constraintsarisefrom the orthonormalityconditionsand, as we shallsee,the\nsolutionwillbe expressedin terms of theeigenvectorexpansionof thecovariance\nmatrix.Beforeconsideringa formalsolution,letus trytoobtainsome intuitionabout\ntheresultby consideringthecaseofa two-dimensionaldataspaceD = 2 and a one­\ndimensionalprincipalsubspace M = 1. We have tochoose a directionU2 so as to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 583,
      "page_label": "564"
    }
  },
  {
    "page_content": "12.1.PrincipalComponent Analysis 565\nminimize J = UISU2' subjecttothenormalizationconstraintuIU2 = 1. Using a\nLagrange multiplierA2 toenforcetheconstraint,we considertheminimizationof\n(12.16)\n(12.18)\nExercise12.2\nAppendix A\nSettingthederivativewithrespectto U2 tozero,we obtainSU2 = A2U2 so thatU2\nisan eigenvectorof S with eigenvalueA2. Thus any eigenvectorwilldefinea sta­\ntionarypointof thedistortionmeasure. To findthevalueof J attheminimum, we\nback-substitutethesolutionforU2 intothedistortionmeasure togiveJ = A2. We\nthereforeobtaintheminimum valueofJ by choosingU2 tobe theeigenvectorcorre­\nspondingtothesmallerofthetwo eigenvalues.Thus we shouldchoosetheprincipal\nsubspacetobe alignedwiththeeigenvectorhavingthelargereigenvalue.Thisresult\naccordswithour intuitionthat,inordertominimize theaveragesquaredprojection\ndistance,we shouldchoose the principalcomponent subspaceto pass throughthe\nmean ofthedatapointsand tobe alignedwiththedirectionsof maximum variance.\nFor thecasewhen theeigenvaluesareequal,any choiceof principal directionwill\ngiverisetothesame valueofJ.\nThe generalsolutiontotheminimizationofJ forarbitraryD and arbitraryM <\nD isobtainedby choosingthe{Ui} tobe eigenvectorsofthecovariancematrixgiven\nby\nSUi = AiUi (12.17)\nwhere i = 1,...,D,and as usualtheeigenvectors{Ui} arechosen tobe orthonor­\nmal.The correspondingvalueofthedistortionmeasure isthengivenby\nD\nJ= L Ai\ni=M+l\nwhich issimplythesum oftheeigenvaluesofthoseeigenvectorsthatareorthogonal",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 584,
      "page_label": "565"
    }
  },
  {
    "page_content": "D\nJ= L Ai\ni=M+l\nwhich issimplythesum oftheeigenvaluesofthoseeigenvectorsthatareorthogonal\ntotheprincipalsubspace.We thereforeobtaintheminimum valueof J by selecting\ntheseeigenvectorstobe thosehavingthe D - M smallesteigenvalues,and hence\ntheeigenvectorsdefiningtheprincipalsubspacearethosecorrespondingto the M\nlargesteigenvalues.\nAlthough we have consideredM < D, thePCA analysisstillholdsifM =\nD, in which casethereisno dimensionalityreductionbut simply a rotationof the\ncoordinateaxestoalignwithprincipalcomponents.\nFinally,itisworthnotingthatthereexistsa closelyrelatedlineardimensionality\nreductiontechniquecalledcanonicalcorrelationanalysis,or CCA (Hotelling,1936;\nBach and Jordan,2002).Whereas PCA works with a singlerandom variable,CCA\nconsiderstwo (ormore) variablesand triesto finda correspondingpairof linear\nsubspacesthathave high cross-correlation,so thateach component withinone ofthe\nsubspacesiscorrelatedwitha singlecomponent from theothersubspace.Itssolution\ncan be expressedintermsofa generalizedeigenvectorproblem.\n12.1.3 ApplicationsofpeA\nWe can illustratetheuse of PCA fordatacompressionby consideringtheoff­\nlinedigitsdataset.Because each eigenvectorof thecovariancematrixisa vector",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 584,
      "page_label": "565"
    }
  },
  {
    "page_content": "566 12.COl\\'TINUOliS LATf;I\\'T\\'ARIAIILES\nFigure12.3 The mean ~'\" x aklogwith!he II\"'tlou'PCA e;gerrvecl<)rllUl,. ..'\" lorthe011-.....\ncligitsdataset.t<>getl'lerwith!hecorrespondi~ ~.\n;ntheOIigi\",,1D-<limensionalspace.we can representthoeigenw:cto<sas imago< of\nthosame silOas ,1>0 data poi\",,_11,.firstIh'e.ig.n,'occOfS.along wich tl>ocorre­\nsponding.igen,'slue,.are <IIo\"'ninFigure12,3,A plO!ofll>ocomplete spect\"'muf\noigo\",·alue,.sone<!intodecreasingorder.isshown in Figure12.4{ai.The di'tortion\nmeasure J aSSQCiatedwilh choo<ing a particularvalueof M isgi.'enby tho sum\nof theeig.n\",luesfrom M + I up to 0 and isptO!tedfordifferent,'aluo<of .\\1in\nFigure12,4(b).\nIf\"'e<utlslitut.(12,12)and (12.13)into(12.10).we can writetheI'CA appro~­\nimationtoa data\"eel'\"x~ i\"thefonn\nM\n\"'-\n~ L{x~\",)u,+ I:(xl'u,)u, (12.19)\n.-. ._M+l\nM\n-x+L(X~U,-XTU,)U; (12.20)\n• ,\n, to' , 10',, ,\n\" ,,\n\"-, \"\"\n\", ~ ~ ; 0 \", ~ ~\n\",., ,.,\nFIIIUre12,4 (a)PIolat!he eJoI;nv.loo.\".,etrumlorthe off·1inedigitsdata set (b) P10t 01 !he sum atthe\n<:liscarded.\"\".Ioos,which \"'l'fesoots!hes.um-ol·SQ\"\",esdistortlonJ i<*~ by projecti<Xlthedataonto\na p<incipalcomponenl slll>spaee'\"dimensionalitvM.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 585,
      "page_label": "566"
    }
  },
  {
    "page_content": "11.1. I'rindpall.:\"l11pon~nt Anal~·.i. 567\nFIIIUr.1:1:.5 An \",>gi\",,1~mpIe IromlI>e011·_ digilsdata...ttOll\"1herwithitsPeA re<:onstnxlions\noblair...:!by 'e1aio\"li!Xl,Ifj)<incipal~n1S10< variousval,,\"01 ,If.As ,IIincreason\n!tiere<:onst,uctiOfI~smore ao::urateand woukl~portee!when .-IfK D ~\n28 x 28 ~ .\"-1.\nwhere we ha\"e made moe of therelation\n\"x= L (x'\",)u;\n,-,\n(12.21)\nAI''''''''/;'\\'A\nSeer/on9.1\nwhich follow.from the completene\" of the {u,I,Thi. represent.a contpre\"ioo\n\"fthe data>ct.Ilttau>eforeach data poim we ha,..repla«d the V·dimensiooal\n\"o<:lorx\" Wilh an ,I[.din>en,ional\"o<:torhavingcomponem, (x~ '\" _ X'\",).11Ie\n'mailerthe\"alueof M. thegreaterthedegreeof comp.-e\",ion.Example. of PeA\n,\"\"on't\"\"tioosofdatapointsforthedigitsdatasetareshown in Figure12.5\nAnolher applicationof priocipalcompcmenl analy,i.i'todatapre-processing.\nInthi'case,lhegoalisnO! dimensionalityredUC1ionbutratherthetmn,formmion of\na dataselinor<k'to standa'lli'.eeenain of ilSpmpenies. This can be in'portanlin\nallowing.ubsequentpallem ,\"\"ognitionalgorithm.10 be appliedsuccessfully10 the\ndata>ct.Typically.ilisdone wilentheoriginal\"ariable.aremea,ured in\"arioosdif.\nferentunil'or !la\"esignificantlydifTerent,'ariabilil}'.For instancein theOld Faithful\ndatasel.thetime betv.-eeneruption.i.typicanyan orderof magni1Ude greaterthan\nlhedUrali\"\"of.nerupt;,,\".When W'e appliedthe \".nlCansalgorill\"\"10 thi<data\nset,\".-efirstmade a separ.telinearre-sealingof the individual\"anable'socb thm",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 586,
      "page_label": "567"
    }
  },
  {
    "page_content": "set,\".-efirstmade a separ.telinearre-sealingof the individual\"anable'socb thm\neach \"ariablehad zero mean and unit\"ariance.llUs isknown as slllNlardiv·.,gthe\ndota.and thecO\\'anancematrixforlhe'lando,di/,eddalahascomponents\n(12,22)\nwhere <1, isthe,'anaoceof:c,.Thisi< known asthe(\",,,el,,,;,,,,matri.'oftheoriginal\ndotaand ha'thepropeny thaiift\"\"orompooent, X; and x,ofthedataareperfee1ly\ncorrel.ted.thenAi _ I.•nd iftheya.-euocorrelated.thenAi _ O.\n11\",,'1\"\"',usingPeA we can make a It>Of'esubst.mialnonnalizat;ooofthedata\ntogi\\'Citzeromean and unitco'·ariance.so thatdifferent\"anablesbecome derorre­\nlate<lTo do this.we first\"\"riletheei8Cn\"cclorequation(12,17)intheform\nsu= UL (12.23)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 586,
      "page_label": "567"
    }
  },
  {
    "page_content": "568 12.CONTINUOUS LATENT VARIABLES\n100 2 2\n90\n00'\nB\nO\n80\n000\n0\n0\n70 0 08 0 0\n,=~o\n0 cPO 0 0 tj ~\n60\nO~50 ~OOID-2 -240\n2 4 6 -2 0 2 -2 0 2\nFigure12.6 Illustrationofthe effectsoflinearpre-processingappliedtotheOld Faithfuldataset.The ploton\ntheleftshows theoriginaldata.The centreplotshows theresultofstandardizingtheindividualvariablestozero\nmean and unitvariance.Alsoshown are the principalaxes ofthisnormalizeddataset,plottedovertherange\n±A~/2. The ploton therightshows theresultofwhiteningofthedatatogiveitzeromean and unitcovariance.\nwhere L isa D x D diagonalmatrixwith elementsAi,and U isa D x D orthog­\nonal matrixwith columns given by Ui. Then we define,foreach datapointX n , a\ntransformedvaluegivenby\n(12.24)\nwhere x isthesample mean definedby (12.1).Clearly,theset{Yn} has zeromean,\nand itscovarianceisgivenby theidentitymatrixbecause\nN\n1~ L L -1/2UT (X n - x)(xn - x)TUL -1/2\nn=l\nL ~1/2UTSUL-1/2 = L -1/2LL -1/2 = I. (12.25)\nAppendix A\nAppendix A\nThis operationisknown as whiteningor sphereingthedataand isillustratedforthe\nOld FaithfuldatasetinFigure12.6.\nItisinterestingtocompare PCA with theFisherlineardiscriminantwhich was\ndiscussedin Section4.1.4.Both methods can be viewed as techniquesforlinear\ndimensionalityreduction.However, PCA isunsupervisedand depends only on the\nvaluesX n whereas Fisherlineardiscriminantalsousesclass-labelinformation.This\ndifferenceishighlightedby theexample inFigure12.7.\nAnother common applicationofprincipalcomponent analysisistodatavisual­",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 587,
      "page_label": "568"
    }
  },
  {
    "page_content": "differenceishighlightedby theexample inFigure12.7.\nAnother common applicationofprincipalcomponent analysisistodatavisual­\nization.Here each datapointisprojectedontoa two-dimensional(M = 2) principal\nsubspace,so thata datapointX n isplottedatCartesiancoordinatesgivenby x'J.U1\nand x'J.U2, where Ul and U2 are theeigenvectorscorrespondingto thelargestand\nsecond largesteigenvalues.An example of such a plot,fortheoilflow dataset,is\nshown inFigure12.8.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 587,
      "page_label": "568"
    }
  },
  {
    "page_content": "12.1.I'<incipalCl)m ...\"n~nt Anal}'s;, 569\n.,':----;!---;-\"_.S 0 3\n'~'\" ~ .•••._',.\".,\n-. .'~.•''''.., ..' ~\n\"\n':r-'---~+·_..-:--'~-J\n~,\n.,\n.\"\nA comparison01 pro:ipalcompo­\nMnt analysis....111 Fisha(s linaar\ndiscriminant101 \"\"\"\",<*man\"\"'\"\nalityr&duclion.Here too data in\ntwo dimansions,belonging to two\nclassessIIOWI1 in red and blue.is\nto be PfOI\"Cledonto a s.ingledi·\nmension. PCA c/>xlsasthe direc·\ntion01 maximum varia\"\"e.sIIOWI1\ntrythama9\"\"ta Co\"\"'.wt11chleads\nto strongclassoverlap.whereas\n!he Fisl>efIiMardiSCfOrnillanttakes\naccoun1 <:A too class labelsand\nleadstoa projectionontotheg<ean\nCUM! giving much t>etlerclass\nseparation\nFig\"\",12.7\nFig\"\",12.8 Visualilatlon01 !he oill'low<latalIetobtained\ntryprojoectingthe<lataonto thelirsttwo prin.\ncipalcompone<1ts.The <ed,blue,and 9r&en\npointscorre-spondto !he 'IamiNI(, 't>omo-\ngenoous',and '8nnula~ flowoonligurations\n\",specriveIy.\n12.1.4 peA forhigh-dimensionaldata\nIn some application.ofpliTlCipalcomponent analysis.the number ofdatapoints\nissmallerthan t!>cdimensionalityoftroedata 'pace.FOI\" example. \",emight want to\napply PeA to a data <el of a few hundred images, each of,,'hichrorrespoOOs to a\n\"eetorin a 'paceof poIentially.....ml milliondimensiOlls(COITespondingtn thfl'e\nenlour\"aluesforeach of thepi.\",lsintroeimage), NOIe thatina D-<limen,ionalspace\na setof jY points.\",'hereN < D. definesa linearsubspa::e\",hosedimensi\"nality\nis at \"\"'stN - 1, and SO there is linlepoint in applying PeA for ,'alue<of M",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 588,
      "page_label": "569"
    }
  },
  {
    "page_content": "is at \"\"'stN - 1, and SO there is linlepoint in applying PeA for ,'alue<of M\nthaI\"'\"greaterthan N - I, Indeed, if\"'epelf\"\",,PeA we willfindthatat least\nD - N + I of theeigen\".luesartlero.eorrespnndingtQ eigenvectorsaloog \",hose\ndireclioosthedata <elhas 10m varianee.Funhem>ore. typicalalgOl\"ithm,forfinding\ntheeigen,'eet\"\"ofa D x D matrixha\"e a computatiooaleoslthm scaleslikeO( D~J.\naOO so forappliealionssuch as the image e,ample.a direc'applicationof PeA will\nbe computatiooallyinfe,,-sibJe.\nW. can resoh'ethisproblem as foIl\",\",'\"Fir;l.letus defineX to be the(N \" DJ·",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 588,
      "page_label": "569"
    }
  },
  {
    "page_content": "570 12.CONTINUOUS LATENT VARIABLES\ndimensionalcentreddatamatrix,whose nth row isgivenby (xn - X)T. The covari­\nance matrix(12.3)can then be writtenas S = N-1 XTX, and thecorresponding\neigenvectorequationbecomes\n1 T\n-X XUi = AiUiN .\nNow pre-multiplyboth sidesby X togive\n1 T\nNXX (XUi) = Ai(XUi)'\nIfwe now defineVi = XUi, we obtain\n1 T\n-XX Vi = AiViN\n(12.26)\n(12.27)\n(12.28)\n(12.30)\nwhich isan eigenvectorequationfortheN x N matrixN- 1XX T . We seethatthis\nhas thesame N -1eigenvaluesastheoriginalcovariancematrix(whichitselfhasan\nadditionalD - N + 1 eigenvaluesofvaluezero).Thus we can solvetheeigenvector\nproblem in spacesof lower dimensionalitywith computationalcostO(N 3 ) instead\nofO(D 3 ).In ordertodeterminetheeigenvectors,we multiplybothsidesof(12.28)\nby X T togive\n( 1 T) T TNX X (X Vi) = Ai(X Vi) (12.29)\nfrom which we see that(XTVi) isan eigenvectorof S with eigenvalueAi. Note,\nhowever,thattheseeigenvectorsneed notbe normalized.To determinetheappropri­\natenormalization,we re-scaleUi ex: X TVi by a constantsuch thatIluiII = 1,which,\nassuming Vi has been normalizedtounitlength,gives\n1 T\nUi = (NA i)1/2X Vi·\nIn summary, toapplythisapproach we firstevaluateXX T and thenfinditseigen­\nvectorsand eigenvaluesand thencompute theeigenvectorsintheoriginaldataspace\nusing(12.30).\n12.2.ProbabilisticpeA\nThe formulationof PCA discussedin the previoussectionwas based on a linear\nprojectionofthedataontoa subspaceoflowerdimensionalitythantheoriginaldata",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 589,
      "page_label": "570"
    }
  },
  {
    "page_content": "The formulationof PCA discussedin the previoussectionwas based on a linear\nprojectionofthedataontoa subspaceoflowerdimensionalitythantheoriginaldata\nspace. We now show thatPCA can alsobe expressedas themaximum likelihood\nsolutionofa probabilisticlatentvariablemodel. ThisreformulationofPCA, known\nasprobabilisticpeA, bringsseveraladvantagescompared withconventionalPCA:\n• ProbabilisticPCA representsa constrainedform of theGaussiandistribution\nin which thenumber of freeparameterscan be restrictedwhilestillallowing\nthemodel tocapturethedominant correlationsina dataset.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 589,
      "page_label": "570"
    }
  },
  {
    "page_content": "Section12.2.2\nSection12.2.3\nSection8.1.4\n12.2.ProbabilisticpeA 571\n• We can derivean EM algorithmforPCA thatiscomputationallyefficientin\nsituationswhere only a few leadingeigenvectorsarerequiredand thatavoids\nhavingtoevaluatethedatacovariancematrixasan intermediatestep.\n• The combinationofa probabilisticmodel and EM allowsus todealwithmiss­\ningvaluesinthedataset.\n• Mixturesof probabilisticPCA models can be formulatedin a principledway\nand trainedusingtheEM algorithm.\n• ProbabilisticPCA forms thebasisfora BayesiantreatmentofPCA inwhich\nthedimensionalityof theprincipalsubspacecan be found automaticallyfrom\nthedata.\n• The existenceof a likelihoodfunctionallowsdirectcomparison with other\nprobabilisticdensitymodels.By contrast,conventionalPCA willassigna low\nreconstructioncosttodatapointsthatareclosetotheprincipalsubspaceeven\niftheyliearbitrarilyfarfrom thetrainingdata.\n• ProbabilisticPCA can be used tomodel class-conditionaldensitiesand hence\nbe appliedtoclassificationproblems.\n• The probabilisticPCA model can be run generativelytoprovidesamplesfrom\nthedistribution.\nThis formulationof PCA as a probabilisticmodel was proposed independentlyby\nTippingand Bishop (1997,1999b) and by Roweis (1998).As we shallsee later,itis\ncloselyrelatedtofactoranalysis(Basilevsky,1994).\nProbabilisticPCA isa simple example of the linear-Gaussianframework, in\nwhich allofthemarginaland conditionaldistributionsareGaussian.We can formu­",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 590,
      "page_label": "571"
    }
  },
  {
    "page_content": "ProbabilisticPCA isa simple example of the linear-Gaussianframework, in\nwhich allofthemarginaland conditionaldistributionsareGaussian.We can formu­\nlateprobabilisticPCA by firstintroducingan explicitlatentvariablez corresponding\ntotheprincipal-componentsubspace.Next we definea Gaussianpriordistribution\np(z)overthelatentvariable,togetherwitha Gaussianconditionaldistributionp(xlz)\nfortheobservedvariablex conditionedon thevalueof thelatentvariable.Specifi­\ncally,thepriordistributionoverz isgivenby a zero-meanunit-covarianceGaussian\np(z)= N(zIO, I). (12.31)\nSimilarly,theconditionaldistributionoftheobservedvariablex,conditionedon the\nvalueofthelatentvariablez,isagainGaussian,oftheform\np(xlz)= N(xlWz + J-L,a2I) (12.32)\nSection8.2.2\nin which themean of x isa generallinearfunctionof z governed by the D x M\nmatrixWand theD-dimensionalvectorJ-L.Note thatthisfactorizeswithrespectto\ntheelementsof x, inotherwords thisisan example of thenaiveBayes model. As\nwe shallseeshortly,thecolumns ofW span a linearsubspacewithinthedataspace\nthatcorrespondstotheprincipalsubspace.The otherparameterinthismodel isthe\nscalara 2 governingthevarianceoftheconditionaldistribution.Note thatthereisno",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 590,
      "page_label": "571"
    }
  },
  {
    "page_content": "572 11.CONTINUOUS LAT!::NT VANIM1LI::S\n,\n/.-,,,,,,,\n,\nFlgu..12.9 I\\n~I\"'tfat\"\" oIlt>eII\"\"\"fativevi&w oI1t>ep<ot>abi!;st\",PeA modeIfof\" two-dimensiooal<!ala\nspace and a on&-<lirnent.ionallat/l<1tspace, An Ob&erved <!alapointx Isgeneratedby firstdrawinga valuei\nfof1t>eIat&n1vafiatlle/f(lm~s priordist,~t\"\" P(~) and Itlendrawinga val\"\"fofx lroman iSO/fopK:Gaussian\ndistr~t\"\" (iijust,al&(lby theredcir<:ie's)havingmean wi +\"and coY8r1.once,,'1The l/f&er\\ellips.&$show l!le\ndensity\"\"\"toors!ofthe marg'''''1dis1r1bulionPIx).\nlossof ge\"\"raJityinassuming a zero mean. unitco\\'arianceGau\"ian forthe latent\ndistributi\"nII{Z)because a more gcneralGau\"i3n di\"ributi\"nwould gi\"erisetoan\nequivalentprobabili\"icn>odel.\nWe can view the probabilisticPeA model from a geoerati\"e\\'iew\"\"intin\"hich\na sampled '-alueof the ob\"\"Yed ,..riableisobIained by firstchoo,ing a ,..Iuefor\nthe latent,'ariahleaod then >ampling the OO\",,,'e;j,-ariablecooditionedon thislao\ntent\\'alue,Specifically,the V-dimen'ional OO\"''''ed'-ariablex isdefinedby a lin·\nea, tran,formati,,\"of the '\\/·dimen,i\"nallatcnt'-ariablez plu, additi'-eGaussian\n'noise',<0 that\n,,=\\VZ+,,+~ (12.33)\nw!>ere z isan M-di\"\"'nsionalGaussian lalentvariable.and ..isa V·dimensi\"nal\n,ero-mean Gau ..ian-distributednoi..,\"ariablewitb co'-ariance,,21.This generative\nprocessisillustratedin Figure 12.9.NOIe thatthisframe\".-orl<isbased on a mapping\nfrom latent,pace10 dataspace.in contrast10 the nl()l'(:C(\"\"'cnti,,,,\"1\"iew \"fI'CA",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 591,
      "page_label": "572"
    }
  },
  {
    "page_content": "from latent,pace10 dataspace.in contrast10 the nl()l'(:C(\"\"'cnti,,,,\"1\"iew \"fI'CA\ndis.cus\"'dalx\",e,11Ie\",,'e=mapping, from data space tothe latentspace.,,-illhe\nOOlained,honlyusingHa ycs·lhwn:m.\nSUf!ll'OSCwe wish 10 deten\"inethe \"aluesofll>oparameters\\V. I'and ,,'uSIng\nmaximum likelihuo<l,To write\"\"\"\"nlhe likeliltoodfunction,we need an \"\"pression\nfortl>omarginaldistributioop{\") of tl>o~,,'ed ...riahle_This isexprt__ sed. fmn'\nthesum aod p,oductrules\"fprobability,in theform\n(11,34)\nE,e,,-ise12,7\nll\"\"auS(:thiscorresponds to a linear·Gau\"i,nlT1(llIcLthi<marginaldi,tribulionis\nagainGaussian.atldisgiven by\n,,(,,)_ N{xllf,C) (IUS)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 591,
      "page_label": "572"
    }
  },
  {
    "page_content": "12.2.ProbabilisticpeA\nwhere theD x D covariancematrixC isdefinedby\nC = WW T + 0-21.\n573\n(12.36)\nThisresultcan alsobe derivedmore directlyby notingthatthepredictivedistribution\nwillbe Gaussian and then evaluatingitsmean and covarianceusing (12.33).This\ngives\nIE[x]\ncov[x]\nIE[Wz + JL + E] = JL\nIE [(Wz + E)(WZ + E)T]\nIE [WZZTW T] + IE[EET]= WW T + 0-21\n(12.37)\n(12.38)\nwhere we have used thefactthatz and E areindependentrandom variablesand hence\nareuncorrelated.\nIntuitively,we can thinkof thedistributionp(x)as being definedby takingan\nisotropicGaussian 'spraycan'and moving itacrosstheprincipalsubspacespraying\nGaussianinkwith densitydeterminedby 0-2 and weightedby thepriordistribution.\nThe accumulatedinkdensitygivesrisetoa 'pancake'shaped distributionrepresent­\ningthemarginaldensityp(x).\nThe predictivedistributionp(x) isgoverned by theparametersJL, W, and 0-2 •\nHowever, thereisredundancy inthisparameterizationcorrespondingtorotationsof\nthelatentspacecoordinates.To seethis,considera matrixW = WR where R is\nan orthogonalmatrix.Using theorthogonalitypropertyRR T = I,we seethatthe\nquantityWW T thatappearsinthecovariancematrixC takestheform\n(12.39)\n(12.41)\nExercise 12.8\nand hence isindependentof R. Thus thereisa whole familyofmatricesW allof\nwhich giverisetothesame predictivedistribution.Thisinvariancecan be understood\ninterms of rotationswithinthelatentspace.We shallreturnto a discussionof the\nnumber ofindependentparametersinthismodel later.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 592,
      "page_label": "573"
    }
  },
  {
    "page_content": "interms of rotationswithinthelatentspace.We shallreturnto a discussionof the\nnumber ofindependentparametersinthismodel later.\nWhen we evaluatethepredictivedistribution,we requireC- 1 , which involves\ntheinversionofa D x D matrix.The computationrequiredtodo thiscan be reduced\nby making useofthematrixinversionidentity(C.7)togive\nC- 1 = 0--11 - 0--2WM- 1W T (12.40)\nwhere theM x M matrixM isdefinedby\nM = WTW + 0-21.\nBecause we invertM ratherthaninvertingC directly,thecostofevaluatingC- 1 is\nreducedfrom O(D 3 ) toO(M 3 ).\nAs well as thepredictivedistributionp(x),we willalsorequirethe posterior\ndistributionp(zlx),which can againbe writtendown directlyusingtheresult(2.116)\nforlinear-Gaussianmodels togive\n(12.42)\nNote thattheposteriormean depends on x, whereas theposteriorcovarianceisin­\ndependentofx.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 592,
      "page_label": "573"
    }
  },
  {
    "page_content": "574 12.CONTINUOUS LATENT VARIABLES\nFigure12.10 The probabilisticpeA model fora datasetofN obser­\nvationsofx can be expressedas a directedgraph in\nwhich each observationX n isassociatedwitha value\nZn ofthelatentvariable.\n..-+--w\nN\n12.2.1 Maximum likelihoodpeA\nWe next considerthe determinationof the model parametersusingmaximum\nlikelihood.Given a datasetX = {xn } of observeddatapoints,theprobabilistic\npeA model can be expressedas a directedgraph,as shown in Figure12.10.The\ncorrespondingloglikelihoodfunctionisgiven,from (12.35),by\nN\nInp(XIJL,W,O' 2 ) = L lnp(xn IW,JL,O'2 )\nn=l\nN\nND N 1\"\" T 1--2-ln(2n)- 2 lnIe[- 2L,..(xn - JL) c- (xn - JL). (12.43)\nn=l\nSettingthederivativeofthelog likelihoodwith respecttoJL equaltozerogivesthe\nexpectedresultJL = xwhere xisthedatamean definedby (12.1).Back-substituting\nwe can thenwritetheloglikelihoodfunctionintheform\nNInp(XIW, JL,0'2) = -2{D In(2n)+ InIe[+ Tr (C-1S)} (12.44)\nwhere S isthedatacovariancematrixdefinedby (12.3).Because theloglikelihood\nisa quadraticfunctionofJL,thissolutionrepresentstheuniquemaximum, ascan be\nconfirmedby computing second derivatives.\nMaximization with respectto W and 0'2 ismore complex but nonethelesshas\nan exact closed-formsolution.Itwas shown by Tippingand Bishop (1999b)thatall\nofthestationarypointsoftheloglikelihoodfunctioncan be writtenas\n(12.45)\nwhere U M isa D x M matrixwhose columns aregivenby any subset(ofsizeM)\nof theeigenvectorsof the datacovariancematrixS, the M x M diagonalmatrix",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 593,
      "page_label": "574"
    }
  },
  {
    "page_content": "where U M isa D x M matrixwhose columns aregivenby any subset(ofsizeM)\nof theeigenvectorsof the datacovariancematrixS, the M x M diagonalmatrix\nL M has elementsgivenby thecorrespondingeigenvalues..\\,and R isan arbitrary\nM x M orthogonalmatrix.\nFurthermore,Tippingand Bishop (1999b)showed thatthemaximum ofthelike­\nlihoodfunctionisobtainedwhen theM eigenvectorsarechosen tobe thosewhose\neigenvaluesaretheM largest(allothersolutionsbeingsaddlepoints).A similarre­\nsultwas conjecturedindependentlyby Roweis (1998),althoughno proofwas given.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 593,
      "page_label": "574"
    }
  },
  {
    "page_content": "12.2.ProbabilisticpeA 575\nAgain,we shallassume thattheeigenvectorshave been arrangedinorderofdecreas­\ningvaluesofthecorrespondingeigenvalues,so thattheM principaleigenvectorsare\nUl,\"\" UM. In thiscase,the columns of W definetheprincipalsubspace of stan­\ndard PCA. The correspondingmaximum likelihoodsolutionfor(J'2isthengivenby\n1 D\n(J'~L = D-M L Ai\ni=M+l\n(12.46)\nSection12.2.2\nso that(J'~L istheaveragevarianceassociatedwith thediscardeddimensions.\nBecause R isorthogonal,itcan be interpretedas a rotationmatrixintheM x M\nlatentspace.Ifwe substitutethesolutionforW intotheexpressionforC, and make\nuse of the orthogonalitypropertyRR T = I,we see thatC isindependentof R.\nThis simply saysthatthepredictivedensityisunchanged by rotationsin the latent\nspaceas discussedearlier.For theparticularcaseofR = I,we seethatthecolumns\nof W arethe principalcomponent eigenvectorsscaledby the varianceparameters\nAi - (J'2.The interpretationof thesescalingfactorsisclearonce we recognizethat\nfora convolutionofindependentGaussian distributions(in thiscasethelatentspace\ndistributionand the noisemodel) the variancesare additive.Thus the varianceAi\nin thedirectionofan eigenvectorUi iscomposed of thesum ofa contributionAi ­\n(J'2from theprojectionof the unit-variancelatentspace distributionintodata space\nthroughthecorrespondingcolumn ofW, plusan isotropiccontributionof variance\n(J'2which isadded inalldirectionsby thenoisemodel.\nItisworth takinga moment to study the form of the covariancematrix given",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 594,
      "page_label": "575"
    }
  },
  {
    "page_content": "(J'2which isadded inalldirectionsby thenoisemodel.\nItisworth takinga moment to study the form of the covariancematrix given\nby (12.36).Considerthevarianceofthepredictivedistributionalong some direction\nspecifiedby the unitvectorv, where vTv = 1, which isgiven by vTCv. First\nsuppose thatv isorthogonalto theprincipalsubspace,inotherwords itisgivenby\nsome linearcombinationof the discardedeigenvectors.Then v TV = 0 and hence\nv TCv = (J'2. Thus themodel predictsa noisevarianceorthogonalto theprincipal\nsubspace,which,from (12.46),isjusttheaverageofthediscardedeigenvalues.Now\nsuppose thatv = Ui where Ui isone of theretainedeigenvectorsdefiningtheprin­\ncipalsubspace.Then vTCv = (Ai- (J'2)+ (J'2 = Ai.In otherwords, thismodel\ncorrectlycapturesthevarianceofthedataalongtheprincipalaxes,and approximates\nthevarianceinallremainingdirectionswith a singleaveragevalue(J'2.\nOne way toconstructthemaximum likelihooddensitymodel would simply be\nto findthe eigenvectorsand eigenvaluesof the data covariancematrix and then to\nevaluateWand (J'2 using the resultsgiven above. In thiscase,we would choose\nR = I forconvenience.However, ifthemaximum likelihoodsolutionisfound by\nnumericaloptimizationof the likelihoodfunction,forinstanceusing an algorithm\nsuch as conjugategradients(Fletcher,1987; Nocedal and Wright, 1999; Bishop and\nNabney, 2008) or through the EM algorithm,then the resultingvalue of R ises­\nsentiallyarbitrary.This impliesthatthecolumns of W need not be orthogonal.If",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 594,
      "page_label": "575"
    }
  },
  {
    "page_content": "sentiallyarbitrary.This impliesthatthecolumns of W need not be orthogonal.If\nan orthogonalbasisisrequired,thematrixW can be post-processedappropriately\n(Golub and Van Loan, 1996). Alternatively,the EM algorithmcan be modified in\nsuch a way as to yieldorthonormalprincipaldirections,sortedin descendingorder\nofthecorrespondingeigenvalues,directly(Ahn and Oh, 2003).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 594,
      "page_label": "575"
    }
  },
  {
    "page_content": "576 12.CONTINUOUS LATENT VARIABLES\nThe rotationalinvarianceinlatentspacerepresentsa form ofstatisticalnoniden­\ntifiability,analogousto thatencounteredformixturemodels inthecaseof discrete\nlatentvariables.Here thereisa continuum of parametersallof which leadto the\nsame predictivedensity,incontrasttothediscretenonidentifiabilityassociatedwith\ncomponent re-labellinginthemixturesetting.\nIfwe considerthecaseof M = D, so thatthereisno reductionofdimension­\nality,then U M = U and L M = L. Making use of the orthogonalityproperties\nUU T = I and RR T = I,we seethatthecovarianceC ofthemarginaldistribution\nforx becomes\n(12.47)\nand so we obtainthe standardmaximum likelihoodsolutionfor anunconstrained\nGaussian distributioninwhich thecovariancematrixisgivenby thesample covari­\nance.\nConventionalPCA isgenerallyformulatedasa projectionofpointsfrom theD­\ndimensionaldataspaceonto an M -dimensionallinearsubspace.ProbabilisticPCA,\nhowever,ismost naturallyexpressedasa mapping from thelatentspaceintothedata\nspace via(12.33).For applicationssuch as visualizationand datacompression,we\ncan reversethismapping usingBayes' theorem.Any pointx indataspacecan then\nbe summarized by itsposteriormean and covariancein latentspace.From (12.42)\nthemean isgivenby\nwhere M isgivenby (12.41).This projectstoa pointindataspacegivenby\nWlE[zlx]+ J-L.\n(12.48)\n(12.49)\nSection3.3.1 Note thatthistakesthesame form as theequationsforregularizedlinearregression",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 595,
      "page_label": "576"
    }
  },
  {
    "page_content": "WlE[zlx]+ J-L.\n(12.48)\n(12.49)\nSection3.3.1 Note thatthistakesthesame form as theequationsforregularizedlinearregression\nand isa consequence of maximizing the likelihoodfunctionfora linearGaussian\nmodel. Similarly,theposteriorcovarianceisgivenfrom (12.42)by 0-2M- 1 and is\nindependentofx.\nIfwe takethelimit0-2 ----t0,thentheposteriormean reducesto\n(12.50)\nExercise 12.11\nExercise 12.12\nSection2.3\nwhich representsan orthogonalprojectionof the datapointonto the latentspace,\nand so we recoverthestandardPCA model. The posteriorcovarianceinthislimitis\nzero,however,and the densitybecomes singular.For 0-2 > 0,thelatentprojection\nisshiftedtowardstheorigin,relativetotheorthogonalprojection.\nFinally,we note thatan importantroleforthe probabilisticPCA model isin\ndefininga multivariateGaussiandistributioninwhich thenumber ofdegreesoffree­\ndom, inotherwords thenumber ofindependentparameters,can be controlledwhilst\nstillallowingthe model to capturethe dominant correlationsin the data. Recall\nthata generalGaussian distributionhas D(D + 1)/2independentparametersinits\ncovariancematrix (plusanotherD parametersin its mean).Thus the number of\nparametersscalesquadraticallywith D and can become excessiveinspacesofhigh",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 595,
      "page_label": "576"
    }
  },
  {
    "page_content": "12.2.ProbabilisticpeA 577\ndimensionality.Ifwe restrictthecovariancematrixtobe diagonal,thenithasonlyD\nindependentparameters,and so thenumber ofparametersnow grows linearlywith\ndimensionality.However, itnow treatsthevariablesasiftheywere independentand\nhence can no longerexpressany correlationsbetween them. ProbabilisticPeA pro­\nvidesan elegantcompromise in which the M most significantcorrelationscan be\ncapturedwhilestillensuringthatthetotalnumber ofparametersgrows onlylinearly\nwith D. We can see thisby evaluatingthe number of degreesof freedom in the\nPPCA model as follows.The covariancematrixC depends on theparametersW,\nwhich hassizeD x M, and a 2 , givinga totalparametercountofDM + 1.However,\nwe have seenthatthereissome redundancyinthisparameterizationassociatedwith\nrotationsofthecoordinatesystem inthelatentspace.The orthogonalmatrixR that\nexpressestheserotationshas sizeM x M. Inthefirstcolumn ofthismatrixthereare\nM - 1 independentparameters,becausethecolumn vectormust be normalizedto\nunitlength.Inthesecondcolumn thereareM - 2 independentparameters,because\nthecolumn must be normalizedand alsomust be orthogonaltothepreviouscolumn,\nand soon.Summing thisarithmeticseries,we seethatR hasa totalofM(M -1)/2\nindependentparameters.Thus thenumber of degreesoffreedom inthecovariance\nmatrixC isgivenby\nDM + 1 - M(M - 1)/2. (12.51)\nExercise12.14\nSection12.2.4\nSection9.4\nThe number of independentparametersinthismodel thereforeonly grows linearly",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 596,
      "page_label": "577"
    }
  },
  {
    "page_content": "DM + 1 - M(M - 1)/2. (12.51)\nExercise12.14\nSection12.2.4\nSection9.4\nThe number of independentparametersinthismodel thereforeonly grows linearly\nwith D, forfixedM. Ifwe takeM = D - 1,thenwe recoverthestandardresult\nfora fullcovarianceGaussian.In thiscase,thevariancealong D - 1 linearlyin­\ndependentdirectionsiscontrolledby thecolumns ofW, and thevariancealongthe\nremainingdirectionisgivenby a2 . IfM = 0,themodel isequivalenttotheisotropic\ncovariancecase.\n12.2.2 EM algorithmforpeA\nAs we have seen,theprobabilisticPCA model can be expressedinterms ofa\nmarginalizationover a continuouslatentspacez in which foreach datapointX n ,\nthereisa correspondinglatentvariableZn. We can thereforemake use of theEM\nalgorithmtofindmaximum likelihoodestimatesofthemodel parameters.This may\nseem ratherpointlessbecause we have alreadyobtainedan exactclosed-formso­\nlutionforthemaximum likelihoodparametervalues.However, in spacesof high\ndimensionality,theremay be computationaladvantagesin using an iterativeEM\nprocedureratherthanworking directlywiththesample covariancematrix.ThisEM\nprocedurecan alsobe extendedto thefactoranalysismodel, forwhich thereisno\nclosed-formsolution.Finally,itallowsmissingdatatobe handled in a principled\nway.\nWe can derivetheEM algorithmforprobabilisticPCA by followingthegeneral\nframework forEM. Thus we writedown thecomplete-datalog likelihoodand take\nitsexpectationwith respectto the posteriordistributionof the latentdistribution",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 596,
      "page_label": "577"
    }
  },
  {
    "page_content": "itsexpectationwith respectto the posteriordistributionof the latentdistribution\nevaluatedusing 'old'parametervalues.Maximization of thisexpectedcomplete­\ndataloglikelihoodthenyieldsthe 'new'parametervalues.Because thedatapoints",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 596,
      "page_label": "577"
    }
  },
  {
    "page_content": "578 12.CONTINUOUS LATENT VARIABLES\nareassumed independent,thecomplete-datalog likelihoodfunctiontakestheform\nN\nInp (X, ZIJL,W, (J2) = L {lnp(xnlzn) + lnp(zn)}\nn=l\n(12.52)\nwhere the nth row of the matrix Z isgiven by Zn. We alreadyknow thattheexact\nmaximum likelihoodsolutionforJL isgivenby thesample mean x definedby (12.1),\nand itisconvenientto substituteforJL atthisstage.Making use of theexpressions\n(12.31)and (12.32)forthelatentand conditionaldistributions,respectively,and tak­\ning theexpectationwith respecttotheposteriordistributionoverthelatentvariables,\nwe obtain\nNote thatthisdepends on theposteriordistributiononly throughthesufficientstatis­\nticsoftheGaussian.Thus in theE step,we use theold parametervaluestoevaluate\nM-1WT(X n - x)\n(J2M- 1 + lE[zn]lE[zn]T\n(12.54)\n(12.55)\nExercise12.15\nwhich follow directlyfrom the posteriordistribution(12.42)togetherwith thestan­\ndard resultlE[znz~] = cov[zn] + JE[zn]JE[zn]T.Here M isdefinedby (12.41).\nIn the M step,we maximize with respectto Wand (J2, keeping the posterior\nstatisticsfixed.Maximization with respectto (T2 isstraightforward.For the maxi­\nmizationwith respecttoW we make use of(C.24),and obtaintheM-step equations\nW new\n2\n(Jnew =\n[t,exn-X)IlIZn]T][t,Il[ZnZ~]]-'\n1 N\nND L {llxn - xl1\n2\n- 2lE[zn]TW~ew(xn - x)\nn=l\n+Tr (JE[znzJ]W~ewW new )}.\n(12.56)\n(12.57)\nThe EM algorithmforprobabilisticPCA proceedsby initializingtheparameters\nand then alternatelycomputing the sufficientstatisticsof the latentspace posterior",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 597,
      "page_label": "578"
    }
  },
  {
    "page_content": "and then alternatelycomputing the sufficientstatisticsof the latentspace posterior\ndistributionusing(12.54)and (12.55)intheE stepand revisingtheparametervalues\nusing (12.56)and (12.57)intheM step.\nOne of the benefitsof the EM algorithmfor PCA iscomputationalefficiency\nforlarge-scaleapplications(Roweis, 1998). Unlike conventionalPCA based on an",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 597,
      "page_label": "578"
    }
  },
  {
    "page_content": "(12.58)\n12.2.ProbabilisticpeA 579\neigenvectordecompositionof the sample covariancematrix,the EM approach is\niterativeand so might appeartobe lessattractive.However, each cycleof theEM\nalgorithmcan be computationallymuch more efficientthan conventionalPCA in\nspacesof highdimensionality.To seethis,we notethattheeigendecompositionof\nthecovariancematrixrequiresO(D 3 ) computation.Often we areinterestedonly\nin thefirstM eigenvectorsand theircorrespondingeigenvalues,in which case we\ncan use algorithmsthatare0 (M D 2 ). However, theevaluationof thecovariance\nmatrixitselftakes0 (ND 2 ) computations,where N isthenumber of datapoints.\nAlgorithmssuch as the snapshotmethod (Sirovich,1987),which assume thatthe\neigenvectorsarelinearcombinationsof thedatavectors,avoiddirectevaluationof\nthecovariancematrixbutareO(N 3 ) and hence unsuitedtolargedatasets.The EM\nalgorithmdescribedhere alsodoes not constructthecovariancematrixexplicitly.\nInstead,themost computationallydemanding stepsarethoseinvolvingsums over\nthedatasetthatare0 (N D M). For largeD, and M « D, thiscan be a significant\nsavingcompared to0 (ND 2 ) and can offsettheiterativenatureoftheEM algorithm.\nNote thatthisEM algorithmcan be implemented in an on-lineform in which\neach D-dimensionaldatapointisreadin and processedand thendiscardedbefore\nthenextdatapointisconsidered.To seethis,notethatthequantitiesevaluatedin\ntheE step(anM-dimensional vectorand an M x M matrix)can be computed for",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 598,
      "page_label": "579"
    }
  },
  {
    "page_content": "thenextdatapointisconsidered.To seethis,notethatthequantitiesevaluatedin\ntheE step(anM-dimensional vectorand an M x M matrix)can be computed for\neachdatapointseparately,and intheM stepwe need toaccumulatesums overdata\npoints,which we can do incrementally.This approachcan be advantageousifboth\nNand D arelarge.\nBecause we now have a fullyprobabilisticmodel forPCA, we can dealwith\nmissingdata,providedthatitismissingatrandom, by marginalizingover thedis­\ntributionof the unobservedvariables.Again thesemissingvaluescan be treated\nusingtheEM algorithm.We givean example of theuse of thisapproachfordata\nvisualizationinFigure12.11.\nAnotherelegantfeatureoftheEM approachisthatwe can takethelimita 2 ----t0,\ncorrespondingtostandardPCA, and stillobtaina validEM-like algorithm(Roweis,\n1998).From (12.55),we seethattheonlyquantitywe need tocompute intheEstep\nisJE[zn].Furthermore,theM stepissimplifie~ becauseM = WTW. To emphasize\nthesimplicityofthealgorithm,letus defineX tobe a matrixofsizeN x D whose\nnth row isgivenby thevectorX n - x and similarlydefine0 tobe a matrixof size\nD x M whose nth row isgivenby thevectorJE[zn].The Estep (12.54)oftheEM\nalgorithmforPCA thenbecomes\no = (W~dWold)-lW~dX\nand theM step(12.56)takestheform\nW new = XTOT(OOT)-l. (12.59)\nAgain thesecan be implementedinan on-lineform.These equationshave a simple\ninterpretationasfollows.From ourearlierdiscussion,we seethattheE stepinvolves\nan orthogonalprojectionofthedatapointsontothecurrentestimatefortheprincipal",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 598,
      "page_label": "579"
    }
  },
  {
    "page_content": "an orthogonalprojectionofthedatapointsontothecurrentestimatefortheprincipal\nsubspace.Correspondingly,theM steprepresentsa re-estimationof theprincipal",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 598,
      "page_label": "579"
    }
  },
  {
    "page_content": "580 12.CONTlNlJOlJS I\"Ht;I'ITVi\\RIARlES\nFig\"..12.11 ProbabilisticPCA visoo,zsbon01 a portion0I1he\"\"!low datasetlo<Ihe!irsl100 (lata»einls,The\nleft..,...ndplotoIIOWS IheI'O\"leoo<mean proj9c1ionsoIlhfI(latapoims on lheprincipalsubspace.The ,;gtrI·hi\\nd\nplotisobtainedby firslran<lomlyomitting30% 0I1he variable.aloo.and lhen us>rlgEM 10 MndIe I\"\"mi......\nvalues.Note I!IaIeac/1data poinl1henNoS allea.,one missingmea.u,ement butlhoallheploti.\"\"ry..mia, to\nlheona obtainedwit\"\"\"lmiss....valL>ll$\nEwrrise /2,/7\nsubspace to minimize !he squared reoonslruCtiooerrorin 'oIhichthe proje<:tion,are\nC.,N.\nWe ean gh'ea ,imple physicalanalogy forthisEM algorithm.which iseasily\nvisualizedfor D = 2 and M = 1. Coo,ider a collectioonf data point',n tWI)\ndimension',aod lettileu\"\"'-dimensiunalprincipalsubspace be representedby a <ohd\nrod. Now atlaCheach data pointtothenxIviaa ,pringoo<:)\"ingHooI;:e',law (\"umJ\nenergy i,propol1ional10 ,liesquareof lilespring\".length).In ll1eE 'tel',we keep\nthe nxIhed and allow the attachment point'tn,Iideup and <I<,wn ll1enxI'\"a,to\nminimize ll1ee\",,'llY,This cau\",.each attachmentpoint(independently)10 position\nItselfat theorthogonalpmjeclion of the c~sponding data pointonto the nxI.In\ntheM 'tel'.we keep theattachment poiOl'fil<edand thenreleasetilenxIand allowit\nto m'>,'e10 tileminimum energy posilion.11IeE and M 'tepsarethenrepeateduntil\na ,uitablec\"\"vergence cri.eri\"\"is..,isfled.a.isilluSlratedin Figure 12.12.\n12.2.3 Bayesian peA",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 599,
      "page_label": "580"
    }
  },
  {
    "page_content": "a ,uitablec\"\"vergence cri.eri\"\"is..,isfled.a.isilluSlratedin Figure 12.12.\n12.2.3 Bayesian peA\nS<J far in OIlrdi\",\"\"iooof PeA. we have \",'.nledIhaltile'\"Ine,IIfor,lie\ndl,nen,ionalit)\"of tileprincipal.ubspace is gi\"en, In praclice.\".-enlmt cOOose a\nsuilable,..I\"\" according10 the application.For ,isuali,a,ion.we ge\"\"\",nychoose\n.\\1 = 2. whereas forOIher application,the approrrialCchoice for ,1/ma)\" be less\ndea,. One appmao:h i.10 pi\",the eigen\"alue'peclrum forlhe data set.analog,•.\"\n10 the example in Figure 12.4 for theoff_linedigitsdala SCI,and look to see iflite\neige\",,,I....nmurallyform two groups comprising a setof ,mall ,'aluesseparatedby\na ,ign;flcantgap from a \",tof relativel)\"large,'alues,indicatinga naturalcholccf<>r\nAI, In practice.such a gap i,oflen '''''seen",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 599,
      "page_label": "580"
    }
  },
  {
    "page_content": ",o-,,o-,,o-,\no\n,\", • , ,,<,\n•,\n0 ~, • 0 0\n....\n-, -, -,\n, 0 , -, 0 -, 0 ,\n,\", , ,\n-,\nFlgu..12.12 Synt\"'elic<lataillustratingtoo EM algorithm!ofPCA definedby (12.58)and (1259).(8)A data\nsetX withthedatapointsshown in1JI'e«l,t\"ll\"tM'W'i1!1l!>et'IMpMdpal \"\"\"\"\"\"\",IS(shown as eigenveclor1\nscaled by It>esquafll'OOIS 04 theeigeJ'l\\lllluel).(b) Initialconfigurat\"'\"01 too principalsul>sl>a<:<tdefinedby W,\nshown inmd, tOO\"lhfIrwiththe fK'(Ijeclions01 the latll<11pointsZ inlotoo <lataspace,giItooby ZW T ,shown in\ncyan,(ojAlter\"\"\"M step,too laten!SI'B«lP>as been update<!wiIhZ r>el(lnxed.(d)Me' tt>esuccess....E Slep,\nIt>e\"\"'-'eo01 Z havu been up<!atll<:1.~'\"Ihogoooalr>rojecliQn$,withW h&k! fixed.(e)Aft....tilese<:o<l<lM\nS!flp.<')Afterl!IeMC()<>;l E st\"l'\nS,uion I.J\nBe<:au,\",th~ pm/xlhili>licPeA modd has a well·definedlikelilloodf\"flCtion,we\n<wId employ cros,-,-.1idationtodelermine the \\\"aJueofdi\"\",nsiooa!ityby \"'Iecting\ntit<:large,tlog likelihoodt>I1a '-alidationdataset Such an opprooch.hov.·~\\-er. can\nbecome computationallyro<lly.p3rticularl)'ifwe CQnsid<:,• probabilisticmiXlUre\nof PeA modds (Tippingand Bishop.1999a) in \"hich we seek 10 <!etermi'\"the\nappropriatedimen,ionalily\",paraltlyfortoch componenl inlt1emixm\"\"\nGi'-enthaiw. ha,-ea probabilislicformulalionof PeA, ils«ms natural10 s«k\nu Buye,ianapproach 10 model seleclion.To do thi,.,,\"'enee<! 10 marginalize001\nthe model paramele\" /'.\\V. und ,,'wilh\"\"peeltoappropriatepriordistribution'.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 600,
      "page_label": "581"
    }
  },
  {
    "page_content": "the model paramele\" /'.\\V. und ,,'wilh\"\"peeltoappropriatepriordistribution'.\nThis Can be done by u,inga ,-ariation.lframework to .pproxim'letheallulylic.lly\nintractablemurginaliUOi;oo,(Bi,hop.1mb). 1lIcmarginallikelihoodv.lues.given\nby ttle,'ari.,ionallowerbour.d,cun lhenbe c<>mpun:d fora r.ngeofdifferent'\"Tue'\n\"f;\\Iar.dItie'\"IuegivingIhtlargestmarginallikelihood\",Iecloo_\nl1erewe consider.simplerapproach introducooby b.asedon therddmu \"p-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 600,
      "page_label": "581"
    }
  },
  {
    "page_content": "582 12.CONTINUOUS LATENT VARIABLES\nFigure12.13 Probabilisticgraphicalmodel forBayesian peA in\nwhich thedistributionovertheparametermatrixW\nisgovernedby a vectora ofhyperparameters.\nw\nN\nproximation,which isappropriatewhen thenumber ofdatapointsisrelativelylarge\nand the correspondingposteriordistributionistightlypeaked (Bishop,1999a). It\ninvolvesa specificchoiceof priorover W thatallowssurplusdimensionsin the\nprincipalsubspacetobe pruned outofthemodel. Thiscorrespondstoan example of\nautomaticrelevancedetermination,orARD, discussedinSection7.2.2.Specifically,\nwe definean independentGaussianpriorovereach column ofW, which represent\nthevectorsdefiningtheprincipalsubspace.Each such Gaussianhas an independent\nvariancegovernedby a precisionhyperparameterO:i so that\n(12.60)\nSection7.2\nwhere Wi istheith column ofW. The resultingmodel can be representedusingthe\ndirectedgraph shown inFigure12.13.\nThe valuesforO:i willbe found iterativelyby maximizing themarginallikeli­\nhood functioninwhich W has been integratedout.As a resultofthisoptimization,\nsome of the O:i may be drivento infinity,with the correspondingparametersvec­\ntorWi being drivento zero (theposteriordistributionbecomes a deltafunctionat\nthe origin)givinga sparsesolution.The effectivedimensionalityof theprincipal\nsubspaceisthendeterminedby thenumber offiniteO:i values,and thecorrespond­\ning vectorsWi can be thoughtof as 'relevant'formodellingthedatadistribution.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 601,
      "page_label": "582"
    }
  },
  {
    "page_content": "ing vectorsWi can be thoughtof as 'relevant'formodellingthedatadistribution.\nIn thisway, the Bayesian approach isautomaticallymaking thetrade-offbetween\nimprovingthefittothedata,by usinga largernumber ofvectorsWi withtheircor­\nrespondingeigenvaluesAi each tuned to the data,and reducingthecomplexityof\nthemodel by suppressingsome oftheWi vectors.The originsof thissparsitywere\ndiscussedearlierinthecontextofrelevancevectormachines.\nThe valuesofO:i arere-estimatedduringtrainingby maximizing thelogmarginal\nlikelihoodgivenby\np(Xla,J-L,0'2)= Jp(XIW, J-L,O'2)p(Wla)dW (12.61)\nwhere thelogofp(XIW, J-L,0'2)isgivenby (12.43).Note thatforsimplicitywe also\ntreatJ-L and 0'2as parametersto be estimated,ratherthandefiningpriorsoverthese\nparameters.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 601,
      "page_label": "582"
    }
  },
  {
    "page_content": "Section4.4\nSection3.5.3\n12.2.ProbabilisticpeA 583\nBecause thisintegrationisintractable,we make use oftheLaplaceapproxima­\ntion.Ifwe assume thattheposteriordistributionissharplypeaked,as willoccurfor\nsufficientlylargedatasets,thenthere-estimationequationsobtainedby maximizing\nthemarginallikelihoodwithrespecttoaitakethesimpleform\n(12.62)\nwhich followsfrom (3.98),notingthatthedimensionalityof Wi isD. These re­\nestimationsareinterleavedwith theEM algorithmupdatesfordeterminingWand\na 2 • The E-stepequationsareagaingivenby (12.54)and (12.55).Similarly,theM­\nstepequationfora 2 isagaingivenby (12.57).The only change isto theM-step\nequationforW, which ismodifiedtogive\n(12.63)\nwhere A = diag(ai)'The valueofI-\"isgivenby thesample mean, as before.\nIfwe choose M = D - 1 then,ifallaivaluesarefinite,themodel represents\na full-covarianceGaussian,whileifalltheaigo toinfinitythemodel isequivalent\ntoan isotropicGaussian,and so themodel can encompass allpennissiblevaluesfor\ntheeffectivedimensionalityoftheprincipalsubspace.Itisalsopossibletoconsider\nsmallervaluesof M, which willsaveon computationalcostbut which willlimit\nthemaximum dimensionalityof thesubspace.A comparison of theresultsof this\nalgorithmwithstandardprobabilisticPCA isshown inFigure12.14.\nBayesianPCA providesan opportunityto illustratetheGibbs samplingalgo­\nrithmdiscussedin Section11.3. Figure12.15 shows an example of the samples\nfrom thehyperparametersInaifora datasetinD = 4 dimensionsinwhich thedi­",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 602,
      "page_label": "583"
    }
  },
  {
    "page_content": "rithmdiscussedin Section11.3. Figure12.15 shows an example of the samples\nfrom thehyperparametersInaifora datasetinD = 4 dimensionsinwhich thedi­\nmensionalityofthelatentspaceisM = 3 butinwhich thedatasetisgeneratedfrom\na probabilisticPCA model havingone directionofhighvariance,withtheremaining\ndirectionscomprisinglow variancenoise.Thisresultshows clearlythepresenceof\nthreedistinctmodes intheposteriordistribution.At eachstepoftheiteration,one of\nthehyperparametershas a smallvalueand theremainingtwo have largevalues,so\nthattwo ofthethreelatentvariablesaresuppressed.During thecourseoftheGibbs\nsampling,thesolutionmakes sharptransitionsbetween thethreemodes.\nThe model describedhere involvesa prioronly over the matrixW. A fully\nBayesiantreatmentof PCA, includingpriorsover 1-\", a 2 , and n, and solvedus­\ning variationalmethods,isdescribedin Bishop (1999b).For a discussionof vari­\nous Bayesianapproachestodetenniningtheappropriatedimensionalityfora PCA\nmodel,seeMinka (2001c).\n12.2.4 Factoranalysis\nFactoranalysisisa linear-Gaussianlatentvariablemodel thatiscloselyrelated\ntoprobabilisticPCA. Itsdefinitiondiffersfrom thatofprobabilisticPCA onlyinthat\ntheconditionaldistributionoftheobservedvariablex giventhelatentvariablez is",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 602,
      "page_label": "583"
    }
  },
  {
    "page_content": "584 12.CONTINUOUS LATENT VARIABLES\n• •\n•• • • •\n• • • • ••••• •• •• • •\n• •••\n• • • •\n••• •\n••\n•• ••\n•\n• •• ••• ••\n• • ••• •\nFigure12.14 'Hinloo'diagramsofthe matrixW inwhich each element01 the matrixisdepictedas\na square (whitelorpositiveand blacklornegativevalues)whose area isproportional\nto the magnitude of thatelement. The syntheticdata selcomprises 300 data pointsin\nD = 10 dimensions sampled from a Gaussian distributionhavingstandarddeviation1.0\nin3 directionsand standarddeviation0.5intheremaining7 directionsfora datasetin\nD = 10 dimensionshaving AT = 3 directionswithlargervariancethan the remaining7\ndirections.The left-handplolshows theresultIrom maximum likelihoodprobabilisticPCA,\nand the left·handplotshows the correspondingresuftfrom Bayesian peA. We see how\nthe Bayesianmodel isabletodiscovertheappropriatedimensionalityby suppressingthe\n6 surplusdegrees offreedom.\ntaken tohave a diagonalratherthan an isotropiccovarianceso that\np(xlz)= N(xlWz + 1'.\\II) (12.64)\nwhere illisa D x D diagonalmatrix.Note thatthefactoranalysismodel, incommon\nwith probabilisticPCA. assumes thattheobserved variablesXl, ...,Xo areindepen­\ndent.given the latentvariablez. In essence.the factoranalysismodel isexplaining\ntheobserved covariancestructureof the data by representingthe independentvari­\nance associatedwith each coordinatein the matrix 1J.'and capturingthecovariance\nbetween variablesin the matrix W. In the factoranalysisliterature.the columns",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 603,
      "page_label": "584"
    }
  },
  {
    "page_content": "between variablesin the matrix W. In the factoranalysisliterature.the columns\nof W. which capturethecorrelationsbetween observed variables.arecalledfaclOr\nloadings.and the diagonal elements of 1J.'.which representthe independent noise\nvariancesforeach ofthevariables,arecalledllniqllenesses.\nThe originsof factoranalysisare as old as those of PCA. and discussionsof\nfactoranalysiscan be found in the books by Everitt(1984). Bartholomew (1987),\nand Basilevsky(1994). Links between factoranalysisand PCA were investigated\nby Lilwley (1953) and Anderson (1963) who showed thatat stationarypointsof\nthe likelihoodfunction.fora faclOranalysismodel with 1J.'= (121, thecolumns of\nW are scaledeigenvectorsof the sample covariancematrix.and (12 isthe average\nof the discardedeigenvalues.Later.Tipping and Bishop (1999b) showed thatthe\nmaximum of the log likelihoodfunctionoccurs when the eigenvectorscomprising\nWare chosen tobe theprincipaleigenvectors.\nMaking use of (2.115).we see thatthe marginal distributionfor theobserved",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 603,
      "page_label": "584"
    }
  },
  {
    "page_content": "12.2.l'ru\":ohilislkI'CA 585\nFllIure12.15Gillbs.,,,,>p!j\"lllo<Bay<lslan\nPCA sh<Ming plots oj Ino,\nversus ~eralion number br\nthree \" values. showing\ntr\"\"\"tions betw..... tbe\nth\"'\"moOts <A !he posterior\ndistribution.\n,-\"riabl,i'gi,-,nby 1'(x) ~ N(Xlj',C) whe ...now\nC=WWT+'i'. (12,6~)\nEurr\"e 12,19\nS\"na\" 12.4\nAs withprobabilisticPC A, thi,moMI isim-\"ri.rrlto'Olalionsin11><0 latent'pace.\nHistoocally,factoranal)',;shas been lhe ,ubjerlof cOl1tro,-ersywroe\" a!tempt<\nh\",-ebttn\"'a<k:toplacean intc'P\"'t\"liooon theind;vidualfaclon(thecOOfdinates\nin z_space).which h3.\\pm\"en problematicdue to lr.e\"\"\"i<lcmifiabililyof factOf\nanalysisassocimedwith Mation' in this'pace.From oor perspeoh-e,howe,-er.we\nshall.iew factoranalysisas a form of lalent\"ariabledensilymodel. in which the\nform of tl>clalent'pace i'of interestbut nO! the particularchoiccof coordinates\nused todescrit>cil.Ifwe wish toremove thedegeneracya'sociatedwithlatent'pace\nroIations.\"\"emu,t con'idernon-Gaussianlatent,-\"riabledi'tribution,.gi\"irrgrise10\nindependentcomponent .n.lysi,(lCA) models.\nWe can detenni\"etheparametersI'.\\V. \"nd ....inthefac!Ofan.ly,i,model by\nmuimum likelihood.11..solutionforI'i'ag\"ingivenby the\",,,,pie\"'ean.How·\neyC'.\"nli~e probabili,ticl'CA.lllcrei'no longera closed-formmaximum likelihood\nsolutionfor\\V.\",'hichmu.\\ltherdorcbe found i'er.li,'c1)'_Because facloranal)',i.is\na latentvariablemodeL thi'can be don. usingan EM algorilhm(R.binand Thayer.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 604,
      "page_label": "585"
    }
  },
  {
    "page_content": "a latentvariablemodeL thi'can be don. usingan EM algorilhm(R.binand Thayer.\n1982)!h\"tis\"nalogou,totheone used(Of pml>;lbili.tiePeA. Specihcally.lheE-'lep\neqnJtioo,areg;'-enby\nwhere \"\"eh\",'edefi\"\"d\nE[zoJ = GW T\",-'(xn - xl\nE[z\"z~J _ G + E[zo]E[z,,]T\n(12,66)\n(1267)\n(l2,6H)\nNOie th\"tthi'i'e.pre,<edina for'\"thaiin,-oh'esinycrsi\"nofmalrices\"fSilO,\\I x,If\nrathe'lhanD x D (ex\"\",,,,fortbeD x D diagooalmatrixoJ'\"'hosein,-ersei.''ri\"ial",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 604,
      "page_label": "585"
    }
  },
  {
    "page_content": "586 12.CONTINUOUS LATENT VARIABLES\nExercise 12.22\ntocompute inO(D) steps),which isconvenientbecauseoftenM « D. Similarly,\ntheM-step equationstaketheform\nw new\n[~(x\" -XllllIZn]\"][~Ill[Znz~I]-'\ndiag {s-W.'w ~ ~1ll[Zn](Xn _ xl\"}\n(12.69)\n(12.70)\nwhere the'diag'operatorsetsallofthenondiagonalelementsofa matrixtozero.A\nBayesiantreatmentofthefactoranalysismodel can be obtainedby a straightforward\napplicationofthetechniquesdiscussedinthisbook.\nAnotherdifferencebetween probabilisticPCA and factoranalysisconcernstheir\nExercise 12.25 differentbehaviourunder transformationsof thedataset.For PCA and probabilis­\nticPCA, ifwe rotatethecoordinatesystem in dataspace,thenwe obtainexactly\nthe same fitto the databut with the W matrixtransformedby thecorresponding\nrotationmatrix.However, forfactoranalysis,theanalogouspropertyisthatifwe\nmake a component-wise re-scalingof thedatavectors,thenthisisabsorbedintoa\ncorrespondingre-scalingoftheelementsof \\)i.\n12.3.KernelpeA\nIn Chapter6,we saw how thetechniqueof kernelsubstitutionallowsus totakean\nalgorithm expressedin terms of scalarproductsof the form xT x' and generalize\nthatalgorithmby replacingthe scalarproductswith a nonlinearkernel.Here we\napplythistechniqueof kernelsubstitutiontoprincipalcomponent analysis,thereby\nobtaininga nonlineargeneralizationcalledkernelpeA (Scholkopfetal.,1998).\nConsidera dataset{xn } of observations,where n = 1,...,N, in a spaceof\ndimensionalityD. In orderto keep thenotationuncluttered,we shallassume that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 605,
      "page_label": "586"
    }
  },
  {
    "page_content": "Considera dataset{xn } of observations,where n = 1,...,N, in a spaceof\ndimensionalityD. In orderto keep thenotationuncluttered,we shallassume that\nwe have alreadysubtractedthe sample mean from each of thevectorsX n , so that\nLn X n= O. The firststepisto expressconventionalPCA in such a form thatthe\ndatavectors{xn } appearonlyintheform ofthescalar productsx~X m . Recallthat\ntheprincipalcomponents aredefinedby theeigenvectorsUi ofthecovariancematrix\nSUi = AiUi (12.71)\nwhere i= 1,...,D. Here theD x D sample covariancematrixS isdefinedby\n(12.72)\nand theeigenvectorsarenormalizedsuch thatuT Ui = 1.\nNow considera nonlineartransformation¢(x)intoan M -dimensionalfeature\nspace,so thateach datapointX n istherebyprojectedonto a point¢(xn ). We can",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 605,
      "page_label": "586"
    }
  },
  {
    "page_content": "12.3.K~mci l'Co'. 587\n...'\\\n(12.73)\nFigu'.12.16 SctIematic_,.lion01 kernelPeA. A <UtllHI In lheOflglnal<Uta space l~'_ plot}..\nPfOlEledby'\"\"\"'*-tranllklfmalion~,,} 1nIo.fa.tur.space (fIght_plot).By I*b'~ PCA inthe\n!Hue 111**. we oblaO'IlhapmeiIlaI~\"llS. \"'who<:tllhatntIe.........inblUe.,..,..oJano4edby lha\n_ v, Tha gr-.....InIMIuN apam indicMaIha _ plOlKlio\".onIO lheIirslpoiridl*'\"\"\".........11.\n\"\"\"'*'_od 110 \"\"'*-poOf&Cllu.in....<lfisIonaIOillll111**. Hole IIuIIIngMefM' lanolpox '..110\n'......1ha,...iIio_poi........00i'........by._1n \"apam.\n__ ptrform)l-.bnlPeA ill fnlllKlopICe...-Iudl,mpIiclIly«lIi'Il'S• -'_\nprinClpaI ....-..model ill onpnllcbuo ~as,1lU>tr*dinFllIft12-16.\nFu \"IO..It..lei'\"lOS!oUlllt1Nl Illt~diu.~ lObo halnromean,\njI)!halL.4>(\"'.J..O. We dWl rctlll'1l10 Itl,~ pol'\".Ihonly.1llt.1f\" .If\"'\"\"\"*\nCO\\-.ullCC mMfU ,n(~.If*'e,~ l\"\"by\n,\nC - .~. L <>(x.j4>(X.)T\n..,\nand ,l~ \",,,,n'\"MOl\"opan,ioni'«linedby\nCv, = A,v, (12,74)\n; = 1...,.M. Our goalis10 soh'\"lhiseigen\"lIlueproblem WilhoUl ha\"inlllOwork\n.\"plici,lyin,hef.lIture'pace.From !hedefinilionof C. lhe.ill\"\"\"\"\"l\"'\"equalions\nlell'U, thaIY, !-ali,fies\n.\"s L <bC\".){<b(x.lT v,}- )\"Y,\n..,\n(12.7~)\n..........-\".-lN1 (proo.idcdA, > 0) tilt\"CC'lorv,isli''nby • Ii_rombllla\"on\nofIlltd>( J.....JO <;.-he \"-\"llmiIllhc(orm\n,\nv, '\"'L 11,.4>(\".).\n...\n(12.76)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 606,
      "page_label": "587"
    }
  },
  {
    "page_content": "588 12.CONTINUOUS LATENT VARIABLES\nSubstitutingthisexpansionback intotheeigenvectorequation,we obtain\n1 N N N\nN L ¢(xn)¢(xn)TL aim¢(X m ) = Ai L ain¢(Xn ),\nn=l m=l n=l\n(12.77)\nThe key stepisnow toexpressthisintermsofthekernelfunctionk(xn ,xm ) =\n¢(Xn)T ¢(xm ),which we do by multiplyingbothsidesby ¢(xZ)T togive\n1 N m N\nN Lk(XI'Xn ) L aimk(Xn,xm ) = Ai Laink(XI'Xn),\nn=l m=l n=l\nThiscan be writteninmatrixnotationas\n(12.78)\n(12.79)\nwhere ai isan N-dimensionalcolumn vectorwithelementsani forn = 1,...,N.\nWe can findsolutionsforaiby solvingthefollowingeigenvalueproblem\n(12.80)\nExercise12.26\nin which we have removed a factorof K from both sidesof (12.79).Note that\nthesolutionsof (12.79)and (12.80)differonly by eigenvectorsof K havingzero\neigenvaluesthatdo notaffecttheprincipalcomponents projection.\nThe normalizationconditionforthecoefficientsai isobtainedby requiringthat\ntheeigenvectorsinfeaturespacebe normalized.Using (12.76)and (12.80),we have\nN N\n1 = V;Vi = L L ainaim¢(xn)T¢(xm ) = a;K~ = AiNa;ai' (12.81)\nn=l m=l\nHaving solvedtheeigenvectorproblem,theresultingprincipalcomponent pro­\njectionscan thenalsobe castintermsof thekernelfunctionso that,using(12.76),\ntheprojectionofa pointx ontoeigenvectoriisgivenby\nN N\nYi(X) = ¢(x)TVi = L ain¢(x)T¢(xn) = L aink(X,x n) (12.82)\nn=l n=l\nand so againisexpressedintermsofthekernelfunction.\nIn theoriginalD-dimensionalx spacethereareD orthogonaleigenvectorsand\nhence we can findatmost D linearprincipalcomponents. The dimensionalityM",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 607,
      "page_label": "588"
    }
  },
  {
    "page_content": "In theoriginalD-dimensionalx spacethereareD orthogonaleigenvectorsand\nhence we can findatmost D linearprincipalcomponents. The dimensionalityM\nof thefeaturespace,however,can be much largerthan D (eveninfinite),and thus\nwe can finda number of nonlinearprincipalcomponents thatcan exceed D. Note,\nhowever,thatthenumber of nonzero eigenvaluescannotexceed thenumber N of\ndatapoints,because(evenifM > N) thecovariancematrixin featurespacehas\nrank atmost equalto N. This isreflectedinthefactthatkernelPCA involvesthe\neigenvectorexpansionoftheN x N matrixK.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 607,
      "page_label": "588"
    }
  },
  {
    "page_content": "12.3.KernelPCA 589\nSo farwe have assumed thatthe projecteddata setgiven by ¢(xn ) has zero\nmean, which in generalwillnot be thecase.We cannot simply compute and then\nsubtractoffthemean, sincewe wish toavoidworking directlyinfeaturespace,and\nso again,we formulatethe algorithmpurelyin-!ermsof the kernelfunction.The\nprojecteddatapointsaftercentralizing,denoted¢(xn ),aregivenby\nand thecorrespondingelementsoftheGram matrixaregivenby\nK nm = ¢(xn)T¢(xm )\n1 N\n¢(xn)T¢(xm ) - N L ¢(xn)T¢(xZ)\nZ=l\n1 N 1 N N\n- N L¢(XZ)T¢(Xm ) + N2 LL¢(Xj)T¢(xZ)\nZ=l j=l Z=l\n1 N\nk(xn,x m ) - N L k(xz,x m )\nZ=l\n1 N 1 N N\n- N Lk(xn,xz)+ N2 LLk(Xj,Xl)'\nZ=l j=l 1=1\nThiscan be expressedinmatrixnotationas\n(12.83)\n(12.84)\n(12.85)\nExercise12.27\nwhere IN denotestheN x N matrixinwhich everyelementtakesthevaluel/N.\n~ ~\nThus we can evaluateK usingonlythekernelfunctionand thenuse K todetermine\ntheeigenvaluesand eigenvectors.Note thatthestandardPCA algorithmisrecovered\nas a specialcaseifwe use a linearkernelk(x,x')= xTx/.Figure12.17shows an\nexample ofkernelPCA appliedtoa syntheticdataset(Scholkopfetal.,1998).Here\na 'Gaussian'kerneloftheform\nk(x,x')= exp(-llx- x/112 /0.1) (12.86)\nisappliedtoa syntheticdataset.The linescorrespondtocontoursalong which the\nprojectionontothecorrespondingprincipalcomponent, definedby\nisconstant.\nN\n¢(X?Vi = L aink(X,xn )\nn=l\n(12.87)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 608,
      "page_label": "589"
    }
  },
  {
    "page_content": "590 12.CONTINUOUS LATENT VAlUABLES\n_.\n(12.88)\nFigure12.11 E\"llmple01 kernelPCA, withaGaussiankernelawIiOO10 asynthetic<latasatintwo <:Iirnensions,\nshowing !he firslflighteigenfunclionsalong w~h l!>eire9tnvailNls.The oootoursam linesalong which !he\nprojoc1iononlot\"\"COffaspMding principalcomponen1ls co<>stam,Nola haw Ihefirsltwo ~....,..rat.\n!he th\"'\"dusters.!he \"\"'\"Ill\"'\"~spIiI\"\"'*'oIlheeluste,intohaMoS. and t\"\"lolIowingIhree\n~again spI~!he duste,\"intohalvesalong directionsorthogonal10 thoprEMouS splils,\nOne obvioo'dls.aJmota~e ofI:emel!'CA Isthafifinvoh'esfindinglheelgen\"e<­\ntorsof the N x N malri>:K raW. Ihan lhe D x D malri,S ofcor,..emionallinear\n!'CA. and!iOIn prac1lceforlargedata\"'1'appro,lmation<areoftenuS(:d\nFinally.\"\"eOOIe thati\"<tandardlinearI'CA,we oftenretainsome redoce<lnum·\nber L < Dof eigenvectorsand then appro,lmale0 datavttl<:>rXn b}'itsprojection\ni~ 0,,1\"lhe L-dimensional principalsubspace,definedby\n,\ni~-L:«\",)\"\"\nI\" kernell'CA.thiswillin gencr~1 not be flO'slble,To see thl',OOIe Ihatthe map­\nping 4'(x)maps the D-dimensional x space i\"t\"0 D-dimensioo.l manijQiIIin lhe\nM-dimemioo.l femure space <1>. TlIe:.'ectorx i'koown a< lhef'\",.imagrof lhe\nc\",\"\"\"pondingpoi\"l4'(x). However, fhe projec1iooof poinl>in feature<J'3C\"\"\"to\nthe linearrcA ,ub,p\"\"\"in that'pace willtypically\"'''lieOn fhe nonlinearD­\ndimensional manifold and !iOwillnul ha.,.a c\"\"\"\",pondlngp\",.lmo~e indOlO spa<.\"C,\nTechnlque< ho.-elhereforebttnproposedforfindingapproximale pre-image<iB\"\"lr\nNat..2(04).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 609,
      "page_label": "590"
    }
  },
  {
    "page_content": "12.4.Nonlinear Latent VariableModels 591\n12.4. NonlinearLatentVariableModels\nExercise12.28\nIn thischapter,we have focussedon thesimplestclassofmodels havingcontinuous\nlatentvariables,namely thosebased on linear-Gaussiandistributions.As well as\nhaving greatpracticalimportance,thesemodels arerelativelyeasy to analyseand\nto fitto dataand can alsobe used as components in more complex models. Here\nwe considerbrieflysome generalizationsofthisframework tomodels thatareeither\nnonlinearor non-Gaussian,or both.\nIn fact,the issuesof nonlinearityand non-Gaussianityare relatedbecause a\ngeneralprobabilitydensitycan be obtainedfrom a simplefixedreferencedensity,\nsuchas a Gaussian,by making a nonlinearchange of variables.Thisideaforms the\nbasisofseveralpracticallatentvariablemodels as we shallseeshortly.\n12.4.1 Independentcomponent analysis\nWe begin by consideringmodels in which the observedvariablesare related\nlinearlytothelatentvariables,butforwhich thelatentdistributionisnon-Gaussian.\nAn importantclassof such models,known as independentcomponent analysis,or\nleA,ariseswhen we considera distributionoverthelatentvariablesthatfactorizes,\nso that\nM\np(z)= IIp(Zj).\nj=l\n(12.89)\nTo understandthe roleof such models, considera situationin which two people\naretalkingatthe same time,and we recordtheirvoices usingtwo microphones.\nIfwe ignoreeffectssuch as time delay and echoes,then the signalsreceivedby\nthemicrophones atany pointin time willbe givenby linearcombinationsof the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 610,
      "page_label": "591"
    }
  },
  {
    "page_content": "Ifwe ignoreeffectssuch as time delay and echoes,then the signalsreceivedby\nthemicrophones atany pointin time willbe givenby linearcombinationsof the\namplitudesof the two voices.The coefficientsof thislinearcombinationwillbe\nconstant,and ifwe can infertheirvaluesfrom sample data,thenwe can invertthe\nmixing process(assuming itisnonsingular)and therebyobtaintwo cleansignals\neachofwhich containsthevoiceofjustone person.Thisisan example ofa problem\ncalledblindsourceseparationin which 'blind'referstothefactthatwe aregiven\nonlythemixed data,and neithertheoriginalsourcesnor themixing coefficientsare\nobserved(Cardoso,1998).\nThis type of problem is sometimes addressedusing the followingapproach\n(MacKay, 2003) inwhich we ignorethetemporalnatureofthesignalsand treatthe\nsuccessivesamplesas i.i.d.We considera generativemodel inwhich therearetwo\nlatentvariablescorrespondingtotheunobservedspeechsignalamplitudes,and there\naretwo observedvariablesgivenby thesignalvaluesatthemicrophones.The latent\nvariableshave ajointdistributionthatfactorizesasabove,and theobservedvariables\naregivenby a linearcombinationofthelatentvariables.There isno need toinclude\na noisedistributionbecausethenumber oflatentvariablesequalsthenumber ofob­\nservedvariables,and thereforethemarginaldistributionof theobservedvariables\nwillnot in generalbe singular,so the observedvariablesare simply deterministic\nlinearcombinationsof the latentvariables.Given a data setof observations,the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 610,
      "page_label": "591"
    }
  },
  {
    "page_content": "592 12.CONTINUOUS LATENT VARIABLES\nExercise12.29\nlikelihoodfunctionforthismodel isa functionof thecoefficientsinthelinearcom­\nbination.The log likelihoodcan be maximized using gradient-basedoptimization\ngivingrisetoa particularversionofindependentcomponent analysis.\nThe successofthisapproachrequiresthatthelatentvariableshave non-Gaussian\ndistributions.To seethis,recallthatinprobabilisticPCA (andinfactoranalysis)the\nlatent-spacedistributionis given by a zero-mean isotropicGaussian. The model\nthereforecannot distinguishbetween two differentchoicesforthe latentvariables\nwhere thesediffersimply by a rotationin latentspace.This can be verifieddirectly\nby notingthatthe marginaldensity(12.35),and hence the likelihoodfunction,is\nunchanged ifwe make the transformationW -) WR where R isan orthogonal\nmatrixsatisfyingRR T = I,becausethematrixC givenby (12.36)isitselfinvariant.\nExtending the model to allow more generalGaussian latentdistributionsdoes not\nchange thisconclusionbecause,as we have seen,such a model isequivalenttothe\nzero-mean isotropicGaussian latentvariablemodel.\nAnother way toseewhy a Gaussian latentvariabledistributionina linearmodel\nisinsufficientto findindependentcomponents isto note thattheprincipalcompo­\nnentsrepresenta rotationofthecoordinatesystem indataspacesuch astodiagonal­\nizethecovariancematrix,so thatthedatadistributioninthenew coordinatesisthen\nuncorrelated.Although zero correlationisa necessaryconditionforindependence",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 611,
      "page_label": "592"
    }
  },
  {
    "page_content": "uncorrelated.Although zero correlationisa necessaryconditionforindependence\nitisnot,however, sufficient.In practice,a common choiceforthe latent-variable\ndistributionisgivenby\n1p(z)= --,.-----,-\nJ 7fcosh(zj)\n1 (12.90)\nwhich has heavy tailscompared toa Gaussian,reflectingtheobservationthatmany\nreal-worlddistributionsalsoexhibitthisproperty.\nThe originalICA model (Belland Sejnowski,1995) was based on theoptimiza­\ntionof an objectivefunctiondefinedby informationmaximization.One advantage\nof a probabilisticlatentvariableformulationisthatithelpsto motivateand formu­\nlategeneralizationsof basicICA. For instance,independentfactoranalysis(Attias,\n1999a) considersa model inwhich thenumber oflatentand observedvariablescan\ndiffer,theobservedvariablesarenoisy,and theindividuallatentvariableshave flex­\nibledistributionsmodelled by mixturesof Gaussians. The log likelihoodforthis\nmodel ismaximized usingEM, and thereconstructionof thelatentvariablesisap­\nproximated using a variationalapproach. Many othertypes of model have been\nconsidered,and thereisnow a huge literatureon ICA and itsapplications(Jutten\nand Herault,1991; Comon etat.,1991; Amari etat.,1996; Pearlmutterand Parra,\n1997; Hyvarinen and Oja, 1997; Hinton etat.,2001; Miskin and MacKay, 2001;\nHojen-Sorensen etat.,2002; Choudrey and Roberts,2003; Chan etat.,2003; Stone,\n2004).\n12.4.2 Autoassociativeneural networks\nIn Chapter 5 we consideredneuralnetworks in thecontextofsupervisedlearn­",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 611,
      "page_label": "592"
    }
  },
  {
    "page_content": "2004).\n12.4.2 Autoassociativeneural networks\nIn Chapter 5 we consideredneuralnetworks in thecontextofsupervisedlearn­\ning,where the roleof the network isto predictthe outputvariablesgiven values",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 611,
      "page_label": "592"
    }
  },
  {
    "page_content": "12.4.NonlinearLatentVariableModels 593\nFigure12.18 An autoassociativemUltilayerperceptronhaving\ntwolayersofweights.Such a networkistrainedto\nmap inputvectorsontothemselvesby minimiza­\ntionot a sum-ot-squareserror.Even withnon­\nlinearunitsin the hiddenlayer,such a network\nisequivalentto linearprincipalcomponent anal­\nysis. Linksrepresentingbiasparametershave\nbeen omittedforclarity.\ninputs outputs\n(12.91)\nforthe inputvariables.However, neuralnetworks have alsobeen appliedto un­\nsupervisedlearningwhere theyhave been used fordimensionalityreduction.This\nisachievedby usinga network having thesame number of outputsas inputs,and\noptimizingtheweightsso as tominimize some measure of thereconstructionerror\nbetween inputsand outputswithrespecttoa setoftrainingdata.\nConsiderfirsta multilayerperceptronof theform shown inFigure12.18,hav­\ning D inputs,D outputunitsand M hidden units,with M < D. The targetsused\nto trainthenetwork are simply the inputvectorsthemselves,so thatthe network\nisattemptingto map each inputvectoronto itself.Such a network issaidtoform\nan autoassociativemapping. Since thenumber of hidden unitsissmallerthanthe\nnumber ofinputs,a perfectreconstructionofallinputvectorsisnotingeneralpos­\nsible.We thereforedeterminethe network parametersw by minimizingan error\nfunctionwhich capturesthedegreeof mismatch between theinputvectorsand their\nreconstructions.In particular,we shallchoose a sum-of-squareserroroftheform\n1 N\nE(w) = \"2 L Ily(xn ,w) - xn 11\n2\n•\nn=l",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 612,
      "page_label": "593"
    }
  },
  {
    "page_content": "reconstructions.In particular,we shallchoose a sum-of-squareserroroftheform\n1 N\nE(w) = \"2 L Ily(xn ,w) - xn 11\n2\n•\nn=l\nIfthehiddenunitshave linearactivationsfunctions, thenitcan be shown thatthe\nerrorfunctionhas a uniqueglobalminimum, and thatatthisminimum thenetwork\nperformsa projectionontotheM -dimensionalsubspacewhich isspannedby thefirst\nM principalcomponents of thedata(Bourlardand Kamp, 1988;Baldiand Hornik,\n1989).Thus,thevectorsofweightswhich leadintothehiddenunitsinFigure12.18\nform a basissetwhich spanstheprincipalsubspace.Note,however,thatthesevec­\ntorsneed not be orthogonalor normalized.This resultisunsurprising,sinceboth\nprincipalcomponent analysisand theneuralnetwork areusinglineardimensionality\nreductionand areminimizingthesame sum-of-squareserrorfunction.\nItmightbe thoughtthatthelimitationsofa lineardimensionalityreductioncould\nbe overcome by usingnonlinear(sigmoidal)activationfunctionsforthehiddenunits\ninthenetworkinFigure12.18.However, even withnonlinearhiddenunits,themin­\nimum errorsolutionisagaingivenby theprojectiononto theprincipalcomponent\nsubspace(Bourlardand Kamp, 1988).There isthereforeno advantageinusingtwo­\nlayerneuralnetworkstoperformdimensionalityreduction.Standardtechniquesfor\nprincipalcomponent analysis(basedon singularvaluedecomposition)areguaran­\nteedtogivethecorrectsolutioninfinitetime,and theyalsogeneratean orderedset\nofeigenvalueswithcorrespondingorthonormaleigenvectors.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 612,
      "page_label": "593"
    }
  },
  {
    "page_content": "594 12.CONTINUOUS LATENT VARIABLES\nFigure12.19 Additionof extrahidden lay­\nersof noolinearunitsgivesan\nauloassocialivenetwork which\ncan performa noolineardimen­\nsiooalityreduction.\ninputs\nx,\nF,\n•\nF,\n•\noutputs\nx,\nThe situationisdifferent,however.ifadditionalhidden layersarepcrmillcdin\nthenetwork.Considerthefour-layerautoassociativcnetwork shown inFigure12.19.\nAgain theoutputunitsarelinear,and theM unitsinthesecond hidden layercan also\nbe linear.however,thefirstand thirdhidden layershave sigmoidalnonlinearactiva­\ntionfunctions.The networkisagaintrainedby minimizationoftheerrorfunction\n(12.91).We can view thisnetwork as two successivefunctionalmappings F] and\nF 2 , as indicatedin Figure 12.19. The firstmapping F] projectstheoriginalD­\ndimensionaldataonto an AI-dimensionalsubspaceS definedby theactivationsof\ntheunitsinthesecond hiddenlayer.Becauseofthepresenceofthefirsthiddenlayer\nofnonlinearunits.thismapping isverygeneral.and inparticularisnotrestrictedto\nbeinglinear.Similarly.thesecond halfofthenetworkdefinesan arbitraryfunctional\nmapping from theM -dimensionalspaceback intotheoriginalD-dimensionalinput\nspace.This hasa simplegeometricalinterpretation.as indicatedforthecaseD = 3\nand M = 2 inFigure12.20.\nSuch a networkeffectivelyperfonnsa nonlinearprincipalcomponent analysis.\nX3\n\"F,\n•\nx, \"\nFigure12.20 Geometricalinterpretationofthemappings performedby thenetworkinFigure12.1g forthecase",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 613,
      "page_label": "594"
    }
  },
  {
    "page_content": "X3\n\"F,\n•\nx, \"\nFigure12.20 Geometricalinterpretationofthemappings performedby thenetworkinFigure12.1g forthecase\nof 0 = 3 inputsand AI = 2 unitsinthe middlehidden layer.The functionF, maps from an M-dimensional\nspace S intoa D-dimensiooalspace and thereforedefinestheway inwhich thespace S isembedded withinthe\noriginalx-space.Sincethe mapping F, can be r\"I()(llinear,theembedding 01 S can be nonplanar,as indicated\ninthefigure.The mapping F. then definesa projectiorlofpointsintheoriginalD-dimensionalspace intothe\nM -dimensionalsubspace S.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 613,
      "page_label": "594"
    }
  },
  {
    "page_content": "12.4.NonlinearLatentVariableModels 595\nIthas theadvantageof notbeinglimitedtolineartransformations, althoughitcon­\ntainsstandardprincipalcomponent analysisas a specialcase. However, training\nthenetworknow involvesa nonlinearoptimizationproblem,sincetheerrorfunction\n(12.91)isno longera quadraticfunctionof thenetworkparameters.Computation­\nallyintensivenonlinearoptimizationtechniquesmust be used,and thereistheriskof\nfindinga suboptimallocalminimum oftheerrorfunction.Also,thedimensionality\nofthesubspacemust be specifiedbeforetrainingthenetwork.\n12.4.3 Modellingnonlinearmanifolds\nAs we have alreadynoted,many naturalsourcesof datacorrespondto low­\ndimensional, possiblynoisy,nonlinearmanifoldsembedded withinthehigherdi­\nmensionalobserveddataspace.Capturingthispropertyexplicitlycan leadtoim­\nproveddensitymodellingcompared withmore generalmethods. Here we consider\nbrieflya rangeoftechniquesthatattempttodo this.\nOne way to model thenonlinearstructureisthrougha combinationof linear\nmodels,sothatwe make a piece-wiselinearapproximationtothemanifold.Thiscan\nbe obtained,forinstance,by usinga clusteringtechniquesuchas K -means basedon\nEuclideandistancetopartitionthedatasetintolocalgroupswithstandardPCA ap­\npliedtoeach group.A betterapproachistouse thereconstructionerrorforcluster\nassignment(Kambhatlaand Leen, 1997;Hintonetal.,1997)asthen acommon cost\nfunctionisbeingoptimizedin each stage.However, theseapproachesstillsuffer",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 614,
      "page_label": "595"
    }
  },
  {
    "page_content": "assignment(Kambhatlaand Leen, 1997;Hintonetal.,1997)asthen acommon cost\nfunctionisbeingoptimizedin each stage.However, theseapproachesstillsuffer\nfrom limitationsdue to theabsenceof an overalldensitymodel. By usingprob­\nabilisticPCA itisstraightforwardto definea fullyprobabilisticmodel simply by\nconsideringa mixturedistributionin which thecomponents areprobabilisticPCA\nmodels (Tippingand Bishop,1999a).Such a model has both discretelatentvari­\nables,correspondingtothediscretemixture,as wellas continuouslatentvariables,\nand the likelihoodfunctioncan be maximized usingthe EM algorithm.A fully\nBayesiantreatment,basedon variationalinference(Bishopand Winn, 2000),allows\nthenumber ofcomponents inthemixture,as wellas theeffectivedimensionalities\nof theindividualmodels,tobe inferredfrom thedata.There aremany variantsof\nthismodel inwhich parameterssuchas theW matrixorthenoisevariancesaretied\nacrosscomponents in themixture,or in which theisotropicnoisedistributionsare\nreplacedby diagonalones,givingrisetoa mixtureoffactoranalysers(Ghahramani\nand Hinton,1996a;Ghahramani and Beal,2000).The mixtureofprobabilisticPCA\nmodels can alsobe extendedhierarchicallytoproducean interactivedatavisualiza­\ntionalgorithm(Bishopand Tipping,1998).\nAn alternativetoconsideringa mixtureoflinearmodels istoconsidera single\nnonlinearmodel. RecallthatconventionalPCA findsa linearsubspacethatpasses\ncloseto thedatain a least-squaressense. This conceptcan be extendedto one­",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 614,
      "page_label": "595"
    }
  },
  {
    "page_content": "nonlinearmodel. RecallthatconventionalPCA findsa linearsubspacethatpasses\ncloseto thedatain a least-squaressense. This conceptcan be extendedto one­\ndimensionalnonlinearsurfacesintheform ofprincipalcurves(Hastieand Stuetzle,\n1989).We candescribea curveina D-dimensionaldataspaceusinga vector-valued\nfunctionf().),which isa vectoreachofwhose elementsisa functionofthescalar)..\nThere aremany possibleways toparameterizethecurve,of which a naturalchoice\nisthearclengthalongthecurve.For any givenpointxindataspace,we can find\nthepointon thecurvethatisclosestinEuclideandistance.We denotethispointby",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 614,
      "page_label": "595"
    }
  },
  {
    "page_content": "596 12.CONTINUOUS LATENT VARIABLES\n>.. = gf(X)because itdepends on theparticularcurve f(>\").For a continuousdata\ndensityp(x),a principalcurveisdefinedas one forwhich everypointon thecurve\nisthemean ofallthosepointsindataspacethatprojecttoit,so that\nJE [Xlgf(X)= >..]= f(>\"). (12.92)\nFor a givencontinuousdensity,therecan be many principalcurves.Inpractice,we\nare interestedin finitedata sets,and we alsowish to restrictattentionto smooth\ncurves.Hastieand Stuetzle(1989)proposea two-stageiterativeprocedureforfind­\ningsuch principalcurves,somewhat reminiscentoftheEM algorithmforPCA. The\ncurveisinitializedusingthefirstprincipalcomponent, and thenthealgorithmalter­\nnatesbetween a dataprojectionstepand curvere-estimationstep.In theprojection\nstep,each datapointisassignedtoa valueof >.. correspondingto theclosestpoint\non the curve. Then in the re-estimationstep,each pointon the curveisgivenby\na weighted averageof thosepointsthatprojectto nearbypointson thecurve,with\npointscloseston thecurvegiventhegreatestweight.Inthecasewhere thesubspace\nisconstrainedtobe linear,theprocedureconvergestothefirstprincipalcomponent\nand isequivalenttothepower method forfindingthelargesteigenvectorof theco­\nvariancematrix.Principalcurvescan be generalizedtomultidimensionalmanifolds\ncalledprincipalsurfacesalthoughthesehave found limiteduse due tothedifficulty\nofdatasmoothing inhigherdimensionseven fortwo-dimensionalmanifolds.\nPCA isoftenused toprojecta datasetonto a lower-dimensionalspace,forex­",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 615,
      "page_label": "596"
    }
  },
  {
    "page_content": "ofdatasmoothing inhigherdimensionseven fortwo-dimensionalmanifolds.\nPCA isoftenused toprojecta datasetonto a lower-dimensionalspace,forex­\nample two dimensional,forthepurposesof visualization.Another lineartechnique\nwitha similaraim ismultidimensionalscaling,orMDS (Cox and Cox, 2000).Itfinds\na low-dimensionalprojectionofthedatasuch as topreserve,as closelyas possible,\nthepairwisedistancesbetween datapoints,and involvesfindingtheeigenvectorsof\nthedistancematrix.Inthecasewhere thedistancesareEuclidean,itgivesequivalent\nresultsto PCA. The MDS conceptcan be extendedto a wide varietyof datatypes\nspecifiedintermsofa similaritymatrix,givingnonmetricMDS.\nTwo othernonprobabilisticmethods fordimensionalityreductionand datavi­\nsualizationareworthy ofmention. Locallylinearembedding,or LLE (Roweis and\nSaul,2000) firstcomputes the setof coefficientsthatbestreconstructseach data\npointfrom itsneighbours.These coefficientsare arrangedto be invariantto rota­\ntions,translations,and scalingsofthatdatapointand itsneighbours,and hence they\ncharacterizethelocalgeometricalpropertiesof theneighbourhood.LLE thenmaps\nthehigh-dimensionaldatapointsdown toa lowerdimensionalspacewhilepreserv­\ning theseneighbourhood coefficients.Ifthe localneighbourhood fora particular\ndatapointcan be consideredlinear,thenthetransformationcan be achievedusing\na combinationof translation,rotation,and scaling,such as to preservetheangles\nformed between thedatapointsand theirneighbours.Because theweightsarein­",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 615,
      "page_label": "596"
    }
  },
  {
    "page_content": "formed between thedatapointsand theirneighbours.Because theweightsarein­\nvarianttothesetransformations,we expectthesame weightvaluestoreconstructthe\ndatapointsin thelow-dimensionalspaceas in thehigh-dimensionaldataspace.In\nspiteof thenonlinearity,theoptimizationforLLE does notexhibitlocalminima.\nIn isometricfeaturemapping, or isomap (Tenenbaum etai.,2000),thegoalis\ntoprojectthedatatoa lower-dimensionalspaceusingMDS, butwhere thedissim­\nilaritiesare definedin terms of the geodesicdistancesmeasured along the mani-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 615,
      "page_label": "596"
    }
  },
  {
    "page_content": "Chapter5\nChapter JJ\n12.4.NonlinearLatentVariableModels 597\nfold.For instance,iftwo pointslieon a circle,thenthegeodesicisthearc-length\ndistancemeasured around thecircumferenceof thecirclenot thestraightlinedis­\ntancemeasured along thechord connectingthem. The algorithmfirstdefinesthe\nneighbourhoodforeachdatapoint,eitherby findingtheK nearestneighboursorby\nfindingallpointswithina sphereof radiusE. A graph isthenconstructedby link­\ning allneighbouringpointsand labellingthem with theirEuclideandistance.The\ngeodesicdistancebetween any pairof pointsisthenapproximatedby the sum of\nthearclengthsalongtheshortestpathconnectingthem (which itselfisfound using\nstandardalgorithms).Finally,metricMDS isappliedtothegeodesicdistancematrix\ntofindthelow-dimensionalprojection.\nOur focus in thischapterhas been on models for which the observedvari­\nablesarecontinuous.We can alsoconsidermodels havingcontinuouslatentvari­\nablestogetherwith discreteobservedvariables,givingriseto latenttraitmodels\n(Bartholomew,1987). In thiscase,themarginalizationover thecontinuouslatent\nvariables,even fora linearrelationshipbetween latentand observedvariables, can­\nnot be performedanalytically,and so more sophisticatedtechniquesarerequired.\nTipping(1999)usesvariationalinferenceina model witha two-dimensionallatent\nspace,allowinga binarydatasettobe visualizedanalogouslytotheuseofPCA to\nvisualizecontinuousdata.Note thatthismodel isthedualof theBayesianlogistic",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 616,
      "page_label": "597"
    }
  },
  {
    "page_content": "space,allowinga binarydatasettobe visualizedanalogouslytotheuseofPCA to\nvisualizecontinuousdata.Note thatthismodel isthedualof theBayesianlogistic\nregressionproblem discussedin Section4.5.In thecaseof logisticregressionwe\nhave N observationsof thefeaturevector<l>n which areparameterizedby a single\nparametervectorw, whereasinthelatentspacevisualizationmodel thereisa single\nlatentspacevariablex (analogousto<1» and N copiesof thelatentvariableW n . A\ngeneralizationof probabilisticlatentvariablemodels togeneralexponentialfamily\ndistributionsisdescribedinCollinsetal.(2002).\nWe have alreadynotedthatan arbitrarydistributioncan be formed by takinga\nGaussianrandom variableand transformingitthrougha suitablenonlinearity.This\nisexploitedin a generallatentvariablemodel calleda densitynetwork (MacKay,\n1995;MacKay and Gibbs,1999) in which thenonlinearfunctionisgoverned by a\nmultilayeredneuralnetwork.Ifthenetworkhasenough hiddenunits,itcan approx­\nimatea givennonlinearfunctiontoany desiredaccuracy.The downside of having\nsucha flexiblemodel isthatthemarginalizationoverthelatentvariables,requiredin\nordertoobtainthelikelihoodfunction,isno longeranalyticallytractable.Instead,\nthelikelihoodisapproximatedusingMonte Carlotechniquesby drawing samples\nfrom theGaussianprior.The marginalizationoverthelatentvariablesthenbecomes\na simplesum with one term foreach sample. However, because a largenumber\nof sample pointsmay be requiredinordertogivean accuraterepresentationof the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 616,
      "page_label": "597"
    }
  },
  {
    "page_content": "a simplesum with one term foreach sample. However, because a largenumber\nof sample pointsmay be requiredinordertogivean accuraterepresentationof the\nmarginal,thisprocedurecan be computationallycostly.\nIfwe considermore restrictedformsforthenonlinearfunction,and make an ap­\npropriatechoiceofthelatentvariabledistribution,thenwe canconstructa latentvari­\nablemodel thatisbothnonlinearand efficienttotrain.The generativetopographic\nmapping, or GTM (BishopetaI.,1996;Bishop etaI.,1997a;Bishop etaI.,1998b)\nusesa latentdistributionthatisdefinedby a finiteregulargridofdeltafunctionsover\nthe(typicallytwo-dimensional)latentspace.Marginalizationoverthelatentspace\nthensimplyinvolvessumming overthecontributionsfrom eachofthegridlocations.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 616,
      "page_label": "597"
    }
  },
  {
    "page_content": "598 12.CONTINUOUS LATENT VA K1AHU':S\n.•.'\"\nFllIu.e12.21 ?lotottrleoillkYw<:lataWllisualiz.edusingPeA on theleftand GTM on Itlengr,tFOf tileGTM\nmodel. each <latapoinIlsplollfldattilemean otitsposM'k><dislributionin..tents;>ace,Tile\"\",\"ineantymlhe\nGTM 1TlOd8I._lhasepamlionbetwoon thegroupsofdata pointstobe .....n \"\"\".ckl.arfy,\nCh\"l'l~f j\nS~etioo /.4\nThe no\"liotarmapping isgi,'enby a linearregressionmodel thaIallow,forgeneral\nIIO/llinearilywhilebeinga linearfuoctionof tileadapli'-eparameler<,NOIe thaItilt\nusuallimitationof linearregressionmodels arisingfrom theen\"\",ofdimen,iooalily\ndoes 1101 ariseintheContr~1 oflheGT~I si\"\"'ethe\"\\3nifoldgenerall)'ha< t,,'odi\"ltn·\nsionsirrespecti'-eof thedimensionalityof the dataspace,A coo\"\"!\",,nceof Illese\n11\"0 cooicesisthatthelikelihoodfunclioncan be e~pressed analyticallyin dosed\nform and can be optimilC<.!efficientlyo,ingtheEM algorithm_The resoltingGTM\nmodel hIsa lwo-dimensionalnonlinearmanifold10 tiledalasel.and by e\"alualing\ntheposteriordistrilJ\",lion(Wer latentspaceforthedatapoi\"\",theycan he projectt<J\nback tothe lalent'JI'K'\"for.'isualilalionpurposes,Figure12,21sl\"\"\"sa comparison\noftheoildata..,1\"isualiredwilhlincarPeA and wilhlheIIO/lhnearGT~I,\nTIltGTM can beseen asa probabilistic\"'rsionofan earliernlOd<lcallM the'''If\norg\"nidng\"\"'p.or SOM (Kohonen. 1982:Kobonen. (995).which alsorepresents\na Iwo-dimensiooalIIO/llincarmanifoid as a regulararrayof disc\"'lepoints.The\nSOM i'somewhat remin;\"\"'ntof the K·trlCan,algorithmin thatdatapointsare",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 617,
      "page_label": "598"
    }
  },
  {
    "page_content": "a Iwo-dimensiooalIIO/llincarmanifoid as a regulararrayof disc\"'lepoints.The\nSOM i'somewhat remin;\"\"'ntof the K·trlCan,algorithmin thatdatapointsare\na.,igr.edto nearby ProlOl)'j>C'-eclonthaIarelhen subsc<juenllyupdale<!.Initially.\nlheproIOI)'jl('Saredistribuledat random, and duringthetrainingprocessthey'selr\norganize'so as toaPl'ro~imale a smoolh manifold. UnlikeK -mean'.how'e\"e..the\nSOM isTIOIoptimizingany well.ddine<!costfunction(Erwin ..al..1992)making\"\ndifficulttos.\"theparametersof themodel and 10 assesscon'-ergence.There i'also\nno guaranteethatthe '\",If-<>rganilalion'willtakeplace..thisisdepen\"\"nl00 the\nchoiceof appropriateparanlttcr\"aloC'f,,,any particulardatasel.\nBy OOfItrast,GTM optimize,theloglikelihoodfunctioo,and theresoltingmodel\ndefine'a probabililyden,ityindma ,pace, In faeL ilcorre,pondsto a con,m,incd\nmi,tureof Gaussian,in which thecomponent.',h.rea COnlnlOn \".riance.•nd the\nmean, are con'trainedto lieon a 'mooIh tw-o-diITlCn,iooaln1anifold.This proba-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 617,
      "page_label": "598"
    }
  },
  {
    "page_content": "Exercises 599\nbilisticfoundationalsomakes itvery straightforwardto definegeneralizationsof\nGTM (Bishopetal.,1998a)such asa Bayesiantreatment,dealingwithmissingval-\nSection6.4 ues,a principledextensionto discretevariables,theuse of Gaussian processesto\ndefinethemanifold,ora hierarchicalGTM model (Tinoand Nabney, 2002).\nBecause themanifoldinGTM isdefinedas a continuoussurface,notjustatthe\nprototypevectorsasintheSOM, itispossibletocompute themagnificationfactors\ncorrespondingtothelocalexpansionsand compressionsof themanifoldneeded to\nfitthedataset(Bishopet al.,1997b) as well as thedirectionalcurvaturesof the\nmanifold(Tinoetal.,2001).These can be visualizedalongwith theprojecteddata\nand provideadditionalinsightintothemodel.\nExercises\n12.1 (**) lIBIn thisexercise,we use proofby inductionto show thatthelinear\nprojectionontoan M -dimensionalsubspacethatmaximizes thevarianceofthepro­\njecteddataisdefinedby theM eigenvectorsof thedatacovariancematrixS, given\nby (12.3),correspondingto theM largesteigenvalues.In Section12.1,thisresult\nwas provenforthecaseof M = 1. Now supposetheresultholdsforsome general\nvalueof M and show thatitconsequentlyholdsfordimensionalityM + 1. To do\nthis,firstsetthederivativeof the varianceof theprojecteddatawith respectto a\nvectorUM+1 definingthe new directionin dataspace equal to zero. This should\nbe done subjectto theconstraintsthatUM +l be orthogonalto theexistingvectors\nU1,\"\" UM, and alsothat itbe normalizedto unitlength.Use Lagrange multipli-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 618,
      "page_label": "599"
    }
  },
  {
    "page_content": "U1,\"\" UM, and alsothat itbe normalizedto unitlength.Use Lagrange multipli-\nAppendix E erstoenforcetheseconstraints.Then make use of theorthonormalitypropertiesof\nthevectorsU1,\"\" UM to show thatthenew vectorUM+1 isan eigenvectorof S.\nFinally,show thatthevarianceismaximized iftheeigenvectorischosen to be the\none correspondingtoeigenvectorAM+1 where theeigenvalueshave been orderedin\ndecreasingvalue.\n12.2 (**) Show thatthe minimum valueof the PCA distortionmeasure J given by\n(12.15)with respectto the Ui, subjectto theorthonormalityconstraints(12.7),is\nobtainedwhen theUi areeigenvectorsof thedatacovariancematrixS. To do this,\nintroducea matrixH of Lagrange multipliers,one foreach constraint,so thatthe\nmodifieddistortionmeasure,inmatrixnotationreads\n] = Tr { UTSU }+ Tr { H(I - UTU) } (12.93)\nwhere U isa m~trix of dimensio~D x (D - M) whose columns aregi:::..enb~Ui.\nNow minimize J withrespecttoU and show thatthes~ution satisfiesSU = UH.\nClearly,one possiblesolutionisthatthecolumns of U are eigenvectorsof S, in\nwhich case H isa diagonalmatrixcontainingthecorrespondingeigenvalues.To\nobtainthegeneralsolution,show thatH can be assumed tobe a symmetr~ ma~ix,\nand by usingitseigenvect£rexpansionshow thatthegeneralsolutiontoSU =~UH\ngivesthesame valueforJ as thespecificsolutionin which thecolumns of U are",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 618,
      "page_label": "599"
    }
  },
  {
    "page_content": "600 12.CONTINUOUS LATENT VARIABLES\ntheeigenvectorsof S. Because thesesolutionsareallequivalent,itisconvenientto\nchoose theeigenvectorsolution.\n12.3 (*) Verifythattheeigenvectorsdefinedby (12.30)arenormalizedto unitlength,\nassuming thattheeigenvectorsVi have unitlength.\n12.4 (*)Imm Suppose we replacethezero-mean,unit-covariancelatentspacedistri­\nbution(12.31)intheprobabilisticPCA model by a generalGaussiandistributionof\ntheformN(zlm, ~). By redefiningtheparametersofthemodel,show thatthisleads\ntoan identicalmodel forthemarginaldistributionp(x)overtheobservedvariables\nforany validchoiceofm and ~.\n12.5 (**) Let x be a D-dimensional random variablehaving a Gaussian distribution\ngiven by N(xIJL,~), and considerthe M-dimensional random variablegivenby\ny = Ax + b where A isan M x D matrix. Show thaty alsohas a Gaussian\ndistribution,and findexpressionsforitsmean and covariance.Discusstheform of\nthisGaussiandistributionforM < D, forM = D, and forM > D.\n12.6 (*)Imm Draw a directedprobabilisticgraph fortheprobabilisticPCA model\ndescribedin Section12.2in which thecomponents of theobservedvariablex are\nshown explicitlyas separatenodes. Hence verifythattheprobabilisticPCA model\nhas the same independence structureas the naiveBayes model discussedin Sec­\ntion8.2.2.\n12.7 (**) By making use oftheresults(2.270)and (2.271)forthemean and covariance\nofa generaldistribution,derivetheresult(12.35)forthemarginaldistributionp(x)\nintheprobabilisticPCA model.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 619,
      "page_label": "600"
    }
  },
  {
    "page_content": "ofa generaldistribution,derivetheresult(12.35)forthemarginaldistributionp(x)\nintheprobabilisticPCA model.\n12.8 (**)Imm By making useoftheresult(2.116),show thattheposteriordistribution\np(zlx)fortheprobabilisticPCA model isgivenby (12.42).\n12.9 (*) Verifythatmaximizing the log likelihood(12.43)forthe probabilisticPCA\nmodel with respectto the parameterJL givesthe resultJLML = x where x isthe\nmean ofthedatavectors.\n12.10 (**) By evaluatingthesecond derivativesoftheloglikelihoodfunction(12.43)for\ntheprobabilisticPCA model withrespecttotheparameterJL,show thatthestationary\npointJLML = x representstheuniquemaximum.\n12.11 (**)Imm Show thatinthelimit(Y2 -.0,theposteriormean fortheprobabilistic\nPCA model becomes an orthogonalprojectiononto the principalsubspace,as in\nconventionalPCA.\n12.12 (**) For (Y2 > 0 show thattheposteriormean in theprobabilisticPCA model is\nshiftedtowardstheoriginrelativetotheorthogonalprojection.\n12.13 (**) Show thattheoptimalreconstructionofa datapointunderprobabilisticPCA,\naccordingtotheleastsquaresprojectioncostofconventionalPCA, isgivenby\n(12.94)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 619,
      "page_label": "600"
    }
  },
  {
    "page_content": "Exercises 601\n12.14 (*) The number ofindependentparametersinthecovariancematrixfortheproba­\nbilisticPCA model with an M -dimensionallatentspaceand a D-dimensionaldata\nspaceisgivenby (12.51).Verifythatin thecase of M = D - 1, thenumber of\nindependentparametersisthesame asina generalcovarianceGaussian,whereas for\nM = °itisthesame as fora Gaussianwithan isotropiccovariance.\n12.15 (**)IIiI!IDerivetheM-step equations(12.56)and (12.57)fortheprobabilistic\nPCA model by maximizationoftheexpectedcomplete-dataloglikelihoodfunction\ngivenby (12.53).\n12.16 (** *) InFigure12.11,we showed an applicationofprobabilisticPCA toa dataset\ninwhich some ofthedatavalues weremissingatrandom. DerivetheEM algorithm\nformaximizing thelikelihoodfunctionfortheprobabilisticPCA model inthissitu­\nation.Note thatthe{zn},as wellas themissingdatavaluesthatarecomponents of\nthevectors{xn },arenow latentvariables.Show thatinthespecialcaseinwhich all\nof thedatavaluesareobserved,thisreducestotheEM algorithmforprobabilistic\nPCA derivedinSection12.2.2.\n12.17 (**)IIiI!ILet W be a D x M matrixwhose columns definea linearsubspace\nof dimensionalityM embedded withina dataspaceofdimensionalityD, and letJ1\nbe a D-dimensionalvector.Given a dataset{xn } where n = 1,...,N, we can\napproximatethedatapointsusinga linearmapping from a setof M -dimensional\nvectors{zn},so thatXn isapproximatedby W Zn + J1. The associatedsum-of­\nsquaresreconstructioncostisgivenby\nN\nJ = L Ilxn - J1- Wz n 11 2 .\nn=l\n(12.95)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 620,
      "page_label": "601"
    }
  },
  {
    "page_content": "vectors{zn},so thatXn isapproximatedby W Zn + J1. The associatedsum-of­\nsquaresreconstructioncostisgivenby\nN\nJ = L Ilxn - J1- Wz n 11 2 .\nn=l\n(12.95)\nFirstshow thatminimizingJ withrespecttoJ1leadstoan analogousexpressionwith\nX n and Zn replacedby zero-meanvariablesX n - x and Zn - Z, respectively,where x\nand Z denotesample means. Then show thatminimizingJ withrespecttoZn,where\nW iskeptfixed,givesriseto thePCA Estep (12.58),and thatminimizing J with\nrespecttoW, where {zn} iskeptfixed,givesrisetothePCA M step(12.59).\n12.18 (*) Derivean expressionforthenumber of independentparametersin thefactor\nanalysismodel describedinSection12.2.4.\n12.19 (**) IIiI!IShow thatthe factoranalysismodel describedin Section12.2.4is\ninvariantunderrotationsofthelatentspacecoordinates.\n12.20 (**) By consideringsecond derivatives,show thatthe only stationarypointof\ntheloglikelihoodfunctionforthefactoranalysismodel discussedinSection12.2.4\nwith respectto the parameterJ1 isgiven by the sample mean definedby (12.1).\nFurthermore,show thatthisstationarypointisa maximum.\n12.21 (**) Derivetheformulae(12.66)and (12.67)fortheE stepof theEM algorithm\nforfactoranalysis.Note thatfrom theresultofExercise 12.20,theparameterJ1 can\nbe replacedby thesample mean x.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 620,
      "page_label": "601"
    }
  },
  {
    "page_content": "602 12.CONTINUOUS LATENT VARIABLES\n12.22 (**) Writedown an expressionfortheexpectedcomplete-dataloglikelihoodfunc­\ntionforthefactoranalysismodel, and hence derivethecorrespondingM stepequa­\ntions(12.69)and (12.70).\n12.23 (*)III!IDraw a directedprobabilisticgraphicalmodel representinga discrete\nmixtureof probabilisticPCA models inwhich each PCA model has itsown values\nof W, JL, and 0-2 • Now draw a modifiedgraphinwhich theseparametervaluesare\nsharedbetween thecomponents ofthemixture.\n12.24 (***) We saw in Section2.3.7thatStudent'st-distributioncan be viewed as an\ninfinitemixtureof Gaussiansin which we marginalizewith respectto a continu­\nous latentvariable.By exploitingthisrepresentation,formulatean EM algorithm\nformaximizing theloglikelihoodfunctionfora multivariateStudent'st-distribution\ngivenan observedsetofdatapoints,and derivetheforms oftheE and M stepequa­\ntions.\n12.25 (**)III!IConsidera linear-Gaussianlatent-variablemodel havinga latentspace\ndistributionp(z)= N(xIO, I)and a conditionaldistributionfortheobservedvari­\nable p(xlz)= N(xlWz + IL,<p) where <P isan arbitrarysymmetric,positive­\ndefinitenoisecovariancematrix.Now supposethatwe make a nonsingularlinear\ntransformationof the datavariablesx ---t Ax, where A isa D x D matrix.If\nJLML' W ML and <PML representthemaximum likelihoodsolutioncorrespondingto\ntheoriginaluntransformeddata,show thatAJLML' AW ML, and A <PMLA T willrep­\nresentthecorrespondingmaximum likelihoodsolutionforthetransformeddataset.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 621,
      "page_label": "602"
    }
  },
  {
    "page_content": "theoriginaluntransformeddata,show thatAJLML' AW ML, and A <PMLA T willrep­\nresentthecorrespondingmaximum likelihoodsolutionforthetransformeddataset.\nFinally,show thattheform ofthemodel ispreservedintwo cases:(i)A isa diagonal\nmatrixand <P isa diagonalmatrix.This correspondstothecaseoffactoranalysis.\nThe transformed<P remainsdiagonal,and hence factoranalysisiscovariantunder\ncomponent-wisere-scalingof thedatavariables;(ii)A isorthogonaland <P ispro­\nportionaltotheunitmatrixso that<P = 0-21. ThiscorrespondstoprobabilisticPCA.\nThe transformed<P matrixremainsproportionaltotheunitmatrix,and henceproba­\nbilisticPCA iscovariantundera rotationoftheaxesofdataspace,asisthecasefor\nconventionalPCA.\n\\\n12.26 (**) Show thatany vectorai thatsatisfies(12.80)willalsosatisfy(12.79).Also,\nshow thatforany solutionof (12.80)havingeigenvalueA, we can add any multiple\nof an eigenvectorof K having zero eigenvalue,and obtaina solutionto (12.79)\nthatalsohas eigenvalueA. Finally,show thatsuch modificationsdo notaffectthe\nprincipal-componentprojectiongivenby (12.82).\n12.27 (**) Show thattheconventionallinearPCA algorithmisrecoveredasa specialcase\nofkernelPCA ifwe choosethelinearkernelfunctiongivenby k(x,x')= xT x'.\n12.28 (**) III!IUse thetransformationproperty(1.27)ofa probabilitydensityunder\na change of variableto show thatany densityp(y)can be obtainedfrom a fixed\ndensityq(x)thatiseverywherenonzero by making a nonlinearchange of variable",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 621,
      "page_label": "602"
    }
  },
  {
    "page_content": "a change of variableto show thatany densityp(y)can be obtainedfrom a fixed\ndensityq(x)thatiseverywherenonzero by making a nonlinearchange of variable\ny = f(x)in which f(x)isa monotonic functionso that0 :::;j'(x)< 00. Write\ndown thedifferentialequationsatisfiedby f(x)and draw a diagramillustratingthe\ntransformationofthedensity.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 621,
      "page_label": "602"
    }
  },
  {
    "page_content": "Exercises 603\n12.29 (**)Em Suppose thattwo variablesZl and Z2 areindependentsothatp(zl'Z2) =\nP(Zl)P(Z2)'Show thatthecovariancematrixbetween thesevariablesisdiagonal.\nThis shows thatindependenceisa sufficientconditionfortwo variablesto be un­\ncorrelated.Now considertwo variablesYl and Y2 in which -1 :0;; Yl :0;; 1 and\nY2 = yg. Write down theconditionaldistributionp(Y2IYl)and observethatthisis\ndependenton Yb showing thatthetwo variablesare not independent.Now show\nthatthecovariancematrixbetween thesetwo variablesisagaindiagonal.To do this,\nuse therelationP(Yl,Y2) = P(YI)p(Y2IYl)to show thattheoff-diagonalterms are\nzero.Thiscounter-exampleshows thatzerocorrelationisnota sufficientcondition\nforindependence.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 622,
      "page_label": "603"
    }
  },
  {
    "page_content": "13\nSequential\nData\nSo far in this book, we have focussed primarily on sets of data points that were as-\nsumed to be independent and identically distributed (i.i.d.). This assumption allowed\nus to express the likelihood function as the product over all data points of the prob-\nability distribution evaluated at each data point. For many applications, however,\nthe i.i.d. assumption will be a poor one. Here we consider a particularly important\nclass of such data sets, namely those that describe sequential data. These often arise\nthrough measurement of time series, for example the rainfall measurements on suc-\ncessive days at a particular location, or the daily values of a currency exchange rate,\nor the acoustic features at successive time frames used for speech recognition. An\nexample involving speech data is shown in Figure 13.1. Sequential data can also\narise in contexts other than time series, for example the sequence of nucleotide base\npairs along a strand of DNA or the sequence of characters in an English sentence.\nFor convenience, we shall sometimes refer to ‘past’ and ‘future’ observations in a\nsequence. However, the models explored in this chapter are equally applicable to all\n605",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 624,
      "page_label": "605"
    }
  },
  {
    "page_content": "606 13. SEQUENTIAL DATA\nFigure 13.1 Example of a spectro-\ngram of the spoken words “Bayes’ theo-\nrem” showing a plot of the intensity of the\nspectral coefﬁcients versus time index.\nforms of sequential data, not just temporal sequences.\nIt is useful to distinguish between stationary and nonstationary sequential dis-\ntributions. In the stationary case, the data evolves in time, but the distribution from\nwhich it is generated remains the same. For the more complex nonstationary situa-\ntion, the generative distribution itself is evolving with time. Here we shall focus on\nthe stationary case.\nFor many applications, such as ﬁnancial forecasting, we wish to be able to pre-\ndict the next value in a time series given observations of the previous values. In-\ntuitively, we expect that recent observations are likely to be more informative than\nmore historical observations in predicting future values. The example in Figure 13.1\nshows that successive observations of the speech spectrum are indeed highly cor-\nrelated. Furthermore, it would be impractical to consider a general dependence of\nfuture observations on all previous observations because the complexity of such a\nmodel would grow without limit as the number of observations increases. This leads\nus to consider Markov models in which we assume that future predictions are inde-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 625,
      "page_label": "606"
    }
  },
  {
    "page_content": "13.1. Markov Models 607\nFigure 13.2 The simplest approach to\nmodelling a sequence of ob-\nservations is to treat them\nas independent, correspond-\ning to a graph without links.\nx1 x2 x3 x4\npendent of all but the most recent observations.\nAlthough such models are tractable, they are also severely limited. We can ob-\ntain a more general framework, while still retaining tractability, by the introduction\nof latent variables, leading to state space models. As in Chapters 9 and 12, we shall\nsee that complex models can thereby be constructed from simpler components (in\nparticular, from distributions belonging to the exponential family) and can be read-\nily characterized using the framework of probabilistic graphical models. Here we\nfocus on the two most important examples of state space models, namely the hid-\nden Markov model, in which the latent variables are discrete, and linear dynamical\nsystems, in which the latent variables are Gaussian. Both models are described by di-\nrected graphs having a tree structure (no loops) for which inference can be performed\nefﬁciently using the sum-product algorithm.\n13.1. Markov Models\nThe easiest way to treat sequential data would be simply to ignore the sequential\naspects and treat the observations as i.i.d., corresponding to the graph in Figure 13.2.\nSuch an approach, however, would fail to exploit the sequential patterns in the data,\nsuch as correlations between observations that are close in the sequence. Suppose,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 626,
      "page_label": "607"
    }
  },
  {
    "page_content": "such as correlations between observations that are close in the sequence. Suppose,\nfor instance, that we observe a binary variable denoting whether on a particular day\nit rained or not. Given a time series of recent observations of this variable, we wish\nto predict whether it will rain on the next day. If we treat the data as i.i.d., then the\nonly information we can glean from the data is the relative frequency of rainy days.\nHowever, we know in practice that the weather often exhibits trends that may last for\nseveral days. Observing whether or not it rains today is therefore of signiﬁcant help\nin predicting if it will rain tomorrow.\nTo express such effects in a probabilistic model, we need to relax the i.i.d. as-\nsumption, and one of the simplest ways to do this is to consider a Markov model.\nFirst of all we note that, without loss of generality, we can use the product rule to\nexpress the joint distribution for a sequence of observations in the form\np(x1,..., xN )=\nN∏\nn=1\np(xn|x1,..., xn−1). (13.1)\nIf we now assume that each of the conditional distributions on the right-hand side\nis independent of all previous observations except the most recent, we obtain the\nﬁrst-order Markov chain, which is depicted as a graphical model in Figure 13.3. The",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 626,
      "page_label": "607"
    }
  },
  {
    "page_content": "608 13. SEQUENTIAL DATA\nFigure 13.3 A ﬁrst-order Markov chain of ob-\nservations {xn} in which the dis-\ntribution p(xn|xn−1) of a particu-\nlar observation xn is conditioned\non the value of the previous ob-\nservation xn−1.\nx1 x2 x3 x4\njoint distribution for a sequence of N observations under this model is given by\np(x1,..., xN )= p(x1)\nN∏\nn=2\np(xn|xn−1). (13.2)\nFrom the d-separation property, we see that the conditional distribution for observa-Section 8.2\ntion xn, given all of the observations up to time n,i sg i v e nb y\np(xn|x1,..., xn−1)= p(xn|xn−1) (13.3)\nwhich is easily veriﬁed by direct evaluation starting from (13.2) and using the prod-\nuct rule of probability. Thus if we use such a model to predict the next observationExercise 13.1\nin a sequence, the distribution of predictions will depend only on the value of the im-\nmediately preceding observation and will be independent of all earlier observations.\nIn most applications of such models, the conditional distributions p(xn|xn−1)\nthat deﬁne the model will be constrained to be equal, corresponding to the assump-\ntion of a stationary time series. The model is then known as a homogeneous Markov\nchain. For instance, if the conditional distributions depend on adjustable parameters\n(whose values might be inferred from a set of training data), then all of the condi-\ntional distributions in the chain will share the same values of those parameters.\nAlthough this is more general than the independence model, it is still very re-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 627,
      "page_label": "608"
    }
  },
  {
    "page_content": "Although this is more general than the independence model, it is still very re-\nstrictive. For many sequential observations, we anticipate that the trends in the data\nover several successive observations will provide important information in predict-\ning the next value. One way to allow earlier observations to have an inﬂuence is to\nmove to higher-order Markov chains. If we allow the predictions to depend also on\nthe previous-but-one value, we obtain a second-order Markov chain, represented by\nthe graph in Figure 13.4. The joint distribution is now given by\np(x1,..., xN )= p(x1)p(x2|x1)\nN∏\nn=3\np(xn|xn−1, xn−2). (13.4)\nAgain, using d-separation or by direct evaluation, we see that the conditional distri-\nbution of xn given xn−1 and xn−2 is independent of all observations x1,... xn−3.\nFigure 13.4 A second-order Markov chain, in\nwhich the conditional distribution\nof a particular observation xn\ndepends on the values of the two\nprevious observations xn−1 and\nxn−2.\nx1 x2 x3 x4",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 627,
      "page_label": "608"
    }
  },
  {
    "page_content": "13.1. Markov Models 609\nFigure 13.5 We can represent sequen-\ntial data using a Markov chain of latent\nvariables, with each observation condi-\ntioned on the state of the corresponding\nlatent variable. This important graphical\nstructure forms the foundation both for the\nhidden Markov model and for linear dy-\nnamical systems.\nzn−1 zn zn+1\nxn−1 xn xn+1\nz1 z2\nx1 x2\nEach observation is now inﬂuenced by two previous observations. We can similarly\nconsider extensions to an Mth order Markov chain in which the conditional distri-\nbution for a particular variable depends on the previous M variables. However, we\nhave paid a price for this increased ﬂexibility because the number of parameters in\nthe model is now much larger. Suppose the observations are discrete variables hav-\ning K states. Then the conditional distribution p(xn|xn−1) in a ﬁrst-order Markov\nchain will be speciﬁed by a set ofK −1 parameters for each of the K states of xn−1\ngiving a total of K(K − 1) parameters. Now suppose we extend the model to an\nMth order Markov chain, so that the joint distribution is built up from conditionals\np(xn|xn−M ,..., xn−1). If the variables are discrete, and if the conditional distri-\nbutions are represented by general conditional probability tables, then the number\nof parameters in such a model will have KM−1(K − 1) parameters. Because this\ngrows exponentially with M, it will often render this approach impractical for larger\nvalues of M.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 628,
      "page_label": "609"
    }
  },
  {
    "page_content": "grows exponentially with M, it will often render this approach impractical for larger\nvalues of M.\nFor continuous variables, we can use linear-Gaussian conditional distributions\nin which each node has a Gaussian distribution whose mean is a linear function\nof its parents. This is known as an autoregressive or AR model (Box et al., 1994;\nThiesson et al., 2004). An alternative approach is to use a parametric model for\np(xn|xn−M ,..., xn−1) such as a neural network. This technique is sometimes\ncalled a tapped delay linebecause it corresponds to storing (delaying) the previous\nM values of the observed variable in order to predict the next value. The number\nof parameters can then be much smaller than in a completely general model (for ex-\nample it may grow linearly with M), although this is achieved at the expense of a\nrestricted family of conditional distributions.\nSuppose we wish to build a model for sequences that is not limited by the\nMarkov assumption to any order and yet that can be speciﬁed using a limited number\nof free parameters. We can achieve this by introducing additional latent variables to\npermit a rich class of models to be constructed out of simple components, as we did\nwith mixture distributions in Chapter 9 and with continuous latent variable models in\nChapter 12. For each observation xn, we introduce a corresponding latent variable\nzn (which may be of different type or dimensionality to the observed variable). We",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 628,
      "page_label": "609"
    }
  },
  {
    "page_content": "zn (which may be of different type or dimensionality to the observed variable). We\nnow assume that it is the latent variables that form a Markov chain, giving rise to the\ngraphical structure known as a state space model, which is shown in Figure 13.5. It\nsatisﬁes the key conditional independence property that zn−1 and zn+1 are indepen-\ndent given zn, so that\nzn+1 ⊥⊥ zn−1 | zn. (13.5)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 628,
      "page_label": "609"
    }
  },
  {
    "page_content": "610 13. SEQUENTIAL DATA\nThe joint distribution for this model is given by\np(x1,..., xN , z1,..., zN )= p(z1)\n[N∏\nn=2\np(zn|zn−1)\n] N∏\nn=1\np(xn|zn). (13.6)\nUsing the d-separation criterion, we see that there is always a path connecting any\ntwo observed variables xn and xm via the latent variables, and that this path is never\nblocked. Thus the predictive distribution p(xn+1|x1,..., xn) for observation xn+1\ngiven all previous observations does not exhibit any conditional independence prop-\nerties, and so our predictions for xn+1 depends on all previous observations. The\nobserved variables, however, do not satisfy the Markov property at any order. We\nshall discuss how to evaluate the predictive distribution in later sections of this chap-\nter.\nThere are two important models for sequential data that are described by this\ngraph. If the latent variables are discrete, then we obtain the hidden Markov model,\nor HMM (Elliott et al., 1995). Note that the observed variables in an HMM maySection 13.2\nbe discrete or continuous, and a variety of different conditional distributions can be\nused to model them. If both the latent and the observed variables are Gaussian (with\na linear-Gaussian dependence of the conditional distributions on their parents), then\nwe obtain the linear dynamical system.Section 13.3\n13.2. Hidden Markov Models\nThe hidden Markov model can be viewed as a speciﬁc instance of the state space\nmodel of Figure 13.5 in which the latent variables are discrete. However, if we",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 629,
      "page_label": "610"
    }
  },
  {
    "page_content": "model of Figure 13.5 in which the latent variables are discrete. However, if we\nexamine a single time slice of the model, we see that it corresponds to a mixture\ndistribution, with component densities given by p(x|z). It can therefore also be\ninterpreted as an extension of a mixture model in which the choice of mixture com-\nponent for each observation is not selected independently but depends on the choice\nof component for the previous observation. The HMM is widely used in speech\nrecognition (Jelinek, 1997; Rabiner and Juang, 1993), natural language modelling\n(Manning and Sch¨utze, 1999), on-line handwriting recognition (Nag et al., 1986),\nand for the analysis of biological sequences such as proteins and DNA (Kroghet al.,\n1994; Durbin et al., 1998; Baldi and Brunak, 2001).\nAs in the case of a standard mixture model, the latent variables are the discrete\nmultinomial variables zn describing which component of the mixture is responsible\nfor generating the corresponding observation xn. Again, it is convenient to use a\n1-of-K coding scheme, as used for mixture models in Chapter 9. We now allow the\nprobability distribution of zn to depend on the state of the previous latent variable\nzn−1 through a conditional distribution p(zn|zn−1). Because the latent variables are\nK-dimensional binary variables, this conditional distribution corresponds to a table\nof numbers that we denote by A, the elements of which are known as transition",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 629,
      "page_label": "610"
    }
  },
  {
    "page_content": "of numbers that we denote by A, the elements of which are known as transition\nprobabilities. They are given by Ajk ≡ p(znk =1 |zn−1,j =1 ), and because they\nare probabilities, they satisfy 0 ⩽ Ajk ⩽ 1 with ∑\nk Ajk =1 , so that the matrix A",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 629,
      "page_label": "610"
    }
  },
  {
    "page_content": "13.2. Hidden Markov Models 611\nFigure 13.6 Transition diagram showing a model whose la-\ntent variables have three possible states corre-\nsponding to the three boxes. The black lines\ndenote the elements of the transition matrix\nAjk. A12\nA23\nA31\nA21\nA32\nA13\nA11\nA22\nA33\nk =1\nk =2\nk =3\nhas K(K−1) independent parameters. We can then write the conditional distribution\nexplicitly in the form\np(zn|zn−1,A)=\nK∏\nk=1\nK∏\nj=1\nAzn−1,jznk\njk . (13.7)\nThe initial latent node z1 is special in that it does not have a parent node, and so\nit has a marginal distribution p(z1) represented by a vector of probabilities π with\nelements πk ≡ p(z1k =1 ), so that\np(z1|π)=\nK∏\nk=1\nπz1k\nk (13.8)\nwhere ∑\nk πk =1 .\nThe transition matrix is sometimes illustrated diagrammatically by drawing the\nstates as nodes in a state transition diagram as shown in Figure 13.6 for the case of\nK =3 . Note that this does not represent a probabilistic graphical model, because\nthe nodes are not separate variables but rather states of a single variable, and so we\nhave shown the states as boxes rather than circles.\nIt is sometimes useful to take a state transition diagram, of the kind shown in\nFigure 13.6, and unfold it over time. This gives an alternative representation of the\ntransitions between latent states, known as a lattice or trellis diagram, and which isSection 8.4.5\nshown for the case of the hidden Markov model in Figure 13.7.\nThe speciﬁcation of the probabilistic model is completed by deﬁning the con-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 630,
      "page_label": "611"
    }
  },
  {
    "page_content": "shown for the case of the hidden Markov model in Figure 13.7.\nThe speciﬁcation of the probabilistic model is completed by deﬁning the con-\nditional distributions of the observed variables p(xn|zn, φ), where φ is a set of pa-\nrameters governing the distribution. These are known as emission probabilities, and\nmight for example be given by Gaussians of the form (9.11) if the elements of x are\ncontinuous variables, or by conditional probability tables if x is discrete. Because\nxn is observed, the distribution p(xn|zn, φ) consists, for a given value of φ,o fa\nvector of K numbers corresponding to the K possible states of the binary vector zn.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 630,
      "page_label": "611"
    }
  },
  {
    "page_content": "612 13. SEQUENTIAL DATA\nFigure 13.7 If we unfold the state transition dia-\ngram of Figure 13.6 over time, we obtain a lattice,\nor trellis, representation of the latent states. Each\ncolumn of this diagram corresponds to one of the\nlatent variables zn.\nk =1\nk =2\nk =3\nn − 2 n − 1 nn +1\nA11 A11 A11\nA33 A33 A33\nWe can represent the emission probabilities in the form\np(xn|zn, φ)=\nK∏\nk=1\np(xn|φk)znk . (13.9)\nWe shall focuss attention on homogeneous models for which all of the condi-\ntional distributions governing the latent variables share the same parameters A, and\nsimilarly all of the emission distributions share the same parametersφ (the extension\nto more general cases is straightforward). Note that a mixture model for an i.i.d. data\nset corresponds to the special case in which the parameters Ajk are the same for all\nvalues of j, so that the conditional distribution p(zn|zn−1) is independent of zn−1.\nThis corresponds to deleting the horizontal links in the graphical model shown in\nFigure 13.5.\nThe joint probability distribution over both latent and observed variables is then\ngiven by\np(X, Z|θ)= p(z1|π)\n[N∏\nn=2\np(zn|zn−1, A)\n] N∏\nm=1\np(xm|zm, φ) (13.10)\nwhere X = {x1,..., xN }, Z = {z1,..., zN }, and θ = {π, A, φ} denotes the set\nof parameters governing the model. Most of our discussion of the hidden Markov\nmodel will be independent of the particular choice of the emission probabilities.\nIndeed, the model is tractable for a wide range of emission distributions including",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 631,
      "page_label": "612"
    }
  },
  {
    "page_content": "Indeed, the model is tractable for a wide range of emission distributions including\ndiscrete tables, Gaussians, and mixtures of Gaussians. It is also possible to exploit\ndiscriminative models such as neural networks. These can be used to model theExercise 13.4\nemission density p(x|z) directly, or to provide a representation for p(z|x) that can\nbe converted into the required emission densityp(x|z) using Bayes’ theorem (Bishop\net al., 2004).\nWe can gain a better understanding of the hidden Markov model by considering\nit from a generative point of view. Recall that to generate samples from a mixture of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 631,
      "page_label": "612"
    }
  },
  {
    "page_content": "13.2. Hidden Markov Models 613\nk =1\nk =2\nk =3\n0 0.5 1\n0\n0.5\n1\n0 0.5 1\n0\n0.5\n1\nFigure 13.8 Illustration of sampling from a hidden Markov model having a 3-state latent variable z and a\nGaussian emission model p(x|z) where x is 2-dimensional. (a) Contours of constant probability density for the\nemission distributions corresponding to each of the three states of the latent variable. (b) A sample of 50 points\ndrawn from the hidden Markov model, colour coded according to the component that generated them and with\nlines connecting the successive observations. Here the transition matrix was ﬁxed so that in any state there is a\n5% probability of making a transition to each of the other states, and consequently a 90% probability of remaining\nin the same state.\nGaussians, we ﬁrst chose one of the components at random with probability given by\nthe mixing coefﬁcients πk and then generate a sample vectorx from the correspond-\ning Gaussian component. This process is repeated N times to generate a data set of\nN independent samples. In the case of the hidden Markov model, this procedure is\nmodiﬁed as follows. We ﬁrst choose the initial latent variable z1 with probabilities\ngoverned by the parameters πk and then sample the corresponding observation x1.\nNow we choose the state of the variable z2 according to the transition probabilities\np(z2|z1) using the already instantiated value ofz1. Thus suppose that the sample for",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 632,
      "page_label": "613"
    }
  },
  {
    "page_content": "p(z2|z1) using the already instantiated value ofz1. Thus suppose that the sample for\nz1 corresponds to state j. Then we choose the state k of z2 with probabilities Ajk\nfor k =1 ,...,K . Once we know z2 we can draw a sample for x2 and also sample\nthe next latent variable z3 and so on. This is an example of ancestral sampling for\na directed graphical model. If, for instance, we have a model in which the diago-Section 8.1.2\nnal transition elements Akk are much larger than the off-diagonal elements, then a\ntypical data sequence will have long runs of points generated from a single compo-\nnent, with infrequent transitions from one component to another. The generation of\nsamples from a hidden Markov model is illustrated in Figure 13.8.\nThere are many variants of the standard HMM model, obtained for instance by\nimposing constraints on the form of the transition matrixA (Rabiner, 1989). Here we\nmention one of particular practical importance called the left-to-right HMM, which\nis obtained by setting the elements Ajk of A to zero if k<j , as illustrated in the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 632,
      "page_label": "613"
    }
  },
  {
    "page_content": "614 13. SEQUENTIAL DATA\nFigure 13.9 Example of the state transition diagram for a 3-state\nleft-to-right hidden Markov model. Note that once a\nstate has been vacated, it cannot later be re-entered.\nk =1 k =2 k =3\nA11 A22 A33\nA12 A23\nA13\nstate transition diagram for a 3-state HMM in Figure 13.9. Typically for such models\nthe initial state probabilities forp(z1) are modiﬁed so thatp(z11)=1 and p(z1j)=0\nfor j ̸=1 , in other words every sequence is constrained to start in state j =1 . The\ntransition matrix may be further constrained to ensure that large changes in the state\nindex do not occur, so that Ajk =0 if k>j +∆ . This type of model is illustrated\nusing a lattice diagram in Figure 13.10.\nMany applications of hidden Markov models, for example speech recognition,\nor on-line character recognition, make use of left-to-right architectures. As an illus-\ntration of the left-to-right hidden Markov model, we consider an example involving\nhandwritten digits. This uses on-line data, meaning that each digit is represented\nby the trajectory of the pen as a function of time in the form of a sequence of pen\ncoordinates, in contrast to the off-line digits data, discussed in Appendix A, which\ncomprises static two-dimensional pixellated images of the ink. Examples of the on-\nline digits are shown in Figure 13.11. Here we train a hidden Markov model on a\nsubset of data comprising 45 examples of the digit ‘2’. There are K =1 6 states,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 633,
      "page_label": "614"
    }
  },
  {
    "page_content": "subset of data comprising 45 examples of the digit ‘2’. There are K =1 6 states,\neach of which can generate a line segment of ﬁxed length having one of 16 possible\nangles, and so the emission distribution is simply a 16 × 16 table of probabilities\nassociated with the allowed angle values for each state index value. Transition prob-\nabilities are all set to zero except for those that keep the state index k the same or\nthat increment it by 1, and the model parameters are optimized using 25 iterations of\nEM. We can gain some insight into the resulting model by running it generatively, as\nshown in Figure 13.11.\nFigure 13.10 Lattice diagram for a 3-state left-\nto-right HMM in which the state index k is allowed\nto increase by at most 1 at each transition. k =1\nk =2\nk =3\nn − 2 n − 1 nn +1\nA11 A11 A11\nA33 A33 A33",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 633,
      "page_label": "614"
    }
  },
  {
    "page_content": "13.2. Hidden Markov Models 615\nFigure 13.11 Top row: examples of on-line handwritten\ndigits. Bottom row: synthetic digits sam-\npled generatively from a left-to-right hid-\nden Markov model that has been trained\non a data set of 45 handwritten digits.\nOne of the most powerful properties of hidden Markov models is their ability to\nexhibit some degree of invariance to local warping (compression and stretching) of\nthe time axis. To understand this, consider the way in which the digit ‘2’ is written\nin the on-line handwritten digits example. A typical digit comprises two distinct\nsections joined at a cusp. The ﬁrst part of the digit, which starts at the top left, has a\nsweeping arc down to the cusp or loop at the bottom left, followed by a second more-\nor-less straight sweep ending at the bottom right. Natural variations in writing style\nwill cause the relative sizes of the two sections to vary, and hence the location of the\ncusp or loop within the temporal sequence will vary. From a generative perspective\nsuch variations can be accommodated by the hidden Markov model through changes\nin the number of transitions to the same state versus the number of transitions to the\nsuccessive state. Note, however, that if a digit ‘2’ is written in the reverse order, that\nis, starting at the bottom right and ending at the top left, then even though the pen tip\ncoordinates may be identical to an example from the training set, the probability of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 634,
      "page_label": "615"
    }
  },
  {
    "page_content": "coordinates may be identical to an example from the training set, the probability of\nthe observations under the model will be extremely small. In the speech recognition\ncontext, warping of the time axis is associated with natural variations in the speed of\nspeech, and again the hidden Markov model can accommodate such a distortion and\nnot penalize it too heavily.\n13.2.1 Maximum likelihood for the HMM\nIf we have observed a data setX = {x1,..., xN }, we can determine the param-\neters of an HMM using maximum likelihood. The likelihood function is obtained\nfrom the joint distribution (13.10) by marginalizing over the latent variables\np(X|θ)=\n∑\nZ\np(X, Z|θ). (13.11)\nBecause the joint distribution p(X, Z|θ) does not factorize over n (in contrast to the\nmixture distribution considered in Chapter 9), we cannot simply treat each of the\nsummations over zn independently. Nor can we perform the summations explicitly\nbecause there are N variables to be summed over, each of which has K states, re-\nsulting in a total of KN terms. Thus the number of terms in the summation grows",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 634,
      "page_label": "615"
    }
  },
  {
    "page_content": "616 13. SEQUENTIAL DATA\nexponentially with the length of the chain. In fact, the summation in (13.11) cor-\nresponds to summing over exponentially many paths through the lattice diagram in\nFigure 13.7.\nWe have already encountered a similar difﬁculty when we considered the infer-\nence problem for the simple chain of variables in Figure 8.32. There we were able\nto make use of the conditional independence properties of the graph to re-order the\nsummations in order to obtain an algorithm whose cost scales linearly, instead of\nexponentially, with the length of the chain. We shall apply a similar technique to the\nhidden Markov model.\nA further difﬁculty with the expression (13.11) for the likelihood function is that,\nbecause it corresponds to a generalization of a mixture distribution, it represents a\nsummation over the emission models for different settings of the latent variables.\nDirect maximization of the likelihood function will therefore lead to complex ex-\npressions with no closed-form solutions, as was the case for simple mixture modelsSection 9.2\n(recall that a mixture model for i.i.d. data is a special case of the HMM).\nWe therefore turn to the expectation maximization algorithm to ﬁnd an efﬁcient\nframework for maximizing the likelihood function in hidden Markov models. The\nEM algorithm starts with some initial selection for the model parameters, which we\ndenote by θold. In the E step, we take these parameter values and ﬁnd the posterior",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 635,
      "page_label": "616"
    }
  },
  {
    "page_content": "denote by θold. In the E step, we take these parameter values and ﬁnd the posterior\ndistribution of the latent variables p(Z|X, θold). We then use this posterior distri-\nbution to evaluate the expectation of the logarithm of the complete-data likelihood\nfunction, as a function of the parameters θ, to give the function Q(θ, θold) deﬁned\nby\nQ(θ, θold)=\n∑\nZ\np(Z|X, θold)l np(X, Z|θ). (13.12)\nAt this point, it is convenient to introduce some notation. We shall use γ(zn) to\ndenote the marginal posterior distribution of a latent variable zn, and ξ(zn−1, zn) to\ndenote the joint posterior distribution of two successive latent variables, so that\nγ(zn)= p(zn|X, θold) (13.13)\nξ(zn−1, zn)= p(zn−1, zn|X, θold). (13.14)\nFor each value of n, we can store γ(zn) using a set of K nonnegative numbers\nthat sum to unity, and similarly we can store ξ(zn−1, zn) using a K × K matrix of\nnonnegative numbers that again sum to unity. We shall also useγ(znk) to denote the\nconditional probability of znk =1 , with a similar use of notation for ξ(zn−1,j,z nk)\nand for other probabilistic variables introduced later. Because the expectation of a\nbinary random variable is just the probability that it takes the value1,w eh a v e\nγ(znk)= E[znk]=\n∑\nz\nγ(z)znk (13.15)\nξ(zn−1,j,z nk)= E[zn−1,jznk]=\n∑\nz\nγ(z)zn−1,jznk. (13.16)\nIf we substitute the joint distribution p(X, Z|θ) given by (13.10) into (13.12),",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 635,
      "page_label": "616"
    }
  },
  {
    "page_content": "13.2. Hidden Markov Models 617\nand make use of the deﬁnitions of γ and ξ , we obtain\nQ(θ, θold)=\nK∑\nk=1\nγ(z1k)l nπk +\nN∑\nn=2\nK∑\nj=1\nK∑\nk=1\nξ(zn−1,j,z nk)l nAjk\n+\nN∑\nn=1\nK∑\nk=1\nγ(znk)l np(xn|φk). (13.17)\nThe goal of the E step will be to evaluate the quantities γ(zn) and ξ(zn−1, zn) efﬁ-\nciently, and we shall discuss this in detail shortly.\nIn the M step, we maximize Q(θ, θold) with respect to the parameters θ =\n{π, A, φ} in which we treat γ(zn) and ξ(zn−1, zn) as constant. Maximization with\nrespect to π and A is easily achieved using appropriate Lagrange multipliers with\nthe resultsExercise 13.5\nπk = γ(z1k)\nK∑\nj=1\nγ(z1j)\n(13.18)\nAjk =\nN∑\nn=2\nξ(zn−1,j,z nk)\nK∑\nl=1\nN∑\nn=2\nξ(zn−1,j,z nl)\n. (13.19)\nThe EM algorithm must be initialized by choosing starting values forπ and A, which\nshould of course respect the summation constraints associated with their probabilis-\ntic interpretation. Note that any elements of π or A that are set to zero initially will\nremain zero in subsequent EM updates. A typical initialization procedure wouldExercise 13.6\ninvolve selecting random starting values for these parameters subject to the summa-\ntion and non-negativity constraints. Note that no particular modiﬁcation to the EM\nresults are required for the case of left-to-right models beyond choosing initial values\nfor the elements Ajk in which the appropriate elements are set to zero, because these\nwill remain zero throughout.\nTo maximize Q(θ, θold) with respect to φk, we notice that only the ﬁnal term",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 636,
      "page_label": "617"
    }
  },
  {
    "page_content": "will remain zero throughout.\nTo maximize Q(θ, θold) with respect to φk, we notice that only the ﬁnal term\nin (13.17) depends on φk, and furthermore this term has exactly the same form as\nthe data-dependent term in the corresponding function for a standard mixture dis-\ntribution for i.i.d. data, as can be seen by comparison with (9.40) for the case of a\nGaussian mixture. Here the quantities γ(znk) are playing the role of the responsibil-\nities. If the parameters φk are independent for the different components, then this\nterm decouples into a sum of terms one for each value of k, each of which can be\nmaximized independently. We are then simply maximizing the weighted log likeli-\nhood function for the emission densityp(x|φk) with weights γ(znk). Here we shall\nsuppose that this maximization can be done efﬁciently. For instance, in the case of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 636,
      "page_label": "617"
    }
  },
  {
    "page_content": "618 13. SEQUENTIAL DATA\nGaussian emission densities we have p(x|φk)= N(x|µk, Σk), and maximization\nof the function Q(θ, θold) then gives\nµk =\nN∑\nn=1\nγ(znk)xn\nN∑\nn=1\nγ(znk)\n(13.20)\nΣk =\nN∑\nn=1\nγ(znk)(xn − µk)(xn − µk)T\nN∑\nn=1\nγ(znk)\n. (13.21)\nFor the case of discrete multinomial observed variables, the conditional distribution\nof the observations takes the form\np(x|z)=\nD∏\ni=1\nK∏\nk=1\nµxizk\nik (13.22)\nand the corresponding M-step equations are given byExercise 13.8\nµik =\nN∑\nn=1\nγ(znk)xni\nN∑\nn=1\nγ(znk)\n. (13.23)\nAn analogous result holds for Bernoulli observed variables.\nThe EM algorithm requires initial values for the parameters of the emission dis-\ntribution. One way to set these is ﬁrst to treat the data initially as i.i.d. and ﬁt the\nemission density by maximum likelihood, and then use the resulting values to ini-\ntialize the parameters for EM.\n13.2.2 The forward-backward algorithm\nNext we seek an efﬁcient procedure for evaluating the quantities γ(znk) and\nξ(zn−1,j,z nk), corresponding to the E step of the EM algorithm. The graph for the\nhidden Markov model, shown in Figure 13.5, is a tree, and so we know that the\nposterior distribution of the latent variables can be obtained efﬁciently using a two-\nstage message passing algorithm. In the particular context of the hidden MarkovSection 8.4\nmodel, this is known as the forward-backward algorithm (Rabiner, 1989), or the\nBaum-Welch algorithm (Baum, 1972). There are in fact several variants of the basic",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 637,
      "page_label": "618"
    }
  },
  {
    "page_content": "Baum-Welch algorithm (Baum, 1972). There are in fact several variants of the basic\nalgorithm, all of which lead to the exact marginals, according to the precise form of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 637,
      "page_label": "618"
    }
  },
  {
    "page_content": "13.2. Hidden Markov Models 619\nthe messages that are propagated along the chain (Jordan, 2007). We shall focus on\nthe most widely used of these, known as the alpha-beta algorithm.\nAs well as being of great practical importance in its own right, the forward-\nbackward algorithm provides us with a nice illustration of many of the concepts\nintroduced in earlier chapters. We shall therefore begin in this section with a ‘con-\nventional’ derivation of the forward-backward equations, making use of the sum\nand product rules of probability, and exploiting conditional independence properties\nwhich we shall obtain from the corresponding graphical model using d-separation.\nThen in Section 13.2.3, we shall see how the forward-backward algorithm can be\nobtained very simply as a speciﬁc example of the sum-product algorithm introduced\nin Section 8.4.4.\nIt is worth emphasizing that evaluation of the posterior distributions of the latent\nvariables is independent of the form of the emission density p(x|z) or indeed of\nwhether the observed variables are continuous or discrete. All we require is the\nvalues of the quantities p(xn|zn) for each value of zn for every n. Also, in this\nsection and the next we shall omit the explicit dependence on the model parameters\nθold because these ﬁxed throughout.\nWe therefore begin by writing down the following conditional independence\nproperties (Jordan, 2007)\np(X|zn)= p(x1,..., xn|zn)\np(xn+1,..., xN |zn) (13.24)\np(x1,..., xn−1|xn, zn)= p(x1,..., xn−1|zn) (13.25)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 638,
      "page_label": "619"
    }
  },
  {
    "page_content": "properties (Jordan, 2007)\np(X|zn)= p(x1,..., xn|zn)\np(xn+1,..., xN |zn) (13.24)\np(x1,..., xn−1|xn, zn)= p(x1,..., xn−1|zn) (13.25)\np(x1,..., xn−1|zn−1, zn)= p(x1,..., xn−1|zn−1) (13.26)\np(xn+1,..., xN |zn, zn+1)= p(xn+1,..., xN |zn+1) (13.27)\np(xn+2,..., xN |zn+1, xn+1)= p(xn+2,..., xN |zn+1) (13.28)\np(X|zn−1, zn)= p(x1,..., xn−1|zn−1)\np(xn|zn)p(xn+1,..., xN |zn) (13.29)\np(xN+1|X, zN+1)= p(xN+1|zN+1) (13.30)\np(zN+1|zN , X)= p(zN+1|zN ) (13.31)\nwhere X = {x1,..., xN }. These relations are most easily proved using d-separation.\nFor instance in the ﬁrst of these results, we note that every path from any one of the\nnodes x1,..., xn−1 to the node xn passes through the node zn, which is observed.\nBecause all such paths are head-to-tail, it follows that the conditional independence\nproperty must hold. The reader should take a few moments to verify each of these\nproperties in turn, as an exercise in the application of d-separation. These relations\ncan also be proved directly, though with signiﬁcantly greater effort, from the joint\ndistribution for the hidden Markov model using the sum and product rules of proba-\nbility.Exercise 13.10\nLet us begin by evaluating γ(znk). Recall that for a discrete multinomial ran-\ndom variable the expected value of one of its components is just the probability of\nthat component having the value1. Thus we are interested in ﬁnding the posterior\ndistribution p(zn|x1,..., xN ) of zn given the observed data set x1,..., xN . This",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 638,
      "page_label": "619"
    }
  },
  {
    "page_content": "620 13. SEQUENTIAL DATA\nrepresents a vector of length K whose entries correspond to the expected values of\nznk. Using Bayes’ theorem, we have\nγ(zn)= p(zn|X)= p(X|zn)p(zn)\np(X) . (13.32)\nNote that the denominator p(X) is implicitly conditioned on the parameters θold\nof the HMM and hence represents the likelihood function. Using the conditional\nindependence property (13.24), together with the product rule of probability, we\nobtain\nγ(zn)= p(x1,..., xn, zn)p(xn+1,..., xN |zn)\np(X) = α(zn)β(zn)\np(X) (13.33)\nwhere we have deﬁned\nα(zn) ≡ p(x1,..., xn, zn) (13.34)\nβ(zn) ≡ p(xn+1,..., xN |zn). (13.35)\nThe quantity α(zn) represents the joint probability of observing all of the given\ndata up to time n and the value of zn, whereas β(zn) represents the conditional\nprobability of all future data from time n +1 up to N given the value of zn. Again,\nα(zn) and β(zn) each represent set of K numbers, one for each of the possible\nsettings of the 1-of-K coded binary vector zn. We shall use the notation α(znk) to\ndenote the value ofα(zn) when znk =1 , with an analogous interpretation ofβ(znk).\nWe now derive recursion relations that allow α(zn) and β(zn) to be evaluated\nefﬁciently. Again, we shall make use of conditional independence properties, in\nparticular (13.25) and (13.26), together with the sum and product rules, allowing us\nto express α(zn) in terms of α(zn−1) as follows\nα(zn)= p(x1,..., xn, zn)\n= p(x1,..., xn|zn)p(zn)\n= p(xn|zn)p(x1,..., xn−1|zn)p(zn)\n= p(xn|zn)p(x1,..., xn−1, zn)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 639,
      "page_label": "620"
    }
  },
  {
    "page_content": "α(zn)= p(x1,..., xn, zn)\n= p(x1,..., xn|zn)p(zn)\n= p(xn|zn)p(x1,..., xn−1|zn)p(zn)\n= p(xn|zn)p(x1,..., xn−1, zn)\n= p(xn|zn)\n∑\nzn−1\np(x1,..., xn−1, zn−1, zn)\n= p(xn|zn)\n∑\nzn−1\np(x1,..., xn−1, zn|zn−1)p(zn−1)\n= p(xn|zn)\n∑\nzn−1\np(x1,..., xn−1|zn−1)p(zn|zn−1)p(zn−1)\n= p(xn|zn)\n∑\nzn−1\np(x1,..., xn−1, zn−1)p(zn|zn−1)\nMaking use of the deﬁnition (13.34) for α(zn), we then obtain\nα(zn)= p(xn|zn)\n∑\nzn−1\nα(zn−1)p(zn|zn−1). (13.36)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 639,
      "page_label": "620"
    }
  },
  {
    "page_content": "13.2. Hidden Markov Models 621\nFigure 13.12 Illustration of the forward recursion (13.36) for\nevaluation of the α variables. In this fragment\nof the lattice, we see that the quantity α(zn1)\nis obtained by taking the elements α(zn−1,j) of\nα(zn−1) at step n−1 and summing them up with\nweights given by Aj1, corresponding to the val-\nues of p(zn|zn−1), and then multiplying by the\ndata contribution p(xn|zn1).\nk =1\nk =2\nk =3\nn − 1 n\nα(zn−1,1)\nα(zn−1,2)\nα(zn−1,3)\nα(zn,1)\nA11\nA21\nA31\np(xn|zn,1)\nIt is worth taking a moment to study this recursion relation in some detail. Note\nthat there are K terms in the summation, and the right-hand side has to be evaluated\nfor each of the K values of zn so each step of the α recursion has computational\ncost that scaled like O(K2). The forward recursion equation for α(zn) is illustrated\nusing a lattice diagram in Figure 13.12.\nIn order to start this recursion, we need an initial condition that is given by\nα(z1)= p(x1, z1)= p(z1)p(x1|z1)=\nK∏\nk=1\n{πkp(x1|φk)}z1k (13.37)\nwhich tells us that α(z1k), for k =1 ,...,K , takes the value πkp(x1|φk). Starting\nat the ﬁrst node of the chain, we can then work along the chain and evaluate α(zn)\nfor every latent node. Because each step of the recursion involves multiplying by a\nK × K matrix, the overall cost of evaluating these quantities for the whole chain is\nof O(K2N).\nWe can similarly ﬁnd a recursion relation for the quantities β(zn) by making",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 640,
      "page_label": "621"
    }
  },
  {
    "page_content": "of O(K2N).\nWe can similarly ﬁnd a recursion relation for the quantities β(zn) by making\nuse of the conditional independence properties (13.27) and (13.28) giving\nβ(zn)= p(xn+1,..., xN |zn)\n=\n∑\nzn+1\np(xn+1,..., xN , zn+1|zn)\n=\n∑\nzn+1\np(xn+1,..., xN |zn, zn+1)p(zn+1|zn)\n=\n∑\nzn+1\np(xn+1,..., xN |zn+1)p(zn+1|zn)\n=\n∑\nzn+1\np(xn+2,..., xN |zn+1)p(xn+1|zn+1)p(zn+1|zn).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 640,
      "page_label": "621"
    }
  },
  {
    "page_content": "622 13. SEQUENTIAL DATA\nFigure 13.13 Illustration of the backward recursion\n(13.38) for evaluation of the β variables. In\nthis fragment of the lattice, we see that the\nquantity β(zn1) is obtained by taking the\ncomponents β(zn+1,k) of β(zn+1) at step\nn +1 and summing them up with weights\ngiven by the products of A1k, correspond-\ning to the values of p(zn+1|zn) and the cor-\nresponding values of the emission density\np(xn|zn+1,k).\nk =1\nk =2\nk =3\nnn +1\nβ(zn,1) β(zn+1,1)\nβ(zn+1,2)\nβ(zn+1,3)\nA11\nA12\nA13\np(xn|zn+1,1)\np(xn|zn+1,2)\np(xn|zn+1,3)\nMaking use of the deﬁnition (13.35) for β(zn), we then obtain\nβ(zn)=\n∑\nzn+1\nβ(zn+1)p(xn+1|zn+1)p(zn+1|zn). (13.38)\nNote that in this case we have a backward message passing algorithm that evaluates\nβ(zn) in terms of β(zn+1). At each step, we absorb the effect of observation xn+1\nthrough the emission probability p(xn+1|zn+1), multiply by the transition matrix\np(zn+1|zn), and then marginalize out zn+1. This is illustrated in Figure 13.13.\nAgain we need a starting condition for the recursion, namely a value forβ(zN ).\nThis can be obtained by setting n = N in (13.33) and replacing α(zN ) with its\ndeﬁnition (13.34) to give\np(zN |X)= p(X, zN )β(zN )\np(X) (13.39)\nwhich we see will be correct provided we take β(zN )=1 for all settings of zN .\nIn the M step equations, the quantity p(X) will cancel out, as can be seen, for\ninstance, in the M-step equation for µk given by (13.20), which takes the form\nµk =\nn∑\nn=1\nγ(znk)xn\nn∑\nn=1\nγ(znk)\n=\nn∑\nn=1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 641,
      "page_label": "622"
    }
  },
  {
    "page_content": "instance, in the M-step equation for µk given by (13.20), which takes the form\nµk =\nn∑\nn=1\nγ(znk)xn\nn∑\nn=1\nγ(znk)\n=\nn∑\nn=1\nα(znk)β(znk)xn\nn∑\nn=1\nα(znk)β(znk)\n. (13.40)\nHowever, the quantity p(X) represents the likelihood function whose value we typ-\nically wish to monitor during the EM optimization, and so it is useful to be able to\nevaluate it. If we sum both sides of (13.33) overzn, and use the fact that the left-hand\nside is a normalized distribution, we obtain\np(X)=\n∑\nzn\nα(zn)β(zn). (13.41)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 641,
      "page_label": "622"
    }
  },
  {
    "page_content": "13.2. Hidden Markov Models 623\nThus we can evaluate the likelihood function by computing this sum, for any conve-\nnient choice of n. For instance, if we only want to evaluate the likelihood function,\nthen we can do this by running the α recursion from the start to the end of the chain,\nand then use this result for n = N, making use of the fact that β(zN ) is a vector of\n1s. In this case no β recursion is required, and we simply have\np(X)=\n∑\nzN\nα(zN ). (13.42)\nLet us take a moment to interpret this result for p(X). Recall that to compute the\nlikelihood we should take the joint distribution p(X, Z) and sum over all possible\nvalues of Z. Each such value represents a particular choice of hidden state for every\ntime step, in other words every term in the summation is a path through the lattice\ndiagram, and recall that there are exponentially many such paths. By expressing\nthe likelihood function in the form (13.42), we have reduced the computational cost\nfrom being exponential in the length of the chain to being linear by swapping the\norder of the summation and multiplications, so that at each time step n we sum\nthe contributions from all paths passing through each of the states znk to give the\nintermediate quantities α(zn).\nNext we consider the evaluation of the quantitiesξ(zn−1, zn), which correspond\nto the values of the conditional probabilities p(zn−1, zn|X) for each of the K × K\nsettings for (zn−1, zn). Using the deﬁnition of ξ(zn−1, zn), and applying Bayes’\ntheorem, we have",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 642,
      "page_label": "623"
    }
  },
  {
    "page_content": "settings for (zn−1, zn). Using the deﬁnition of ξ(zn−1, zn), and applying Bayes’\ntheorem, we have\nξ(zn−1, zn)= p(zn−1, zn|X)\n= p(X|zn−1, zn)p(zn−1, zn)\np(X)\n= p(x1,..., xn−1|zn−1)p(xn|zn)p(xn+1,..., xN |zn)p(zn|zn−1)p(zn−1)\np(X)\n= α(zn−1)p(xn|zn)p(zn|zn−1)β(zn)\np(X) (13.43)\nwhere we have made use of the conditional independence property (13.29) together\nwith the deﬁnitions of α(zn) and β(zn) given by (13.34) and (13.35). Thus we can\ncalculate the ξ(zn−1, zn) directly by using the results of the α and β recursions.\nLet us summarize the steps required to train a hidden Markov model using\nthe EM algorithm. We ﬁrst make an initial selection of the parameters θold where\nθ ≡ (π, A, φ). The A and π parameters are often initialized either uniformly or\nrandomly from a uniform distribution (respecting their non-negativity and summa-\ntion constraints). Initialization of the parameters φ will depend on the form of the\ndistribution. For instance in the case of Gaussians, the parameters µk might be ini-\ntialized by applying the K-means algorithm to the data, and Σk might be initialized\nto the covariance matrix of the corresponding K means cluster. Then we run both\nthe forward α recursion and the backward β recursion and use the results to evaluate\nγ(zn) and ξ(zn−1, zn). At this stage, we can also evaluate the likelihood function.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 642,
      "page_label": "623"
    }
  },
  {
    "page_content": "624 13. SEQUENTIAL DATA\nThis completes the E step, and we use the results to ﬁnd a revised set of parameters\nθnew using the M-step equations from Section 13.2.1. We then continue to alternate\nbetween E and M steps until some convergence criterion is satisﬁed, for instance\nwhen the change in the likelihood function is below some threshold.\nNote that in these recursion relations the observations enter through conditional\ndistributions of the form p(xn|zn). The recursions are therefore independent of\nthe type or dimensionality of the observed variables or the form of this conditional\ndistribution, so long as its value can be computed for each of the K possible states\nof zn. Since the observed variables {xn} are ﬁxed, the quantities p(xn|zn) can be\npre-computed as functions of zn at the start of the EM algorithm, and remain ﬁxed\nthroughout.\nWe have seen in earlier chapters that the maximum likelihood approach is most\neffective when the number of data points is large in relation to the number of parame-\nters. Here we note that a hidden Markov model can be trained effectively, using max-\nimum likelihood, provided the training sequence is sufﬁciently long. Alternatively,\nwe can make use of multiple shorter sequences, which requires a straightforward\nmodiﬁcation of the hidden Markov model EM algorithm. In the case of left-to-rightExercise 13.12\nmodels, this is particularly important because, in a given observation sequence, a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 643,
      "page_label": "624"
    }
  },
  {
    "page_content": "models, this is particularly important because, in a given observation sequence, a\ngiven state transition corresponding to a nondiagonal element ofA will seen at most\nonce.\nAnother quantity of interest is the predictive distribution, in which the observed\ndata is X = {x1,..., xN } and we wish to predict xN+1, which would be important\nfor real-time applications such as ﬁnancial forecasting. Again we make use of the\nsum and product rules together with the conditional independence properties (13.29)\nand (13.31) giving\np(xN+1|X)=\n∑\nzN+1\np(xN+1, zN+1|X)\n=\n∑\nzN+1\np(xN+1|zN+1)p(zN+1|X)\n=\n∑\nzN+1\np(xN+1|zN+1)\n∑\nzN\np(zN+1, zN |X)\n=\n∑\nzN+1\np(xN+1|zN+1)\n∑\nzN\np(zN+1|zN )p(zN |X)\n=\n∑\nzN+1\np(xN+1|zN+1)\n∑\nzN\np(zN+1|zN )p(zN , X)\np(X)\n= 1\np(X)\n∑\nzN+1\np(xN+1|zN+1)\n∑\nzN\np(zN+1|zN )α(zN ) (13.44)\nwhich can be evaluated by ﬁrst running a forward α recursion and then computing\nthe ﬁnal summations over zN and zN+1. The result of the ﬁrst summation over zN\ncan be stored and used once the value of xN+1 is observed in order to run the α\nrecursion forward to the next step in order to predict the subsequent value xN+2.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 643,
      "page_label": "624"
    }
  },
  {
    "page_content": "13.2. Hidden Markov Models 625\nFigure 13.14 A fragment of the fac-\ntor graph representation for the hidden\nMarkov model.\nχψ n\ng1 gn−1 gn\nz1 zn−1 zn\nx1 xn−1 xn\nNote that in (13.44), the inﬂuence of all data from x1 to xN is summarized in the K\nvalues of α(zN ). Thus the predictive distribution can be carried forward indeﬁnitely\nusing a ﬁxed amount of storage, as may be required for real-time applications.\nHere we have discussed the estimation of the parameters of an HMM using max-\nimum likelihood. This framework is easily extended to regularized maximum likeli-\nhood by introducing priors over the model parameters π, A and φ whose values are\nthen estimated by maximizing their posterior probability. This can again be done us-\ning the EM algorithm in which the E step is the same as discussed above, and the M\nstep involves adding the log of the prior distribution p(θ) to the function Q(θ, θold)\nbefore maximization and represents a straightforward application of the techniques\ndeveloped at various points in this book. Furthermore, we can use variational meth-\nods to give a fully Bayesian treatment of the HMM in which we marginalize over theSection 10.1\nparameter distributions (MacKay, 1997). As with maximum likelihood, this leads to\na two-pass forward-backward recursion to compute posterior probabilities.\n13.2.3 The sum-product algorithm for the HMM\nThe directed graph that represents the hidden Markov model, shown in Fig-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 644,
      "page_label": "625"
    }
  },
  {
    "page_content": "13.2.3 The sum-product algorithm for the HMM\nThe directed graph that represents the hidden Markov model, shown in Fig-\nure 13.5, is a tree and so we can solve the problem of ﬁnding local marginals for the\nhidden variables using the sum-product algorithm. Not surprisingly, this turns out toSection 8.4.4\nbe equivalent to the forward-backward algorithm considered in the previous section,\nand so the sum-product algorithm therefore provides us with a simple way to derive\nthe alpha-beta recursion formulae.\nWe begin by transforming the directed graph of Figure 13.5 into a factor graph,\nof which a representative fragment is shown in Figure 13.14. This form of the fac-\ntor graph shows all variables, both latent and observed, explicitly. However, for\nthe purpose of solving the inference problem, we shall always be conditioning on\nthe variables x1,..., xN , and so we can simplify the factor graph by absorbing the\nemission probabilities into the transition probability factors. This leads to the sim-\npliﬁed factor graph representation in Figure 13.15, in which the factors are given\nby\nh(z1)= p(z1)p(x1|z1) (13.45)\nfn(zn−1, zn)= p(zn|zn−1)p(xn|zn). (13.46)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 644,
      "page_label": "625"
    }
  },
  {
    "page_content": "626 13. SEQUENTIAL DATA\nFigure 13.15 A simpliﬁed form of fac-\ntor graph to describe the hidden Markov\nmodel.\nh fn\nz1 zn−1 zn\nTo derive the alpha-beta algorithm, we denote the ﬁnal hidden variable zN as\nthe root node, and ﬁrst pass messages from the leaf node h to the root. From the\ngeneral results (8.66) and (8.69) for message propagation, we see that the messages\nwhich are propagated in the hidden Markov model take the form\nµzn−1→fn (zn−1)= µfn−1→zn−1 (zn−1) (13.47)\nµfn→zn (zn)=\n∑\nzn−1\nfn(zn−1, zn)µzn−1→fn (zn−1) (13.48)\nThese equations represent the propagation of messages forward along the chain and\nare equivalent to the alpha recursions derived in the previous section, as we shall\nnow show. Note that because the variable nodes zn have only two neighbours, they\nperform no computation.\nWe can eliminate µzn−1→fn (zn−1) from (13.48) using (13.47) to give a recur-\nsion for the f → z messages of the form\nµfn→zn (zn)=\n∑\nzn−1\nfn(zn−1, zn)µfn−1→zn−1 (zn−1). (13.49)\nIf we now recall the deﬁnition (13.46), and if we deﬁne\nα(zn)= µfn→zn (zn) (13.50)\nthen we obtain the alpha recursion given by (13.36). We also need to verify that\nthe quantities α(zn) are themselves equivalent to those deﬁned previously. This\nis easily done by using the initial condition (8.71) and noting that α(z1) is given\nby h(z1)= p(z1)p(x1|z1) which is identical to (13.37). Because the initial α is\nthe same, and because they are iteratively computed using the same equation, all",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 645,
      "page_label": "626"
    }
  },
  {
    "page_content": "the same, and because they are iteratively computed using the same equation, all\nsubsequent α quantities must be the same.\nNext we consider the messages that are propagated from the root node back to\nthe leaf node. These take the form\nµfn+1→fn (zn)=\n∑\nzn+1\nfn+1(zn, zn+1)µfn+2→fn+1 (zn+1) (13.51)\nwhere, as before, we have eliminated the messages of the type z → f since the\nvariable nodes perform no computation. Using the deﬁnition (13.46) to substitute\nfor fn+1(zn, zn+1), and deﬁning\nβ(zn)= µfn+1→zn (zn) (13.52)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 645,
      "page_label": "626"
    }
  },
  {
    "page_content": "13.2. Hidden Markov Models 627\nwe obtain the beta recursion given by (13.38). Again, we can verify that the beta\nvariables themselves are equivalent by noting that (8.70) implies that the initial mes-\nsage send by the root variable node isµzN →fN (zN )=1 , which is identical to the\ninitialization of β(zN ) given in Section 13.2.2.\nThe sum-product algorithm also speciﬁes how to evaluate the marginals once all\nthe messages have been evaluated. In particular, the result (8.63) shows that the local\nmarginal at the node zn is given by the product of the incoming messages. Because\nwe have conditioned on the variables X = {x1,..., xN }, we are computing the\njoint distribution\np(zn, X)= µfn→zn (zn)µfn+1→zn (zn)= α(zn)β(zn). (13.53)\nDividing both sides by p(X), we then obtain\nγ(zn)= p(zn, X)\np(X) = α(zn)β(zn)\np(X) (13.54)\nin agreement with (13.33). The result (13.43) can similarly be derived from (8.72).Exercise 13.11\n13.2.4 Scaling factors\nThere is an important issue that must be addressed before we can make use of the\nforward backward algorithm in practice. From the recursion relation (13.36), we note\nthat at each step the new value α(zn) is obtained from the previous value α(zn−1)\nby multiplying by quantities p(zn|zn−1) and p(xn|zn). Because these probabilities\nare often signiﬁcantly less than unity, as we work our way forward along the chain,\nthe values of α(zn) can go to zero exponentially quickly. For moderate lengths of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 646,
      "page_label": "627"
    }
  },
  {
    "page_content": "the values of α(zn) can go to zero exponentially quickly. For moderate lengths of\nchain (say 100 or so), the calculation of the α(zn) will soon exceed the dynamic\nrange of the computer, even if double precision ﬂoating point is used.\nIn the case of i.i.d. data, we implicitly circumvented this problem with the eval-\nuation of likelihood functions by taking logarithms. Unfortunately, this will not help\nhere because we are forming sums of products of small numbers (we are in fact im-\nplicitly summing over all possible paths through the lattice diagram of Figure 13.7).\nWe therefore work with re-scaled versions ofα(zn) and β(zn) whose values remain\nof order unity. As we shall see, the corresponding scaling factors cancel out when\nwe use these re-scaled quantities in the EM algorithm.\nIn (13.34), we deﬁned α(zn)= p(x1,..., xn, zn) representing the joint distri-\nbution of all the observations up to xn and the latent variable zn. Now we deﬁne a\nnormalized version of α given by\nˆα(zn)= p(zn|x1,..., xn)= α(zn)\np(x1,..., xn) (13.55)\nwhich we expect to be well behaved numerically because it is a probability distribu-\ntion over K variables for any value ofn. In order to relate the scaled and original al-\npha variables, we introduce scaling factors deﬁned by conditional distributions over\nthe observed variables\ncn = p(xn|x1,..., xn−1). (13.56)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 646,
      "page_label": "627"
    }
  },
  {
    "page_content": "628 13. SEQUENTIAL DATA\nFrom the product rule, we then have\np(x1,..., xn)=\nn∏\nm=1\ncm (13.57)\nand so\nα(zn)= p(zn|x1,..., xn)p(x1,..., xn)=\n( n∏\nm=1\ncm\n)\nˆα(zn). (13.58)\nWe can then turn the recursion equation (13.36) for α into one for ˆα given by\ncnˆα(zn)= p(xn|zn)\n∑\nzn−1\nˆα(zn−1)p(zn|zn−1). (13.59)\nNote that at each stage of the forward message passing phase, used to evaluateˆα(zn),\nwe have to evaluate and store cn, which is easily done because it is the coefﬁcient\nthat normalizes the right-hand side of (13.59) to give ˆα(zn).\nWe can similarly deﬁne re-scaled variables ˆβ(zn) using\nβ(zn)=\n( N∏\nm=n+1\ncm\n)\nˆβ(zn) (13.60)\nwhich will again remain within machine precision because, from (13.35), the quan-\ntities ˆβ(zn) are simply the ratio of two conditional probabilities\nˆβ(zn)= p(xn+1,..., xN |zn)\np(xn+1,..., xN |x1,..., xn). (13.61)\nThe recursion result (13.38) forβ then gives the following recursion for the re-scaled\nvariables\ncn+1ˆβ(zn)=\n∑\nzn+1\nˆβ(zn+1)p(xn+1|zn+1)p(zn+1|zn). (13.62)\nIn applying this recursion relation, we make use of the scaling factors cn that were\npreviously computed in the α phase.\nFrom (13.57), we see that the likelihood function can be found using\np(X)=\nN∏\nn=1\ncn. (13.63)\nSimilarly, using (13.33) and (13.43), together with (13.63), we see that the required\nmarginals are given byExercise 13.15\nγ(zn)= ˆα(zn)ˆβ(zn) (13.64)\nξ(zn−1, zn)= cnˆα(zn−1)p(xn|zn)p(zn|z−1)ˆβ(zn). (13.65)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 647,
      "page_label": "628"
    }
  },
  {
    "page_content": "13.2. Hidden Markov Models 629\nFinally, we note that there is an alternative formulation of the forward-backward\nalgorithm (Jordan, 2007) in which the backward pass is deﬁned by a recursion based\nthe quantities γ(zn)= ˆα(zn)ˆβ(zn) instead of using ˆβ(zn). This α–γ recursion\nrequires that the forward pass be completed ﬁrst so that all the quantities ˆα(zn)\nare available for the backward pass, whereas the forward and backward passes of\nthe α–β algorithm can be done independently. Although these two algorithms have\ncomparable computational cost, the α–β version is the most commonly encountered\none in the case of hidden Markov models, whereas for linear dynamical systems aSection 13.3\nrecursion analogous to the α–γ form is more usual.\n13.2.5 The Viterbi algorithm\nIn many applications of hidden Markov models, the latent variables have some\nmeaningful interpretation, and so it is often of interest to ﬁnd the most probable\nsequence of hidden states for a given observation sequence. For instance in speech\nrecognition, we might wish to ﬁnd the most probable phoneme sequence for a given\nseries of acoustic observations. Because the graph for the hidden Markov model is\na directed tree, this problem can be solved exactly using the max-sum algorithm.\nWe recall from our discussion in Section 8.4.5 that the problem of ﬁnding the most\nprobable sequence of latent states is not the same as that of ﬁnding the set of states",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 648,
      "page_label": "629"
    }
  },
  {
    "page_content": "probable sequence of latent states is not the same as that of ﬁnding the set of states\nthat are individually the most probable. The latter problem can be solved by ﬁrst\nrunning the forward-backward (sum-product) algorithm to ﬁnd the latent variable\nmarginals γ(zn) and then maximizing each of these individually (Dudaet al., 2001).\nHowever, the set of such states will not, in general, correspond to the most probable\nsequence of states. In fact, this set of states might even represent a sequence having\nzero probability, if it so happens that two successive states, which in isolation are\nindividually the most probable, are such that the transition matrix element connecting\nthem is zero.\nIn practice, we are usually interested in ﬁnding the most probable sequence of\nstates, and this can be solved efﬁciently using the max-sum algorithm, which in the\ncontext of hidden Markov models is known as the Viterbi algorithm (Viterbi, 1967).\nNote that the max-sum algorithm works with log probabilities and so there is no\nneed to use re-scaled variables as was done with the forward-backward algorithm.\nFigure 13.16 shows a fragment of the hidden Markov model expanded as lattice\ndiagram. As we have already noted, the number of possible paths through the lattice\ngrows exponentially with the length of the chain. The Viterbi algorithm searches this\nspace of paths efﬁciently to ﬁnd the most probable path with a computational cost\nthat grows only linearly with the length of the chain.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 648,
      "page_label": "629"
    }
  },
  {
    "page_content": "space of paths efﬁciently to ﬁnd the most probable path with a computational cost\nthat grows only linearly with the length of the chain.\nAs with the sum-product algorithm, we ﬁrst represent the hidden Markov model\nas a factor graph, as shown in Figure 13.15. Again, we treat the variable node zN\nas the root, and pass messages to the root starting with the leaf nodes. Using the\nresults (8.93) and (8.94), we see that the messages passed in the max-sum algorithm\nare given by\nµzn→fn+1 (zn)= µfn→zn (zn) (13.66)\nµfn+1→zn+1 (zn+1) = max\nzn\n{\nlnfn+1(zn, zn+1)+ µzn→fn+1 (zn)\n}\n. (13.67)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 648,
      "page_label": "629"
    }
  },
  {
    "page_content": "630 13. SEQUENTIAL DATA\nFigure 13.16 A fragment of the HMM lattice\nshowing two possible paths. The Viterbi algorithm\nefﬁciently determines the most probable path from\namongst the exponentially many possibilities. For\nany given path, the corresponding probability is\ngiven by the product of the elements of the tran-\nsition matrix Ajk, corresponding to the probabil-\nities p(zn+1|zn) for each segment of the path,\nalong with the emission densities p(xn|k) asso-\nciated with each node on the path.\nk =1\nk =2\nk =3\nn − 2 n − 1 nn +1\nIf we eliminateµzn→fn+1 (zn) between these two equations, and make use of (13.46),\nwe obtain a recursion for the f → z messages of the form\nω(zn+1)=l n p(xn+1|zn+1) + max\nzn\n{lnp(x+1|zn)+ ω(zn)} (13.68)\nwhere we have introduced the notation ω(zn) ≡ µfn→zn (zn).\nFrom (8.95) and (8.96), these messages are initialized using\nω(z1)=l n p(z1)+l n p(x1|z1). (13.69)\nwhere we have used (13.45). Note that to keep the notation uncluttered, we omit\nthe dependence on the model parameters θ that are held ﬁxed when ﬁnding the most\nprobable sequence.\nThe Viterbi algorithm can also be derived directly from the deﬁnition (13.6) of\nthe joint distribution by taking the logarithm and then exchanging maximizations\nand summations. It is easily seen that the quantities ω(zn) have the probabilisticExercise 13.16\ninterpretation\nω(zn)= m a x\nz1,...,zn−1\np(x1,..., xn, z1,..., zn). (13.70)\nOnce we have completed the ﬁnal maximization over zN , we will obtain the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 649,
      "page_label": "630"
    }
  },
  {
    "page_content": "interpretation\nω(zn)= m a x\nz1,...,zn−1\np(x1,..., xn, z1,..., zn). (13.70)\nOnce we have completed the ﬁnal maximization over zN , we will obtain the\nvalue of the joint distribution p(X, Z) corresponding to the most probable path. We\nalso wish to ﬁnd the sequence of latent variable values that corresponds to this path.\nTo do this, we simply make use of the back-tracking procedure discussed in Sec-\ntion 8.4.5. Speciﬁcally, we note that the maximization over zn must be performed\nfor each of the K possible values of zn+1. Suppose we keep a record of the values\nof zn that correspond to the maxima for each value of the K values of zn+1. Let us\ndenote this function by ψ(kn) where k ∈{ 1,...,K }. Once we have passed mes-\nsages to the end of the chain and found the most probable state of zN , we can then\nuse this function to backtrack along the chain by applying it recursively\nkmax\nn = ψ(kmax\nn+1). (13.71)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 649,
      "page_label": "630"
    }
  },
  {
    "page_content": "13.2. Hidden Markov Models 631\nIntuitively, we can understand the Viterbi algorithm as follows. Naively, we\ncould consider explicitly all of the exponentially many paths through the lattice,\nevaluate the probability for each, and then select the path having the highest proba-\nbility. However, we notice that we can make a dramatic saving in computational cost\nas follows. Suppose that for each path we evaluate its probability by summing up\nproducts of transition and emission probabilities as we work our way forward along\neach path through the lattice. Consider a particular time step n and a particular state\nk at that time step. There will be many possible paths converging on the correspond-\ning node in the lattice diagram. However, we need only retain that particular path\nthat so far has the highest probability. Because there are K states at time step n,w e\nneed to keep track of K such paths. At time step n +1 , there will be K2 possible\npaths to consider, comprising K possible paths leading out of each of the K current\nstates, but again we need only retain K of these corresponding to the best path for\neach state at timen+1. When we reach the ﬁnal time stepN we will discover which\nstate corresponds to the overall most probable path. Because there is a unique path\ncoming into that state we can trace the path back to step N − 1 to see what state it\noccupied at that time, and so on back through the lattice to the state n =1 .\n13.2.6 Extensions of the hidden Markov model",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 650,
      "page_label": "631"
    }
  },
  {
    "page_content": "occupied at that time, and so on back through the lattice to the state n =1 .\n13.2.6 Extensions of the hidden Markov model\nThe basic hidden Markov model, along with the standard training algorithm\nbased on maximum likelihood, has been extended in numerous ways to meet the\nrequirements of particular applications. Here we discuss a few of the more important\nexamples.\nWe see from the digits example in Figure 13.11 that hidden Markov models can\nbe quite poor generative models for the data, because many of the synthetic digits\nlook quite unrepresentative of the training data. If the goal is sequence classiﬁca-\ntion, there can be signiﬁcant beneﬁt in determining the parameters of hidden Markov\nmodels using discriminative rather than maximum likelihood techniques. Suppose\nwe have a training set of R observation sequences Xr, where r =1 ,...,R , each of\nwhich is labelled according to its class m, where m =1 ,...,M . For each class, we\nhave a separate hidden Markov model with its own parameters θm, and we treat the\nproblem of determining the parameter values as a standard classiﬁcation problem in\nwhich we optimize the cross-entropy\nR∑\nr=1\nln p(mr|Xr). (13.72)\nUsing Bayes’ theorem this can be expressed in terms of the sequence probabilities\nassociated with the hidden Markov models\nR∑\nr=1\nln\n{\np(Xr|θr)p(mr)\n∑ M\nl=1 p(Xr|θl)p(lr)\n}\n(13.73)\nwhere p(m) is the prior probability of class m. Optimization of this cost function",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 650,
      "page_label": "631"
    }
  },
  {
    "page_content": "R∑\nr=1\nln\n{\np(Xr|θr)p(mr)\n∑ M\nl=1 p(Xr|θl)p(lr)\n}\n(13.73)\nwhere p(m) is the prior probability of class m. Optimization of this cost function\nis more complex than for maximum likelihood (Kapadia, 1998), and in particular",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 650,
      "page_label": "631"
    }
  },
  {
    "page_content": "632 13. SEQUENTIAL DATA\nFigure 13.17 Section of an autoregressive hidden\nMarkov model, in which the distribution\nof the observation xn depends on a\nsubset of the previous observations as\nwell as on the hidden state zn. In this\nexample, the distribution ofxn depends\non the two previous observations xn−1\nand xn−2.\nzn−1 zn zn+1\nxn−1 xn xn+1\nrequires that every training sequence be evaluated under each of the models in or-\nder to compute the denominator in (13.73). Hidden Markov models, coupled with\ndiscriminative training methods, are widely used in speech recognition (Kapadia,\n1998).\nA signiﬁcant weakness of the hidden Markov model is the way in which it rep-\nresents the distribution of times for which the system remains in a given state. To see\nthe problem, note that the probability that a sequence sampled from a given hidden\nMarkov model will spend precisely T steps in state k and then make a transition to a\ndifferent state is given by\np(T)=( Akk)T (1 − Akk) ∝ exp (−T lnAkk) (13.74)\nand so is an exponentially decaying function of T. For many applications, this will\nbe a very unrealistic model of state duration. The problem can be resolved by mod-\nelling state duration directly in which the diagonal coefﬁcientsAkk are all set to zero,\nand each state k is explicitly associated with a probability distributionp(T|k) of pos-\nsible duration times. From a generative point of view, when a state k is entered, a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 651,
      "page_label": "632"
    }
  },
  {
    "page_content": "sible duration times. From a generative point of view, when a state k is entered, a\nvalue T representing the number of time steps that the system will remain in state k\nis then drawn from p(T|k). The model then emits T values of the observed variable\nxt, which are generally assumed to be independent so that the corresponding emis-\nsion density is simply ∏T\nt=1 p(xt|k). This approach requires some straightforward\nmodiﬁcations to the EM optimization procedure (Rabiner, 1989).\nAnother limitation of the standard HMM is that it is poor at capturing long-\nrange correlations between the observed variables (i.e., between variables that are\nseparated by many time steps) because these must be mediated via the ﬁrst-order\nMarkov chain of hidden states. Longer-range effects could in principle be included\nby adding extra links to the graphical model of Figure 13.5. One way to address this\nis to generalize the HMM to give theautoregressive hidden Markov model(Ephraim\net al., 1989), an example of which is shown in Figure 13.17. For discrete observa-\ntions, this corresponds to expanded tables of conditional probabilities for the emis-\nsion distributions. In the case of a Gaussian emission density, we can use the linear-\nGaussian framework in which the conditional distribution for xn given the values\nof the previous observations, and the value of zn, is a Gaussian whose mean is a\nlinear combination of the values of the conditioning variables. Clearly the number",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 651,
      "page_label": "632"
    }
  },
  {
    "page_content": "linear combination of the values of the conditioning variables. Clearly the number\nof additional links in the graph must be limited to avoid an excessive the number of\nfree parameters. In the example shown in Figure 13.17, each observation depends on",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 651,
      "page_label": "632"
    }
  },
  {
    "page_content": "13.2. Hidden Markov Models 633\nFigure 13.18 Example of an input-output hidden\nMarkov model. In this case, both the\nemission probabilities and the transition\nprobabilities depend on the values of a\nsequence of observations u1,..., uN .\nzn−1 zn zn+1\nxn−1 xn xn+1\nun−1 un un+1\nthe two preceding observed variables as well as on the hidden state. Although this\ngraph looks messy, we can again appeal to d-separation to see that in fact it still has\na simple probabilistic structure. In particular, if we imagine conditioning on zn we\nsee that, as with the standard HMM, the values of zn−1 and zn+1 are independent,\ncorresponding to the conditional independence property (13.5). This is easily veri-\nﬁed by noting that every path from node zn−1 to node zn+1 passes through at least\none observed node that is head-to-tail with respect to that path. As a consequence,\nwe can again use a forward-backward recursion in the E step of the EM algorithm to\ndetermine the posterior distributions of the latent variables in a computational time\nthat is linear in the length of the chain. Similarly, the M step involves only a minor\nmodiﬁcation of the standard M-step equations. In the case of Gaussian emission\ndensities this involves estimating the parameters using the standard linear regression\nequations, discussed in Chapter 3.\nWe have seen that the autoregressive HMM appears as a natural extension of the\nstandard HMM when viewed as a graphical model. In fact the probabilistic graphical",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 652,
      "page_label": "633"
    }
  },
  {
    "page_content": "standard HMM when viewed as a graphical model. In fact the probabilistic graphical\nmodelling viewpoint motivates a plethora of different graphical structures based on\nthe HMM. Another example is the input-output hidden Markov model (Bengio and\nFrasconi, 1995), in which we have a sequence of observed variables u1,..., uN ,i n\naddition to the output variables x1,..., xN , whose values inﬂuence either the dis-\ntribution of latent variables or output variables, or both. An example is shown in\nFigure 13.18. This extends the HMM framework to the domain of supervised learn-\ning for sequential data. It is again easy to show, through the use of the d-separation\ncriterion, that the Markov property (13.5) for the chain of latent variables still holds.\nTo verify this, simply note that there is only one path from node zn−1 to node zn+1\nand this is head-to-tail with respect to the observed node zn. This conditional inde-\npendence property again allows the formulation of a computationally efﬁcient learn-\ning algorithm. In particular, we can determine the parameters θ of the model by\nmaximizing the likelihood function L(θ)= p(X|U, θ) where U is a matrix whose\nrows are given by uT\nn. As a consequence of the conditional independence property\n(13.5) this likelihood function can be maximized efﬁciently using an EM algorithm\nin which the E step involves forward and backward recursions.Exercise 13.18\nAnother variant of the HMM worthy of mention is the factorial hidden Markov",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 652,
      "page_label": "633"
    }
  },
  {
    "page_content": "Another variant of the HMM worthy of mention is the factorial hidden Markov\nmodel (Ghahramani and Jordan, 1997), in which there are multiple independent",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 652,
      "page_label": "633"
    }
  },
  {
    "page_content": "634 13. SEQUENTIAL DATA\nFigure 13.19 A factorial hidden Markov model com-\nprising two Markov chains of latent vari-\nables. For continuous observed variables\nx, one possible choice of emission model\nis a linear-Gaussian density in which the\nmean of the Gaussian is a linear combi-\nnation of the states of the corresponding\nlatent variables.\nz(1)\nn−1 z(1)\nn z(1)\nn+1\nz(2)\nn−1 z(2)\nn z(2)\nn+1\nxn−1 xn xn+1\nMarkov chains of latent variables, and the distribution of the observed variable at\na given time step is conditional on the states of all of the corresponding latent vari-\nables at that same time step. Figure 13.19 shows the corresponding graphical model.\nThe motivation for considering factorial HMM can be seen by noting that in order to\nrepresent, say, 10 bits of information at a given time step, a standard HMM would\nneed K =2 10 = 1024 latent states, whereas a factorial HMM could make use of 10\nbinary latent chains. The primary disadvantage of factorial HMMs, however, lies in\nthe additional complexity of training them. The M step for the factorial HMM model\nis straightforward. However, observation of thex variables introduces dependencies\nbetween the latent chains, leading to difﬁculties with the E step. This can be seen\nby noting that in Figure 13.19, the variables z(1)\nn and z(2)\nn are connected by a path\nwhich is head-to-head at node xn and hence they are not d-separated. The exact E\nstep for this model does not correspond to running forward and backward recursions",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 653,
      "page_label": "634"
    }
  },
  {
    "page_content": "step for this model does not correspond to running forward and backward recursions\nalong the M Markov chains independently. This is conﬁrmed by noting that the key\nconditional independence property (13.5) is not satisﬁed for the individual Markov\nchains in the factorial HMM model, as is shown using d-separation in Figure 13.20.\nNow suppose that there are M chains of hidden nodes and for simplicity suppose\nthat all latent variables have the same numberK of states. Then one approach would\nbe to note that there are KM combinations of latent variables at a given time step\nFigure 13.20 Example of a path, highlighted in green,\nwhich is head-to-head at the observed\nnodes xn−1 and xn+1, and head-to-tail\nat the unobserved nodes z(2)\nn−1, z(2)\nn and\nz(2)\nn+1. Thus the path is not blocked and\nso the conditional independence property\n(13.5) does not hold for the individual la-\ntent chains of the factorial HMM model.\nAs a consequence, there is no efﬁcient\nexact E step for this model.\nz(1)\nn−1 z(1)\nn z(1)\nn+1\nz(2)\nn−1 z(2)\nn z(2)\nn+1\nxn−1 xn xn+1",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 653,
      "page_label": "634"
    }
  },
  {
    "page_content": "13.3. Linear Dynamical Systems 635\nand so we can transform the model into an equivalent standard HMM having a single\nchain of latent variables each of which has KM latent states. We can then run the\nstandard forward-backward recursions in the E step. This has computational com-\nplexity O(NK 2M ) that is exponential in the number M of latent chains and so will\nbe intractable for anything other than small values of M. One solution would be\nto use sampling methods (discussed in Chapter 11). As an elegant deterministic al-\nternative, Ghahramani and Jordan (1997) exploited variational inference techniquesSection 10.1\nto obtain a tractable algorithm for approximate inference. This can be done using\na simple variational posterior distribution that is fully factorized with respect to the\nlatent variables, or alternatively by using a more powerful approach in which the\nvariational distribution is described by independent Markov chains corresponding to\nthe chains of latent variables in the original model. In the latter case, the variational\ninference algorithms involves running independent forward and backward recursions\nalong each chain, which is computationally efﬁcient and yet is also able to capture\ncorrelations between variables within the same chain.\nClearly, there are many possible probabilistic structures that can be constructed\naccording to the needs of particular applications. Graphical models provide a general",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 654,
      "page_label": "635"
    }
  },
  {
    "page_content": "according to the needs of particular applications. Graphical models provide a general\ntechnique for motivating, describing, and analysing such structures, and variational\nmethods provide a powerful framework for performing inference in those models for\nwhich exact solution is intractable.\n13.3. Linear Dynamical Systems\nIn order to motivate the concept of linear dynamical systems, let us consider the\nfollowing simple problem, which often arises in practical settings. Suppose we wish\nto measure the value of an unknown quantity z using a noisy sensor that returns a\nobservation x representing the value of z plus zero-mean Gaussian noise. Given a\nsingle measurement, our best guess for z is to assume that z = x. However, we\ncan improve our estimate for z by taking lots of measurements and averaging them,\nbecause the random noise terms will tend to cancel each other. Now let’s make the\nsituation more complicated by assuming that we wish to measure a quantityz that\nis changing over time. We can take regular measurements of x so that at some point\nin time we have obtained x1,..., xN and we wish to ﬁnd the corresponding values\nz1,..., xN . If we simply average the measurements, the error due to random noise\nwill be reduced, but unfortunately we will just obtain a single averaged estimate, in\nwhich we have averaged over the changing value ofz, thereby introducing a new\nsource of error.\nIntuitively, we could imagine doing a bit better as follows. To estimate the value",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 654,
      "page_label": "635"
    }
  },
  {
    "page_content": "source of error.\nIntuitively, we could imagine doing a bit better as follows. To estimate the value\nof zN , we take only the most recent few measurements, say xN−L,..., xN and just\naverage these. If z is changing slowly, and the random noise level in the sensor is\nhigh, it would make sense to choose a relatively long window of observations to\naverage. Conversely, if the signal is changing quickly, and the noise levels are small,\nwe might be better just to use xN directly as our estimate of zN . Perhaps we could\ndo even better if we take a weighted average, in which more recent measurements",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 654,
      "page_label": "635"
    }
  },
  {
    "page_content": "636 13. SEQUENTIAL DATA\nmake a greater contribution than less recent ones.\nAlthough this sort of intuitive argument seems plausible, it does not tell us how\nto form a weighted average, and any sort of hand-crafted weighing is hardly likely\nto be optimal. Fortunately, we can address problems such as this much more sys-\ntematically by deﬁning a probabilistic model that captures the time evolution and\nmeasurement processes and then applying the inference and learning methods devel-\noped in earlier chapters. Here we shall focus on a widely used model known as a\nlinear dynamical system.\nAs we have seen, the HMM corresponds to the state space model shown in\nFigure 13.5 in which the latent variables are discrete but with arbitrary emission\nprobability distributions. This graph of course describes a much broader class of\nprobability distributions, all of which factorize according to (13.6). We now consider\nextensions to other distributions for the latent variables. In particular, we consider\ncontinuous latent variables in which the summations of the sum-product algorithm\nbecome integrals. The general form of the inference algorithms will, however, be\nthe same as for the hidden Markov model. It is interesting to note that, historically,\nhidden Markov models and linear dynamical systems were developed independently.\nOnce they are both expressed as graphical models, however, the deep relationship\nbetween them immediately becomes apparent.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 655,
      "page_label": "636"
    }
  },
  {
    "page_content": "Once they are both expressed as graphical models, however, the deep relationship\nbetween them immediately becomes apparent.\nOne key requirement is that we retain an efﬁcient algorithm for inference which\nis linear in the length of the chain. This requires that, for instance, when we take\na quantity ˆα(zn−1), representing the posterior probability of zn given observations\nx1,..., xn, and multiply by the transition probability p(zn|zn−1) and the emission\nprobability p(xn|zn) and then marginalize over zn−1, we obtain a distribution over\nzn that is of the same functional form as that over ˆα(zn−1). That is to say, the\ndistribution must not become more complex at each stage, but must only change in\nits parameter values. Not surprisingly, the only distributions that have this property\nof being closed under multiplication are those belonging to the exponential family.\nHere we consider the most important example from a practical perspective,\nwhich is the Gaussian. In particular, we consider a linear-Gaussian state space model\nso that the latent variables{zn}, as well as the observed variables {xn}, are multi-\nvariate Gaussian distributions whose means are linear functions of the states of their\nparents in the graph. We have seen that a directed graph of linear-Gaussian units\nis equivalent to a joint Gaussian distribution over all of the variables. Furthermore,\nmarginals such as ˆα(zn) are also Gaussian, so that the functional form of the mes-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 655,
      "page_label": "636"
    }
  },
  {
    "page_content": "marginals such as ˆα(zn) are also Gaussian, so that the functional form of the mes-\nsages is preserved and we will obtain an efﬁcient inference algorithm. By contrast,\nsuppose that the emission densitiesp(xn|zn) comprise a mixture of K Gaussians\neach of which has a mean that is linear in zn. Then even if ˆα(z1) is Gaussian, the\nquantity ˆα(z2) will be a mixture of K Gaussians, ˆα(z3) will be a mixture of K2\nGaussians, and so on, and exact inference will not be of practical value.\nWe have seen that the hidden Markov model can be viewed as an extension of\nthe mixture models of Chapter 9 to allow for sequential correlations in the data.\nIn a similar way, we can view the linear dynamical system as a generalization of the\ncontinuous latent variable models of Chapter 12 such as probabilistic PCA and factor\nanalysis. Each pair of nodes {zn, xn} represents a linear-Gaussian latent variable",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 655,
      "page_label": "636"
    }
  },
  {
    "page_content": "13.3. Linear Dynamical Systems 637\nmodel for that particular observation. However, the latent variables {zn} are no\nlonger treated as independent but now form a Markov chain.\nBecause the model is represented by a tree-structured directed graph, inference\nproblems can be solved efﬁciently using the sum-product algorithm. The forward re-\ncursions, analogous to theα messages of the hidden Markov model, are known as the\nKalman ﬁlter equations (Kalman, 1960; Zarchan and Musoff, 2005), and the back-\nward recursions, analogous to the β messages, are known as the Kalman smoother\nequations, or the Rauch-Tung-Striebel (RTS) equations (Rauch et al., 1965). The\nKalman ﬁlter is widely used in many real-time tracking applications.\nBecause the linear dynamical system is a linear-Gaussian model, the joint distri-\nbution over all variables, as well as all marginals and conditionals, will be Gaussian.\nIt follows that the sequence of individually most probable latent variable values is\nthe same as the most probable latent sequence. There is thus no need to consider theExercise 13.19\nanalogue of the Viterbi algorithm for the linear dynamical system.\nBecause the model has linear-Gaussian conditional distributions, we can write\nthe transition and emission distributions in the general form\np(zn|zn−1)= N(zn|Azn−1, Γ) (13.75)\np(xn|zn)= N(xn|Czn, Σ). (13.76)\nThe initial latent variable also has a Gaussian distribution which we write as\np(z1)= N(z1|µ0, V0). (13.77)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 656,
      "page_label": "637"
    }
  },
  {
    "page_content": "p(xn|zn)= N(xn|Czn, Σ). (13.76)\nThe initial latent variable also has a Gaussian distribution which we write as\np(z1)= N(z1|µ0, V0). (13.77)\nNote that in order to simplify the notation, we have omitted additive constant terms\nfrom the means of the Gaussians. In fact, it is straightforward to include them if\ndesired. Traditionally, these distributions are more commonly expressed in an equiv-Exercise 13.24\nalent form in terms of noisy linear equations given by\nzn = Azn−1 + wn (13.78)\nxn = Czn + vn (13.79)\nz1 = µ0 + u (13.80)\nwhere the noise terms have the distributions\nw ∼N (w|0,Γ) (13.81)\nv ∼N (v|0,Σ) (13.82)\nu ∼N (u|0,V0). (13.83)\nThe parameters of the model, denoted by θ = {A, Γ, C, Σ, µ0, V0}, can be\ndetermined using maximum likelihood through the EM algorithm. In the E step, we\nneed to solve the inference problem of determining the local posterior marginals for\nthe latent variables, which can be solved efﬁciently using the sum-product algorithm,\nas we discuss in the next section.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 656,
      "page_label": "637"
    }
  },
  {
    "page_content": "638 13. SEQUENTIAL DATA\n13.3.1 Inference in LDS\nWe now turn to the problem of ﬁnding the marginal distributions for the latent\nvariables conditional on the observation sequence. For given parameter settings, we\nalso wish to make predictions of the next latent state zn and of the next observation\nxn conditioned on the observed data x1,..., xn−1 for use in real-time applications.\nThese inference problems can be solved efﬁciently using the sum-product algorithm,\nwhich in the context of the linear dynamical system gives rise to the Kalman ﬁlter\nand Kalman smoother equations.\nIt is worth emphasizing that because the linear dynamical system is a linear-\nGaussian model, the joint distribution over all latent and observed variables is simply\na Gaussian, and so in principle we could solve inference problems by using the\nstandard results derived in previous chapters for the marginals and conditionals of a\nmultivariate Gaussian. The role of the sum-product algorithm is to provide a more\nefﬁcient way to perform such computations.\nLinear dynamical systems have the identical factorization, given by (13.6), to\nhidden Markov models, and are again described by the factor graphs in Figures 13.14\nand 13.15. Inference algorithms therefore take precisely the same form except that\nsummations over latent variables are replaced by integrations. We begin by consid-\nering the forward equations in which we treat zN as the root node, and propagate",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 657,
      "page_label": "638"
    }
  },
  {
    "page_content": "ering the forward equations in which we treat zN as the root node, and propagate\nmessages from the leaf nodeh(z1) to the root. From (13.77), the initial message will\nbe Gaussian, and because each of the factors is Gaussian, all subsequent messages\nwill also be Gaussian. By convention, we shall propagate messages that are nor-\nmalized marginal distributions corresponding top(zn|x1,..., xn), which we denote\nby\nˆα(zn)= N(zn|µn, Vn). (13.84)\nThis is precisely analogous to the propagation of scaled variables ˆα(zn) given by\n(13.59) in the discrete case of the hidden Markov model, and so the recursion equa-\ntion now takes the form\ncnˆα(zn)= p(xn|zn)\n∫\nˆα(zn−1)p(zn|zn−1)d zn−1. (13.85)\nSubstituting for the conditionalsp(zn|zn−1) and p(xn|zn), using (13.75) and (13.76),\nrespectively, and making use of (13.84), we see that (13.85) becomes\ncnN(zn|µn, Vn)= N(xn|Czn, Σ)∫\nN(zn|Azn−1, Γ)N(zn−1|µn−1, Vn−1)d zn−1. (13.86)\nHere we are supposing that µn−1 and Vn−1 are known, and by evaluating the inte-\ngral in (13.86), we wish to determine values for µn and Vn. The integral is easily\nevaluated by making use of the result (2.115), from which it follows that\n∫\nN(zn|Azn−1, Γ)N(zn−1|µn−1, Vn−1)d zn−1\n= N(zn|Aµn−1, Pn−1) (13.87)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 657,
      "page_label": "638"
    }
  },
  {
    "page_content": "13.3. Linear Dynamical Systems 639\nwhere we have deﬁned\nPn−1 = AVn−1AT + Γ. (13.88)\nWe can now combine this result with the ﬁrst factor on the right-hand side of (13.86)\nby making use of (2.115) and (2.116) to give\nµn = Aµn−1 + Kn(xn − CAµn−1) (13.89)\nVn =( I − KnC)Pn−1 (13.90)\ncn = N(xn|CAµn−1, CPn−1CT + Σ). (13.91)\nHere we have made use of the matrix inverse identities (C.5) and (C.7) and also\ndeﬁned the Kalman gain matrix\nKn = Pn−1CT (\nCPn−1CT + Σ\n)−1\n. (13.92)\nThus, given the values of µn−1 and Vn−1, together with the new observation xn,\nwe can evaluate the Gaussian marginal for zn having mean µn and covariance Vn,\nas well as the normalization coefﬁcient cn.\nThe initial conditions for these recursion equations are obtained from\nc1ˆα(z1)= p(z1)p(x1|z1). (13.93)\nBecause p(z1) is given by (13.77), and p(x1|z1) is given by (13.76), we can again\nmake use of (2.115) to calculate c1 and (2.116) to calculate µ1 and V1 giving\nµ1 = µ0 + K1(x1 − Cµ0) (13.94)\nV1 =( I − K1C)V0 (13.95)\nc1 = N(x1|Cµ0, CV0CT + Σ) (13.96)\nwhere\nK1 = V0CT (\nCV0CT + Σ\n)−1\n. (13.97)\nSimilarly, the likelihood function for the linear dynamical system is given by (13.63)\nin which the factors cn are found using the Kalman ﬁltering equations.\nWe can interpret the steps involved in going from the posterior marginal over\nzn−1 to the posterior marginal over zn as follows. In (13.89), we can view the\nquantity Aµn−1 as the prediction of the mean overzn obtained by simply taking the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 658,
      "page_label": "639"
    }
  },
  {
    "page_content": "quantity Aµn−1 as the prediction of the mean overzn obtained by simply taking the\nmean over zn−1 and projecting it forward one step using the transition probability\nmatrix A. This predicted mean would give a predicted observation for xn given by\nCAzn−1 obtained by applying the emission probability matrix C to the predicted\nhidden state mean. We can view the update equation (13.89) for the mean of the\nhidden variable distribution as taking the predicted mean Aµn−1 and then adding\na correction that is proportional to the error xn − CAzn−1 between the predicted\nobservation and the actual observation. The coefﬁcient of this correction is given by\nthe Kalman gain matrix. Thus we can view the Kalman ﬁlter as a process of making\nsuccessive predictions and then correcting these predictions in the light of the new\nobservations. This is illustrated graphically in Figure 13.21.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 658,
      "page_label": "639"
    }
  },
  {
    "page_content": "640 13. SEQUENTIAL DATA\nzn−1 zn zn\nFigure 13.21 The linear dynamical system can be viewed as a sequence of steps in which increasing un-\ncertainty in the state variable due to diffusion is compensated by the arrival of new data. In the left-hand plot,\nthe blue curve shows the distribution p(zn−1|x1,..., xn−1), which incorporates all the data up to step n − 1.\nThe diffusion arising from the nonzero variance of the transition probability p(zn|zn−1) gives the distribution\np(zn|x1,..., xn−1), shown in red in the centre plot. Note that this is broader and shifted relative to the blue curve\n(which is shown dashed in the centre plot for comparison). The next data observationxn contributes through the\nemission density p(xn|zn), which is shown as a function ofzn in green on the right-hand plot. Note that this is not\na density with respect to zn and so is not normalized to one. Inclusion of this new data point leads to a revised\ndistribution p(zn|x1,..., xn) for the state density shown in blue. We see that observation of the data has shifted\nand narrowed the distribution compared to p(zn|x1,..., xn−1) (which is shown in dashed in the right-hand plot\nfor comparison).\nIf we consider a situation in which the measurement noise is small compared\nto the rate at which the latent variable is evolving, then we ﬁnd that the posterior\ndistribution for zn depends only on the current measurement xn, in accordance withExercise 13.27",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 659,
      "page_label": "640"
    }
  },
  {
    "page_content": "distribution for zn depends only on the current measurement xn, in accordance withExercise 13.27\nthe intuition from our simple example at the start of the section. Similarly, if the\nlatent variable is evolving slowly relative to the observation noise level, we ﬁnd that\nthe posterior mean for zn is obtained by averaging all of the measurements obtained\nup to that time.Exercise 13.28\nOne of the most important applications of the Kalman ﬁlter is to tracking, and\nthis is illustrated using a simple example of an object moving in two dimensions in\nFigure 13.22.\nSo far, we have solved the inference problem of ﬁnding the posterior marginal\nfor a node zn given observations from x1 up to xn. Next we turn to the problem of\nﬁnding the marginal for a node zn given all observations x1 to xN . For temporal\ndata, this corresponds to the inclusion of future as well as past observations. Al-\nthough this cannot be used for real-time prediction, it plays a key role in learning the\nparameters of the model. By analogy with the hidden Markov model, this problem\ncan be solved by propagating messages from node xN back to node x1 and com-\nbining this information with that obtained during the forward message passing stage\nused to compute the ˆα(zn).\nIn the LDS literature, it is usual to formulate this backward recursion in terms\nof γ(zn)= ˆα(zn)ˆβ(zn) rather than in terms of ˆβ(zn). Because γ(zn) must also be\nGaussian, we write it in the form\nγ(zn)= ˆα(zn)ˆβ(zn)= N(zn|ˆµn, ˆVn). (13.98)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 659,
      "page_label": "640"
    }
  },
  {
    "page_content": "Gaussian, we write it in the form\nγ(zn)= ˆα(zn)ˆβ(zn)= N(zn|ˆµn, ˆVn). (13.98)\nTo derive the required recursion, we start from the backward recursion (13.62) for",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 659,
      "page_label": "640"
    }
  },
  {
    "page_content": "13.3. Linear Dynamical Systems 641\nFigure 13.22 An illustration of a linear dy-\nnamical system being used to\ntrack a moving object. The blue\npoints indicate the true positions\nof the object in a two-dimensional\nspace at successive time steps,\nthe green points denote noisy\nmeasurements of the positions,\nand the red crosses indicate the\nmeans of the inferred posterior\ndistributions of the positions ob-\ntained by running the Kalman ﬁl-\ntering equations. The covari-\nances of the inferred positions\nare indicated by the red ellipses,\nwhich correspond to contours\nhaving one standard deviation.\nˆβ(zn), which, for continuous latent variables, can be written in the form\ncn+1ˆβ(zn)=\n∫\nˆβ(zn+1)p(xn+1|zn+1)p(zn+1|zn)d zn+1. (13.99)\nWe now multiply both sides of (13.99) by ˆα(zn) and substitute for p(xn+1|zn+1)\nand p(zn+1|zn) using (13.75) and (13.76). Then we make use of (13.89), (13.90)\nand (13.91), together with (13.98), and after some manipulation we obtainExercise 13.29\nˆµn = µn + Jn\n(ˆµn+1 − AµN\n)\n(13.100)\nˆVn = Vn + Jn\n(\nˆVn+1 − Pn\n)\nJT\nn (13.101)\nwhere we have deﬁned\nJn = VnAT (Pn)−1 (13.102)\nand we have made use of AVn = PnJT\nn. Note that these recursions require that the\nforward pass be completed ﬁrst so that the quantities µn and Vn will be available\nfor the backward pass.\nFor the EM algorithm, we also require the pairwise posterior marginals, which\ncan be obtained from (13.65) in the form\nξ(zn−1, zn)=( cn)−1 ˆα(zn−1)p(xn|zn)p(zn|z−1)ˆβ(zn)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 660,
      "page_label": "641"
    }
  },
  {
    "page_content": "can be obtained from (13.65) in the form\nξ(zn−1, zn)=( cn)−1 ˆα(zn−1)p(xn|zn)p(zn|z−1)ˆβ(zn)\n= N(zn−1|µn−1, Vn−1)N(zn|Azn−1, Γ)N(xn|Czn, Σ)N(zn|ˆµn, ˆVn)\ncnˆα(zn) .\n(13.103)\nSubstituting for ˆα(zn) using (13.84) and rearranging, we see that ξ(zn−1, zn) is a\nGaussian with mean given with components γ(zn−1) and γ(zn), and a covariance\nbetween zn and zn−1 given byExercise 13.31\ncov[zn, zn−1]= Jn−1 ˆVn. (13.104)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 660,
      "page_label": "641"
    }
  },
  {
    "page_content": "642 13. SEQUENTIAL DATA\n13.3.2 Learning in LDS\nSo far, we have considered the inference problem for linear dynamical systems,\nassuming that the model parametersθ = {A, Γ, C, Σ, µ0, V0} are known. Next, we\nconsider the determination of these parameters using maximum likelihood (Ghahra-\nmani and Hinton, 1996b). Because the model has latent variables, this can be ad-\ndressed using the EM algorithm, which was discussed in general terms in Chapter 9.\nWe can derive the EM algorithm for the linear dynamical system as follows. Let\nus denote the estimated parameter values at some particular cycle of the algorithm\nby θold. For these parameter values, we can run the inference algorithm to determine\nthe posterior distribution of the latent variablesp(Z|X, θold), or more precisely those\nlocal posterior marginals that are required in the M step. In particular, we shall\nrequire the following expectations\nE [zn]= ˆµn (13.105)\nE\n[\nznzT\nn−1\n]\n= Jn−1 ˆVn + ˆµnˆµT\nn−1 (13.106)\nE\n[\nznzT\nn\n]\n= ˆVn + ˆµnˆµT\nn (13.107)\nwhere we have used (13.104).\nNow we consider the complete-data log likelihood function, which is obtained\nby taking the logarithm of (13.6) and is therefore given by\nlnp(X, Z|θ)=l n p(z1|µ0, V0)+\nN∑\nn=2\nlnp(zn|zn−1, A, Γ)\n+\nN∑\nn=1\nlnp(xn|zn, C, Σ) (13.108)\nin which we have made the dependence on the parameters explicit. We now take the\nexpectation of the complete-data log likelihood with respect to the posterior distri-\nbution p(Z|X, θold) which deﬁnes the function",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 661,
      "page_label": "642"
    }
  },
  {
    "page_content": "expectation of the complete-data log likelihood with respect to the posterior distri-\nbution p(Z|X, θold) which deﬁnes the function\nQ(θ, θold)= EZ|θold [lnp(X, Z|θ)]. (13.109)\nIn the M step, this function is maximized with respect to the components of θ.\nConsider ﬁrst the parameters µ0 and V0. If we substitute for p(z1|µ0, V0) in\n(13.108) using (13.77), and then take the expectation with respect to Z, we obtain\nQ(θ, θold)= −1\n2 ln|V0|− EZ|θold\n[1\n2(z1 − µ0)TV−1\n0 (z1 − µ0)\n]\n+c o n s t\nwhere all terms not dependent on µ0 or V0 have been absorbed into the additive\nconstant. Maximization with respect to µ0 and V0 is easily performed by making\nuse of the maximum likelihood solution for a Gaussian distribution discussed in\nSection 2.3.4, givingExercise 13.32",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 661,
      "page_label": "642"
    }
  },
  {
    "page_content": "13.3. Linear Dynamical Systems 643\nµnew\n0 = E[z1] (13.110)\nVnew\n0 = E[z1zT\n1 ] − E[z1]E[zT\n1 ]. (13.111)\nSimilarly, to optimize A and Γ, we substitute for p(zn|zn−1, A, Γ) in (13.108)\nusing (13.75) giving\nQ(θ, θold)= −N − 1\n2 ln|Γ|\n−EZ|θold\n[\n1\n2\nN∑\nn=2\n(zn − Azn−1)TΓ−1(zn − Azn−1)\n]\n+c o n s t(13.112)\nin which the constant comprises terms that are independent ofA and Γ. Maximizing\nwith respect to these parameters then givesExercise 13.33\nAnew =\n( N∑\nn=2\nE\n[\nznzT\nn−1\n]\n)( N∑\nn=2\nE\n[\nzn−1zT\nn−1\n]\n) −1\n(13.113)\nΓnew = 1\nN − 1\nN∑\nn=2\n{\nE\n[\nznzT\nn\n]\n− AnewE\n[\nzn−1zT\nn\n]\n−E\n[\nznzT\nn−1\n]\nAnew + AnewE\n[\nzn−1zT\nn−1\n]\n(Anew)T\n}\n. (13.114)\nNote that Anew must be evaluated ﬁrst, and the result can then be used to determine\nΓnew.\nFinally, in order to determine the new values of C and Σ, we substitute for\np(xn|zn, C, Σ) in (13.108) using (13.76) giving\nQ(θ, θold)= −N\n2 ln|Σ|\n−EZ|θold\n[\n1\n2\nN∑\nn=1\n(xn − Czn)TΣ−1(xn − Czn)\n]\n+c o n s t.\nMaximizing with respect to C and Σ then givesExercise 13.34\nCnew =\n( N∑\nn=1\nxnE\n[\nzT\nn\n]\n)( N∑\nn=1\nE\n[\nznzT\nn\n]\n) −1\n(13.115)\nΣnew = 1\nN\nN∑\nn=1\n{\nxnxT\nn − CnewE [zn]xT\nn\n−xnE\n[\nzT\nn\n]\nCnew + CnewE\n[\nznzT\nn\n]\nCnew}\n. (13.116)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 662,
      "page_label": "643"
    }
  },
  {
    "page_content": "644 13. SEQUENTIAL DATA\nWe have approached parameter learning in the linear dynamical system using\nmaximum likelihood. Inclusion of priors to give a MAP estimate is straightforward,\nand a fully Bayesian treatment can be found by applying the analytical approxima-\ntion techniques discussed in Chapter 10, though a detailed treatment is precluded\nhere due to lack of space.\n13.3.3 Extensions of LDS\nAs with the hidden Markov model, there is considerable interest in extending\nthe basic linear dynamical system in order to increase its capabilities. Although the\nassumption of a linear-Gaussian model leads to efﬁcient algorithms for inference\nand learning, it also implies that the marginal distribution of the observed variables\nis simply a Gaussian, which represents a signiﬁcant limitation. One simple extension\nof the linear dynamical system is to use a Gaussian mixture as the initial distribution\nfor z1. If this mixture has K components, then the forward recursion equations\n(13.85) will lead to a mixture of K Gaussians over each hidden variable zn, and so\nthe model is again tractable.\nFor many applications, the Gaussian emission density is a poor approximation.\nIf instead we try to use a mixture of K Gaussians as the emission density, then the\nposterior ˆα(z1) will also be a mixture of K Gaussians. However, from (13.85) the\nposterior ˆα(z2) will comprise a mixture of K2 Gaussians, and so on, with ˆα(zn)\nbeing given by a mixture of Kn Gaussians. Thus the number of components grows",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 663,
      "page_label": "644"
    }
  },
  {
    "page_content": "being given by a mixture of Kn Gaussians. Thus the number of components grows\nexponentially with the length of the chain, and so this model is impractical.\nMore generally, introducing transition or emission models that depart from the\nlinear-Gaussian (or other exponential family) model leads to an intractable infer-\nence problem. We can make deterministic approximations such as assumed den-\nsity ﬁltering or expectation propagation, or we can make use of sampling methods,Chapter 10\nas discussed in Section 13.3.4. One widely used approach is to make a Gaussian\napproximation by linearizing around the mean of the predicted distribution, which\ngives rise to theextended Kalman ﬁlter(Zarchan and Musoff, 2005).\nAs with hidden Markov models, we can develop interesting extensions of the ba-\nsic linear dynamical system by expanding its graphical representation. For example,\ntheswitching state space model(Ghahramani and Hinton, 1998) can be viewed as\na combination of the hidden Markov model with a set of linear dynamical systems.\nThe model has multiple Markov chains of continuous linear-Gaussian latent vari-\nables, each of which is analogous to the latent chain of the linear dynamical system\ndiscussed earlier, together with a Markov chain of discrete variables of the form used\nin a hidden Markov model. The output at each time step is determined by stochas-\ntically choosing one of the continuous latent chains, using the state of the discrete",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 663,
      "page_label": "644"
    }
  },
  {
    "page_content": "tically choosing one of the continuous latent chains, using the state of the discrete\nlatent variable as a switch, and then emitting an observation from the corresponding\nconditional output distribution. Exact inference in this model is intractable, but vari-\national methods lead to an efﬁcient inference scheme involving forward-backward\nrecursions along each of the continuous and discrete Markov chains independently.\nNote that, if we consider multiple chains of discrete latent variables, and use one as\nthe switch to select from the remainder, we obtain an analogous model having only\ndiscrete latent variables known as the switching hidden Markov model.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 663,
      "page_label": "644"
    }
  },
  {
    "page_content": "13.3. Linear Dynamical Systems 645\n13.3.4 Particle ﬁlters\nFor dynamical systems which do not have a linear-Gaussian, for example, if\nthey use a non-Gaussian emission density, we can turn to sampling methods in orderChapter 11\nto ﬁnd a tractable inference algorithm. In particular, we can apply the sampling-\nimportance-resampling formalism of Section 11.1.5 to obtain a sequential Monte\nCarlo algorithm known as the particle ﬁlter.\nConsider the class of distributions represented by the graphical model in Fig-\nure 13.5, and suppose we are given the observed values Xn =( x1,..., xn) and\nwe wish to draw L samples from the posterior distribution p(zn|Xn). Using Bayes’\ntheorem, we have\nE[f(zn)] =\n∫\nf(zn)p(zn|Xn)d zn\n=\n∫\nf(zn)p(zn|xn, Xn−1)d zn\n=\n∫\nf(zn)p(xn|zn)p(zn|Xn−1)d zn\n∫\np(xn|zn)p(zn|Xn−1)d zn\n≃\nL∑\nl=1\nw(l)\nn f(z(l)\nn ) (13.117)\nwhere {z(l)\nn } is a set of samples drawn from p(zn|Xn−1) and we have made use of\nthe conditional independence property p(xn|zn, Xn−1)= p(xn|zn), which follows\nfrom the graph in Figure 13.5. The sampling weights {w(l)\nn } are deﬁned by\nw(l)\nn = p(xn|z(l)\nn )\n∑ L\nm=1 p(xn|z(m)\nn )\n(13.118)\nwhere the same samples are used in the numerator as in the denominator. Thus the\nposterior distribution p(zn|xn) is represented by the set of samples {z(l)\nn } together\nwith the corresponding weights {w(l)\nn }. Note that these weights satisfy 0 ⩽ w(l)\nn 1\nand ∑\nl w(l)\nn =1 .\nBecause we wish to ﬁnd a sequential sampling scheme, we shall suppose that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 664,
      "page_label": "645"
    }
  },
  {
    "page_content": "n }. Note that these weights satisfy 0 ⩽ w(l)\nn 1\nand ∑\nl w(l)\nn =1 .\nBecause we wish to ﬁnd a sequential sampling scheme, we shall suppose that\na set of samples and weights have been obtained at time step n, and that we have\nsubsequently observed the value of xn+1, and we wish to ﬁnd the weights and sam-\nples at time step n +1 . We ﬁrst sample from the distribution p(zn+1|Xn). This is",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 664,
      "page_label": "645"
    }
  },
  {
    "page_content": "646 13. SEQUENTIAL DATA\nstraightforward since, again using Bayes’ theorem\np(zn+1|Xn)=\n∫\np(zn+1|zn, Xn)p(zn|Xn)d zn\n=\n∫\np(zn+1|zn)p(zn|Xn)d zn\n=\n∫\np(zn+1|zn)p(zn|xn, Xn−1)d zn\n=\n∫\np(zn+1|zn)p(xn|zn)p(zn|Xn−1)d zn\n∫\np(xn|zn)p(zn|Xn−1)d zn\n=\n∑\nl\nw(l)\nn p(zn+1|z(l)\nn ) (13.119)\nwhere we have made use of the conditional independence properties\np(zn+1|zn, Xn)= p(zn+1|zn) (13.120)\np(xn|zn, Xn−1)= p(xn|zn) (13.121)\nwhich follow from the application of the d-separation criterion to the graph in Fig-\nure 13.5. The distribution given by (13.119) is a mixture distribution, and samples\ncan be drawn by choosing a componentl with probability given by the mixing coef-\nﬁcients w(l) and then drawing a sample from the corresponding component.\nIn summary, we can view each step of the particle ﬁlter algorithm as comprising\ntwo stages. At time step n, we have a sample representation of the posterior dis-\ntribution p(zn|Xn) expressed as samples {z(l)\nn } with corresponding weights {w(l)\nn }.\nThis can be viewed as a mixture representation of the form (13.119). To obtain the\ncorresponding representation for the next time step, we ﬁrst draw L samples from\nthe mixture distribution (13.119), and then for each sample we use the new obser-\nvation xn+1 to evaluate the corresponding weights w(l)\nn+1 ∝ p(xn+1|z(l)\nn+1). This is\nillustrated, for the case of a single variable z, in Figure 13.23.\nThe particle ﬁltering, or sequential Monte Carlo, approach has appeared in the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 665,
      "page_label": "646"
    }
  },
  {
    "page_content": "illustrated, for the case of a single variable z, in Figure 13.23.\nThe particle ﬁltering, or sequential Monte Carlo, approach has appeared in the\nliterature under various names including the bootstrap ﬁlter (Gordon et al., 1993),\nsurvival of the ﬁttest(Kanazawa et al., 1995), and the condensation algorithm (Isard\nand Blake, 1998).\nExercises\n13.1 (⋆) www Use the technique of d-separation, discussed in Section 8.2, to verify\nthat the Markov model shown in Figure 13.3 having N nodes in total satisﬁes the\nconditional independence properties (13.3) for n =2 ,...,N . Similarly, show that\na model described by the graph in Figure 13.4 in which there are N nodes in total",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 665,
      "page_label": "646"
    }
  },
  {
    "page_content": "Exercises 647\np(zn|Xn)\np(zn+1|Xn)\np(xn+1|zn+1)\np(zn+1|Xn+1) z\nFigure 13.23 Schematic illustration of the operation of the particle ﬁlter for a one-dimensional latent\nspace. At time step n, the posterior p(zn|xn) is represented as a mixture distribution,\nshown schematically as circles whose sizes are proportional to the weights w(l)\nn .As e to f\nL samples is then drawn from this distribution and the new weightsw(l)\nn+1 evaluated using\np(xn+1|z(l)\nn+1).\nsatisﬁes the conditional independence properties\np(xn|x1,..., xn−1)= p(xn|xn−1, xn−2) (13.122)\nfor n =3 ,...,N .\n13.2 (⋆⋆ ) Consider the joint probability distribution (13.2) corresponding to the directed\ngraph of Figure 13.3. Using the sum and product rules of probability, verify that\nthis joint distribution satisﬁes the conditional independence property (13.3) for n =\n2,...,N . Similarly, show that the second-order Markov model described by the\njoint distribution (13.4) satisﬁes the conditional independence property\np(xn|x1,..., xn−1)= p(xn|xn−1, xn−2) (13.123)\nfor n =3 ,...,N .\n13.3 (⋆) By using d-separation, show that the distributionp(x1,..., xN ) of the observed\ndata for the state space model represented by the directed graph in Figure 13.5 does\nnot satisfy any conditional independence properties and hence does not exhibit the\nMarkov property at any ﬁnite order.\n13.4 (⋆⋆ ) www Consider a hidden Markov model in which the emission densities are\nrepresented by a parametric model p(x|z, w), such as a linear regression model or",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 666,
      "page_label": "647"
    }
  },
  {
    "page_content": "represented by a parametric model p(x|z, w), such as a linear regression model or\na neural network, in which w is a vector of adaptive parameters. Describe how the\nparameters w can be learned from data using maximum likelihood.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 666,
      "page_label": "647"
    }
  },
  {
    "page_content": "648 13. SEQUENTIAL DATA\n13.5 (⋆⋆ ) Verify the M-step equations (13.18) and (13.19) for the initial state probabili-\nties and transition probability parameters of the hidden Markov model by maximiza-\ntion of the expected complete-data log likelihood function (13.17), using appropriate\nLagrange multipliers to enforce the summation constraints on the components of π\nand A.\n13.6 (⋆) Show that if any elements of the parameters π or A for a hidden Markov\nmodel are initially set to zero, then those elements will remain zero in all subsequent\nupdates of the EM algorithm.\n13.7 (⋆) Consider a hidden Markov model with Gaussian emission densities. Show that\nmaximization of the function Q(θ, θold) with respect to the mean and covariance\nparameters of the Gaussians gives rise to the M-step equations (13.20) and (13.21).\n13.8 (⋆⋆ ) www For a hidden Markov model having discrete observations governed by\na multinomial distribution, show that the conditional distribution of the observations\ngiven the hidden variables is given by (13.22) and the corresponding M step equa-\ntions are given by (13.23). Write down the analogous equations for the conditional\ndistribution and the M step equations for the case of a hidden Markov with multiple\nbinary output variables each of which is governed by a Bernoulli conditional dis-\ntribution. Hint: refer to Sections 2.1 and 2.2 for a discussion of the corresponding\nmaximum likelihood solutions for i.i.d. data if required.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 667,
      "page_label": "648"
    }
  },
  {
    "page_content": "tribution. Hint: refer to Sections 2.1 and 2.2 for a discussion of the corresponding\nmaximum likelihood solutions for i.i.d. data if required.\n13.9 (⋆⋆ ) www Use the d-separation criterion to verify that the conditional indepen-\ndence properties (13.24)–(13.31) are satisﬁed by the joint distribution for the hidden\nMarkov model deﬁned by (13.6).\n13.10 (⋆⋆⋆ ) By applying the sum and product rules of probability, verify that the condi-\ntional independence properties (13.24)–(13.31) are satisﬁed by the joint distribution\nfor the hidden Markov model deﬁned by (13.6).\n13.11 (⋆⋆ ) Starting from the expression (8.72) for the marginal distribution over the vari-\nables of a factor in a factor graph, together with the results for the messages in the\nsum-product algorithm obtained in Section 13.2.3, derive the result (13.43) for the\njoint posterior distribution over two successive latent variables in a hidden Markov\nmodel.\n13.12 (⋆⋆ ) Suppose we wish to train a hidden Markov model by maximum likelihood\nusing data that comprises R independent sequences of observations, which we de-\nnote by X(r) where r =1 ,...,R . Show that in the E step of the EM algorithm,\nwe simply evaluate posterior probabilities for the latent variables by running the α\nand β recursions independently for each of the sequences. Also show that in the\nM step, the initial probability and transition probability parameters are re-estimated",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 667,
      "page_label": "648"
    }
  },
  {
    "page_content": "Exercises 649\nusing modiﬁed forms of (13.18 ) and (13.19) given by\nπk =\nR∑\nr=1\nγ(z(r)\n1k )\nR∑\nr=1\nK∑\nj=1\nγ(z(r)\n1j )\n(13.124)\nAjk =\nR∑\nr=1\nN∑\nn=2\nξ(z(r)\nn−1,j,z (r)\nn,k)\nR∑\nr=1\nK∑\nl=1\nN∑\nn=2\nξ(z(r)\nn−1,j,z (r)\nn,l)\n(13.125)\nwhere, for notational convenience, we have assumed that the sequences are of the\nsame length (the generalization to sequences of different lengths is straightforward).\nSimilarly, show that the M-step equation for re-estimation of the means of Gaussian\nemission models is given by\nµk =\nR∑\nr=1\nN∑\nn=1\nγ(z(r)\nnk )x(r)\nn\nR∑\nr=1\nN∑\nn=1\nγ(z(r)\nnk )\n. (13.126)\nNote that the M-step equations for other emission model parameters and distributions\ntake an analogous form.\n13.13 (⋆⋆ ) www Use the deﬁnition (8.64) of the messages passed from a factor node\nto a variable node in a factor graph, together with the expression (13.6) for the joint\ndistribution in a hidden Markov model, to show that the deﬁnition (13.50) of the\nalpha message is the same as the deﬁnition (13.34).\n13.14 (⋆⋆ ) Use the deﬁnition (8.67) of the messages passed from a factor node to a\nvariable node in a factor graph, together with the expression (13.6) for the joint\ndistribution in a hidden Markov model, to show that the deﬁnition (13.52) of the\nbeta message is the same as the deﬁnition (13.35).\n13.15 (⋆⋆ ) Use the expressions (13.33) and (13.43) for the marginals in a hidden Markov\nmodel to derive the corresponding results (13.64) and (13.65) expressed in terms of\nre-scaled variables.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 668,
      "page_label": "649"
    }
  },
  {
    "page_content": "model to derive the corresponding results (13.64) and (13.65) expressed in terms of\nre-scaled variables.\n13.16 (⋆⋆⋆ ) In this exercise, we derive the forward message passing equation for the\nViterbi algorithm directly from the expression (13.6) for the joint distribution. This\ninvolves maximizing over all of the hidden variablesz1,..., zN . By taking the log-\narithm and then exchanging maximizations and summations, derive the recursion",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 668,
      "page_label": "649"
    }
  },
  {
    "page_content": "650 13. SEQUENTIAL DATA\n(13.68) where the quantities ω(zn) are deﬁned by (13.70). Show that the initial\ncondition for this recursion is given by (13.69).\n13.17 (⋆) www Show that the directed graph for the input-output hidden Markov model,\ngiven in Figure 13.18, can be expressed as a tree-structured factor graph of the form\nshown in Figure 13.15 and write down expressions for the initial factor h(z1) and\nfor the general factor fn(zn−1, zn) where 2 ⩽ n ⩽ N.\n13.18 (⋆⋆⋆ ) Using the result of Exercise 13.17, derive the recursion equations, includ-\ning the initial conditions, for the forward-backward algorithm for the input-output\nhidden Markov model shown in Figure 13.18.\n13.19 (⋆) www The Kalman ﬁlter and smoother equations allow the posterior distribu-\ntions over individual latent variables, conditioned on all of the observed variables,\nto be found efﬁciently for linear dynamical systems. Show that the sequence of\nlatent variable values obtained by maximizing each of these posterior distributions\nindividually is the same as the most probable sequence of latent values. To do this,\nsimply note that the joint distribution of all latent and observed variables in a linear\ndynamical system is Gaussian, and hence all conditionals and marginals will also be\nGaussian, and then make use of the result (2.98).\n13.20 (⋆⋆ ) www Use the result (2.115) to prove (13.87).\n13.21 (⋆⋆ ) Use the results (2.115) and (2.116), together with the matrix identities (C.5)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 669,
      "page_label": "650"
    }
  },
  {
    "page_content": "13.20 (⋆⋆ ) www Use the result (2.115) to prove (13.87).\n13.21 (⋆⋆ ) Use the results (2.115) and (2.116), together with the matrix identities (C.5)\nand (C.7), to derive the results (13.89), (13.90), and (13.91), where the Kalman gain\nmatrix Kn is deﬁned by (13.92).\n13.22 (⋆⋆ ) www Using (13.93), together with the deﬁnitions (13.76) and (13.77) and\nthe result (2.115), derive (13.96).\n13.23 (⋆⋆ ) Using (13.93), together with the deﬁnitions (13.76) and (13.77) and the result\n(2.116), derive (13.94), (13.95) and (13.97).\n13.24 (⋆⋆ ) www Consider a generalization of (13.75) and (13.76) in which we include\nconstant terms a and c in the Gaussian means, so that\np(zn|zn−1)= N(zn|Azn−1 + a, Γ) (13.127)\np(xn|zn)= N(xn|Czn + c, Σ). (13.128)\nShow that this extension can be re-case in the framework discussed in this chapter by\ndeﬁning a state vector z with an additional component ﬁxed at unity, and then aug-\nmenting the matrices A and C using extra columns corresponding to the parameters\na and c.\n13.25 (⋆⋆ ) In this exercise, we show that when the Kalman ﬁlter equations are applied\nto independent observations, they reduce to the results given in Section 2.3 for the\nmaximum likelihood solution for a single Gaussian distribution. Consider the prob-\nlem of ﬁnding the meanµ of a single Gaussian random variable x, in which we are\ngiven a set of independent observations {x1,...,x N }. To model this we can use",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 669,
      "page_label": "650"
    }
  },
  {
    "page_content": "Exercises 651\na linear dynamical system governed by (13.75) and (13.76), with latent variables\n{z1,...,z N } in which C becomes the identity matrix and where the transition prob-\nability A = 0 because the observations are independent. Let the parameters m0\nand V0 of the initial state be denoted by µ0 and σ2\n0, respectively, and suppose that\nΣ becomes σ2. Write down the corresponding Kalman ﬁlter equations starting from\nthe general results (13.89) and (13.90), together with (13.94) and (13.95). Show that\nthese are equivalent to the results (2.141) and (2.142) obtained directly by consider-\ning independent data.\n13.26 (⋆⋆⋆ ) Consider a special case of the linear dynamical system of Section 13.3 that is\nequivalent to probabilistic PCA, so that the transition matrix A = 0, the covariance\nΓ = I, and the noise covariance Σ = σ2I. By making use of the matrix inversion\nidentity (C.7) show that, if the emission density matrix C is denoted W, then the\nposterior distribution over the hidden states deﬁned by (13.89) and (13.90) reduces\nto the result (12.42) for probabilistic PCA.\n13.27 (⋆) www Consider a linear dynamical system of the form discussed in Sec-\ntion 13.3 in which the amplitude of the observation noise goes to zero, so thatΣ = 0.\nShow that the posterior distribution for zn has mean xn and zero variance. This\naccords with our intuition that if there is no noise, we should just use the current\nobservation xn to estimate the state variablezn and ignore all previous observations.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 670,
      "page_label": "651"
    }
  },
  {
    "page_content": "observation xn to estimate the state variablezn and ignore all previous observations.\n13.28 (⋆⋆⋆ ) Consider a special case of the linear dynamical system of Section 13.3 in\nwhich the state variable zn is constrained to be equal to the previous state variable,\nwhich corresponds to A = I and Γ = 0. For simplicity, assume also that V0 →∞\nso that the initial conditions forz are unimportant, and the predictions are determined\npurely by the data. Use proof by induction to show that the posterior mean for state\nzn is determined by the average of x1,..., xn. This corresponds to the intuitive\nresult that if the state variable is constant, our best estimate is obtained by averaging\nthe observations.\n13.29 (⋆⋆⋆ ) Starting from the backwards recursion equation (13.99), derive the RTS\nsmoothing equations (13.100) and (13.101) for the Gaussian linear dynamical sys-\ntem.\n13.30 (⋆⋆ ) Starting from the result (13.65) for the pairwise posterior marginal in a state\nspace model, derive the speciﬁc form (13.103) for the case of the Gaussian linear\ndynamical system.\n13.31 (⋆⋆ ) Starting from the result (13.103) and by substituting for ˆα(zn) using (13.84),\nverify the result (13.104) for the covariance between zn and zn−1.\n13.32 (⋆⋆ ) www Verify the results (13.110) and (13.111) for the M-step equations for\nµ0 and V0 in the linear dynamical system.\n13.33 (⋆⋆ ) Verify the results (13.113) and (13.114) for the M-step equations forA and Γ\nin the linear dynamical system.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 670,
      "page_label": "651"
    }
  },
  {
    "page_content": "652 13. SEQUENTIAL DATA\n13.34 (⋆⋆ ) Verify the results (13.115) and (13.116) for the M-step equations forC and Σ\nin the linear dynamical system.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 671,
      "page_label": "652"
    }
  },
  {
    "page_content": "14\nCombining\nModels\nIn earlier chapters, we have explored a range of different models for solving classiﬁ-\ncation and regression problems. It is often found that improved performance can be\nobtained by combining multiple models together in some way, instead of just using\na single model in isolation. For instance, we might train L different models and then\nmake predictions using the average of the predictions made by each model. Such\ncombinations of models are sometimes calledcommittees. In Section 14.2, we dis-\ncuss ways to apply the committee concept in practice, and we also give some insight\ninto why it can sometimes be an effective procedure.\nOne important variant of the committee method, known as boosting, involves\ntraining multiple models in sequence in which the error function used to train a par-\nticular model depends on the performance of the previous models. This can produce\nsubstantial improvements in performance compared to the use of a single model and\nis discussed in Section 14.3.\nInstead of averaging the predictions of a set of models, an alternative form of\n653",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 672,
      "page_label": "653"
    }
  },
  {
    "page_content": "654 14. COMBINING MODELS\nmodel combination is to select one of the models to make the prediction, in which\nthe choice of model is a function of the input variables. Thus different models be-\ncome responsible for making predictions in different regions of input space. One\nwidely used framework of this kind is known as a decision tree in which the selec-\ntion process can be described as a sequence of binary selections corresponding to\nthe traversal of a tree structure and is discussed in Section 14.4. In this case, the\nindividual models are generally chosen to be very simple, and the overall ﬂexibility\nof the model arises from the input-dependent selection process. Decision trees can\nbe applied to both classiﬁcation and regression problems.\nOne limitation of decision trees is that the division of input space is based on\nhard splits in which only one model is responsible for making predictions for any\ngiven value of the input variables. The decision process can be softened by moving\nto a probabilistic framework for combining models, as discussed in Section 14.5. For\nexample, if we have a set ofK models for a conditional distribution p(t|x,k ) where\nx is the input variable, t is the target variable, and k =1 ,...,K indexes the model,\nthen we can form a probabilistic mixture of the form\np(t|x)=\nK∑\nk=1\nπk(x)p(t|x,k ) (14.1)\nin which πk(x)= p(k|x) represent the input-dependent mixing coefﬁcients. Such\nmodels can be viewed as mixture distributions in which the component densities, as",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 673,
      "page_label": "654"
    }
  },
  {
    "page_content": "models can be viewed as mixture distributions in which the component densities, as\nwell as the mixing coefﬁcients, are conditioned on the input variables and are known\nas mixtures of experts. They are closely related to the mixture density network model\ndiscussed in Section 5.6.\n14.1. Bayesian Model Averaging\nIt is important to distinguish between model combination methods and Bayesian\nmodel averaging, as the two are often confused. To understand the difference, con-\nsider the example of density estimation using a mixture of Gaussians in which severalSection 9.2\nGaussian components are combined probabilistically. The model contains a binary\nlatent variable z that indicates which component of the mixture is responsible for\ngenerating the corresponding data point. Thus the model is speciﬁed in terms of a\njoint distribution\np(x, z) (14.2)\nand the corresponding density over the observed variable x is obtained by marginal-\nizing over the latent variable\np(x)=\n∑\nz\np(x, z). (14.3)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 673,
      "page_label": "654"
    }
  },
  {
    "page_content": "14.2. Committees 655\nIn the case of our Gaussian mixture example, this leads to a distribution of the form\np(x)=\nK∑\nk=1\nπkN(x|µk, Σk) (14.4)\nwith the usual interpretation of the symbols. This is an example of model combi-\nnation. For independent, identically distributed data, we can use (14.3) to write the\nmarginal probability of a data setX = {x1,..., xN } in the form\np(X)=\nN∏\nn=1\np(xn)=\nN∏\nn=1\n[∑\nzn\np(xn, zn)\n]\n. (14.5)\nThus we see that each observed data pointxn has a corresponding latent variablezn.\nNow suppose we have several different models indexed by h =1 ,...,H with\nprior probabilities p(h). For instance one model might be a mixture of Gaussians and\nanother model might be a mixture of Cauchy distributions. The marginal distribution\nover the data set is given by\np(X)=\nH∑\nh=1\np(X|h)p(h). (14.6)\nThis is an example of Bayesian model averaging. The interpretation of this summa-\ntion over h is that just one model is responsible for generating the whole data set,\nand the probability distribution over h simply reﬂects our uncertainty as to which\nmodel that is. As the size of the data set increases, this uncertainty reduces, and\nthe posterior probabilities p(h|X) become increasingly focussed on just one of the\nmodels.\nThis highlights the key difference between Bayesian model averaging and model\ncombination, because in Bayesian model averaging the whole data set is generated\nby a single model. By contrast, when we combine multiple models, as in (14.5), we",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 674,
      "page_label": "655"
    }
  },
  {
    "page_content": "by a single model. By contrast, when we combine multiple models, as in (14.5), we\nsee that different data points within the data set can potentially be generated from\ndifferent values of the latent variable z and hence by different components.\nAlthough we have considered the marginal probability p(X), the same consid-\nerations apply for the predictive densityp(x|X) or for conditional distributions such\nas p(t|x, X, T).Exercise 14.1\n14.2. Committees\nThe simplest way to construct a committee is to average the predictions of a set of\nindividual models. Such a procedure can be motivated from a frequentist perspective\nby considering the trade-off between bias and variance, which decomposes the er-Section 3.2\nror due to a model into the bias component that arises from differences between the\nmodel and the true function to be predicted, and the variance component that repre-\nsents the sensitivity of the model to the individual data points. Recall from Figure 3.5",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 674,
      "page_label": "655"
    }
  },
  {
    "page_content": "656 14. COMBINING MODELS\nthat when we trained multiple polynomials using the sinusoidal data, and then aver-\naged the resulting functions, the contribution arising from the variance term tended to\ncancel, leading to improved predictions. When we averaged a set of low-bias mod-\nels (corresponding to higher order polynomials), we obtained accurate predictions\nfor the underlying sinusoidal function from which the data were generated.\nIn practice, of course, we have only a single data set, and so we have to ﬁnd\na way to introduce variability between the different models within the committee.\nOne approach is to use bootstrap data sets, discussed in Section 1.2.3. Consider a\nregression problem in which we are trying to predict the value of a single continuous\nvariable, and suppose we generate M bootstrap data sets and then use each to train\na separate copy ym(x) of a predictive model where m =1 ,...,M . The committee\nprediction is given by\nyCOM(x)= 1\nM\nM∑\nm=1\nym(x). (14.7)\nThis procedure is known as bootstrap aggregation or bagging (Breiman, 1996).\nSuppose the true regression function that we are trying to predict is given by\nh(x), so that the output of each of the models can be written as the true value plus\nan error in the form\nym(x)= h(x)+ ϵm(x). (14.8)\nThe average sum-of-squares error then takes the form\nEx\n[\n{ym(x) − h(x)}2]\n= Ex\n[\nϵm(x)2]\n(14.9)\nwhere Ex[·] denotes a frequentist expectation with respect to the distribution of the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 675,
      "page_label": "656"
    }
  },
  {
    "page_content": "Ex\n[\n{ym(x) − h(x)}2]\n= Ex\n[\nϵm(x)2]\n(14.9)\nwhere Ex[·] denotes a frequentist expectation with respect to the distribution of the\ninput vector x. The average error made by the models acting individually is therefore\nEAV = 1\nM\nM∑\nm=1\nEx\n[\nϵm(x)2]\n. (14.10)\nSimilarly, the expected error from the committee (14.7) is given by\nECOM = Ex\n⎡\n⎣\n{\n1\nM\nM∑\nm=1\nym(x) − h(x)\n}2⎤\n⎦\n= Ex\n⎡\n⎣\n{\n1\nM\nM∑\nm=1\nϵm(x)\n}2⎤\n⎦ (14.11)\nIf we assume that the errors have zero mean and are uncorrelated, so that\nEx [ϵm(x) ]=0 (14.12)\nEx [ϵm(x)ϵl(x) ]=0 ,m ̸=l (14.13)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 675,
      "page_label": "656"
    }
  },
  {
    "page_content": "14.3. Boosting 657\nthen we obtainExercise 14.2\nECOM = 1\nM EAV. (14.14)\nThis apparently dramatic result suggests that the average error of a model can be\nreduced by a factor of M simply by averaging M versions of the model. Unfortu-\nnately, it depends on the key assumption that the errors due to the individual models\nare uncorrelated. In practice, the errors are typically highly correlated, and the reduc-\ntion in overall error is generally small. It can, however, be shown that the expected\ncommittee error will not exceed the expected error of the constituent models, so that\nECOM ⩽ EAV. In order to achieve more signiﬁcant improvements, we turn to a moreExercise 14.3\nsophisticated technique for building committees, known as boosting.\n14.3. Boosting\nBoosting is a powerful technique for combining multiple ‘base’ classiﬁers to produce\na form of committee whose performance can be signiﬁcantly better than that of any\nof the base classiﬁers. Here we describe the most widely used form of boosting\nalgorithm called AdaBoost, short for ‘adaptive boosting’, developed by Freund and\nSchapire (1996). Boosting can give good results even if the base classiﬁers have a\nperformance that is only slightly better than random, and hence sometimes the base\nclassiﬁers are known asweak learners. Originally designed for solving classiﬁcation\nproblems, boosting can also be extended to regression (Friedman, 2001).\nThe principal difference between boosting and the committee methods such as",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 676,
      "page_label": "657"
    }
  },
  {
    "page_content": "problems, boosting can also be extended to regression (Friedman, 2001).\nThe principal difference between boosting and the committee methods such as\nbagging discussed above, is that the base classiﬁers are trained in sequence, and\neach base classiﬁer is trained using a weighted form of the data set in which the\nweighting coefﬁcient associated with each data point depends on the performance\nof the previous classiﬁers. In particular, points that are misclassiﬁed by one of the\nbase classiﬁers are given greater weight when used to train the next classiﬁer in\nthe sequence. Once all the classiﬁers have been trained, their predictions are then\ncombined through a weighted majority voting scheme, as illustrated schematically\nin Figure 14.1.\nConsider a two-class classiﬁcation problem, in which the training data comprises\ninput vectors x1,..., xN along with corresponding binary target variablest1,...,t N\nwhere tn ∈{ − 1, 1}. Each data point is given an associated weighting parameter\nwn, which is initially set 1/N for all data points. We shall suppose that we have\na procedure available for training a base classiﬁer using weighted data to give a\nfunction y(x) ∈{ − 1, 1}. At each stage of the algorithm, AdaBoost trains a new\nclassiﬁer using a data set in which the weighting coefﬁcients are adjusted according\nto the performance of the previously trained classiﬁer so as to give greater weight\nto the misclassiﬁed data points. Finally, when the desired number of base classiﬁers",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 676,
      "page_label": "657"
    }
  },
  {
    "page_content": "to the misclassiﬁed data points. Finally, when the desired number of base classiﬁers\nhave been trained, they are combined to form a committee using coefﬁcients that\ngive different weight to different base classiﬁers. The precise form of the AdaBoost\nalgorithm is given below.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 676,
      "page_label": "657"
    }
  },
  {
    "page_content": "658 14. COMBINING MODELS\nFigure 14.1 Schematic illustration of the\nboosting framework. Each\nbase classiﬁer ym(x) is trained\non a weighted form of the train-\ning set (blue arrows) in which\nthe weights w(m)\nn depend on\nthe performance of the pre-\nvious base classiﬁer ym−1(x)\n(green arrows). Once all base\nclassiﬁers have been trained,\nthey are combined to give\nthe ﬁnal classiﬁer YM (x) (red\narrows).\n{w(1)\nn }{ w(2)\nn }{ w(M)\nn }\ny1(x) y2(x) yM (x)\nYM (x)=s i g n\n( M∑\nm\nαmym(x)\n)\nAdaBoost\n1. Initialize the data weighting coefﬁcients {wn} by setting w(1)\nn =1 /N for\nn =1 ,...,N .\n2. For m =1 ,...,M :\n(a) Fit a classiﬁer ym(x) to the training data by minimizing the weighted\nerror function\nJm =\nN∑\nn=1\nw(m)\nn I(ym(xn) ̸= tn) (14.15)\nwhere I(ym(xn) ̸= tn) is the indicator function and equals 1 when\nym(xn) ̸= tn and 0 otherwise.\n(b) Evaluate the quantities\nϵm =\nN∑\nn=1\nw(m)\nn I(ym(xn) ̸= tn)\nN∑\nn=1\nw(m)\nn\n(14.16)\nand then use these to evaluate\nαm =l n\n{1 − ϵm\nϵm\n}\n. (14.17)\n(c) Update the data weighting coefﬁcients\nw(m+1)\nn = w(m)\nn exp {αmI(ym(xn) ̸= tn)} (14.18)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 677,
      "page_label": "658"
    }
  },
  {
    "page_content": "14.3. Boosting 659\n3. Make predictions using the ﬁnal model, which is given by\nYM (x) = sign\n( M∑\nm=1\nαmym(x)\n)\n. (14.19)\nWe see that the ﬁrst base classiﬁer y1(x) is trained using weighting coefﬁ-\ncients w(1)\nn that are all equal, which therefore corresponds to the usual procedure\nfor training a single classiﬁer. From (14.18), we see that in subsequent iterations\nthe weighting coefﬁcients w(m)\nn are increased for data points that are misclassiﬁed\nand decreased for data points that are correctly classiﬁed. Successive classiﬁers are\ntherefore forced to place greater emphasis on points that have been misclassiﬁed by\nprevious classiﬁers, and data points that continue to be misclassiﬁed by successive\nclassiﬁers receive ever greater weight. The quantities ϵm represent weighted mea-\nsures of the error rates of each of the base classiﬁers on the data set. We therefore\nsee that the weighting coefﬁcients αm deﬁned by (14.17) give greater weight to the\nmore accurate classiﬁers when computing the overall output given by (14.19).\nThe AdaBoost algorithm is illustrated in Figure 14.2, using a subset of 30 data\npoints taken from the toy classiﬁcation data set shown in Figure A.7. Here each base\nlearners consists of a threshold on one of the input variables. This simple classiﬁer\ncorresponds to a form of decision tree known as a ‘decision stumps’, i.e., a deci-Section 14.4\nsion tree with a single node. Thus each base learner classiﬁes an input according to",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 678,
      "page_label": "659"
    }
  },
  {
    "page_content": "sion tree with a single node. Thus each base learner classiﬁes an input according to\nwhether one of the input features exceeds some threshold and therefore simply parti-\ntions the space into two regions separated by a linear decision surface that is parallel\nto one of the axes.\n14.3.1 Minimizing exponential error\nBoosting was originally motivated using statistical learning theory, leading to\nupper bounds on the generalization error. However, these bounds turn out to be too\nloose to have practical value, and the actual performance of boosting is much better\nthan the bounds alone would suggest. Friedman et al. (2000) gave a different and\nvery simple interpretation of boosting in terms of the sequential minimization of an\nexponential error function.\nConsider the exponential error function deﬁned by\nE =\nN∑\nn=1\nexp{−tnfm(xn)} (14.20)\nwhere fm(x) is a classiﬁer deﬁned in terms of a linear combination of base classiﬁers\nyl(x) of the form\nfm(x)= 1\n2\nm∑\nl=1\nαlyl(x) (14.21)\nand tn ∈{ −1, 1} are the training set target values. Our goal is to minimize E with\nrespect to both the weighting coefﬁcientsαl and the parameters of the base classiﬁers\nyl(x).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 678,
      "page_label": "659"
    }
  },
  {
    "page_content": "660 14. COMBINING MODELS\nm =1\n−1 0 1 2\n−2\n0\n2 m =2\n−1 0 1 2\n−2\n0\n2 m =3\n−1 0 1 2\n−2\n0\n2\nm =6\n−1 0 1 2\n−2\n0\n2 m =1 0\n−1 0 1 2\n−2\n0\n2 m = 150\n−1 0 1 2\n−2\n0\n2\nFigure 14.2 Illustration of boosting in which the base learners consist of simple thresholds applied to one or\nother of the axes. Each ﬁgure shows the number m of base learners trained so far, along with the decision\nboundary of the most recent base learner (dashed black line) and the combined decision boundary of the en-\nsemble (solid green line). Each data point is depicted by a circle whose radius indicates the weight assigned to\nthat data point when training the most recently added base learner. Thus, for instance, we see that points that\nare misclassiﬁed by the m =1 base learner are given greater weight when training the m =2 base learner.\nInstead of doing a global error function minimization, however, we shall sup-\npose that the base classiﬁers y1(x),...,y m−1(x) are ﬁxed, as are their coefﬁcients\nα1,...,α m−1, and so we are minimizing only with respect to αm and ym(x). Sep-\narating off the contribution from base classiﬁer ym(x), we can then write the error\nfunction in the form\nE =\nN∑\nn=1\nexp\n{\n−tnfm−1(xn) − 1\n2tnαmym(xn)\n}\n=\nN∑\nn=1\nw(m)\nn exp\n{\n−1\n2tnαmym(xn)\n}\n(14.22)\nwhere the coefﬁcients w(m)\nn =e x p{−tnfm−1(xn)} can be viewed as constants\nbecause we are optimizing only αm and ym(x). If we denote by Tm the set of\ndata points that are correctly classiﬁed by ym(x), and if we denote the remaining",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 679,
      "page_label": "660"
    }
  },
  {
    "page_content": "data points that are correctly classiﬁed by ym(x), and if we denote the remaining\nmisclassiﬁed points by Mm, then we can in turn rewrite the error function in the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 679,
      "page_label": "660"
    }
  },
  {
    "page_content": "14.3. Boosting 661\nform\nE = e−αm/2\n∑\nn∈Tm\nw(m)\nn + eαm/2\n∑\nn∈Mm\nw(m)\nn\n=( eαm/2 − e−αm/2)\nN∑\nn=1\nw(m)\nn I(ym(xn) ̸=tn)+ e−αm/2\nN∑\nn=1\nw(m)\nn .\n(14.23)\nWhen we minimize this with respect to ym(x), we see that the second term is con-\nstant, and so this is equivalent to minimizing (14.15) because the overall multiplica-\ntive factor in front of the summation does not affect the location of the minimum.\nSimilarly, minimizing with respect to αm, we obtain (14.17) in which ϵm is deﬁned\nby (14.16).Exercise 14.6\nFrom (14.22) we see that, having found αm and ym(x), the weights on the data\npoints are updated using\nw(m+1)\nn = w(m)\nn exp\n{\n−1\n2tnαmym(xn)\n}\n. (14.24)\nMaking use of the fact that\ntnym(xn)=1 − 2I(ym(xn) ̸=tn) (14.25)\nwe see that the weights w(m)\nn are updated at the next iteration using\nw(m+1)\nn = w(m)\nn exp(−αm/2) exp{αmI(ym(xn) ̸=tn)}. (14.26)\nBecause the term exp(−αm/2) is independent of n, we see that it weights all data\npoints by the same factor and so can be discarded. Thus we obtain (14.18).\nFinally, once all the base classiﬁers are trained, new data points are classiﬁed by\nevaluating the sign of the combined function deﬁned according to (14.21). Because\nthe factor of1/2 does not affect the sign it can be omitted, giving (14.19).\n14.3.2 Error functions for boosting\nThe exponential error function that is minimized by the AdaBoost algorithm\ndiffers from those considered in previous chapters. To gain some insight into the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 680,
      "page_label": "661"
    }
  },
  {
    "page_content": "differs from those considered in previous chapters. To gain some insight into the\nnature of the exponential error function, we ﬁrst consider the expected error given\nby\nEx,t [exp{−ty(x)}]=\n∑\nt\n∫\nexp{−ty(x)}p(t|x)p(x)dx. (14.27)\nIf we perform a variational minimization with respect to all possible functions y(x),\nwe obtainExercise 14.7\ny(x)= 1\n2 ln\n{ p(t =1 |x)\np(t = −1|x)\n}\n(14.28)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 680,
      "page_label": "661"
    }
  },
  {
    "page_content": "662 14. COMBINING MODELS\nFigure 14.3 Plot of the exponential (green) and\nrescaled cross-entropy (red) error\nfunctions along with the hinge er-\nror (blue) used in support vector\nmachines, and the misclassiﬁcation\nerror (black). Note that for large\nnegative values of z = ty(x), the\ncross-entropy gives a linearly in-\ncreasing penalty, whereas the expo-\nnential loss gives an exponentially in-\ncreasing penalty.\n−2 −1 012 z\nE(z)\nwhich is half the log-odds. Thus the AdaBoost algorithm is seeking the best approx-\nimation to the log odds ratio, within the space of functions represented by the linear\ncombination of base classiﬁers, subject to the constrained minimization resulting\nfrom the sequential optimization strategy. This result motivates the use of the sign\nfunction in (14.19) to arrive at the ﬁnal classiﬁcation decision.\nWe have already seen that the minimizer y(x) of the cross-entropy error (4.90)\nfor two-class classiﬁcation is given by the posterior class probability. In the case\nof a target variable t ∈{ − 1, 1}, we have seen that the error function is given bySection 7.1.2\nln(1 + exp(−yt)). This is compared with the exponential error function in Fig-\nure 14.3, where we have divided the cross-entropy error by a constant factor ln(2)\nso that it passes through the point (0, 1) for ease of comparison. We see that both\ncan be seen as continuous approximations to the ideal misclassiﬁcation error func-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 681,
      "page_label": "662"
    }
  },
  {
    "page_content": "can be seen as continuous approximations to the ideal misclassiﬁcation error func-\ntion. An advantage of the exponential error is that its sequential minimization leads\nto the simple AdaBoost scheme. One drawback, however, is that it penalizes large\nnegative values of ty(x) much more strongly than cross-entropy. In particular, we\nsee that for large negative values of ty, the cross-entropy grows linearly with |ty|,\nwhereas the exponential error function grows exponentially with |ty|. Thus the ex-\nponential error function will be much less robust to outliers or misclassiﬁed data\npoints. Another important difference between cross-entropy and the exponential er-\nror function is that the latter cannot be interpreted as the log likelihood function of\nany well-deﬁned probabilistic model. Furthermore, the exponential error does notExercise 14.8\ngeneralize to classiﬁcation problems having K> 2 classes, again in contrast to the\ncross-entropy for a probabilistic model, which is easily generalized to give (4.108).Section 4.3.4\nThe interpretation of boosting as the sequential optimization of an additive model\nunder an exponential error (Friedman et al., 2000) opens the door to a wide range\nof boosting-like algorithms, including multiclass extensions, by altering the choice\nof error function. It also motivates the extension to regression problems (Friedman,\n2001). If we consider a sum-of-squares error function for regression, then sequential",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 681,
      "page_label": "662"
    }
  },
  {
    "page_content": "2001). If we consider a sum-of-squares error function for regression, then sequential\nminimization of an additive model of the form (14.21) simply involves ﬁtting each\nnew base classiﬁer to the residual errorstn −fm−1(xn) from the previous model. AsExercise 14.9\nwe have noted, however, the sum-of-squares error is not robust to outliers, and this",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 681,
      "page_label": "662"
    }
  },
  {
    "page_content": "14.4. Tree-based Models 663\nFigure 14.4 Comparison of the squared error\n(green) with the absolute error (red)\nshowing how the latter places much\nless emphasis on large errors and\nhence is more robust to outliers and\nmislabelled data points.\n0 z\nE(z)\n−11\ncan be addressed by basing the boosting algorithm on the absolute deviation |y − t|\ninstead. These two error functions are compared in Figure 14.4.\n14.4. Tree-based Models\nThere are various simple, but widely used, models that work by partitioning the\ninput space into cuboid regions, whose edges are aligned with the axes, and then\nassigning a simple model (for example, a constant) to each region. They can be\nviewed as a model combination method in which only one model is responsible\nfor making predictions at any given point in input space. The process of selecting\na speciﬁc model, given a new input x, can be described by a sequential decision\nmaking process corresponding to the traversal of a binary tree (one that splits into\ntwo branches at each node). Here we focus on a particular tree-based framework\ncalled classiﬁcation and regression trees,o r CART (Breiman et al., 1984), although\nthere are many other variants going by such names as ID3 and C4.5 (Quinlan, 1986;\nQuinlan, 1993).\nFigure 14.5 shows an illustration of a recursive binary partitioning of the input\nspace, along with the corresponding tree structure. In this example, the ﬁrst step\nFigure 14.5 Illustration of a two-dimensional in-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 682,
      "page_label": "663"
    }
  },
  {
    "page_content": "space, along with the corresponding tree structure. In this example, the ﬁrst step\nFigure 14.5 Illustration of a two-dimensional in-\nput space that has been partitioned\ninto ﬁve regions using axis-aligned\nboundaries.\nA\nB\nCD\nE\nθ1 θ4\nθ2\nθ3\nx1\nx2",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 682,
      "page_label": "663"
    }
  },
  {
    "page_content": "664 14. COMBINING MODELS\nFigure 14.6 Binary tree corresponding to the par-\ntitioning of input space shown in Fig-\nure 14.5.\nx1 >θ 1\nx2 >θ 3\nx1 ⩽ θ4\nx2 ⩽ θ2\nABCDE\ndivides the whole of the input space into two regions according to whether x1 ⩽ θ1\nor x1 >θ 1 where θ1 is a parameter of the model. This creates two subregions, each\nof which can then be subdivided independently. For instance, the region x1 ⩽ θ1\nis further subdivided according to whether x2 ⩽ θ2 or x2 >θ 2, giving rise to the\nregions denoted A and B. The recursive subdivision can be described by the traversal\nof the binary tree shown in Figure 14.6. For any new input x, we determine which\nregion it falls into by starting at the top of the tree at the root node and following\na path down to a speciﬁc leaf node according to the decision criteria at each node.\nNote that such decision trees are not probabilistic graphical models.\nWithin each region, there is a separate model to predict the target variable. For\ninstance, in regression we might simply predict a constant over each region, or in\nclassiﬁcation we might assign each region to a speciﬁc class. A key property of tree-\nbased models, which makes them popular in ﬁelds such as medical diagnosis, for\nexample, is that they are readily interpretable by humans because they correspond\nto a sequence of binary decisions applied to the individual input variables. For in-\nstance, to predict a patient’s disease, we might ﬁrst ask “is their temperature greater",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 683,
      "page_label": "664"
    }
  },
  {
    "page_content": "stance, to predict a patient’s disease, we might ﬁrst ask “is their temperature greater\nthan some threshold?”. If the answer is yes, then we might next ask “is their blood\npressure less than some threshold?”. Each leaf of the tree is then associated with a\nspeciﬁc diagnosis.\nIn order to learn such a model from a training set, we have to determine the\nstructure of the tree, including which input variable is chosen at each node to form\nthe split criterion as well as the value of the threshold parameter θi for the split. We\nalso have to determine the values of the predictive variable within each region.\nConsider ﬁrst a regression problem in which the goal is to predict a single target\nvariable t from a D-dimensional vector x =( x1,...,x D)T of input variables. The\ntraining data consists of input vectors {x1,..., xN } along with the corresponding\ncontinuous labels {t1,...,t N }. If the partitioning of the input space is given, and we\nminimize the sum-of-squares error function, then the optimal value of the predictive\nvariable within any given region is just given by the average of the values of tn for\nthose data points that fall in that region.Exercise 14.10\nNow consider how to determine the structure of the decision tree. Even for a\nﬁxed number of nodes in the tree, the problem of determining the optimal structure\n(including choice of input variable for each split as well as the corresponding thresh-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 683,
      "page_label": "664"
    }
  },
  {
    "page_content": "14.4. Tree-based Models 665\nolds) to minimize the sum-of-squares error is usually computationally infeasible due\nto the combinatorially large number of possible solutions. Instead, a greedy opti-\nmization is generally done by starting with a single root node, corresponding to the\nwhole input space, and then growing the tree by adding nodes one at a time. At each\nstep there will be some number of candidate regions in input space that can be split,\ncorresponding to the addition of a pair of leaf nodes to the existing tree. For each\nof these, there is a choice of which of the D input variables to split, as well as the\nvalue of the threshold. The joint optimization of the choice of region to split, and the\nchoice of input variable and threshold, can be done efﬁciently by exhaustive search\nnoting that, for a given choice of split variable and threshold, the optimal choice of\npredictive variable is given by the local average of the data, as noted earlier. This\nis repeated for all possible choices of variable to be split, and the one that gives the\nsmallest residual sum-of-squares error is retained.\nGiven a greedy strategy for growing the tree, there remains the issue of when\nto stop adding nodes. A simple approach would be to stop when the reduction in\nresidual error falls below some threshold. However, it is found empirically that often\nnone of the available splits produces a signiﬁcant reduction in error, and yet after",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 684,
      "page_label": "665"
    }
  },
  {
    "page_content": "none of the available splits produces a signiﬁcant reduction in error, and yet after\nseveral more splits a substantial error reduction is found. For this reason, it is com-\nmon practice to grow a large tree, using a stopping criterion based on the number\nof data points associated with the leaf nodes, and then prune back the resulting tree.\nThe pruning is based on a criterion that balances residual error against a measure of\nmodel complexity. If we denote the starting tree for pruning by T0, then we deﬁne\nT ⊂ T0 to be a subtree of T0 if it can be obtained by pruning nodes from T0 (in\nother words, by collapsing internal nodes by combining the corresponding regions).\nSuppose the leaf nodes are indexed by τ =1 ,..., |T|, with leaf node τ representing\na region Rτ of input space having Nτ data points, and |T| denoting the total number\nof leaf nodes. The optimal prediction for region Rτ is then given by\nyτ = 1\nNτ\n∑\nxn∈Rτ\ntn (14.29)\nand the corresponding contribution to the residual sum-of-squares is then\nQτ (T)=\n∑\nxn∈Rτ\n{tn − yτ }2 . (14.30)\nThe pruning criterion is then given by\nC(T)=\n|T|∑\nτ=1\nQτ (T)+ λ|T| (14.31)\nThe regularization parameter λ determines the trade-off between the overall residual\nsum-of-squares error and the complexity of the model as measured by the number\n|T| of leaf nodes, and its value is chosen by cross-validation.\nFor classiﬁcation problems, the process of growing and pruning the tree is sim-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 684,
      "page_label": "665"
    }
  },
  {
    "page_content": "|T| of leaf nodes, and its value is chosen by cross-validation.\nFor classiﬁcation problems, the process of growing and pruning the tree is sim-\nilar, except that the sum-of-squares error is replaced by a more appropriate measure",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 684,
      "page_label": "665"
    }
  },
  {
    "page_content": "666 14. COMBINING MODELS\nof performance. If we deﬁne pτk to be the proportion of data points in region Rτ\nassigned to class k, where k =1 ,...,K , then two commonly used choices are the\ncross-entropy\nQτ (T)=\nK∑\nk=1\npτk lnpτk (14.32)\nand the Gini index\nQτ (T)=\nK∑\nk=1\npτk (1 − pτk ) . (14.33)\nThese both vanish forpτk =0 and pτk =1 and have a maximum atpτk =0 .5. They\nencourage the formation of regions in which a high proportion of the data points are\nassigned to one class. The cross entropy and the Gini index are better measures than\nthe misclassiﬁcation rate for growing the tree because they are more sensitive to the\nnode probabilities. Also, unlike misclassiﬁcation rate, they are differentiable andExercise 14.11\nhence better suited to gradient based optimization methods. For subsequent pruning\nof the tree, the misclassiﬁcation rate is generally used.\nThe human interpretability of a tree model such as CART is often seen as its\nmajor strength. However, in practice it is found that the particular tree structure that\nis learned is very sensitive to the details of the data set, so that a small change to the\ntraining data can result in a very different set of splits (Hastieet al., 2001).\nThere are other problems with tree-based methods of the kind considered in\nthis section. One is that the splits are aligned with the axes of the feature space,\nwhich may be very suboptimal. For instance, to separate two classes whose optimal",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 685,
      "page_label": "666"
    }
  },
  {
    "page_content": "which may be very suboptimal. For instance, to separate two classes whose optimal\ndecision boundary runs at 45 degrees to the axes would need a large number of\naxis-parallel splits of the input space as compared to a single non-axis-aligned split.\nFurthermore, the splits in a decision tree are hard, so that each region of input space\nis associated with one, and only one, leaf node model. The last issue is particularly\nproblematic in regression where we are typically aiming to model smooth functions,\nand yet the tree model produces piecewise-constant predictions with discontinuities\nat the split boundaries.\n14.5. Conditional Mixture Models\nWe have seen that standard decision trees are restricted by hard, axis-aligned splits of\nthe input space. These constraints can be relaxed, at the expense of interpretability,\nby allowing soft, probabilistic splits that can be functions of all of the input variables,\nnot just one of them at a time. If we also give the leaf models a probabilistic inter-\npretation, we arrive at a fully probabilistic tree-based model called the hierarchical\nmixture of experts, which we consider in Section 14.5.3.\nAn alternative way to motivate the hierarchical mixture of experts model is to\nstart with a standard probabilistic mixtures of unconditional density models such as\nGaussians and replace the component densities with conditional distributions. HereChapter 9\nwe consider mixtures of linear regression models (Section 14.5.1) and mixtures of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 685,
      "page_label": "666"
    }
  },
  {
    "page_content": "14.5. Conditional Mixture Models 667\nlogistic regression models (Section 14.5.2). In the simplest case, the mixing coefﬁ-\ncients are independent of the input variables. If we make a further generalization to\nallow the mixing coefﬁcients also to depend on the inputs then we obtain amixture\nof experts model. Finally, if we allow each component in the mixture model to be\nitself a mixture of experts model, then we obtain a hierarchical mixture of experts.\n14.5.1 Mixtures of linear regression models\nOne of the many advantages of giving a probabilistic interpretation to the lin-\near regression model is that it can then be used as a component in more complex\nprobabilistic models. This can be done, for instance, by viewing the conditional\ndistribution representing the linear regression model as a node in a directed prob-\nabilistic graph. Here we consider a simple example corresponding to a mixture of\nlinear regression models, which represents a straightforward extension of the Gaus-\nsian mixture model discussed in Section 9.2 to the case of conditional Gaussian\ndistributions.\nWe therefore consider K linear regression models, each governed by its own\nweight parameter wk. In many applications, it will be appropriate to use a common\nnoise variance, governed by a precision parameterβ, for all K components, and this\nis the case we consider here. We will once again restrict attention to a single target",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 686,
      "page_label": "667"
    }
  },
  {
    "page_content": "is the case we consider here. We will once again restrict attention to a single target\nvariablet, though the extension to multiple outputs is straightforward. If we denoteExercise 14.12\nthe mixing coefﬁcients by πk, then the mixture distribution can be written\np(t|θ)=\nK∑\nk=1\nπkN(t|wT\nk φ,β −1) (14.34)\nwhere θ denotes the set of all adaptive parameters in the model, namelyW = {wk},\nπ = {πk}, and β. The log likelihood function for this model, given a data set of\nobservations {φn,t n}, then takes the form\nlnp(t|θ)=\nN∑\nn=1\nln\n( K∑\nk=1\nπkN(tn|wT\nk φn,β −1)\n)\n(14.35)\nwhere t =( t1,...,t N )T denotes the vector of target variables.\nIn order to maximize this likelihood function, we can once again appeal to the\nEM algorithm, which will turn out to be a simple extension of the EM algorithm for\nunconditional Gaussian mixtures of Section 9.2. We can therefore build on our expe-\nrience with the unconditional mixture and introduce a setZ = {zn} of binary latent\nvariables where znk ∈{ 0, 1} in which, for each data point n, all of the elements\nk =1 ,...,K are zero except for a single value of 1 indicating which component\nof the mixture was responsible for generating that data point. The joint distribution\nover latent and observed variables can be represented by the graphical model shown\nin Figure 14.7.\nThe complete-data log likelihood function then takes the formExercise 14.13\nlnp(t, Z|θ)=\nN∑\nn=1\nK∑\nk=1\nznk ln\n{\nπkN(tn|wT\nk φn,β −1)\n}\n. (14.36)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 686,
      "page_label": "667"
    }
  },
  {
    "page_content": "668 14. COMBINING MODELS\nFigure 14.7 Probabilistic directed graph representing a mixture of\nlinear regression models, deﬁned by (14.35).\nzn\ntn\nφn\nNW\nβ\nπ\nThe EM algorithm begins by ﬁrst choosing an initial valueθold for the model param-\neters. In the E step, these parameter values are then used to evaluate the posterior\nprobabilities, or responsibilities, of each component k for every data point n given\nby\nγnk = E[znk]= p(k|φn, θold)= πkN(tn|wT\nk φn,β −1)∑\nj πjN(tn|wT\nj φn,β −1). (14.37)\nThe responsibilities are then used to determine the expectation, with respect to the\nposterior distribution p(Z|t, θold), of the complete-data log likelihood, which takes\nthe form\nQ(θ, θold)= EZ [lnp(t, Z|θ)] =\nN∑\nn=1\nK∑\nk=1\nγnk\n{\nln πk +l nN(tn|wT\nk φn,β −1)\n}\n.\nIn the M step, we maximize the function Q(θ, θold) with respect to θ, keeping the\nγnk ﬁxed. For the optimization with respect to the mixing coefﬁcients πk we need\nto take account of the constraint ∑\nk πk =1 , which can be done with the aid of a\nLagrange multiplier, leading to an M-step re-estimation equation for πk in the formExercise 14.14\nπk = 1\nN\nN∑\nn=1\nγnk. (14.38)\nNote that this has exactly the same form as the corresponding result for a simple\nmixture of unconditional Gaussians given by (9.22).\nNext consider the maximization with respect to the parameter vector wk of the\nkth linear regression model. Substituting for the Gaussian distribution, we see that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 687,
      "page_label": "668"
    }
  },
  {
    "page_content": "kth linear regression model. Substituting for the Gaussian distribution, we see that\nthe function Q(θ, θold), as a function of the parameter vector wk, takes the form\nQ(θ, θold)=\nN∑\nn=1\nγnk\n{\n−β\n2\n(\ntn − wT\nk φn\n)2\n}\n+c o n s t (14.39)\nwhere the constant term includes the contributions from other weight vectorswj for\nj ̸= k. Note that the quantity we are maximizing is similar to the (negative of the)\nstandard sum-of-squares error (3.12) for a single linear regression model, but with\nthe inclusion of the responsibilities γnk. This represents a weighted least squares",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 687,
      "page_label": "668"
    }
  },
  {
    "page_content": "14.5. Conditional Mixture Models 669\nproblem, in which the term corresponding to the nth data point carries a weighting\ncoefﬁcient given by βγnk, which could be interpreted as an effective precision for\neach data point. We see that each component linear regression model in the mixture,\ngoverned by its own parameter vectorwk, is ﬁtted separately to the whole data set in\nthe M step, but with each data point n weighted by the responsibility γnk that model\nk takes for that data point. Setting the derivative of (14.39) with respect to wk equal\nto zero gives\n0=\nN∑\nn=1\nγnk\n(\ntn − wT\nk φn\n)\nφn (14.40)\nwhich we can write in matrix notation as\n0= ΦTRk(t − Φwk) (14.41)\nwhere Rk = diag(γnk) is a diagonal matrix of size N × N. Solving for wk,w e\nobtain\nwk =\n(\nΦTRkΦ\n)−1\nΦTRkt. (14.42)\nThis represents a set of modiﬁed normal equations corresponding to the weighted\nleast squares problem, of the same form as (4.99) found in the context of logistic\nregression. Note that after each E step, the matrixRk will change and so we will\nhave to solve the normal equations afresh in the subsequent M step.\nFinally, we maximize Q(θ, θold) with respect to β. Keeping only terms that\ndepend on β, the function Q(θ, θold) can be written\nQ(θ, θold)=\nN∑\nn=1\nK∑\nk=1\nγnk\n{1\n2 lnβ − β\n2\n(\ntn − wT\nk φn\n)2\n}\n. (14.43)\nSetting the derivative with respect to β equal to zero, and rearranging, we obtain the\nM-step equation for β in the form\n1\nβ = 1\nN\nN∑\nn=1\nK∑\nk=1\nγnk\n(\ntn − wT\nk φn\n)2\n. (14.44)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 688,
      "page_label": "669"
    }
  },
  {
    "page_content": "M-step equation for β in the form\n1\nβ = 1\nN\nN∑\nn=1\nK∑\nk=1\nγnk\n(\ntn − wT\nk φn\n)2\n. (14.44)\nIn Figure 14.8, we illustrate this EM algorithm using the simple example of\nﬁtting a mixture of two straight lines to a data set having one input variable x and\none target variable t. The predictive density (14.34) is plotted in Figure 14.9 using\nthe converged parameter values obtained from the EM algorithm, corresponding to\nthe right-hand plot in Figure 14.8. Also shown in this ﬁgure is the result of ﬁtting\na single linear regression model, which gives a unimodal predictive density. We see\nthat the mixture model gives a much better representation of the data distribution,\nand this is reﬂected in the higher likelihood value. However, the mixture model\nalso assigns signiﬁcant probability mass to regions where there is no data because its\npredictive distribution is bimodal for all values ofx. This problem can be resolved by\nextending the model to allow the mixture coefﬁcients themselves to be functions of\nx, leading to models such as the mixture density networks discussed in Section 5.6,\nand hierarchical mixture of experts discussed in Section 14.5.3.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 688,
      "page_label": "669"
    }
  },
  {
    "page_content": "670 14. COMBINING MODELS\n−1 −0.5 0 0.5 1\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n−1 −0.5 0 0.5 1\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n−1 −0.5 0 0.5 1\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n−1 −0.5 0 0.5 1\n0\n0.2\n0.4\n0.6\n0.8\n1\n−1 −0.5 0 0.5 1\n0\n0.2\n0.4\n0.6\n0.8\n1\n−1 −0.5 0 0.5 1\n0\n0.2\n0.4\n0.6\n0.8\n1\nFigure 14.8 Example of a synthetic data set, shown by the green points, having one input variable x and one\ntarget variable t, together with a mixture of two linear regression models whose mean functions y(x, wk), where\nk ∈{ 1, 2}, are shown by the blue and red lines. The upper three plots show the initial conﬁguration (left), the\nresult of running 30 iterations of EM (centre), and the result after 50 iterations of EM (right). Hereβ was initialized\nto the reciprocal of the true variance of the set of target values. The lower three plots show the corresponding\nresponsibilities plotted as a vertical line for each data point in which the length of the blue segment gives the\nposterior probability of the blue line for that data point (and similarly for the red segment).\n14.5.2 Mixtures of logistic models\nBecause the logistic regression model deﬁnes a conditional distribution for the\ntarget variable, given the input vector, it is straightforward to use it as the component\ndistribution in a mixture model, thereby giving rise to a richer family of conditional\ndistributions compared to a single logistic regression model. This example involves\na straightforward combination of ideas encountered in earlier sections of the book",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 689,
      "page_label": "670"
    }
  },
  {
    "page_content": "a straightforward combination of ideas encountered in earlier sections of the book\nand will help consolidate these for the reader.\nThe conditional distribution of the target variable, for a probabilistic mixture of\nK logistic regression models, is given by\np(t|φ, θ)=\nK∑\nk=1\nπkyt\nk [1 − yk]1−t (14.45)\nwhere φ is the feature vector, yk = σ\n(\nwT\nk φ\n)\nis the output of component k, and θ\ndenotes the adjustable parameters namely {πk} and {wk}.\nNow suppose we are given a data set {φn,t n}. The corresponding likelihood",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 689,
      "page_label": "670"
    }
  },
  {
    "page_content": "14.5. Conditional Mixture Models 671\nFigure 14.9 The left plot shows the predictive conditional density corresponding to the converged solution in\nFigure 14.8. This gives a log likelihood value of −3.0. A vertical slice through one of these plots at a particular\nvalue of x represents the corresponding conditional distribution p(t|x), which we see is bimodal. The plot on the\nright shows the predictive density for a single linear regression model ﬁtted to the same data set using maximum\nlikelihood. This model has a smaller log likelihood of −27.6.\nfunction is then given by\np(t|θ)=\nN∏\nn=1\n( K∑\nk=1\nπkytn\nnk [1 − ynk]1−tn\n)\n(14.46)\nwhere ynk = σ(wT\nk φn) and t =( t1,...,t N )T. We can maximize this likelihood\nfunction iteratively by making use of the EM algorithm. This involves introducing\nlatent variables znk that correspond to a 1-of-K coded binary indicator variable for\neach data point n. The complete-data likelihood function is then given by\np(t, Z|θ)=\nN∏\nn=1\nK∏\nk=1\n{\nπkytn\nnk [1 − ynk]1−tn\n}znk\n(14.47)\nwhere Z is the matrix of latent variables with elements znk. We initialize the EM\nalgorithm by choosing an initial value θold for the model parameters. In the E step,\nwe then use these parameter values to evaluate the posterior probabilities of the com-\nponentsk for each data point n, which are given by\nγnk = E[znk]= p(k|φn, θold)= πkytn\nnk [1 − ynk]1−tn\n∑\nj πjytn\nnj [1 − ynj]1−tn\n. (14.48)\nThese responsibilities are then used to ﬁnd the expected complete-data log likelihood",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 690,
      "page_label": "671"
    }
  },
  {
    "page_content": "nk [1 − ynk]1−tn\n∑\nj πjytn\nnj [1 − ynj]1−tn\n. (14.48)\nThese responsibilities are then used to ﬁnd the expected complete-data log likelihood\nas a function of θ, given by\nQ(θ, θold)= EZ [lnp(t, Z|θ)]\n=\nN∑\nn=1\nK∑\nk=1\nγnk {lnπk + tn lnynk +( 1− tn)l n( 1− ynk)}. (14.49)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 690,
      "page_label": "671"
    }
  },
  {
    "page_content": "672 14. COMBINING MODELS\nThe M step involves maximization of this function with respect to θ, keeping θold,\nand hence γnk, ﬁxed. Maximization with respect to πk can be done in the usual way,\nwith a Lagrange multiplier to enforce the summation constraint ∑\nk πk =1 , giving\nthe familiar result\nπk = 1\nN\nN∑\nn=1\nγnk. (14.50)\nTo determine the {wk}, we note that the Q(θ, θold) function comprises a sum\nover terms indexed by k each of which depends only on one of the vectors wk,s o\nthat the different vectors are decoupled in the M step of the EM algorithm. In other\nwords, the different components interact only via the responsibilities, which are ﬁxed\nduring the M step. Note that the M step does not have a closed-form solution and\nmust be solved iteratively using, for instance, the iterative reweighted least squares\n(IRLS) algorithm. The gradient and the Hessian for the vector wk are given bySection 4.3.3\n∇kQ =\nN∑\nn=1\nγnk(tn − ynk)φn (14.51)\nHk = −∇k∇kQ =\nN∑\nn=1\nγnkynk(1 − ynk)φnφT\nn (14.52)\nwhere ∇k denotes the gradient with respect to wk. For ﬁxed γnk, these are indepen-\ndent of {wj} for j ̸=k and so we can solve for each wk separately using the IRLS\nalgorithm. Thus the M-step equations for component k correspond simply to ﬁttingSection 4.3.3\na single logistic regression model to a weighted data set in which data pointn carries\na weight γnk. Figure 14.10 shows an example of the mixture of logistic regression",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 691,
      "page_label": "672"
    }
  },
  {
    "page_content": "a weight γnk. Figure 14.10 shows an example of the mixture of logistic regression\nmodels applied to a simple classiﬁcation problem. The extension of this model to a\nmixture of softmax models for more than two classes is straightforward.Exercise 14.16\n14.5.3 Mixtures of experts\nIn Section 14.5.1, we considered a mixture of linear regression models, and in\nSection 14.5.2 we discussed the analogous mixture of linear classiﬁers. Although\nthese simple mixtures extend the ﬂexibility of linear models to include more com-\nplex (e.g., multimodal) predictive distributions, they are still very limited. We can\nfurther increase the capability of such models by allowing the mixing coefﬁcients\nthemselves to be functions of the input variable, so that\np(t|x)=\nK∑\nk=1\nπk(x)pk(t|x). (14.53)\nThis is known as a mixture of expertsmodel (Jacobs et al., 1991) in which the mix-\ning coefﬁcients πk(x) are known as gating functions and the individual component\ndensities pk(t|x) are called experts. The notion behind the terminology is that differ-\nent components can model the distribution in different regions of input space (they",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 691,
      "page_label": "672"
    }
  },
  {
    "page_content": "14.5. Conditional Mixture Models 673\nFigure 14.10 Illustration of a mixture of logistic regression models. The left plot shows data points drawn\nfrom two classes denoted red and blue, in which the background colour (which varies from pure red to pure blue)\ndenotes the true probability of the class label. The centre plot shows the result of ﬁtting a single logistic regression\nmodel using maximum likelihood, in which the background colour denotes the corresponding probability of the\nclass label. Because the colour is a near-uniform purple, we see that the model assigns a probability of around\n0.5 to each of the classes over most of input space. The right plot shows the result of ﬁtting a mixture of two\nlogistic regression models, which now gives much higher probability to the correct labels for many of the points\nin the blue class.\nare ‘experts’ at making predictions in their own regions), and the gating functions\ndetermine which components are dominant in which region.\nThe gating functions πk(x) must satisfy the usual constraints for mixing co-\nefﬁcients, namely 0 ⩽ πk(x) ⩽ 1 and ∑\nk πk(x)=1 . They can therefore be\nrepresented, for example, by linear softmax models of the form (4.104) and (4.105).\nIf the experts are also linear (regression or classiﬁcation) models, then the whole\nmodel can be ﬁtted efﬁciently using the EM algorithm, with iterative reweighted\nleast squares being employed in the M step (Jordan and Jacobs, 1994).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 692,
      "page_label": "673"
    }
  },
  {
    "page_content": "model can be ﬁtted efﬁciently using the EM algorithm, with iterative reweighted\nleast squares being employed in the M step (Jordan and Jacobs, 1994).\nSuch a model still has signiﬁcant limitations due to the use of linear models\nfor the gating and expert functions. A much more ﬂexible model is obtained by\nusing a multilevel gating function to give thehierarchical mixture of experts,o r\nHME model (Jordan and Jacobs, 1994). To understand the structure of this model,\nimagine a mixture distribution in which each component in the mixture is itself a\nmixture distribution. For simple unconditional mixtures, this hierarchical mixture is\ntrivially equivalent to a single ﬂat mixture distribution. However, when the mixingExercise 14.17\ncoefﬁcients are input dependent, this hierarchical model becomes nontrivial. The\nHME model can also be viewed as a probabilistic version ofdecision treesdiscussed\nin Section 14.4 and can again be trained efﬁciently by maximum likelihood using an\nEM algorithm with IRLS in the M step. A Bayesian treatment of the HME has beenSection 4.3.3\ngiven by Bishop and Svens´en (2003) based on variational inference.\nWe shall not discuss the HME in detail here. However, it is worth pointing out\nthe close connection with the mixture density networkdiscussed in Section 5.6. The\nprincipal advantage of the mixtures of experts model is that it can be optimized by\nEM in which the M step for each mixture component and gating model involves",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 692,
      "page_label": "673"
    }
  },
  {
    "page_content": "EM in which the M step for each mixture component and gating model involves\na convex optimization (although the overall optimization is nonconvex). By con-\ntrast, the advantage of the mixture density network approach is that the component",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 692,
      "page_label": "673"
    }
  },
  {
    "page_content": "674 14. COMBINING MODELS\ndensities and the mixing coefﬁcients share the hidden units of the neural network.\nFurthermore, in the mixture density network, the splits of the input space are further\nrelaxed compared to the hierarchical mixture of experts in that they are not only soft,\nand not constrained to be axis aligned, but they can also be nonlinear.\nExercises\n14.1 (⋆⋆ ) www Consider a set models of the form p(t|x, zh, θh,h ) in which x is the\ninput vector, t is the target vector, h indexes the different models, zh is a latent vari-\nable for model h, and θh is the set of parameters for model h. Suppose the models\nhave prior probabilities p(h) and that we are given a training set X = {x1,..., xN }\nand T = {t1,..., tN }. Write down the formulae needed to evaluate the predic-\ntive distribution p(t|x, X, T) in which the latent variables and the model index are\nmarginalized out. Use these formulae to highlight the difference between Bayesian\naveraging of different models and the use of latent variables within a single model.\n14.2 (⋆) The expected sum-of-squares error EAV for a simple committee model can\nbe deﬁned by (14.10), and the expected error of the committee itself is given by\n(14.11). Assuming that the individual errors satisfy (14.12) and (14.13), derive the\nresult (14.14).\n14.3 (⋆) www By making use of Jensen’s inequality (1.115), for the special case of\nthe convex function f(x)= x2, show that the average expected sum-of-squares",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 693,
      "page_label": "674"
    }
  },
  {
    "page_content": "the convex function f(x)= x2, show that the average expected sum-of-squares\nerror EAV of the members of a simple committee model, given by (14.10), and the\nexpected error ECOM of the committee itself, given by (14.11), satisfy\nECOM ⩽ EAV. (14.54)\n14.4 (⋆⋆ ) By making use of Jensen’s in equality (1.115), show that the result (14.54)\nderived in the previous exercise hods for any error function E(y), not just sum-of-\nsquares, provided it is a convex function of y.\n14.5 (⋆⋆ ) www Consider a committee in which we allow unequal weighting of the\nconstituent models, so that\nyCOM(x)=\nM∑\nm=1\nαmym(x). (14.55)\nIn order to ensure that the predictions yCOM(x) remain within sensible limits, sup-\npose that we require that they be bounded at each value of x by the minimum and\nmaximum values given by any of the members of the committee, so that\nymin(x) ⩽ yCOM(x) ⩽ ymax(x). (14.56)\nShow that a necessary and sufﬁcient condition for this constraint is that the coefﬁ-\ncients αm satisfy\nαm ⩾ 0,\nM∑\nm=1\nαm =1 . (14.57)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 693,
      "page_label": "674"
    }
  },
  {
    "page_content": "Exercises 675\n14.6 (⋆) www By differentiating the error function (14.23) with respect to αm, show\nthat the parameters αm in the AdaBoost algorithm are updated using (14.17) in\nwhich ϵm is deﬁned by (14.16).\n14.7 (⋆) By making a variational minimization of the expected exponential error function\ngiven by (14.27) with respect to all possible functionsy(x), show that the minimizing\nfunction is given by (14.28).\n14.8 (⋆) Show that the exponential error function (14.20), which is minimized by the\nAdaBoost algorithm, does not correspond to the log likelihood of any well-behaved\nprobabilistic model. This can be done by showing that the corresponding conditional\ndistributionp(t|x) cannot be correctly normalized.\n14.9 (⋆) www Show that the sequential minimization of the sum-of-squares error func-\ntion for an additive model of the form (14.21) in the style of boosting simply involves\nﬁtting each new base classiﬁer to the residual errorstn−fm−1(xn) from the previous\nmodel.\n14.10 (⋆) Verify that if we minimize the sum-of-squares error between a set of training\nvalues {tn} and a single predictive value t, then the optimal solution for t is given\nby the mean of the {tn}.\n14.11 (⋆⋆ ) Consider a data set comprising 400 data points from class C1 and 400 data\npoints from class C2. Suppose that a tree model A splits these into (300, 100) at\nthe ﬁrst leaf node and (100, 300) at the second leaf node, where (n, m) denotes that",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 694,
      "page_label": "675"
    }
  },
  {
    "page_content": "the ﬁrst leaf node and (100, 300) at the second leaf node, where (n, m) denotes that\nn points are assigned to C1 and m points are assigned to C2. Similarly, suppose\nthat a second tree model B splits them into (200, 400) and (200, 0). Evaluate the\nmisclassiﬁcation rates for the two trees and hence show that they are equal. Similarly,\nevaluate the cross-entropy (14.32) and Gini index (14.33) for the two trees and show\nthat they are both lower for tree B than for tree A.\n14.12 (⋆⋆ ) Extend the results of Section 14.5.1 for a mixture of linear regression models\nto the case of multiple target values described by a vector t. To do this, make use of\nthe results of Section 3.1.5.\n14.13 (⋆) www Verify that the complete-data log likelihood function for the mixture of\nlinear regression models is given by (14.36).\n14.14 (⋆) Use the technique of Lagrange multipliers (Appendix E) to show that the M-step\nre-estimation equation for the mixing coefﬁcients in the mixture of linear regression\nmodels trained by maximum likelihood EM is given by (14.38).\n14.15 (⋆) www We have already noted that if we use a squared loss function in a regres-\nsion problem, the corresponding optimal prediction of the target variable for a new\ninput vector is given by the conditional mean of the predictive distribution. Show\nthat the conditional mean for the mixture of linear regression models discussed in\nSection 14.5.1 is given by a linear combination of the means of each component dis-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 694,
      "page_label": "675"
    }
  },
  {
    "page_content": "Section 14.5.1 is given by a linear combination of the means of each component dis-\ntribution. Note that if the conditional distribution of the target data is multimodal,\nthe conditional mean can give poor predictions.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 694,
      "page_label": "675"
    }
  },
  {
    "page_content": "676 14. COMBINING MODELS\n14.16 (⋆⋆⋆ ) Extend the logistic regression mixture model of Section 14.5.2 to a mixture\nof softmax classiﬁers representing C ⩾ 2 classes. Write down the EM algorithm for\ndetermining the parameters of this model through maximum likelihood.\n14.17 (⋆⋆ ) www Consider a mixture model for a conditional distribution p(t|x) of the\nform\np(t|x)=\nK∑\nk=1\nπkψk(t|x) (14.58)\nin which each mixture component ψk(t|x) is itself a mixture model. Show that this\ntwo-level hierarchical mixture is equivalent to a conventional single-level mixture\nmodel. Now suppose that the mixing coefﬁcients in both levels of such a hierar-\nchical model are arbitrary functions of x. Again, show that this hierarchical model\nis again equivalent to a single-level model with x-dependent mixing coefﬁcients.\nFinally, consider the case in which the mixing coefﬁcients at both levels of the hi-\nerarchical mixture are constrained to be linear classiﬁcation (logistic or softmax)\nmodels. Show that the hierarchical mixture cannot in general be represented by a\nsingle-level mixture having linear classiﬁcation models for the mixing coefﬁcients.\nHint: to do this it is sufﬁcient to construct a single counter-example, so consider a\nmixture of two components in which one of those components is itself a mixture of\ntwo components, with mixing coefﬁcients given by linear-logistic models. Show that\nthis cannot be represented by a single-level mixture of 3 components having mixing",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 695,
      "page_label": "676"
    }
  },
  {
    "page_content": "this cannot be represented by a single-level mixture of 3 components having mixing\ncoefﬁcients determined by a linear-softmax model.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 695,
      "page_label": "676"
    }
  },
  {
    "page_content": "Appendix A. Data Sets\nIn this appendix, we give a brief introduction to the data sets used to illustrate some\nof the algorithms described in this book. Detailed information on ﬁle formats for\nthese data sets, as well as the data ﬁles themselves, can be obtained from the book\nweb site:\nhttp://research.microsoft.com/∼cmbishop/PRML\nHandwritten Digits\nThe digits data used in this book is taken from the MNIST data set (LeCun et al.,\n1998), which itself was constructed by modifying a subset of the much larger data\nset produced by NIST (the National Institute of Standards and Technology). It com-\nprises a training set of 60, 000 examples and a test set of 10, 000 examples. Some\nof the data was collected from Census Bureau employees and the rest was collected\nfrom high-school children, and care was taken to ensure that the test examples were\nwritten by different individuals to the training examples.\nThe original NIST data had binary (black or white) pixels. To create MNIST,\nthese images were size normalized to ﬁt in a20×20 pixel box while preserving their\naspect ratio. As a consequence of the anti-aliasing used to change the resolution of\nthe images, the resulting MNIST digits are grey scale. These images were then\ncentred in a 28 × 28 box. Examples of the MNIST digits are shown in Figure A.1.\nError rates for classifying the digits range from 12% for a simple linear classi-\nﬁer, through 0.56% for a carefully designed support vector machine, to 0.4% for a",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 696,
      "page_label": "677"
    }
  },
  {
    "page_content": "ﬁer, through 0.56% for a carefully designed support vector machine, to 0.4% for a\nconvolutional neural network (LeCun et al., 1998).\n677",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 696,
      "page_label": "677"
    }
  },
  {
    "page_content": "678 A. DATA SETS\nFigure A.1 One hundred examples of the\nMNIST digits chosen at ran-\ndom from the training set.\nOil Flow\nThis is a synthetic data set that arose out of a project aimed at measuring nonin-\nvasively the proportions of oil, water, and gas in North Sea oil transfer pipelines\n(Bishop and James, 1993). It is based on the principle ofdual-energy gamma densit-\nometry. The ideas is that if a narrow beam of gamma rays is passed through the pipe,\nthe attenuation in the intensity of the beam provides information about the density of\nmaterial along its path. Thus, for instance, the beam will be attenuated more strongly\nby oil than by gas.\nA single attenuation measurement alone is not sufﬁcient because there are two\ndegrees of freedom corresponding to the fraction of oil and the fraction of water (the\nfraction of gas is redundant because the three fractions must add to one). To address\nthis, two gamma beams of different energies (in other words different frequencies or\nwavelengths) are passed through the pipe along the same path, and the attenuation of\neach is measured. Because the absorbtion properties of different materials vary dif-\nferently as a function of energy, measurement of the attenuations at the two energies\nprovides two independent pieces of information. Given the known absorbtion prop-\nerties of oil, water, and gas at the two energies, it is then a simple matter to calculate\nthe average fractions of oil and water (and hence of gas) measured along the pathof",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 697,
      "page_label": "678"
    }
  },
  {
    "page_content": "the average fractions of oil and water (and hence of gas) measured along the pathof\nthe gamma beams.\nThere is a further complication, however, associated with the motion of the ma-\nterials along the pipe. If the ﬂow velocity is small, then the oil ﬂoats on top of the\nwater with the gas sitting above the oil. This is known as a laminar or stratiﬁed",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 697,
      "page_label": "678"
    }
  },
  {
    "page_content": "A. DATA SETS 679\nFigure A.2 The three geometrical conﬁgurations of the oil,\nwater, and gas phases used to generate the oil-\nﬂow data set. For each conﬁguration, the pro-\nportions of the three phases can vary.\nMix\nGas\nWater\nOil\nHomogeneous\nStratiﬁed Annular\nﬂow conﬁguration and is illustrated in Figure A.2. As the ﬂow velocity is increased,\nmore complex geometrical conﬁgurations of the oil, water, and gas can arise. For the\npurposes of this data set, two speciﬁc idealizations are considered. In the annular\nconﬁguration the oil, water, and gas form concentric cylinders with the water around\nthe outside and the gas in the centre, whereas in the homogeneous conﬁguration the\noil, water and gas are assumed to be intimately mixed as might occur at high ﬂow\nvelocities under turbulent conditions. These conﬁgurations are also illustrated in\nFigure A.2.\nWe have seen that a single dual-energy beam gives the oil and water fractions\nmeasured along the path length, whereas we are interested in the volume fractions of\noil and water. This can be addressed by using multiple dual-energy gamma densit-\nometers whose beams pass through different regions of the pipe. For this particular\ndata set, there are six such beams, and their spatial arrangement is shown in Fig-\nure A.3. A single observation is therefore represented by a 12-dimensional vector\ncomprising the fractions of oil and water measured along the paths of each of the",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 698,
      "page_label": "679"
    }
  },
  {
    "page_content": "comprising the fractions of oil and water measured along the paths of each of the\nbeams. We are, however, interested in obtaining the overall volume fractions of the\nthree phases in the pipe. This is much like the classical problem of tomographic re-\nconstruction, used in medical imaging for example, in which a two-dimensional dis-\nFigure A.3 Cross section of the pipe showing the arrangement of the\nsix beam lines, each of which comprises a single dual-\nenergy gamma densitometer. Note that the vertical beams\nare asymmetrically arranged relative to the central axis\n(shown by the dotted line).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 698,
      "page_label": "679"
    }
  },
  {
    "page_content": "680 A. DATA SETS\ntribution is to be reconstructed from an number of one-dimensional averages. Here\nthere are far fewer line measurements than in a typical tomography application. On\nthe other hand the range of geometrical conﬁgurations is much more limited, and so\nthe conﬁguration, as well as the phase fractions, can be predicted with reasonable\naccuracy from the densitometer data.\nFor safety reasons, the intensity of the gamma beams is kept relatively weak and\nso to obtain an accurate measurement of the attenuation, the measured beam intensity\nis integrated over a speciﬁc time interval. For a ﬁnite integration time, there are\nrandom ﬂuctuations in the measured intensity due to the fact that the gamma beams\ncomprise discrete packets of energy called photons. In practice, the integration time\nis chosen as a compromise between reducing the noise level (which requires a long\nintegration time) and detecting temporal variations in the ﬂow (which requires a short\nintegration time). The oil ﬂow data set is generated using realistic known values for\nthe absorption properties of oil, water, and gas at the two gamma energies used, and\nwith a speciﬁc choice of integration time ( 10 seconds) chosen as characteristic of a\ntypical practical setup.\nEach point in the data set is generated independently using the following steps:\n1. Choose one of the three phase conﬁgurations at random with equal probability.\n2. Choose three random numbersf1, f2 and f3 from the uniform distribution over",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 699,
      "page_label": "680"
    }
  },
  {
    "page_content": "2. Choose three random numbersf1, f2 and f3 from the uniform distribution over\n(0, 1) and deﬁne\nfoil = f1\nf1 + f2 + f3\n,f water = f2\nf1 + f2 + f3\n. (A.1)\nThis treats the three phases on an equal footing and ensures that the volume\nfractions add to one.\n3. For each of the six beam lines, calculate the effective path lengths through oil\nand water for the given phase conﬁguration.\n4. Perturb the path lengths using the Poisson distribution based on the known\nbeam intensities and integration time to allow for the effect of photon statistics.\nEach point in the data set comprises the 12 path length measurements, together\nwith the fractions of oil and water and a binary label describing the phase conﬁgu-\nration. The data set is divided into training, validation, and test sets, each of which\ncomprises 1, 000 independent data points. Details of the data format are available\nfrom the book web site.\nIn Bishop and James (1993), statistical machine learning techniques were used\nto predict the volume fractions and also the geometrical conﬁguration of the phases\nshown in Figure A.2, from the 12-dimensional vector of measurements. The 12-\ndimensional observation vectors can also be used to test data visualization algo-\nrithms.\nThis data set has a rich and interesting structure, as follows. For any given\nconﬁguration there are two degrees of freedom corresponding to the fractions of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 699,
      "page_label": "680"
    }
  },
  {
    "page_content": "A. DATA SETS 681\noil and water, and so for inﬁnite integration time the data will locally live on a two-\ndimensional manifold. For a ﬁnite integration time, the individual data points will be\nperturbed away from the manifold by the photon noise. In the homogeneous phase\nconﬁguration, the path lengths in oil and water are linearly related to the fractions of\noil and water, and so the data points lie close to a linear manifold. For the annular\nconﬁguration, the relationship between phase fraction and path length is nonlinear\nand so the manifold will be nonlinear. In the case of the laminar conﬁguration the\nsituation is even more complex because small variations in the phase fractions can\ncause one of the horizontal phase boundaries to move across one of the horizontal\nbeam lines leading to a discontinuous jump in the12-dimensional observation space.\nIn this way, the two-dimensional nonlinear manifold for the laminar conﬁguration is\nbroken into six distinct segments. Note also that some of the manifolds for different\nphase conﬁgurations meet at speciﬁc points, for example if the pipe is ﬁlled entirely\nwith oil, it corresponds to speciﬁc instances of the laminar, annular, and homoge-\nneous conﬁgurations.\nOld Faithful\nOld Faithful, shown in Figure A.4, is a hydrothermal geyser in Yellowstone National\nPark in the state of Wyoming, U.S.A., and is a popular tourist attraction. Its name\nstems from the supposed regularity of its eruptions.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 700,
      "page_label": "681"
    }
  },
  {
    "page_content": "Park in the state of Wyoming, U.S.A., and is a popular tourist attraction. Its name\nstems from the supposed regularity of its eruptions.\nThe data set comprises272 observations, each of which represents a single erup-\ntion and contains two variables corresponding to the duration in minutes of the erup-\ntion, and the time until the next eruption, also in minutes. Figure A.5 shows a plot of\nthe time to the next eruption versus the duration of the eruptions. It can be seen that\nthe time to the next eruption varies considerably, although knowledge of the duration\nof the current eruption allows it to be predicted more accurately. Note that there exist\nseveral other data sets relating to the eruptions of Old Faithful.\nFigure A.4 The Old Faithful geyser\nin Y ellowstone National\nPark. c⃝ Bruce T. Gourley\nwww.brucegourley.com.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 700,
      "page_label": "681"
    }
  },
  {
    "page_content": "682 A. DATA SETS\nFigure A.5 Plot of the time to the next eruption\nin minutes (vertical axis) versus the\nduration of the eruption in minutes\n(horizontal axis) for the Old Faithful\ndata set.\n1 2 3 4 5 6\n40\n50\n60\n70\n80\n90\n100\nSynthetic Data\nThroughout the book, we use two simple synthetic data sets to illustrate many of the\nalgorithms. The ﬁrst of these is a regression problem, based on the sinusoidal func-\ntion, shown in Figure A.6. The input values {xn} are generated uniformly in range\n(0, 1), and the corresponding target values {tn} are obtained by ﬁrst computing the\ncorresponding values of the function sin(2πx), and then adding random noise with\na Gaussian distribution having standard deviation0.3. Various forms of this data set,\nhaving different numbers of data points, are used in the book.\nThe second data set is a classiﬁcation problem having two classes, with equal\nprior probabilities, and is shown in Figure A.7. The blue class is generated from a\nsingle Gaussian while the red class comes from a mixture of two Gaussians. Be-\ncause we know the class priors and the class-conditional densities, it is straightfor-\nward to evaluate and plot the true posterior probabilities as well as the minimum\nmisclassiﬁcation-rate decision boundary, as shown in Figure A.7.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 701,
      "page_label": "682"
    }
  },
  {
    "page_content": "A. DATA SETS 683\nx\nt\n0 1\n−1\n0\n1\nx\nt\n0 1\n−1\n0\n1\nFigure A.6 The left-hand plot shows the synthetic regression data set along with the underlying sinusoidal\nfunction from which the data points were generated. The right-hand plot shows the true conditional distribution\np(t|x) from which the labels are generated, in which the green curve denotes the mean, and the shaded region\nspans one standard deviation on each side of the mean.\n−2 0 2\n−2\n0\n2\nFigure A.7 The left plot shows the synthetic classiﬁcation data set with data from the two classes shown in\nred and blue. On the right is a plot of the true posterior probabilities, shown on a colour scale going from pure\nred denoting probability of the red class is 1 to pure blue denoting probability of the red class is 0. Because\nthese probabilities are known, the optimal decision boundary for minimizing the misclassiﬁcation rate (which\ncorresponds to the contour along which the posterior probabilities for each class equal 0.5) can be evaluated\nand is shown by the green curve. This decision boundary is also plotted on the left-hand ﬁgure.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 702,
      "page_label": "683"
    }
  },
  {
    "page_content": "Appendix B. Probability Distributions\nIn this appendix, we summarize the main properties of some of the most widely used\nprobability distributions, and for each distribution we list some key statistics such as\nthe expectation E[x], the variance (or covariance), the mode, and the entropy H[x].\nAll of these distributions are members of the exponential family and are widely used\nas building blocks for more sophisticated probabilistic models.\nBernoulli\nThis is the distribution for a single binary variable x ∈{ 0, 1} representing, for\nexample, the result of ﬂipping a coin. It is governed by a single continuous parameter\nµ ∈ [0,1] that represents the probability of x =1 .\nBern(x|µ)= µx(1 − µ)1−x (B.1)\nE[x]= µ (B.2)\nvar[x]= µ(1 − µ) (B.3)\nmode[x]=\n{\n1 if µ ⩾ 0.5,\n0 otherwise (B.4)\nH[x]= −µlnµ − (1 − µ)l n ( 1− µ). (B.5)\nThe Bernoulli is a special case of the binomial distribution for the case of a single\nobservation. Its conjugate prior for µ is the beta distribution.\n685",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 704,
      "page_label": "685"
    }
  },
  {
    "page_content": "686 B. PROBABILITY DISTRIBUTIONS\nBeta\nThis is a distribution over a continuous variable µ ∈ [0,1], which is often used to\nrepresent the probability for some binary event. It is governed by two parameters a\nand b that are constrained by a> 0 and b> 0 to ensure that the distribution can be\nnormalized.\nBeta(µ|a, b)= Γ(a + b)\nΓ(a)Γ(b)µa−1(1 − µ)b−1 (B.6)\nE[µ]= a\na + b (B.7)\nvar[µ]= ab\n(a + b)2(a + b +1 ) (B.8)\nmode[µ]= a − 1\na + b − 2. (B.9)\nThe beta is the conjugate prior for the Bernoulli distribution, for which a and b can\nbe interpreted as the effective prior number of observations of x =1 and x =0 ,\nrespectively. Its density is ﬁnite if a ⩾ 1 and b ⩾ 1, otherwise there is a singularity\nat µ =0 and/or µ =1 .F o ra = b =1 , it reduces to a uniform distribution. The beta\ndistribution is a special case of the K-state Dirichlet distribution for K =2 .\nBinomial\nThe binomial distribution gives the probability of observingm occurrences of x =1\nin a set of N samples from a Bernoulli distribution, where the probability of observ-\ning x =1 is µ ∈ [0,1].\nBin(m|N,µ )=\n(N\nm\n)\nµm(1 − µ)N−m (B.10)\nE[m]= Nµ (B.11)\nvar[m]= Nµ(1 − µ) (B.12)\nmode[m]= ⌊(N +1 )µ⌋ (B.13)\nwhere ⌊(N +1 )µ⌋denotes the largest integer that is less than or equal to (N +1 )µ,\nand the quantity (N\nm\n)\n= N!\nm!(N − m)! (B.14)\ndenotes the number of ways of choosing m objects out of a total of N identical\nobjects. Here m!, pronounced ‘factorial m’, denotes the product m × (m − 1) ×",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 705,
      "page_label": "686"
    }
  },
  {
    "page_content": "objects. Here m!, pronounced ‘factorial m’, denotes the product m × (m − 1) ×\n..., ×2 × 1. The particular case of the binomial distribution for N =1 is known as\nthe Bernoulli distribution, and for largeN the binomial distribution is approximately\nGaussian. The conjugate prior for µ is the beta distribution.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 705,
      "page_label": "686"
    }
  },
  {
    "page_content": "B. PROBABILITY DISTRIBUTIONS 687\nDirichlet\nThe Dirichlet is a multivariate distribution overK random variables 0 ⩽ µk ⩽ 1,\nwhere k =1 ,...,K , subject to the constraints\n0 ⩽ µk ⩽ 1,\nK∑\nk=1\nµk =1 . (B.15)\nDenoting µ =( µ1,...,µ K)T and α =( α1,...,α K)T,w eh a v e\nDir(µ|α)= C(α)\nK∏\nk=1\nµαk−1\nk (B.16)\nE[µk]= αk\nˆα (B.17)\nvar[µk]= αk(ˆα − αk)\nˆα2(ˆα +1 ) (B.18)\ncov[µjµk]= − αjαk\nˆα2(ˆα +1 ) (B.19)\nmode[µk]= αk − 1\nˆα − K (B.20)\nE[ln µk]= ψ(αk) − ψ(ˆα) (B.21)\nH[µ]= −\nK∑\nk=1\n(αk − 1) {ψ(αk) − ψ(ˆα)}− lnC(α) (B.22)\nwhere\nC(α)= Γ(ˆα)\nΓ(α1) ··· Γ(αK) (B.23)\nand\nˆα =\nK∑\nk=1\nαk. (B.24)\nHere\nψ(a) ≡ d\nda ln Γ(a) (B.25)\nis known as thedigamma function (Abramowitz and Stegun, 1965). The parameters\nαk are subject to the constraintαk > 0 in order to ensure that the distribution can be\nnormalized.\nThe Dirichlet forms the conjugate prior for the multinomial distribution and rep-\nresents a generalization of the beta distribution. In this case, the parametersαk can\nbe interpreted as effective numbers of observations of the corresponding values of\nthe K-dimensional binary observation vector x. As with the beta distribution, the\nDirichlet has ﬁnite density everywhere providedαk ⩾ 1 for all k.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 706,
      "page_label": "687"
    }
  },
  {
    "page_content": "688 B. PROBABILITY DISTRIBUTIONS\nGamma\nThe Gamma is a probability distribution over a positive random variable τ> 0\ngoverned by parameters a and b that are subject to the constraints a> 0 and b> 0\nto ensure that the distribution can be normalized.\nGam(τ|a, b)= 1\nΓ(a)baτa−1e−bτ (B.26)\nE[τ]= a\nb (B.27)\nvar[τ]= a\nb2 (B.28)\nmode[τ]= a − 1\nb for α ⩾ 1 (B.29)\nE[lnτ]= ψ(a) − lnb (B.30)\nH[τ]=l n Γ ( a) − (a − 1)ψ(a) − lnb + a (B.31)\nwhere ψ(·) is the digamma function deﬁned by (B.25). The gamma distribution is\nthe conjugate prior for the precision (inverse variance) of a univariate Gaussian. For\na ⩾ 1 the density is everywhere ﬁnite, and the special case of a =1 is known as the\nexponential distribution.\nGaussian\nThe Gaussian is the most widely used distribution for continuous variables. It is also\nknown as the normal distribution. In the case of a single variable x ∈ (−∞, ∞) it is\ngoverned by two parameters, the mean µ ∈ (−∞, ∞) and the variance σ2 > 0.\nN(x|µ, σ2)= 1\n(2πσ2)1/2 exp\n{\n− 1\n2σ2 (x − µ)2\n}\n(B.32)\nE[x]= µ (B.33)\nvar[x]= σ2 (B.34)\nmode[x]= µ (B.35)\nH[x]= 1\n2 ln σ2 + 1\n2 (1 + ln(2π)) . (B.36)\nThe inverse of the variance τ =1 /σ2 is called the precision, and the square root\nof the variance σ is called the standard deviation. The conjugate prior for µ is the\nGaussian, and the conjugate prior for τ is the gamma distribution. If both µ and τ\nare unknown, their joint conjugate prior is the Gaussian-gamma distribution.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 707,
      "page_label": "688"
    }
  },
  {
    "page_content": "are unknown, their joint conjugate prior is the Gaussian-gamma distribution.\nFor a D-dimensional vector x, the Gaussian is governed by a D-dimensional\nmean vector µ and a D × D covariance matrix Σ that must be symmetric and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 707,
      "page_label": "688"
    }
  },
  {
    "page_content": "B. PROBABILITY DISTRIBUTIONS 689\npositive-deﬁnite.\nN(x|µ,Σ)= 1\n(2π)D/2\n1\n|Σ|1/2 exp\n{\n−1\n2(x − µ)TΣ−1(x − µ)\n}\n(B.37)\nE[x]= µ (B.38)\ncov[x]= Σ (B.39)\nmode[x]= µ (B.40)\nH[x]= 1\n2 ln|Σ| + D\n2 (1 + ln(2π)). (B.41)\nThe inverse of the covariance matrixΛ = Σ−1 is the precision matrix, which is also\nsymmetric and positive deﬁnite. Averages of random variables tend to a Gaussian, by\nthe central limit theorem, and the sum of two Gaussian variables is again Gaussian.\nThe Gaussian is the distribution that maximizes the entropy for a given variance\n(or covariance). Any linear transformation of a Gaussian random variable is again\nGaussian. The marginal distribution of a multivariate Gaussian with respect to a\nsubset of the variables is itself Gaussian, and similarly the conditional distribution is\nalso Gaussian. The conjugate prior for µ is the Gaussian, the conjugate prior for Λ\nis the Wishart, and the conjugate prior for (µ, Λ) is the Gaussian-Wishart.\nIf we have a marginal Gaussian distribution for x and a conditional Gaussian\ndistribution for y given x in the form\np(x)= N(x|µ, Λ−1) (B.42)\np(y|x)= N(y|Ax + b, L−1) (B.43)\nthen the marginal distribution of y, and the conditional distribution of x given y, are\ngiven by\np(y)= N(y|Aµ + b, L−1 + AΛ−1AT) (B.44)\np(x|y)= N(x|Σ{ATL(y − b)+ Λµ}, Σ) (B.45)\nwhere\nΣ =( Λ + ATLA)−1. (B.46)\nIf we have a joint Gaussian distribution N(x|µ, Σ) with Λ ≡ Σ−1 and we\ndeﬁne the following partitions\nx =\n(\nxa\nxb\n)\n, µ =\n(\nµa\nµb\n)\n(B.47)\nΣ =\n(\nΣaa Σab\nΣba Σbb\n)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 708,
      "page_label": "689"
    }
  },
  {
    "page_content": "deﬁne the following partitions\nx =\n(\nxa\nxb\n)\n, µ =\n(\nµa\nµb\n)\n(B.47)\nΣ =\n(\nΣaa Σab\nΣba Σbb\n)\n, Λ =\n(\nΛaa Λab\nΛba Λbb\n)\n(B.48)\nthen the conditional distribution p(xa|xb) is given by\np(xa|xb)= N(x|µa|b, Λ−1\naa ) (B.49)\nµa|b = µa − Λ−1\naa Λab(xb − µb) (B.50)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 708,
      "page_label": "689"
    }
  },
  {
    "page_content": "690 B. PROBABILITY DISTRIBUTIONS\nand the marginal distribution p(xa) is given by\np(xa)= N(xa|µa, Σaa). (B.51)\nGaussian-Gamma\nThis is the conjugate prior distribution for a univariate Gaussian N(x|µ, λ−1) in\nwhich the mean µ and the precision λ are both unknown and is also called the\nnormal-gamma distribution. It comprises the product of a Gaussian distribution for\nµ, whose precision is proportional to λ, and a gamma distribution over λ.\np(µ, λ|µ0,β,a ,b )= N\n(\nµ|µo, (βλ)−1)\nGam(λ|a, b). (B.52)\nGaussian-Wishart\nThis is the conjugate prior distribution for a multivariate Gaussian N(x|µ, Λ) in\nwhich both the mean µ and the precision Λ are unknown, and is also called the\nnormal-Wishart distribution. It comprises the product of a Gaussian distribution for\nµ, whose precision is proportional to Λ, and a Wishart distribution over Λ.\np(µ, Λ|µ0,β, W,ν )= N\n(\nµ|µ0, (βΛ)−1)\nW(Λ|W,ν ). (B.53)\nFor the particular case of a scalar x, this is equivalent to the Gaussian-gamma distri-\nbution.\nMultinomial\nIf we generalize the Bernoulli distribution to an K-dimensional binary variable x\nwith components xk ∈{ 0, 1} such that ∑\nk xk =1 , then we obtain the following\ndiscrete distribution\np(x)=\nK∏\nk=1\nµxk\nk (B.54)\nE[xk]= µk (B.55)\nvar[xk]= µk(1 − µk) (B.56)\ncov[xjxk]= Ijkµk (B.57)\nH[x]= −\nM∑\nk=1\nµk ln µk (B.58)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 709,
      "page_label": "690"
    }
  },
  {
    "page_content": "B. PROBABILITY DISTRIBUTIONS 691\nwhere Ijk is the j, kelement of the identity matrix. Because p(xk =1 )= µk, the\nparameters must satisfy 0 ⩽ µk ⩽ 1 and ∑\nk µk =1 .\nThe multinomial distribution is a multivariate generalization of the binomial and\ngives the distribution over counts mk for a K-state discrete variable to be in state k\ngiven a total number of observations N.\nMult(m1,m 2,...,m K|µ,N )=\n( N\nm1m2 ...m M\n) M∏\nk=1\nµmk\nk (B.59)\nE[mk]= Nµk (B.60)\nvar[mk]= Nµk(1 − µk) (B.61)\ncov[mjmk]= −Nµjµk (B.62)\nwhere µ =( µ1,...,µ K)T, and the quantity\n( N\nm1m2 ...m K\n)\n= N!\nm1! ...m K! (B.63)\ngives the number of ways of taking N identical objects and assigning mk of them to\nbin k for k =1 ,...,K . The value of µk gives the probability of the random variable\ntaking state k, and so these parameters are subject to the constraints 0 ⩽ µk ⩽ 1\nand ∑\nk µk =1 . The conjugate prior distribution for the parameters {µk} is the\nDirichlet.\nNormal\nThe normal distribution is simply another name for the Gaussian. In this book, we\nuse the term Gaussian throughout, although we retain the conventional use of the\nsymbol N to denote this distribution. For consistency, we shall refer to the normal-\ngamma distribution as the Gaussian-gamma distribution, and similarly the normal-\nWishart is called the Gaussian-Wishart.\nStudent’s t\nThis distribution was published by William Gosset in 1908, but his employer, Gui-\nness Breweries, required him to publish under a pseudonym, so he chose ‘Student’.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 710,
      "page_label": "691"
    }
  },
  {
    "page_content": "ness Breweries, required him to publish under a pseudonym, so he chose ‘Student’.\nIn the univariate form, Student’s t-distribution is obtained by placing a conjugate\ngamma prior over the precision of a univariate Gaussian distribution and then inte-\ngrating out the precision variable. It can therefore be viewed as an inﬁnite mixture",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 710,
      "page_label": "691"
    }
  },
  {
    "page_content": "692 B. PROBABILITY DISTRIBUTIONS\nof Gaussians having the same mean but different variances.\nSt(x|µ, λ, ν)= Γ(ν/2+1 /2)\nΓ(ν/2)\n( λ\nπν\n)1/2 [\n1+ λ(x − µ)2\nν\n]−ν/2−1/2\n(B.64)\nE[x]= µ for ν> 1 (B.65)\nvar[x]= 1\nλ\nν\nν − 2 for ν> 2 (B.66)\nmode[x]= µ. (B.67)\nHere ν> 0 is called the number of degrees of freedom of the distribution. The\nparticular case of ν =1 is called the Cauchy distribution.\nFor aD-dimensional variablex, Student’s t-distribution corresponds to marginal-\nizing the precision matrix of a multivariate Gaussian with respect to a conjugate\nWishart prior and takes the form\nSt(x|µ,Λ,ν )= Γ(ν/2+ D/2)\nΓ(ν/2)\n|Λ|1/2\n(νπ)D/2\n[\n1+ ∆2\nν\n]−ν/2−D/2\n(B.68)\nE[x]= µ for ν> 1 (B.69)\ncov[x]= ν\nν − 2Λ−1 for ν> 2 (B.70)\nmode[x]= µ (B.71)\nwhere ∆2 is the squared Mahalanobis distance deﬁned by\n∆2 =( x − µ)TΛ(x − µ). (B.72)\nIn the limit ν →∞ , the t-distribution reduces to a Gaussian with mean µ and pre-\ncision Λ. Student’s t-distribution provides a generalization of the Gaussian whose\nmaximum likelihood parameter values are robust to outliers.\nUniform\nThis is a simple distribution for a continuous variable x deﬁned over a ﬁnite interval\nx ∈ [a, b] where b>a .\nU(x|a, b)= 1\nb − a (B.73)\nE[x]= (b + a)\n2 (B.74)\nvar[x]= (b − a)2\n12 (B.75)\nH[x]=l n ( b − a). (B.76)\nIf x has distribution U(x|0, 1), then a +( b − a)x will have distribution U(x|a, b).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 711,
      "page_label": "692"
    }
  },
  {
    "page_content": "B. PROBABILITY DISTRIBUTIONS 693\nVon Mises\nThe von Mises distribution, also known as the circular normal or the circular Gaus-\nsian, is a univariate Gaussian-like periodic distribution for a variable θ ∈ [0,2π).\np(θ|θ0,m )= 1\n2πI0(m) exp {m cos(θ − θ0)} (B.77)\nwhere I0(m) is the zeroth-order Bessel function of the ﬁrst kind. The distribution\nhas period 2π so that p(θ +2 π)= p(θ) for all θ. Care must be taken in interpret-\ning this distribution because simple expectations will be dependent on the (arbitrary)\nchoice of origin for the variable θ. The parameter θ0 is analogous to the mean of a\nunivariate Gaussian, and the parameter m> 0, known as the concentration param-\neter, is analogous to the precision (inverse variance). For large m, the von Mises\ndistribution is approximately a Gaussian centred on θ0.\nWishart\nThe Wishart distribution is the conjugate prior for the precision matrix of a multi-\nvariate Gaussian.\nW(Λ|W,ν )= B(W,ν )|Λ|(ν−D−1)/2 exp\n(\n−1\n2Tr(W−1Λ)\n)\n(B.78)\nwhere\nB(W,ν ) ≡| W|−ν/2\n(\n2νD/2 πD(D−1)/4\nD∏\ni=1\nΓ\n( ν +1 − i\n2\n)) −1\n(B.79)\nE[Λ]= νW (B.80)\nE [ln|Λ|]=\nD∑\ni=1\nψ\n( ν +1 − i\n2\n)\n+ D ln 2 + ln|W| (B.81)\nH[Λ]= −lnB(W,ν ) − (ν − D − 1)\n2 E [ln|Λ|]+ νD\n2 (B.82)\nwhere W is a D × D symmetric, positive deﬁnite matrix, and ψ(·) is the digamma\nfunction deﬁned by (B.25). The parameter ν is called the number of degrees of\nfreedom of the distribution and is restricted to ν>D − 1 to ensure that the Gamma",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 712,
      "page_label": "693"
    }
  },
  {
    "page_content": "freedom of the distribution and is restricted to ν>D − 1 to ensure that the Gamma\nfunction in the normalization factor is well-deﬁned. In one dimension, the Wishart\nreduces to the gamma distribution Gam(λ|a, b) given by (B.26) with parameters\na = ν/2 and b =1 /2W.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 712,
      "page_label": "693"
    }
  },
  {
    "page_content": "Appendix C. Properties of Matrices\nIn this appendix, we gather together some useful properties and identities involving\nmatrices and determinants. This is not intended to be an introductory tutorial, and\nit is assumed that the reader is already familiar with basic linear algebra. For some\nresults, we indicate how to prove them, whereas in more complex cases we leave\nthe interested reader to refer to standard textbooks on the subject. In all cases, we\nassume that inverses exist and that matrix dimensions are such that the formulae\nare correctly deﬁned. A comprehensive discussion of linear algebra can be found in\nGolub and Van Loan (1996), and an extensive collection of matrix properties is given\nby L ¨utkepohl (1996). Matrix derivatives are discussed in Magnus and Neudecker\n(1999).\nBasic Matrix Identities\nA matrix A has elements Aij where i indexes the rows, and j indexes the columns.\nWe use IN to denote the N × N identity matrix (also called the unit matrix), and\nwhere there is no ambiguity over dimensionality we simply use I. The transpose\nmatrix AT has elements (AT)ij = Aji. From the deﬁnition of transpose, we have\n(AB)T = BTAT (C.1)\nwhich can be veriﬁed by writing out the indices. The inverse of A, denoted A−1,\nsatisﬁes\nAA−1 = A−1A = I. (C.2)\nBecause ABB−1A−1 = I,w eh a v e\n(AB)−1 = B−1A−1. (C.3)\nAlso we have (\nAT)−1\n=\n(\nA−1)T\n(C.4)\n695",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 714,
      "page_label": "695"
    }
  },
  {
    "page_content": "696 C. PROPERTIES OF MATRICES\nwhich is easily proven by taking the transpose of (C.2) and applying (C.1).\nA useful identity involving matrix inverses is the following\n(P−1 + BTR−1B)−1BTR−1 = PBT(BPBT + R)−1. (C.5)\nwhich is easily veriﬁed by right multiplying both sides by (BPBT + R). Suppose\nthat P has dimensionality N × N while R has dimensionality M × M, so that B is\nM × N. Then if M ≪ N, it will be much cheaper to evaluate the right-hand side of\n(C.5) than the left-hand side. A special case that sometimes arises is\n(I + AB)−1A = A(I + BA)−1. (C.6)\nAnother useful identity involving inverses is the following:\n(A + BD−1C)−1 = A−1 − A−1B(D + CA−1B)−1CA−1 (C.7)\nwhich is known as the W oodbury identityand which can be veriﬁed by multiplying\nboth sides by (A + BD−1C). This is useful, for instance, when A is large and\ndiagonal, and hence easy to invert, while B has many rows but few columns (and\nconversely for C) so that the right-hand side is much cheaper to evaluate than the\nleft-hand side.\nA set of vectors {a1,..., aN } is said to be linearly independent if the relation∑\nn αnan =0 holds only if all αn =0 . This implies that none of the vectors\ncan be expressed as a linear combination of the remainder. The rank of a matrix is\nthe maximum number of linearly independent rows (or equivalently the maximum\nnumber of linearly independent columns).\nTraces and Determinants\nTrace and determinant apply to square matrices. The trace Tr (A) of a matrix A",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 715,
      "page_label": "696"
    }
  },
  {
    "page_content": "number of linearly independent columns).\nTraces and Determinants\nTrace and determinant apply to square matrices. The trace Tr (A) of a matrix A\nis deﬁned as the sum of the elements on the leading diagonal. By writing out the\nindices, we see that\nTr(AB)= Tr(BA). (C.8)\nBy applying this formula multiple times to the product of three matrices, we see that\nTr(ABC)= Tr(CAB)= Tr(BCA) (C.9)\nwhich is known as thecyclic property of the trace operator and which clearly extends\nto the product of any number of matrices. The determinant |A| of an N × N matrix\nA is deﬁned by\n|A| =\n∑\n(±1)A1i1 A2i2 ··· ANiN (C.10)\nin which the sum is taken over all products consisting of precisely one element from\neach row and one element from each column, with a coefﬁcient+1 or −1 according",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 715,
      "page_label": "696"
    }
  },
  {
    "page_content": "C. PROPERTIES OF MATRICES 697\nto whether the permutation i1i2 ...i N is even or odd, respectively. Note that|I| =1 .\nThus, for a 2 × 2 matrix, the determinant takes the form\n|A| =\n⏐⏐⏐⏐\na11 a12\na21 a22\n⏐⏐⏐⏐= a11a22 − a12a21. (C.11)\nThe determinant of a product of two matrices is given by\n|AB| = |A||B| (C.12)\nas can be shown from (C.10). Also, the determinant of an inverse matrix is given by\n⏐⏐A−1⏐⏐= 1\n|A| (C.13)\nwhich can be shown by taking the determinant of (C.2) and applying (C.12).\nIf A and B are matrices of size N × M, then\n⏐⏐IN + ABT⏐⏐=\n⏐⏐IM + ATB\n⏐⏐. (C.14)\nA useful special case is ⏐⏐IN + abT⏐⏐=1+ aTb (C.15)\nwhere a and b are N-dimensional column vectors.\nMatrix Derivatives\nSometimes we need to consider derivatives of vectors and matrices with respect to\nscalars. The derivative of a vectora with respect to a scalar x is itself a vector whose\ncomponents are given by ( ∂a\n∂x\n)\ni\n= ∂ai\n∂x (C.16)\nwith an analogous deﬁnition for the derivative of a matrix. Derivatives with respect\nto vectors and matrices can also be deﬁned, for instance\n( ∂x\n∂a\n)\ni\n= ∂x\n∂ai\n(C.17)\nand similarly ( ∂a\n∂b\n)\nij\n= ∂ai\n∂bj\n. (C.18)\nThe following is easily proven by writing out the components\n∂\n∂x\n(\nxTa\n)\n= ∂\n∂x\n(\naTx\n)\n= a. (C.19)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 716,
      "page_label": "697"
    }
  },
  {
    "page_content": "698 C. PROPERTIES OF MATRICES\nSimilarly\n∂\n∂x (AB)= ∂A\n∂x B + A∂B\n∂x . (C.20)\nThe derivative of the inverse of a matrix can be expressed as\n∂\n∂x\n(\nA−1)\n= −A−1 ∂A\n∂x A−1 (C.21)\nas can be shown by differentiating the equation A−1A = I using (C.20) and then\nright multiplying by A−1. Also\n∂\n∂x ln |A| = Tr\n(\nA−1 ∂A\n∂x\n)\n(C.22)\nwhich we shall prove later. If we choose x to be one of the elements of A,w eh a v e\n∂\n∂Aij\nTr (AB)= Bji (C.23)\nas can be seen by writing out the matrices using index notation. We can write this\nresult more compactly in the form\n∂\n∂ATr (AB)= BT. (C.24)\nWith this notation, we have the following properties\n∂\n∂ATr\n(\nATB\n)\n= B (C.25)\n∂\n∂ATr(A)= I (C.26)\n∂\n∂ATr(ABAT)= A(B + BT) (C.27)\nwhich can again be proven by writing out the matrix indices. We also have\n∂\n∂A ln|A| =\n(\nA−1)T\n(C.28)\nwhich follows from (C.22) and (C.26).\nEigenvector Equation\nFor a square matrix A of size M × M, the eigenvector equation is deﬁned by\nAui = λiui (C.29)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 717,
      "page_label": "698"
    }
  },
  {
    "page_content": "C. PROPERTIES OF MATRICES 699\nfor i =1 ,...,M , where ui is an eigenvector and λi is the correspondingeigenvalue.\nThis can be viewed as a set of M simultaneous homogeneous linear equations, and\nthe condition for a solution is that\n|A − λiI| =0 (C.30)\nwhich is known as thecharacteristic equation. Because this is a polynomial of order\nM in λi, it must have M solutions (though these need not all be distinct). The rank\nof A is equal to the number of nonzero eigenvalues.\nOf particular interest are symmetric matrices, which arise as covariance ma-\ntrices, kernel matrices, and Hessians. Symmetric matrices have the property that\nAij = Aji, or equivalentlyAT = A. The inverse of a symmetric matrix is also sym-\nmetric, as can be seen by taking the transpose of A−1A = I and using AA−1 = I\ntogether with the symmetry of I.\nIn general, the eigenvalues of a matrix are complex numbers, but for symmetric\nmatrices the eigenvalues λi are real. This can be seen by ﬁrst left multiplying (C.29)\nby (u⋆\ni )T, where ⋆ denotes the complex conjugate, to give\n(u⋆\ni )T Aui = λi (u⋆\ni )T ui. (C.31)\nNext we take the complex conjugate of (C.29) and left multiply by uT\ni to give\nuT\ni Au⋆\ni = λ⋆\ni uT\ni u⋆\ni. (C.32)\nwhere we have used A⋆ = A because we consider only real matrices A. Taking\nthe transpose of the second of these equations, and using AT = A, we see that the\nleft-hand sides of the two equations are equal, and hence that λ⋆\ni = λi and so λi\nmust be real.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 718,
      "page_label": "699"
    }
  },
  {
    "page_content": "left-hand sides of the two equations are equal, and hence that λ⋆\ni = λi and so λi\nmust be real.\nThe eigenvectors ui of a real symmetric matrix can be chosen to be orthonormal\n(i.e., orthogonal and of unit length) so that\nuT\ni uj = Iij (C.33)\nwhere Iij are the elements of the identity matrixI. To show this, we ﬁrst left multiply\n(C.29) by uT\nj to give\nuT\nj Aui = λiuT\nj ui (C.34)\nand hence, by exchange of indices, we have\nuT\ni Auj = λjuT\ni uj. (C.35)\nWe now take the transpose of the second equation and make use of the symmetry\nproperty AT = A, and then subtract the two equations to give\n(λi − λj) uT\ni uj =0 . (C.36)\nHence, for λi ̸=λj,w eh a v euT\ni uj =0 , and hence ui and uj are orthogonal. If the\ntwo eigenvalues are equal, then any linear combination αui + βuj is also an eigen-\nvector with the same eigenvalue, so we can select one linear combination arbitrarily,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 718,
      "page_label": "699"
    }
  },
  {
    "page_content": "700 C. PROPERTIES OF MATRICES\nand then choose the second to be orthogonal to the ﬁrst (it can be shown that the de-\ngenerate eigenvectors are never linearly dependent). Hence the eigenvectors can be\nchosen to be orthogonal, and by normalizing can be set to unit length. Because there\nare M eigenvalues, the corresponding M orthogonal eigenvectors form a complete\nset and so any M-dimensional vector can be expressed as a linear combination of\nthe eigenvectors.\nWe can take the eigenvectors ui to be the columns of an M × M matrix U,\nwhich from orthonormality satisﬁes\nUTU = I. (C.37)\nSuch a matrix is said to be orthogonal. Interestingly, the rows of this matrix are also\northogonal, so that UUT = I. To show this, note that (C.37) implies UTUU−1 =\nU−1 = UT and so UU−1 = UUT = I. Using (C.12), it also follows that |U| =1 .\nThe eigenvector equation (C.29) can be expressed in terms of U in the form\nAU = UΛ (C.38)\nwhere Λ is an M × M diagonal matrix whose diagonal elements are given by the\neigenvalues λi.\nIf we consider a column vector x that is transformed by an orthogonal matrix U\nto give a new vector\n˜x = Ux (C.39)\nthen the length of the vector is preserved because\n˜xT˜x = xTUTUx = xTx (C.40)\nand similarly the angle between any two such vectors is preserved because\n˜xT˜y = xTUTUy = xTy. (C.41)\nThus, multiplication by U can be interpreted as a rigid rotation of the coordinate\nsystem.\nFrom (C.38), it follows that\nUTAU = Λ (C.42)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 719,
      "page_label": "700"
    }
  },
  {
    "page_content": "Thus, multiplication by U can be interpreted as a rigid rotation of the coordinate\nsystem.\nFrom (C.38), it follows that\nUTAU = Λ (C.42)\nand because Λ is a diagonal matrix, we say that the matrix A is diagonalized by the\nmatrix U. If we left multiply by U and right multiply by UT, we obtain\nA = UΛUT (C.43)\nTaking the inverse of this equation, and using (C.3) together with U−1 = UT,w e\nhave\nA−1 = UΛ−1UT. (C.44)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 719,
      "page_label": "700"
    }
  },
  {
    "page_content": "C. PROPERTIES OF MATRICES 701\nThese last two equations can also be written in the form\nA =\nM∑\ni=1\nλiuiuT\ni (C.45)\nA−1 =\nM∑\ni=1\n1\nλi\nuiuT\ni . (C.46)\nIf we take the determinant of (C.43), and use (C.12), we obtain\n|A| =\nM∏\ni=1\nλi. (C.47)\nSimilarly, taking the trace of (C.43), and using the cyclic property (C.8) of the trace\noperator together with UTU = I,w eh a v e\nTr(A)=\nM∑\ni=1\nλi. (C.48)\nWe leave it as an exercise for the reader to verify (C.22) by making use of the results\n(C.33), (C.45), (C.46), and (C.47).\nA matrix A is said to be positive deﬁnite, denoted by A ≻ 0,i f wTAw > 0 for\nall values of the vector w. Equivalently, a positive deﬁnite matrix has λi > 0 for all\nof its eigenvalues (as can be seen by setting w to each of the eigenvectors in turn,\nand by noting that an arbitrary vector can be expanded as a linear combination of the\neigenvectors). Note that positive deﬁnite is not the same as all the elements being\npositive. For example, the matrix\n(\n12\n34\n)\n(C.49)\nhas eigenvalues λ1 ≃ 5.37 and λ2 ≃− 0.37. A matrix is said to be positive semidef-\ninite if wTAw ⩾ 0 holds for all values of w, which is denoted A ⪰ 0, and is\nequivalent to λi ⩾ 0.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 720,
      "page_label": "701"
    }
  },
  {
    "page_content": "Appendix D. Calculus of Variations\nWe can think of a function y(x) as being an operator that, for any input value x,\nreturns an output value y. In the same way, we can deﬁne a functional F[y] to be\nan operator that takes a function y(x) and returns an output value F. An example of\na functional is the length of a curve drawn in a two-dimensional plane in which the\npath of the curve is deﬁned in terms of a function. In the context of machine learning,\na widely used functional is the entropy H[x] for a continuous variable x because, for\nany choice of probability density functionp(x), it returns a scalar value representing\nthe entropy of x under that density. Thus the entropy ofp(x) could equally well have\nbeen written as H[p].\nA common problem in conventional calculus is to ﬁnd a value of x that max-\nimizes (or minimizes) a function y(x). Similarly, in the calculus of variations we\nseek a function y(x) that maximizes (or minimizes) a functional F[y]. That is, of all\npossible functions y(x), we wish to ﬁnd the particular function for which the func-\ntional F[y] is a maximum (or minimum). The calculus of variations can be used, for\ninstance, to show that the shortest path between two points is a straight line or that\nthe maximum entropy distribution is a Gaussian.\nIf we weren’t familiar with the rules of ordinary calculus, we could evaluate a\nconventional derivative dy/ dx by making a small change ϵ to the variable x and\nthen expanding in powers of ϵ, so that\ny(x + ϵ)= y(x)+ dy",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 722,
      "page_label": "703"
    }
  },
  {
    "page_content": "conventional derivative dy/ dx by making a small change ϵ to the variable x and\nthen expanding in powers of ϵ, so that\ny(x + ϵ)= y(x)+ dy\ndxϵ + O(ϵ2) (D.1)\nand ﬁnally taking the limit ϵ → 0. Similarly, for a function of several variables\ny(x1,...,x D), the corresponding partial derivatives are deﬁned by\ny(x1 + ϵ1,...,x D + ϵD)= y(x1,...,x D)+\nD∑\ni=1\n∂y\n∂xi\nϵi + O(ϵ2). (D.2)\nThe analogous deﬁnition of a functional derivative arises when we consider how\nmuch a functional F[y] changes when we make a small changeϵη(x) to the function\n703",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 722,
      "page_label": "703"
    }
  },
  {
    "page_content": "704 D. CALCULUS OF V ARIATIONS\nFigure D.1 A functional derivative can be deﬁned by\nconsidering how the value of a functional\nF[y] changes when the function y(x) is\nchanged to y(x)+ ϵη(x) where η(x) is an\narbitrary function of x.\ny(x)\ny(x)+ ϵη(x)\nx\ny(x), where η(x) is an arbitrary function ofx, as illustrated in Figure D.1. We denote\nthe functional derivative of E[f] with respect to f(x) by δF/δf (x), and deﬁne it by\nthe following relation:\nF[y(x)+ ϵη(x)] = F[y(x)] +ϵ\n∫ δF\nδy(x)η(x)d x + O(ϵ2). (D.3)\nThis can be seen as a natural extension of (D.2) in which F[y] now depends on a\ncontinuous set of variables, namely the values ofy at all points x. Requiring that the\nfunctional be stationary with respect to small variations in the function y(x) gives\n∫ δE\nδy(x)η(x)d x =0 . (D.4)\nBecause this must hold for an arbitrary choice of η(x), it follows that the functional\nderivative must vanish. To see this, imagine choosing a perturbationη(x) that is zero\neverywhere except in the neighbourhood of a point ˆx, in which case the functional\nderivative must be zero at x = ˆx. However, because this must be true for every\nchoice of ˆx, the functional derivative must vanish for all values ofx.\nConsider a functional that is deﬁned by an integral over a function G(y,y ′,x)\nthat depends on both y(x) and its derivative y′(x) as well as having a direct depen-\ndence on x\nF[y]=\n∫\nG (y(x),y ′(x),x)d x (D.5)\nwhere the value of y(x) is assumed to be ﬁxed at the boundary of the region of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 723,
      "page_label": "704"
    }
  },
  {
    "page_content": "dence on x\nF[y]=\n∫\nG (y(x),y ′(x),x)d x (D.5)\nwhere the value of y(x) is assumed to be ﬁxed at the boundary of the region of\nintegration (which might be at inﬁnity). If we now consider variations in the function\ny(x), we obtain\nF[y(x)+ ϵη(x)] = F[y(x)] +ϵ\n∫ {∂G\n∂y η(x)+ ∂G\n∂y′η′(x)\n}\ndx + O(ϵ2). (D.6)\nWe now have to cast this in the form (D.3). To do so, we integrate the second term by\nparts and make use of the fact that η(x) must vanish at the boundary of the integral\n(because y(x) is ﬁxed at the boundary). This gives\nF[y(x)+ ϵη(x)] = F[y(x)] +ϵ\n∫ {∂G\n∂y − d\ndx\n( ∂G\n∂y′\n)}\nη(x)d x + O(ϵ2) (D.7)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 723,
      "page_label": "704"
    }
  },
  {
    "page_content": "D. CALCULUS OF V ARIATIONS 705\nfrom which we can read off the functional derivative by comparison with (D.3).\nRequiring that the functional derivative vanishes then gives\n∂G\n∂y − d\ndx\n( ∂G\n∂y′\n)\n=0 (D.8)\nwhich are known as the Euler-Lagrange equations. For example, if\nG = y(x)2 +( y′(x))\n2\n(D.9)\nthen the Euler-Lagrange equations take the form\ny(x) − d2y\ndx2 =0 . (D.10)\nThis second order differential equation can be solved for y(x) by making use of the\nboundary conditions on y(x).\nOften, we consider functionals deﬁned by integrals whose integrands take the\nform G(y,x ) and that do not depend on the derivatives ofy(x). In this case, station-\narity simply requires that ∂G/∂y(x)=0 for all values of x.\nIf we are optimizing a functional with respect to a probability distribution, then\nwe need to maintain the normalization constraint on the probabilities. This is often\nmost conveniently done using a Lagrange multiplier, which then allows an uncon-Appendix E\nstrained optimization to be performed.\nThe extension of the above results to a multidimensional variable x is straight-\nforward. For a more comprehensive discussion of the calculus of variations, see\nSagan (1969).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 724,
      "page_label": "705"
    }
  },
  {
    "page_content": "Appendix E. Lagrange Multipliers\nLagrange multipliers, also sometimes called undetermined multipliers, are used to\nﬁnd the stationary points of a function of several variables subject to one or more\nconstraints.\nConsider the problem of ﬁnding the maximum of a functionf(x1,x2) subject to\na constraint relating x1 and x2, which we write in the form\ng(x1,x2)=0 . (E.1)\nOne approach would be to solve the constraint equation (E.1) and thus expressx2 as\na function of x1 in the form x2 = h(x1). This can then be substituted into f(x1,x2)\nto give a function of x1 alone of the form f(x1,h (x1)). The maximum with respect\nto x1 could then be found by differentiation in the usual way, to give the stationary\nvalue x⋆\n1, with the corresponding value of x2 given by x⋆\n2 = h(x⋆\n1).\nOne problem with this approach is that it may be difﬁcult to ﬁnd an analytic\nsolution of the constraint equation that allowsx2 to be expressed as an explicit func-\ntion of x1. Also, this approach treats x1 and x2 differently and so spoils the natural\nsymmetry between these variables.\nA more elegant, and often simpler, approach is based on the introduction of a\nparameter λ called a Lagrange multiplier. We shall motivate this technique from\na geometrical perspective. Consider a D-dimensional variable x with components\nx1,...,x D. The constraint equation g(x)=0 then represents a(D−1)-dimensional\nsurface in x-space as indicated in Figure E.1.\nWe ﬁrst note that at any point on the constraint surface the gradient ∇g(x) of",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 726,
      "page_label": "707"
    }
  },
  {
    "page_content": "surface in x-space as indicated in Figure E.1.\nWe ﬁrst note that at any point on the constraint surface the gradient ∇g(x) of\nthe constraint function will be orthogonal to the surface. To see this, consider a point\nx that lies on the constraint surface, and consider a nearby point x + ϵ that also lies\non the surface. If we make a Taylor expansion around x,w eh a v e\ng(x + ϵ) ≃ g(x)+ ϵT∇g(x). (E.2)\nBecause both x and x+ϵ lie on the constraint surface, we haveg(x)= g(x+ϵ) and\nhence ϵT∇g(x) ≃ 0. In the limit ∥ϵ∥→ 0 we have ϵT∇g(x)=0 , and because ϵ is\n707",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 726,
      "page_label": "707"
    }
  },
  {
    "page_content": "708 E. LAGRANGE MULTIPLIERS\nFigure E.1 A geometrical picture of the technique of La-\ngrange multipliers in which we seek to maximize a\nfunction f(x), subject to the constraint g(x)=0 .\nIf x is D dimensional, the constraint g(x)=0 cor-\nresponds to a subspace of dimensionality D − 1,\nindicated by the red curve. The problem can\nbe solved by optimizing the Lagrangian function\nL(x,λ )= f(x)+ λg(x).\n∇f(x)\n∇g(x)\nxA\ng(x)=0\nthen parallel to the constraint surface g(x)=0 , we see that the vector ∇g is normal\nto the surface.\nNext we seek a point x⋆ on the constraint surface such that f(x) is maximized.\nSuch a point must have the property that the vector ∇f(x) is also orthogonal to the\nconstraint surface, as illustrated in Figure E.1, because otherwise we could increase\nthe value of f(x) by moving a short distance along the constraint surface. Thus ∇f\nand ∇g are parallel (or anti-parallel) vectors, and so there must exist a parameter λ\nsuch that\n∇f + λ∇g =0 (E.3)\nwhere λ ̸=0 is known as a Lagrange multiplier. Note that λ can have either sign.\nAt this point, it is convenient to introduce the Lagrangian function deﬁned by\nL(x,λ ) ≡ f(x)+ λg(x). (E.4)\nThe constrained stationarity condition (E.3) is obtained by setting ∇xL =0 . Fur-\nthermore, the condition ∂L/∂λ =0 leads to the constraint equation g(x)=0 .\nThus to ﬁnd the maximum of a functionf(x) subject to the constraint g(x)=0 ,\nwe deﬁne the Lagrangian function given by (E.4) and we then ﬁnd the stationary",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 727,
      "page_label": "708"
    }
  },
  {
    "page_content": "we deﬁne the Lagrangian function given by (E.4) and we then ﬁnd the stationary\npoint of L(x,λ ) with respect to both x and λ. For a D-dimensional vector x, this\ngives D +1 equations that determine both the stationary pointx⋆ and the value of λ.\nIf we are only interested in x⋆ , then we can eliminate λ from the stationarity equa-\ntions without needing to ﬁnd its value (hence the term ‘undetermined multiplier’).\nAs a simple example, suppose we wish to ﬁnd the stationary point of the function\nf(x1,x2)=1 − x2\n1 − x2\n2 subject to the constraint g(x1,x2)= x1 + x2 − 1=0 ,a s\nillustrated in Figure E.2. The corresponding Lagrangian function is given by\nL(x,λ )=1 − x2\n1 − x2\n2 + λ(x1 + x2 − 1). (E.5)\nThe conditions for this Lagrangian to be stationary with respect tox1, x2, and λ give\nthe following coupled equations:\n−2x1 + λ =0 (E.6)\n−2x2 + λ =0 (E.7)\nx1 + x2 − 1=0 . (E.8)",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 727,
      "page_label": "708"
    }
  },
  {
    "page_content": "E. LAGRANGE MULTIPLIERS 709\nFigure E.2 A simple example of the use of Lagrange multipli-\ners in which the aim is to maximize f(x1,x 2)=\n1 − x2\n1 − x2\n2 subject to the constraint g(x1,x 2)=0\nwhere g(x1,x 2)= x1 + x2 − 1. The circles show\ncontours of the function f(x1,x 2), and the diagonal\nline shows the constraint surface g(x1,x 2)=0 .\ng(x1,x2)=0\nx1\nx2\n(x⋆\n1,x⋆\n2)\nSolution of these equations then gives the stationary point as (x⋆\n1,x⋆\n2)=( 1\n2, 1\n2), and\nthe corresponding value for the Lagrange multiplier is λ =1 .\nSo far, we have considered the problem of maximizing a function subject to an\nequality constraint of the form g(x)=0 . We now consider the problem of maxi-\nmizing f(x) subject to an inequality constraint of the form g(x) ⩾ 0, as illustrated\nin Figure E.3.\nThere are now two kinds of solution possible, according to whether the con-\nstrained stationary point lies in the region where g(x) > 0, in which case the con-\nstraint is inactive, or whether it lies on the boundary g(x)=0 , in which case the\nconstraint is said to be active. In the former case, the function g(x) plays no role\nand so the stationary condition is simply ∇f(x)=0 . This again corresponds to\na stationary point of the Lagrange function (E.4) but this time with λ =0 . The\nlatter case, where the solution lies on the boundary, is analogous to the equality con-\nstraint discussed previously and corresponds to a stationary point of the Lagrange",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 728,
      "page_label": "709"
    }
  },
  {
    "page_content": "straint discussed previously and corresponds to a stationary point of the Lagrange\nfunction (E.4) with λ ̸=0 . Now, however, the sign of the Lagrange multiplier is\ncrucial, because the function f(x) will only be at a maximum if its gradient is ori-\nented away from the regiong(x) > 0, as illustrated in Figure E.3. We therefore have\n∇f(x)= −λ∇g(x) for some value of λ> 0.\nFor either of these two cases, the product λg(x)=0 . Thus the solution to the\nFigure E.3 Illustration of the problem of maximizing\nf(x) subject to the inequality constraint\ng(x) ⩾ 0.\n∇f(x)\n∇g(x)\nxA\nxB\ng(x)=0g(x) > 0",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 728,
      "page_label": "709"
    }
  },
  {
    "page_content": "710 E. LAGRANGE MULTIPLIERS\nproblem of maximizing f(x) subject to g(x) ⩾ 0 is obtained by optimizing the\nLagrange function (E.4) with respect to x and λ subject to the conditions\ng(x) ⩾ 0 (E.9)\nλ ⩾ 0 (E.10)\nλg(x)=0 (E.11)\nThese are known as theKarush-Kuhn-Tucker(KKT) conditions (Karush, 1939; Kuhn\nand Tucker, 1951).\nNote that if we wish to minimize (rather than maximize) the function f(x) sub-\nject to an inequality constraint g(x) ⩾ 0, then we minimize the Lagrangian function\nL(x,λ )= f(x) − λg(x) with respect to x, again subject to λ ⩾ 0.\nFinally, it is straightforward to extend the technique of Lagrange multipliers to\nthe case of multiple equality and inequality constraints. Suppose we wish to maxi-\nmize f(x) subject to gj(x)=0 for j =1 ,...,J , and hk(x) ⩾ 0 for k =1 ,...,K .\nWe then introduce Lagrange multipliers {λj} and {µk}, and then optimize the La-\ngrangian function given by\nL(x,{λj}, {µk})= f(x)+\nJ∑\nj=1\nλjgj(x)+\nK∑\nk=1\nµkhk(x) (E.12)\nsubject to µk ⩾ 0 and µkhk(x)=0 for k =1 ,...,K . Extensions to constrained\nfunctional derivatives are similarly straightforward. For a more detailed discussionAppendix D\nof the technique of Lagrange multipliers, see Nocedal and Wright (1999).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 729,
      "page_label": "710"
    }
  },
  {
    "page_content": "REFERENCES 711\nReferences\nAbramowitz, M. and I. A. Stegun (1965). Handbook\nof Mathematical Functions. Dover.\nAdler, S. L. (1981). Over-relaxation method for the\nMonte Carlo evaluation of the partition func-\ntion for multiquadratic actions.Physical Review\nD 23, 2901–2904.\nAhn, J. H. and J. H. Oh (2003). A constrained EM\nalgorithm for principal component analysis.Neu-\nral Computation 15(1), 57–65.\nAizerman, M. A., E. M. Braverman, and L. I. Rozo-\nnoer (1964). The probability problem of pattern\nrecognition learning and the method of potential\nfunctions. Automation and Remote Control 25,\n1175–1190.\nAkaike, H. (1974). A new look at statistical model\nidentiﬁcation. IEEE Transactions on Automatic\nControl 19, 716–723.\nAli, S. M. and S. D. Silvey (1966). A general class\nof coefﬁcients of divergence of one distribution\nfrom another.Journal of the Royal Statistical So-\nciety, B28(1), 131–142.\nAllwein, E. L., R. E. Schapire, and Y . Singer (2000).\nReducing multiclass to binary: a unifying ap-\nproach for margin classiﬁers.Journal of Machine\nLearning Research 1, 113–141.\nAmari, S. (1985). Differential-Geometrical Methods\nin Statistics. Springer.\nAmari, S., A. Cichocki, and H. H. Yang (1996). A\nnew learning algorithm for blind signal separa-\ntion. In D. S. Touretzky, M. C. Mozer, and M. E.\nHasselmo (Eds.), Advances in Neural Informa-\ntion Processing Systems, V olume 8, pp. 757–763.\nMIT Press.\nAmari, S. I. (1998). Natural gradient works efﬁ-\nciently in learning. Neural Computation 10,",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 730,
      "page_label": "711"
    }
  },
  {
    "page_content": "MIT Press.\nAmari, S. I. (1998). Natural gradient works efﬁ-\nciently in learning. Neural Computation 10,\n251–276.\nAnderson, J. A. and E. Rosenfeld (Eds.) (1988).\nNeurocomputing: F oundations of Research. MIT\nPress.\nAnderson, T. W. (1963). Asymptotic theory for prin-\ncipal component analysis. Annals of Mathemati-\ncal Statistics 34, 122–148.\nAndrieu, C., N. de Freitas, A. Doucet, and M. I. Jor-\ndan (2003). An introduction to MCMC for ma-\nchine learning.Machine Learning 50, 5–43.\nAnthony, M. and N. Biggs (1992). An Introduction\nto Computational Learning Theory. Cambridge\nUniversity Press.\nAttias, H. (1999a). Independent factor analysis.Neu-\nral Computation 11(4), 803–851.\nAttias, H. (1999b). Inferring parameters and struc-\nture of latent variable models by variational\nBayes. In K. B. Laskey and H. Prade (Eds.),",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 730,
      "page_label": "711"
    }
  },
  {
    "page_content": "712 REFERENCES\nUncertainty in Artiﬁcial Intelligence: Proceed-\nings of the Fifth Conference, pp. 21–30. Morgan\nKaufmann.\nBach, F. R. and M. I. Jordan (2002). Kernel inde-\npendent component analysis.Journal of Machine\nLearning Research 3, 1–48.\nBakir, G. H., J. Weston, and B. Sch ¨olkopf (2004).\nLearning to ﬁnd pre-images. In S. Thrun, L. K.\nSaul, and B. Sch¨olkopf (Eds.), Advances in Neu-\nral Information Processing Systems, V olume 16,\npp. 449–456. MIT Press.\nBaldi, P. and S. Brunak (2001). Bioinformatics: The\nMachine Learning Approach(Second ed.). MIT\nPress.\nBaldi, P. and K. Hornik (1989). Neural networks\nand principal component analysis: learning from\nexamples without local minima.Neural Net-\nworks 2(1), 53–58.\nBarber, D. and C. M. Bishop (1997). Bayesian\nmodel comparison by Monte Carlo chaining. In\nM. Mozer, M. Jordan, and T. Petsche (Eds.), Ad-\nvances in Neural Information Processing Sys-\ntems, V olume 9, pp. 333–339. MIT Press.\nBarber, D. and C. M. Bishop (1998a). Ensemble\nlearning for multi-layer networks. In M. I. Jor-\ndan, K. J. Kearns, and S. A. Solla (Eds.), Ad-\nvances in Neural Information Processing Sys-\ntems, V olume 10, pp. 395–401.\nBarber, D. and C. M. Bishop (1998b). Ensemble\nlearning in Bayesian neural networks. In C. M.\nBishop (Ed.),Generalization in Neural Networks\nand Machine Learning, pp. 215–237. Springer.\nBartholomew, D. J. (1987). Latent V ariable Models\nand Factor Analysis. Charles Grifﬁn.\nBasilevsky, A. (1994). Statistical Factor Analysis",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 731,
      "page_label": "712"
    }
  },
  {
    "page_content": "Bartholomew, D. J. (1987). Latent V ariable Models\nand Factor Analysis. Charles Grifﬁn.\nBasilevsky, A. (1994). Statistical Factor Analysis\nand Related Methods: Theory and Applications.\nWiley.\nBather, J. (2000). Decision Theory: An Introduction\nto Dynamic Programming and Sequential Deci-\nsions. Wiley.\nBaudat, G. and F. Anouar (2000). Generalized dis-\ncriminant analysis using a kernel approach. Neu-\nral Computation 12(10), 2385–2404.\nBaum, L. E. (1972). An inequality and associated\nmaximization technique in statistical estimation\nof probabilistic functions of Markov processes.\nInequalities 3, 1–8.\nBecker, S. and Y . Le Cun (1989). Improving the con-\nvergence of back-propagation learning with sec-\nond order methods. In D. Touretzky, G. E. Hin-\nton, and T. J. Sejnowski (Eds.), Proceedings of\nthe 1988 Connectionist Models Summer School,\npp. 29–37. Morgan Kaufmann.\nBell, A. J. and T. J. Sejnowski (1995). An infor-\nmation maximization approach to blind separa-\ntion and blind deconvolution. Neural Computa-\ntion 7(6), 1129–1159.\nBellman, R. (1961). Adaptive Control Processes: A\nGuided T our. Princeton University Press.\nBengio, Y . and P. Frasconi (1995). An input output\nHMM architecture. In G. Tesauro, D. S. Touret-\nzky, and T. K. Leen (Eds.), Advances in Neural\nInformation Processing Systems, V olume 7, pp.\n427–434. MIT Press.\nBennett, K. P. (1992). Robust linear programming\ndiscrimination of two linearly separable sets.Op-\ntimization Methods and Software1, 23–34.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 731,
      "page_label": "712"
    }
  },
  {
    "page_content": "Bennett, K. P. (1992). Robust linear programming\ndiscrimination of two linearly separable sets.Op-\ntimization Methods and Software1, 23–34.\nBerger, J. O. (1985).Statistical Decision Theory and\nBayesian Analysis (Second ed.). Springer.\nBernardo, J. M. and A. F. M. Smith (1994).Bayesian\nTheory. Wiley.\nBerrou, C., A. Glavieux, and P. Thitimajshima\n(1993). Near Shannon limit error-correcting cod-\ning and decoding: Turbo-codes (1). InProceed-\nings ICC’93, pp. 1064–1070.\nBesag, J. (1974). On spatio-temporal models and\nMarkov ﬁelds. In Transactions of the 7th Prague\nConference on Information Theory, Statistical\nDecision Functions and Random Processes, pp.\n47–75. Academia.\nBesag, J. (1986). On the statistical analysis of dirty\npictures. Journal of the Royal Statistical Soci-\nety B-48, 259–302.\nBesag, J., P. J. Green, D. Hidgon, and K. Megersen\n(1995). Bayesian computation and stochastic\nsystems. Statistical Science 10(1), 3–66.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 731,
      "page_label": "712"
    }
  },
  {
    "page_content": "REFERENCES 713\nBishop, C. M. (1991). A fast procedure for retraining\nthe multilayer perceptron. International Journal\nof Neural Systems2(3), 229–236.\nBishop, C. M. (1992). Exact calculation of the Hes-\nsian matrix for the multilayer perceptron. Neural\nComputation 4(4), 494–501.\nBishop, C. M. (1993). Curvature-driven smoothing:\na learning algorithm for feedforward networks.\nIEEE Transactions on Neural Networks 4(5),\n882–884.\nBishop, C. M. (1994). Novelty detection and neu-\nral network validation. IEE Proceedings: Vision,\nImage and Signal Processing 141(4), 217–222.\nSpecial issue on applications of neural networks.\nBishop, C. M. (1995a). Neural Networks for Pattern\nRecognition. Oxford University Press.\nBishop, C. M. (1995b). Training with noise is equiv-\nalent to Tikhonov regularization.Neural Compu-\ntation 7(1), 108–116.\nBishop, C. M. (1999a). Bayesian PCA. In M. S.\nKearns, S. A. Solla, and D. A. Cohn (Eds.), Ad-\nvances in Neural Information Processing Sys-\ntems, V olume 11, pp. 382–388. MIT Press.\nBishop, C. M. (1999b). Variational principal\ncomponents. In Proceedings Ninth Interna-\ntional Conference on Artiﬁcial Neural Networks,\nICANN’99, V olume 1, pp. 509–514. IEE.\nBishop, C. M. and G. D. James (1993). Analysis of\nmultiphase ﬂows using dual-energy gamma den-\nsitometry and neural networks. Nuclear Instru-\nments and Methods in Physics Research A327,\n580–593.\nBishop, C. M. and I. T. Nabney (1996). Modelling\nconditional probability distributions for periodic",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 732,
      "page_label": "713"
    }
  },
  {
    "page_content": "580–593.\nBishop, C. M. and I. T. Nabney (1996). Modelling\nconditional probability distributions for periodic\nvariables. Neural Computation8(5), 1123–1133.\nBishop, C. M. and I. T. Nabney (2008). Pattern\nRecognition and Machine Learning: A Matlab\nCompanion. Springer. In preparation.\nBishop, C. M., D. Spiegelhalter, and J. Winn\n(2003). VIBES: A variational inference engine\nfor Bayesian networks. In S. Becker, S. Thrun,\nand K. Obermeyer (Eds.), Advances in Neural\nInformation Processing Systems, V olume 15, pp.\n793–800. MIT Press.\nBishop, C. M. and M. Svens ´en (2003). Bayesian hi-\nerarchical mixtures of experts. In U. Kjaerulff\nand C. Meek (Eds.), Proceedings Nineteenth\nConference on Uncertainty in Artiﬁcial Intelli-\ngence, pp. 57–64. Morgan Kaufmann.\nBishop, C. M., M. Svens ´en, and G. E. Hinton\n(2004). Distinguishing text from graphics in on-\nline handwritten ink. In F. Kimura and H. Fu-\njisawa (Eds.), Proceedings Ninth International\nW orkshop on Frontiers in Handwriting Recogni-\ntion, IWFHR-9, Tokyo, Japan, pp. 142–147.\nBishop, C. M., M. Svens ´en, and C. K. I. Williams\n(1996). EM optimization of latent variable den-\nsity models. In D. S. Touretzky, M. C. Mozer,\nand M. E. Hasselmo (Eds.), Advances in Neural\nInformation Processing Systems, V olume 8, pp.\n465–471. MIT Press.\nBishop, C. M., M. Svens ´en, and C. K. I. Williams\n(1997a). GTM: a principled alternative to the\nSelf-Organizing Map. In M. C. Mozer, M. I. Jor-\ndan, and T. Petche (Eds.), Advances in Neural",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 732,
      "page_label": "713"
    }
  },
  {
    "page_content": "(1997a). GTM: a principled alternative to the\nSelf-Organizing Map. In M. C. Mozer, M. I. Jor-\ndan, and T. Petche (Eds.), Advances in Neural\nInformation Processing Systems, V olume 9, pp.\n354–360. MIT Press.\nBishop, C. M., M. Svens ´en, and C. K. I. Williams\n(1997b). Magniﬁcation factors for the GTM al-\ngorithm. In Proceedings IEE Fifth International\nConference on Artiﬁcial Neural Networks, Cam-\nbridge, U.K., pp. 64–69. Institute of Electrical\nEngineers.\nBishop, C. M., M. Svens ´en, and C. K. I. Williams\n(1998a). Developments of the Generative To-\npographic Mapping. Neurocomputing 21, 203–\n224.\nBishop, C. M., M. Svens ´en, and C. K. I. Williams\n(1998b). GTM: the Generative Topographic\nMapping.Neural Computation 10(1), 215–234.\nBishop, C. M. and M. E. Tipping (1998). A hier-\narchical latent variable model for data visualiza-\ntion.IEEE Transactions on Pattern Analysis and\nMachine Intelligence20(3), 281–293.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 732,
      "page_label": "713"
    }
  },
  {
    "page_content": "714 REFERENCES\nBishop, C. M. and J. Winn (2000). Non-linear\nBayesian image modelling. In Proceedings Sixth\nEuropean Conference on Computer Vision,\nDublin, V olume 1, pp. 3–17. Springer.\nBlei, D. M., M. I. Jordan, and A. Y . Ng (2003). Hi-\nerarchical Bayesian models for applications in\ninformation retrieval. In J. M. B. et al. (Ed.),\nBayesian Statistics, 7, pp. 25–43. Oxford Uni-\nversity Press.\nBlock, H. D. (1962). The perceptron: a model\nfor brain functioning. Reviews of Modern\nPhysics 34(1), 123–135. Reprinted in Anderson\nand Rosenfeld (1988).\nBlum, J. A. (1965). Multidimensional stochastic ap-\nproximation methods. Annals of Mathematical\nStatistics 25, 737–744.\nBodlaender, H. (1993). A tourist guide through\ntreewidth. Acta Cybernetica 11, 1–21.\nBoser, B. E., I. M. Guyon, and V . N. Vapnik (1992).\nA training algorithm for optimal margin classi-\nﬁers. In D. Haussler (Ed.), Proceedings Fifth An-\nnual W orkshop on Computational Learning The-\nory (COLT), pp. 144–152. ACM.\nBourlard, H. and Y . Kamp (1988). Auto-association\nby multilayer perceptrons and singular value de-\ncomposition. Biological Cybernetics 59, 291–\n294.\nBox, G. E. P., G. M. Jenkins, and G. C. Reinsel\n(1994). Time Series Analysis. Prentice Hall.\nBox, G. E. P. and G. C. Tao (1973). Bayesian Infer-\nence in Statistical Analysis. Wiley.\nBoyd, S. and L. Vandenberghe (2004).Convex Opti-\nmization. Cambridge University Press.\nBoyen, X. and D. Koller (1998). Tractable inference",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 733,
      "page_label": "714"
    }
  },
  {
    "page_content": "Boyd, S. and L. Vandenberghe (2004).Convex Opti-\nmization. Cambridge University Press.\nBoyen, X. and D. Koller (1998). Tractable inference\nfor complex stochastic processes. In G. F. Cooper\nand S. Moral (Eds.), Proceedings 14th Annual\nConference on Uncertainty in Artiﬁcial Intelli-\ngence (UAI), pp. 33–42. Morgan Kaufmann.\nBoykov, Y ., O. Veksler, and R. Zabih (2001). Fast\napproximate energy minimization via graph cuts.\nIEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence23(11), 1222–1239.\nBreiman, L. (1996). Bagging predictors. Machine\nLearning 26, 123–140.\nBreiman, L., J. H. Friedman, R. A. Olshen, and\nP. J. Stone (1984). Classiﬁcation and Regression\nTrees. Wadsworth.\nBrooks, S. P. (1998). Markov chain Monte\nCarlo method and its application. The Statisti-\ncian 47(1), 69–100.\nBroomhead, D. S. and D. Lowe (1988). Multivari-\nable functional interpolation and adaptive net-\nworks. Complex Systems 2, 321–355.\nBuntine, W. and A. Weigend (1991). Bayesian back-\npropagation. Complex Systems 5, 603–643.\nBuntine, W. L. and A. S. Weigend (1993). Com-\nputing second derivatives in feed-forward net-\nworks: a review. IEEE Transactions on Neural\nNetworks 5(3), 480–488.\nBurges, C. J. C. (1998). A tutorial on support vec-\ntor machines for pattern recognition. Knowledge\nDiscovery and Data Mining2(2), 121–167.\nCardoso, J.-F. (1998). Blind signal separation: statis-\ntical principles. Proceedings of the IEEE 9(10),\n2009–2025.\nCasella, G. and R. L. Berger (2002). Statistical In-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 733,
      "page_label": "714"
    }
  },
  {
    "page_content": "tical principles. Proceedings of the IEEE 9(10),\n2009–2025.\nCasella, G. and R. L. Berger (2002). Statistical In-\nference (Second ed.). Duxbury.\nCastillo, E., J. M. Guti ´errez, and A. S. Hadi (1997).\nExpert Systems and Probabilistic Network Mod-\nels. Springer.\nChan, K., T. Lee, and T. J. Sejnowski (2003). Vari-\national Bayesian learning of ICA with missing\ndata. Neural Computation 15(8), 1991–2011.\nChen, A. M., H. Lu, and R. Hecht-Nielsen (1993).\nOn the geometry of feedforward neural network\nerror surfaces. Neural Computation 5(6), 910–\n927.\nChen, M. H., Q. M. Shao, and J. G. Ibrahim (Eds.)\n(2001). Monte Carlo Methods for Bayesian Com-\nputation. Springer.\nChen, S., C. F. N. Cowan, and P. M. Grant (1991).\nOrthogonal least squares learning algorithm for\nradial basis function networks.IEEE Transac-\ntions on Neural Networks2(2), 302–309.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 733,
      "page_label": "714"
    }
  },
  {
    "page_content": "REFERENCES 715\nChoudrey, R. A. and S. J. Roberts (2003). Variational\nmixture of Bayesian independent component an-\nalyzers.Neural Computation 15(1), 213–252.\nClifford, P. (1990). Markov random ﬁelds in statis-\ntics. In G. R. Grimmett and D. J. A. Welsh (Eds.),\nDisorder in Physical Systems. A V olume in Hon-\nour of John M. Hammersley, pp. 19–32. Oxford\nUniversity Press.\nCollins, M., S. Dasgupta, and R. E. Schapire (2002).\nA generalization of principal component analy-\nsis to the exponential family. In T. G. Dietterich,\nS. Becker, and Z. Ghahramani (Eds.), Advances\nin Neural Information Processing Systems, V ol-\nume 14, pp. 617–624. MIT Press.\nComon, P., C. Jutten, and J. Herault (1991). Blind\nsource separation, 2: problems statement. Signal\nProcessing 24(1), 11–20.\nCorduneanu, A. and C. M. Bishop (2001). Vari-\national Bayesian model selection for mixture\ndistributions. In T. Richardson and T. Jaakkola\n(Eds.), Proceedings Eighth International Confer-\nence on Artiﬁcial Intelligence and Statistics, pp.\n27–34. Morgan Kaufmann.\nCormen, T. H., C. E. Leiserson, R. L. Rivest, and\nC. Stein (2001). Introduction to Algorithms(Sec-\nond ed.). MIT Press.\nCortes, C. and V . N. Vapnik (1995). Support vector\nnetworks. Machine Learning 20, 273–297.\nCotter, N. E. (1990). The Stone-Weierstrass theo-\nrem and its application to neural networks. IEEE\nTransactions on Neural Networks1(4), 290–295.\nCover, T. and P. Hart (1967). Nearest neighbor pat-\ntern classiﬁcation. IEEE Transactions on Infor-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 734,
      "page_label": "715"
    }
  },
  {
    "page_content": "Transactions on Neural Networks1(4), 290–295.\nCover, T. and P. Hart (1967). Nearest neighbor pat-\ntern classiﬁcation. IEEE Transactions on Infor-\nmation Theory IT-11, 21–27.\nCover, T. M. and J. A. Thomas (1991). Elements of\nInformation Theory. Wiley.\nCowell, R. G., A. P. Dawid, S. L. Lauritzen, and D. J.\nSpiegelhalter (1999). Probabilistic Networks and\nExpert Systems. Springer.\nCox, R. T. (1946). Probability, frequency and\nreasonable expectation. American Journal of\nPhysics 14(1), 1–13.\nCox, T. F. and M. A. A. Cox (2000). Multidimen-\nsional Scaling (Second ed.). Chapman and Hall.\nCressie, N. (1993).Statistics for Spatial Data. Wiley.\nCristianini, N. and J. Shawe-Taylor (2000). Support\nvector machines and other kernel-based learning\nmethods. Cambridge University Press.\nCsat´o, L. and M. Opper (2002). Sparse on-line Gaus-\nsian processes. Neural Computation 14(3), 641–\n668.\nCsisz`ar, I. and G. Tusn `ady (1984). Information ge-\nometry and alternating minimization procedures.\nStatistics and Decisions1(1), 205–237.\nCybenko, G. (1989). Approximation by superposi-\ntions of a sigmoidal function. Mathematics of\nControl, Signals and Systems2, 304–314.\nDawid, A. P. (1979). Conditional independence in\nstatistical theory (with discussion).Journal of the\nRoyal Statistical Society, Series B4, 1–31.\nDawid, A. P. (1980). Conditional independence for\nstatistical operations. Annals of Statistics8, 598–\n617.\ndeFinetti, B. (1970). Theory of Probability. Wiley\nand Sons.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 734,
      "page_label": "715"
    }
  },
  {
    "page_content": "statistical operations. Annals of Statistics8, 598–\n617.\ndeFinetti, B. (1970). Theory of Probability. Wiley\nand Sons.\nDempster, A. P., N. M. Laird, and D. B. Rubin\n(1977). Maximum likelihood from incomplete\ndata via the EM algorithm.Journal of the Royal\nStatistical Society, B39(1), 1–38.\nDenison, D. G. T., C. C. Holmes, B. K. Mallick,\nand A. F. M. Smith (2002).Bayesian Methods for\nNonlinear Classiﬁcation and Regression. Wiley.\nDiaconis, P. and L. Saloff-Coste (1998). What do we\nknow about the Metropolis algorithm? Journal\nof Computer and System Sciences57, 20–36.\nDietterich, T. G. and G. Bakiri (1995). Solving\nmulticlass learning problems via error-correcting\noutput codes.Journal of Artiﬁcial Intelligence\nResearch 2, 263–286.\nDuane, S., A. D. Kennedy, B. J. Pendleton, and\nD. Roweth (1987). Hybrid Monte Carlo. Physics\nLetters B 195(2), 216–222.\nDuda, R. O. and P. E. Hart (1973). Pattern Classiﬁ-\ncation and Scene Analysis. Wiley.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 734,
      "page_label": "715"
    }
  },
  {
    "page_content": "716 REFERENCES\nDuda, R. O., P. E. Hart, and D. G. Stork (2001).Pat-\ntern Classiﬁcation (Second ed.). Wiley.\nDurbin, R., S. Eddy, A. Krogh, and G. Mitchi-\nson (1998). Biological Sequence Analysis. Cam-\nbridge University Press.\nDybowski, R. and S. Roberts (2005). An anthology\nof probabilistic models for medical informatics.\nIn D. Husmeier, R. Dybowski, and S. Roberts\n(Eds.), Probabilistic Modeling in Bioinformatics\nand Medical Informatics, pp. 297–349. Springer.\nEfron, B. (1979). Bootstrap methods: another look\nat the jackknife. Annals of Statistics 7, 1–26.\nElkan, C. (2003). Using the triangle inequality to ac-\ncelerate k-means. In Proceedings of the Twelfth\nInternational Conference on Machine Learning,\npp. 147–153. AAAI.\nElliott, R. J., L. Aggoun, and J. B. Moore (1995).\nHidden Markov Models: Estimation and Con-\ntrol. Springer.\nEphraim, Y ., D. Malah, and B. H. Juang (1989).\nOn the application of hidden Markov models for\nenhancing noisy speech.IEEE Transactions on\nAcoustics, Speech and Signal Processing37(12),\n1846–1856.\nErwin, E., K. Obermayer, and K. Schulten (1992).\nSelf-organizing maps: ordering, convergence\nproperties and energy functions. Biological Cy-\nbernetics 67, 47–55.\nEveritt, B. S. (1984). An Introduction to Latent V ari-\nable Models. Chapman and Hall.\nFaul, A. C. and M. E. Tipping (2002). Analysis of\nsparse Bayesian learning. In T. G. Dietterich,\nS. Becker, and Z. Ghahramani (Eds.), Advances\nin Neural Information Processing Systems, V ol-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 735,
      "page_label": "716"
    }
  },
  {
    "page_content": "sparse Bayesian learning. In T. G. Dietterich,\nS. Becker, and Z. Ghahramani (Eds.), Advances\nin Neural Information Processing Systems, V ol-\nume 14, pp. 383–389. MIT Press.\nFeller, W. (1966). An Introduction to Probability\nTheory and its Applications (Second ed.), V ol-\nume 2. Wiley.\nFeynman, R. P., R. B. Leighton, and M. Sands\n(1964). The Feynman Lectures of Physics, V ol-\nume Two. Addison-Wesley. Chapter 19.\nFletcher, R. (1987). Practical Methods of Optimiza-\ntion (Second ed.). Wiley.\nForsyth, D. A. and J. Ponce (2003). Computer Vi-\nsion: A Modern Approach. Prentice Hall.\nFreund, Y . and R. E. Schapire (1996). Experiments\nwith a new boosting algorithm. In L. Saitta (Ed.),\nThirteenth International Conference on Machine\nLearning, pp. 148–156. Morgan Kaufmann.\nFrey, B. J. (1998). Graphical Models for Ma-\nchine Learning and Digital Communication .\nMIT Press.\nFrey, B. J. and D. J. C. MacKay (1998). A revolu-\ntion: Belief propagation in graphs with cycles. In\nM. I. Jordan, M. J. Kearns, and S. A. Solla (Eds.),\nAdvances in Neural Information Processing Sys-\ntems, V olume 10. MIT Press.\nFriedman, J. H. (2001). Greedy function approxi-\nmation: a gradient boosting machine. Annals of\nStatistics 29(5), 1189–1232.\nFriedman, J. H., T. Hastie, and R. Tibshirani (2000).\nAdditive logistic regression: a statistical view of\nboosting. Annals of Statistics 28, 337–407.\nFriedman, N. and D. Koller (2003). Being Bayesian\nabout network structure: A Bayesian approach",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 735,
      "page_label": "716"
    }
  },
  {
    "page_content": "boosting. Annals of Statistics 28, 337–407.\nFriedman, N. and D. Koller (2003). Being Bayesian\nabout network structure: A Bayesian approach\nto structure discovery in Bayesian networks.Ma-\nchine Learning 50, 95–126.\nFrydenberg, M. (1990). The chain graph Markov\nproperty. Scandinavian Journal of Statistics 17,\n333–353.\nFukunaga, K. (1990). Introduction to Statistical Pat-\ntern Recognition (Second ed.). Academic Press.\nFunahashi, K. (1989). On the approximate realiza-\ntion of continuous mappings by neural networks.\nNeural Networks 2(3), 183–192.\nFung, R. and K. C. Chang (1990). Weighting and\nintegrating evidence for stochastic simulation in\nBayesian networks. In P. P. Bonissone, M. Hen-\nrion, L. N. Kanal, and J. F. Lemmer (Eds.),Un-\ncertainty in Artiﬁcial Intelligence, V olume 5, pp.\n208–219. Elsevier.\nGallager, R. G. (1963). Low-Density Parity-Check\nCodes. MIT Press.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 735,
      "page_label": "716"
    }
  },
  {
    "page_content": "REFERENCES 717\nGamerman, D. (1997). Markov Chain Monte Carlo:\nStochastic Simulation for Bayesian Inference .\nChapman and Hall.\nGelman, A., J. B. Carlin, H. S. Stern, and D. B. Ru-\nbin (2004). Bayesian Data Analysis(Second ed.).\nChapman and Hall.\nGeman, S. and D. Geman (1984). Stochastic re-\nlaxation, Gibbs distributions, and the Bayesian\nrestoration of images. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence 6(1),\n721–741.\nGhahramani, Z. and M. J. Beal (2000). Variational\ninference for Bayesian mixtures of factor ana-\nlyzers. In S. A. Solla, T. K. Leen, and K. R.\nM¨uller (Eds.), Advances in Neural Information\nProcessing Systems, V olume 12, pp. 449–455.\nMIT Press.\nGhahramani, Z. and G. E. Hinton (1996a). The\nEM algorithm for mixtures of factor analyzers.\nTechnical Report CRG-TR-96-1, University of\nToronto.\nGhahramani, Z. and G. E. Hinton (1996b). Param-\neter estimation for linear dynamical systems.\nTechnical Report CRG-TR-96-2, University of\nToronto.\nGhahramani, Z. and G. E. Hinton (1998). Variational\nlearning for switching state-space models. Neu-\nral Computation 12(4), 963–996.\nGhahramani, Z. and M. I. Jordan (1994). Super-\nvised learning from incomplete data via an EM\nappproach. In J. D. Cowan, G. T. Tesauro, and\nJ. Alspector (Eds.), Advances in Neural Informa-\ntion Processing Systems, V olume 6, pp. 120–127.\nMorgan Kaufmann.\nGhahramani, Z. and M. I. Jordan (1997). Factorial\nhidden Markov models. Machine Learning 29,\n245–275.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 736,
      "page_label": "717"
    }
  },
  {
    "page_content": "Morgan Kaufmann.\nGhahramani, Z. and M. I. Jordan (1997). Factorial\nhidden Markov models. Machine Learning 29,\n245–275.\nGibbs, M. N. (1997). Bayesian Gaussian processes\nfor regression and classiﬁcation. Phd thesis, Uni-\nversity of Cambridge.\nGibbs, M. N. and D. J. C. MacKay (2000). Varia-\ntional Gaussian process classiﬁers. IEEE Trans-\nactions on Neural Networks11, 1458–1464.\nGilks, W. R. (1992). Derivative-free adaptive\nrejection sampling for Gibbs sampling. In\nJ. Bernardo, J. Berger, A. P. Dawid, and A. F. M.\nSmith (Eds.), Bayesian Statistics, V olume 4. Ox-\nford University Press.\nGilks, W. R., N. G. Best, and K. K. C. Tan (1995).\nAdaptive rejection Metropolis sampling. Applied\nStatistics 44, 455–472.\nGilks, W. R., S. Richardson, and D. J. Spiegelhal-\nter (Eds.) (1996). Markov Chain Monte Carlo in\nPractice. Chapman and Hall.\nGilks, W. R. and P. Wild (1992). Adaptive rejection\nsampling for Gibbs sampling. Applied Statis-\ntics 41, 337–348.\nGill, P. E., W. Murray, and M. H. Wright (1981).\nPractical Optimization. Academic Press.\nGoldberg, P. W., C. K. I. Williams, and C. M.\nBishop (1998). Regression with input-dependent\nnoise: A Gaussian process treatment. In Ad-\nvances in Neural Information Processing Sys-\ntems, V olume 10, pp. 493–499. MIT Press.\nGolub, G. H. and C. F. Van Loan (1996). Matrix\nComputations (Third ed.). John Hopkins Univer-\nsity Press.\nGood, I. (1950). Probability and the W eighing of Ev-\nidence. Hafners.\nGordon, N. J., D. J. Salmond, and A. F. M. Smith",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 736,
      "page_label": "717"
    }
  },
  {
    "page_content": "sity Press.\nGood, I. (1950). Probability and the W eighing of Ev-\nidence. Hafners.\nGordon, N. J., D. J. Salmond, and A. F. M. Smith\n(1993). Novel approach to nonlinear/non-\nGaussian Bayesian state estimation. IEE\nProceedings-F 140(2), 107–113.\nGraepel, T. (2003). Solving noisy linear operator\nequations by Gaussian processes: Application\nto ordinary and partial differential equations. In\nProceedings of the Twentieth International Con-\nference on Machine Learning, pp. 234–241.\nGreig, D., B. Porteous, and A. Seheult (1989). Ex-\nact maximum a-posteriori estimation for binary\nimages. Journal of the Royal Statistical Society,\nSeries B 51(2), 271–279.\nGull, S. F. (1989). Developments in maximum en-\ntropy data analysis. In J. Skilling (Ed.), Maxi-\nmum Entropy and Bayesian Methods, pp. 53–71.\nKluwer.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 736,
      "page_label": "717"
    }
  },
  {
    "page_content": "718 REFERENCES\nHassibi, B. and D. G. Stork (1993). Second order\nderivatives for network pruning: optimal brain\nsurgeon. In S. J. Hanson, J. D. Cowan, and\nC. L. Giles (Eds.), Advances in Neural Informa-\ntion Processing Systems, V olume 5, pp. 164–171.\nMorgan Kaufmann.\nHastie, T. and W. Stuetzle (1989). Principal curves.\nJournal of the American Statistical Associa-\ntion84(106), 502–516.\nHastie, T., R. Tibshirani, and J. Friedman (2001).\nThe Elements of Statistical Learning. Springer.\nHastings, W. K. (1970). Monte Carlo sampling\nmethods using Markov chains and their applica-\ntions. Biometrika 57, 97–109.\nHathaway, R. J. (1986). Another interpretation of the\nEM algorithm for mixture distributions.Statistics\nand Probability Letters 4, 53–56.\nHaussler, D. (1999). Convolution kernels on discrete\nstructures. Technical Report UCSC-CRL-99-10,\nUniversity of California, Santa Cruz, Computer\nScience Department.\nHenrion, M. (1988). Propagation of uncertainty by\nlogic sampling in Bayes’ networks. In J. F. Lem-\nmer and L. N. Kanal (Eds.),Uncertainty in Arti-\nﬁcial Intelligence, V olume 2, pp. 149–164. North\nHolland.\nHerbrich, R. (2002). Learning Kernel Classiﬁers .\nMIT Press.\nHertz, J., A. Krogh, and R. G. Palmer (1991). In-\ntroduction to the Theory of Neural Computation.\nAddison Wesley.\nHinton, G. E., P. Dayan, and M. Revow (1997).\nModelling the manifolds of images of handwrit-\nten digits.IEEE Transactions on Neural Net-\nworks 8(1), 65–74.\nHinton, G. E. and D. van Camp (1993). Keeping",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 737,
      "page_label": "718"
    }
  },
  {
    "page_content": "ten digits.IEEE Transactions on Neural Net-\nworks 8(1), 65–74.\nHinton, G. E. and D. van Camp (1993). Keeping\nneural networks simple by minimizing the de-\nscription length of the weights. InProceedings of\nthe Sixth Annual Conference on Computational\nLearning Theory, pp. 5–13. ACM.\nHinton, G. E., M. Welling, Y . W. Teh, and S. Osin-\ndero (2001). A new view of ICA. InProceedings\nof the International Conference on Independent\nComponent Analysis and Blind Signal Separa-\ntion, V olume 3.\nHodgson, M. E. (1998). Reducing computational re-\nquirements of the minimum-distance classiﬁer.\nRemote Sensing of Environments25, 117–128.\nHoerl, A. E. and R. Kennard (1970). Ridge regres-\nsion: biased estimation for nonorthogonal prob-\nlems. T echnometrics12, 55–67.\nHofmann, T. (2000). Learning the similarity of doc-\numents: an information-geometric approach to\ndocument retrieval and classiﬁcation. In S. A.\nSolla, T. K. Leen, and K. R. M¨uller (Eds.), Ad-\nvances in Neural Information Processing Sys-\ntems, V olume 12, pp. 914–920. MIT Press.\nHojen-Sorensen, P. A., O. Winther, and L. K. Hansen\n(2002). Mean ﬁeld approaches to independent\ncomponent analysis. Neural Computation 14(4),\n889–918.\nHornik, K. (1991). Approximation capabilities of\nmultilayer feedforward networks. Neural Net-\nworks 4(2), 251–257.\nHornik, K., M. Stinchcombe, and H. White (1989).\nMultilayer feedforward networks are universal\napproximators.Neural Networks 2(5), 359–366.\nHotelling, H. (1933). Analysis of a complex of statis-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 737,
      "page_label": "718"
    }
  },
  {
    "page_content": "Multilayer feedforward networks are universal\napproximators.Neural Networks 2(5), 359–366.\nHotelling, H. (1933). Analysis of a complex of statis-\ntical variables into principal components. Jour-\nnal of Educational Psychology24, 417–441.\nHotelling, H. (1936). Relations between two sets of\nvariables. Biometrika 28, 321–377.\nHyv¨arinen, A. and E. Oja (1997). A fast ﬁxed-point\nalgorithm for independent component analysis.\nNeural Computation 9(7), 1483–1492.\nIsard, M. and A. Blake (1998). CONDENSATION\n– conditional density propagation for visual\ntracking.International Journal of Computer Vi-\nsion 29(1), 5–18.\nIto, Y . (1991). Representation of functions by su-\nperpositions of a step or sigmoid function and\ntheir applications to neural network theory.Neu-\nral Networks 4(3), 385–394.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 737,
      "page_label": "718"
    }
  },
  {
    "page_content": "REFERENCES 719\nJaakkola, T. and M. I. Jordan (2000). Bayesian\nparameter estimation via variational methods.\nStatistics and Computing10, 25–37.\nJaakkola, T. S. (2001). Tutorial on variational ap-\nproximation methods. In M. Opper and D. Saad\n(Eds.), Advances in Mean Field Methods , pp.\n129–159. MIT Press.\nJaakkola, T. S. and D. Haussler (1999). Exploiting\ngenerative models in discriminative classiﬁers. In\nM. S. Kearns, S. A. Solla, and D. A. Cohn (Eds.),\nAdvances in Neural Information Processing Sys-\ntems, V olume 11. MIT Press.\nJacobs, R. A., M. I. Jordan, S. J. Nowlan, and G. E.\nHinton (1991). Adaptive mixtures of local ex-\nperts. Neural Computation 3(1), 79–87.\nJaynes, E. T. (2003). Probability Theory: The Logic\nof Science. Cambridge University Press.\nJebara, T. (2004). Machine Learning: Discrimina-\ntive and Generative. Kluwer.\nJeffries, H. (1946). An invariant form for the prior\nprobability in estimation problems. Pro. Roy.\nSoc. AA 186, 453–461.\nJelinek, F. (1997). Statistical Methods for Speech\nRecognition. MIT Press.\nJensen, C., A. Kong, and U. Kjaerulff (1995). Block-\ning gibbs sampling in very large probabilistic\nexpert systems. International Journal of Human\nComputer Studies. Special Issue on Real-W orld\nApplications of Uncertain Reasoning. 42, 647–\n666.\nJensen, F. V . (1996). An Introduction to Bayesian\nNetworks. UCL Press.\nJerrum, M. and A. Sinclair (1996). The Markov\nchain Monte Carlo method: an approach to ap-\nproximate counting and integration. In D. S.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 738,
      "page_label": "719"
    }
  },
  {
    "page_content": "Jerrum, M. and A. Sinclair (1996). The Markov\nchain Monte Carlo method: an approach to ap-\nproximate counting and integration. In D. S.\nHochbaum (Ed.), Approximation Algorithms for\nNP-Hard Problems. PWS Publishing.\nJolliffe, I. T. (2002). Principal Component Analysis\n(Second ed.). Springer.\nJordan, M. I. (1999). Learning in Graphical Models.\nMIT Press.\nJordan, M. I. (2007). An Introduction to Probabilis-\ntic Graphical Models. In preparation.\nJordan, M. I., Z. Ghahramani, T. S. Jaakkola, and\nL. K. Saul (1999). An introduction to variational\nmethods for graphical models. In M. I. Jordan\n(Ed.), Learning in Graphical Models, pp. 105–\n162. MIT Press.\nJordan, M. I. and R. A. Jacobs (1994). Hierarchical\nmixtures of experts and the EM algorithm. Neu-\nral Computation 6(2), 181–214.\nJutten, C. and J. Herault (1991). Blind separation of\nsources, 1: An adaptive algorithm based on neu-\nromimetic architecture.Signal Processing24(1),\n1–10.\nKalman, R. E. (1960). A new approach to linear ﬁl-\ntering and prediction problems. Transactions of\nthe American Society for Mechanical Engineer-\ning, Series D, Journal of Basic Engineering82,\n35–45.\nKambhatla, N. and T. K. Leen (1997). Dimension\nreduction by local principal component analysis.\nNeural Computation 9(7), 1493–1516.\nKanazawa, K., D. Koller, and S. Russel (1995).\nStochastic simulation algorithms for dynamic\nprobabilistic networks. InUncertainty in Artiﬁ-\ncial Intelligence, V olume 11. Morgan Kaufmann.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 738,
      "page_label": "719"
    }
  },
  {
    "page_content": "Stochastic simulation algorithms for dynamic\nprobabilistic networks. InUncertainty in Artiﬁ-\ncial Intelligence, V olume 11. Morgan Kaufmann.\nKapadia, S. (1998). Discriminative Training of Hid-\nden Markov Models. Phd thesis, University of\nCambridge, U.K.\nKapur, J. (1989). Maximum entropy methods in sci-\nence and engineering. Wiley.\nKarush, W. (1939). Minima of functions of several\nvariables with inequalities as side constraints.\nMaster’s thesis, Department of Mathematics,\nUniversity of Chicago.\nKass, R. E. and A. E. Raftery (1995). Bayes fac-\ntors. Journal of the American Statistical Associ-\nation 90, 377–395.\nKearns, M. J. and U. V . Vazirani (1994). An Intro-\nduction to Computational Learning Theory.M I T\nPress.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 738,
      "page_label": "719"
    }
  },
  {
    "page_content": "720 REFERENCES\nKindermann, R. and J. L. Snell (1980).Markov Ran-\ndom Fields and Their Applications . American\nMathematical Society.\nKittler, J. and J. F ¨oglein (1984). Contextual classiﬁ-\ncation of multispectral pixel data. Image and Vi-\nsion Computing 2, 13–29.\nKohonen, T. (1982). Self-organized formation of\ntopologically correct feature maps. Biological\nCybernetics 43, 59–69.\nKohonen, T. (1995). Self-Organizing Maps .\nSpringer.\nKolmogorov, V . and R. Zabih (2004). What en-\nergy functions can be minimized via graph cuts?\nIEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence26(2), 147–159.\nKreinovich, V . Y . (1991). Arbitrary nonlinearity is\nsufﬁcient to represent all functions by neural net-\nworks: a theorem.Neural Networks 4(3), 381–\n383.\nKrogh, A., M. Brown, I. S. Mian, K. Sj ¨olander, and\nD. Haussler (1994). Hidden Markov models in\ncomputational biology: Applications to protein\nmodelling. Journal of Molecular Biology 235,\n1501–1531.\nKschischnang, F. R., B. J. Frey, and H. A. Loeliger\n(2001). Factor graphs and the sum-product algo-\nrithm. IEEE Transactions on Information The-\nory 47(2), 498–519.\nKuhn, H. W. and A. W. Tucker (1951). Nonlinear\nprogramming. In Proceedings of the 2nd Berke-\nley Symposium on Mathematical Statistics and\nProbabilities, pp. 481–492. University of Cali-\nfornia Press.\nKullback, S. and R. A. Leibler (1951). On infor-\nmation and sufﬁciency. Annals of Mathematical\nStatistics 22(1), 79–86.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 739,
      "page_label": "720"
    }
  },
  {
    "page_content": "fornia Press.\nKullback, S. and R. A. Leibler (1951). On infor-\nmation and sufﬁciency. Annals of Mathematical\nStatistics 22(1), 79–86.\nK˙urkov´a, V . and P. C. Kainen (1994). Functionally\nequivalent feed-forward neural networks. Neural\nComputation 6(3), 543–558.\nKuss, M. and C. Rasmussen (2006). Assessing ap-\nproximations for Gaussian process classiﬁcation.\nIn Advances in Neural Information Processing\nSystems, Number 18. MIT Press. in press.\nLasserre, J., C. M. Bishop, and T. Minka (2006).\nPrincipled hybrids of generative and discrimina-\ntive models. InProceedings 2006 IEEE Confer-\nence on Computer Vision and Pattern Recogni-\ntion, New Y ork.\nLauritzen, S. and N. Wermuth (1989). Graphical\nmodels for association between variables, some\nof which are qualitative some quantitative.An-\nnals of Statistics 17, 31–57.\nLauritzen, S. L. (1992). Propagation of probabilities,\nmeans and variances in mixed graphical associa-\ntion models. Journal of the American Statistical\nAssociation 87, 1098–1108.\nLauritzen, S. L. (1996). Graphical Models. Oxford\nUniversity Press.\nLauritzen, S. L. and D. J. Spiegelhalter (1988). Lo-\ncal computations with probabailities on graphical\nstructures and their application to expert systems.\nJournal of the Royal Statistical Society50, 157–\n224.\nLawley, D. N. (1953). A modiﬁed method of esti-\nmation in factor analysis and some large sam-\nple results. In Uppsala Symposium on Psycho-\nlogical Factor Analysis, Number 3 in Nordisk",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 739,
      "page_label": "720"
    }
  },
  {
    "page_content": "mation in factor analysis and some large sam-\nple results. In Uppsala Symposium on Psycho-\nlogical Factor Analysis, Number 3 in Nordisk\nPsykologi Monograph Series, pp. 35–42. Upp-\nsala: Almqvist and Wiksell.\nLawrence, N. D., A. I. T. Rowstron, C. M. Bishop,\nand M. J. Taylor (2002). Optimising synchro-\nnisation times for mobile devices. In T. G. Di-\netterich, S. Becker, and Z. Ghahramani (Eds.),\nAdvances in Neural Information Processing Sys-\ntems, V olume 14, pp. 1401–1408. MIT Press.\nLazarsfeld, P. F. and N. W. Henry (1968). Latent\nStructure Analysis. Houghton Mifﬂin.\nLe Cun, Y ., B. Boser, J. S. Denker, D. Henderson,\nR. E. Howard, W. Hubbard, and L. D. Jackel\n(1989). Backpropagation applied to handwritten\nzip code recognition.Neural Computation 1(4),\n541–551.\nLe Cun, Y ., J. S. Denker, and S. A. Solla (1990).\nOptimal brain damage. In D. S. Touretzky (Ed.),",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 739,
      "page_label": "720"
    }
  },
  {
    "page_content": "REFERENCES 721\nAdvances in Neural Information Processing Sys-\ntems, V olume 2, pp. 598–605. Morgan Kauf-\nmann.\nLeCun, Y ., L. Bottou, Y . Bengio, and P. Haffner\n(1998). Gradient-based learning applied to doc-\nument recognition. Proceedings of the IEEE86,\n2278–2324.\nLee, Y ., Y . Lin, and G. Wahba (2001). Multicategory\nsupport vector machines. Technical Report 1040,\nDepartment of Statistics, University of Madison,\nWisconsin.\nLeen, T. K. (1995). From data distributions to regu-\nlarization in invariant learning.Neural Computa-\ntion 7, 974–981.\nLindley, D. V . (1982). Scoring rules and the in-\nevitability of probability. International Statisti-\ncal Review 50, 1–26.\nLiu, J. S. (Ed.) (2001). Monte Carlo Strategies in\nScientiﬁc Computing. Springer.\nLloyd, S. P. (1982). Least squares quantization in\nPCM. IEEE Transactions on Information The-\nory 28(2), 129–137.\nL¨utkepohl, H. (1996). Handbook of Matrices. Wiley.\nMacKay, D. J. C. (1992a). Bayesian interpolation.\nNeural Computation 4(3), 415–447.\nMacKay, D. J. C. (1992b). The evidence framework\napplied to classiﬁcation networks. Neural Com-\nputation 4(5), 720–736.\nMacKay, D. J. C. (1992c). A practical Bayesian\nframework for back-propagation networks. Neu-\nral Computation 4(3), 448–472.\nMacKay, D. J. C. (1994). Bayesian methods for\nbackprop networks. In E. Domany, J. L. van\nHemmen, and K. Schulten (Eds.), Models of\nNeural Networks, III, Chapter 6, pp. 211–254.\nSpringer.\nMacKay, D. J. C. (1995). Bayesian neural networks",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 740,
      "page_label": "721"
    }
  },
  {
    "page_content": "Hemmen, and K. Schulten (Eds.), Models of\nNeural Networks, III, Chapter 6, pp. 211–254.\nSpringer.\nMacKay, D. J. C. (1995). Bayesian neural networks\nand density networks. Nuclear Instruments and\nMethods in Physics Research, A354(1), 73–80.\nMacKay, D. J. C. (1997). Ensemble learning for hid-\nden Markov models. Unpublished manuscript,\nDepartment of Physics, University of Cam-\nbridge.\nMacKay, D. J. C. (1998). Introduction to Gaus-\nsian processes. In C. M. Bishop (Ed.), Neural\nNetworks and Machine Learning, pp. 133–166.\nSpringer.\nMacKay, D. J. C. (1999). Comparison of approx-\nimate methods for handling hyperparameters.\nNeural Computation 11(5), 1035–1068.\nMacKay, D. J. C. (2003). Information Theory, Infer-\nence and Learning Algorithms. Cambridge Uni-\nversity Press.\nMacKay, D. J. C. and M. N. Gibbs (1999). Den-\nsity networks. In J. W. Kay and D. M. Tittering-\nton (Eds.),Statistics and Neural Networks: Ad-\nvances at the Interface, Chapter 5, pp. 129–145.\nOxford University Press.\nMacKay, D. J. C. and R. M. Neal (1999). Good error-\ncorrecting codes based on very sparse matrices.\nIEEE Transactions on Information Theory 45,\n399–431.\nMacQueen, J. (1967). Some methods for classiﬁca-\ntion and analysis of multivariate observations. In\nL. M. LeCam and J. Neyman (Eds.), Proceed-\nings of the Fifth Berkeley Symposium on Mathe-\nmatical Statistics and Probability, V olume I, pp.\n281–297. University of California Press.\nMagnus, J. R. and H. Neudecker (1999). Matrix Dif-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 740,
      "page_label": "721"
    }
  },
  {
    "page_content": "matical Statistics and Probability, V olume I, pp.\n281–297. University of California Press.\nMagnus, J. R. and H. Neudecker (1999). Matrix Dif-\nferential Calculus with Applications in Statistics\nand Econometrics. Wiley.\nMallat, S. (1999). A W avelet T our of Signal Process-\ning (Second ed.). Academic Press.\nManning, C. D. and H. Sch¨utze (1999). F oundations\nof Statistical Natural Language Processing. MIT\nPress.\nMardia, K. V . and P. E. Jupp (2000). Directional\nStatistics. Wiley.\nMaybeck, P. S. (1982). Stochastic models, estima-\ntion and control. Academic Press.\nMcAllester, D. A. (2003). PAC-Bayesian stochastic\nmodel selection. Machine Learning 51(1), 5–21.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 740,
      "page_label": "721"
    }
  },
  {
    "page_content": "722 REFERENCES\nMcCullagh, P. and J. A. Nelder (1989). Generalized\nLinear Models (Second ed.). Chapman and Hall.\nMcCulloch, W. S. and W. Pitts (1943). A logical\ncalculus of the ideas immanent in nervous ac-\ntivity. Bulletin of Mathematical Biophysics 5,\n115–133. Reprinted in Anderson and Rosenfeld\n(1988).\nMcEliece, R. J., D. J. C. MacKay, and J. F. Cheng\n(1998). Turbo decoding as an instance of Pearl’s\n‘Belief Ppropagation’ algorithm. IEEE Journal\non Selected Areas in Communications 16, 140–\n152.\nMcLachlan, G. J. and K. E. Basford (1988). Mixture\nModels: Inference and Applications to Cluster-\ning. Marcel Dekker.\nMcLachlan, G. J. and T. Krishnan (1997). The EM\nAlgorithm and its Extensions. Wiley.\nMcLachlan, G. J. and D. Peel (2000). Finite Mixture\nModels. Wiley.\nMeng, X. L. and D. B. Rubin (1993). Maximum like-\nlihood estimation via the ECM algorithm: a gen-\neral framework. Biometrika 80, 267–278.\nMetropolis, N., A. W. Rosenbluth, M. N. Rosen-\nbluth, A. H. Teller, and E. Teller (1953). Equa-\ntion of state calculations by fast computing\nmachines. Journal of Chemical Physics 21(6),\n1087–1092.\nMetropolis, N. and S. Ulam (1949). The Monte\nCarlo method. Journal of the American Statisti-\ncal Association 44(247), 335–341.\nMika, S., G. R ¨atsch, J. Weston, and B. Sch ¨olkopf\n(1999). Fisher discriminant analysis with ker-\nnels. In Y . H. Hu, J. Larsen, E. Wilson, and\nS. Douglas (Eds.),Neural Networks for Signal\nProcessing IX, pp. 41–48. IEEE.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 741,
      "page_label": "722"
    }
  },
  {
    "page_content": "nels. In Y . H. Hu, J. Larsen, E. Wilson, and\nS. Douglas (Eds.),Neural Networks for Signal\nProcessing IX, pp. 41–48. IEEE.\nMinka, T. (2001a). Expectation propagation for ap-\nproximate Bayesian inference. In J. Breese and\nD. Koller (Eds.), Proceedings of the Seventeenth\nConference on Uncertainty in Artiﬁcial Intelli-\ngence, pp. 362–369. Morgan Kaufmann.\nMinka, T. (2001b). A family of approximate al-\ngorithms for Bayesian inference. Ph. D. thesis,\nMIT.\nMinka, T. (2004). Power EP. Technical Report\nMSR-TR-2004-149, Microsoft Research Cam-\nbridge.\nMinka, T. (2005). Divergence measures and mes-\nsage passing. Technical Report MSR-TR-2005-\n173, Microsoft Research Cambridge.\nMinka, T. P. (2001c). Automatic choice of dimen-\nsionality for PCA. In T. K. Leen, T. G. Diet-\nterich, and V . Tresp (Eds.), Advances in Neural\nInformation Processing Systems, V olume 13, pp.\n598–604. MIT Press.\nMinsky, M. L. and S. A. Papert (1969). Perceptrons.\nMIT Press. Expanded edition 1990.\nMiskin, J. W. and D. J. C. MacKay (2001). Ensem-\nble learning for blind source separation. In S. J.\nRoberts and R. M. Everson (Eds.), Independent\nComponent Analysis: Principles and Practice .\nCambridge University Press.\nMøller, M. (1993). Efﬁcient Training of Feed-\nForward Neural Networks. Ph. D. thesis, Aarhus\nUniversity, Denmark.\nMoody, J. and C. J. Darken (1989). Fast learning in\nnetworks of locally-tuned processing units. Neu-\nral Computation 1(2), 281–294.\nMoore, A. W. (2000). The anchors hierarch: us-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 741,
      "page_label": "722"
    }
  },
  {
    "page_content": "networks of locally-tuned processing units. Neu-\nral Computation 1(2), 281–294.\nMoore, A. W. (2000). The anchors hierarch: us-\ning the triangle inequality to survive high dimen-\nsional data. In Proceedings of the Twelfth Con-\nference on Uncertainty in Artiﬁcial Intelligence,\npp. 397–405.\nM¨uller, K. R., S. Mika, G. R ¨atsch, K. Tsuda, and\nB. Sch ¨olkopf (2001). An introduction to kernel-\nbased learning algorithms. IEEE Transactions on\nNeural Networks 12(2), 181–202.\nM¨uller, P. and F. A. Quintana (2004). Nonparametric\nBayesian data analysis. Statistical Science19(1),\n95–110.\nNabney, I. T. (2002).Netlab: Algorithms for Pattern\nRecognition. Springer.\nNadaraya, ´E. A. (1964). On estimating regression.\nTheory of Probability and its Applications9(1),\n141–142.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 741,
      "page_label": "722"
    }
  },
  {
    "page_content": "REFERENCES 723\nNag, R., K. Wong, and F. Fallside (1986). Script\nrecognition using hidden markov models. In\nICASSP86, pp. 2071–2074. IEEE.\nNeal, R. M. (1993). Probabilistic inference using\nMarkov chain Monte Carlo methods. Technical\nReport CRG-TR-93-1, Department of Computer\nScience, University of Toronto, Canada.\nNeal, R. M. (1996). Bayesian Learning for Neural\nNetworks. Springer. Lecture Notes in Statistics\n118.\nNeal, R. M. (1997). Monte Carlo implementation of\nGaussian process models for Bayesian regression\nand classiﬁcation. Technical Report 9702, De-\npartment of Computer Statistics, University of\nToronto.\nNeal, R. M. (1999). Suppressing random walks in\nMarkov chain Monte Carlo using ordered over-\nrelaxation. In M. I. Jordan (Ed.), Learning in\nGraphical Models, pp. 205–228. MIT Press.\nNeal, R. M. (2000). Markov chain sampling for\nDirichlet process mixture models. Journal of\nComputational and Graphical Statistics9, 249–\n265.\nNeal, R. M. (2003). Slice sampling.Annals of Statis-\ntics 31, 705–767.\nNeal, R. M. and G. E. Hinton (1999). A new view of\nthe EM algorithm that justiﬁes incremental and\nother variants. In M. I. Jordan (Ed.), Learning in\nGraphical Models, pp. 355–368. MIT Press.\nNelder, J. A. and R. W. M. Wedderburn (1972). Gen-\neralized linear models. Journal of the Royal Sta-\ntistical Society, A 135, 370–384.\nNilsson, N. J. (1965). Learning Machines. McGraw-\nHill. Reprinted as The Mathematical F ounda-\ntions of Learning Machines, Morgan Kaufmann,\n(1990).",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 742,
      "page_label": "723"
    }
  },
  {
    "page_content": "Nilsson, N. J. (1965). Learning Machines. McGraw-\nHill. Reprinted as The Mathematical F ounda-\ntions of Learning Machines, Morgan Kaufmann,\n(1990).\nNocedal, J. and S. J. Wright (1999). Numerical Op-\ntimization. Springer.\nNowlan, S. J. and G. E. Hinton (1992). Simplifying\nneural networks by soft weight sharing. Neural\nComputation 4(4), 473–493.\nOgden, R. T. (1997). Essential W avelets for Statisti-\ncal Applications and Data Analysis. Birkh¨auser.\nOpper, M. and O. Winther (1999). A Bayesian ap-\nproach to on-line learning. In D. Saad (Ed.), On-\nLine Learning in Neural Networks, pp. 363–378.\nCambridge University Press.\nOpper, M. and O. Winther (2000a). Gaussian\nprocesses and SVM: mean ﬁeld theory and\nleave-one-out. In A. J. Smola, P. L. Bartlett,\nB. Sch ¨olkopf, and D. Shuurmans (Eds.), Ad-\nvances in Large Margin Classiﬁers, pp. 311–326.\nMIT Press.\nOpper, M. and O. Winther (2000b). Gaussian\nprocesses for classiﬁcation. Neural Computa-\ntion 12(11), 2655–2684.\nOsuna, E., R. Freund, and F. Girosi (1996). Support\nvector machines: training and applications. A.I.\nMemo AIM-1602, MIT.\nPapoulis, A. (1984). Probability, Random V ariables,\nand Stochastic Processes(Second ed.). McGraw-\nHill.\nParisi, G. (1988). Statistical Field Theory. Addison-\nWesley.\nPearl, J. (1988). Probabilistic Reasoning in Intelli-\ngent Systems. Morgan Kaufmann.\nPearlmutter, B. A. (1994). Fast exact multiplication\nby the Hessian. Neural Computation 6(1), 147–\n160.\nPearlmutter, B. A. and L. C. Parra (1997). Maximum",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 742,
      "page_label": "723"
    }
  },
  {
    "page_content": "by the Hessian. Neural Computation 6(1), 147–\n160.\nPearlmutter, B. A. and L. C. Parra (1997). Maximum\nlikelihood source separation: a context-sensitive\ngeneralization of ICA. In M. C. Mozer, M. I. Jor-\ndan, and T. Petsche (Eds.), Advances in Neural\nInformation Processing Systems, V olume 9, pp.\n613–619. MIT Press.\nPearson, K. (1901). On lines and planes of closest ﬁt\nto systems of points in space. The London, Edin-\nburgh and Dublin Philosophical Magazine and\nJournal of Science, Sixth Series2, 559–572.\nPlatt, J. C. (1999). Fast training of support vector\nmachines using sequential minimal optimization.\nIn B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola\n(Eds.), Advances in Kernel Methods – Support\nV ector Learning, pp. 185–208. MIT Press.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 742,
      "page_label": "723"
    }
  },
  {
    "page_content": "724 REFERENCES\nPlatt, J. C. (2000). Probabilities for SV machines.\nIn A. J. Smola, P. L. Bartlett, B. Sch ¨olkopf, and\nD. Shuurmans (Eds.), Advances in Large Margin\nClassiﬁers, pp. 61–73. MIT Press.\nPlatt, J. C., N. Cristianini, and J. Shawe-Taylor\n(2000). Large margin DAGs for multiclass clas-\nsiﬁcation. In S. A. Solla, T. K. Leen, and K. R.\nM¨uller (Eds.), Advances in Neural Information\nProcessing Systems, V olume 12, pp. 547–553.\nMIT Press.\nPoggio, T. and F. Girosi (1990). Networks for ap-\nproximation and learning. Proceedings of the\nIEEE 78(9), 1481–1497.\nPowell, M. J. D. (1987). Radial basis functions for\nmultivariable interpolation: a review. In J. C.\nMason and M. G. Cox (Eds.), Algorithms for\nApproximation, pp. 143–167. Oxford University\nPress.\nPress, W. H., S. A. Teukolsky, W. T. Vetterling, and\nB. P. Flannery (1992). Numerical Recipes in C:\nThe Art of Scientiﬁc Computing (Second ed.).\nCambridge University Press.\nQazaz, C. S., C. K. I. Williams, and C. M. Bishop\n(1997). An upper bound on the Bayesian error\nbars for generalized linear regression. In S. W.\nEllacott, J. C. Mason, and I. J. Anderson (Eds.),\nMathematics of Neural Networks: Models, Algo-\nrithms and Applications, pp. 295–299. Kluwer.\nQuinlan, J. R. (1986). Induction of decision trees.\nMachine Learning 1(1), 81–106.\nQuinlan, J. R. (1993). C4.5: Programs for Machine\nLearning. Morgan Kaufmann.\nRabiner, L. and B. H. Juang (1993). Fundamentals\nof Speech Recognition. Prentice Hall.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 743,
      "page_label": "724"
    }
  },
  {
    "page_content": "Learning. Morgan Kaufmann.\nRabiner, L. and B. H. Juang (1993). Fundamentals\nof Speech Recognition. Prentice Hall.\nRabiner, L. R. (1989). A tutorial on hidden Markov\nmodels and selected applications in speech\nrecognition.Proceedings of the IEEE 77(2),\n257–285.\nRamasubramanian, V . and K. K. Paliwal (1990). A\ngeneralized optimization of the k-d tree for fast\nnearest-neighbour search. In Proceedings F ourth\nIEEE Region 10 International Conference (TEN-\nCON’89), pp. 565–568.\nRamsey, F. (1931). Truth and probability. In\nR. Braithwaite (Ed.), The F oundations of Math-\nematics and other Logical Essays . Humanities\nPress.\nRao, C. R. and S. K. Mitra (1971). Generalized In-\nverse of Matrices and Its Applications. Wiley.\nRasmussen, C. E. (1996). Evaluation of Gaussian\nProcesses and Other Methods for Non-Linear\nRegression. Ph. D. thesis, University of Toronto.\nRasmussen, C. E. and J. Qui˜nonero-Candela (2005).\nHealing the relevance vector machine by aug-\nmentation. In L. D. Raedt and S. Wrobel (Eds.),\nProceedings of the 22nd International Confer-\nence on Machine Learning, pp. 689–696.\nRasmussen, C. E. and C. K. I. Williams (2006).\nGaussian Processes for Machine Learning. MIT\nPress.\nRauch, H. E., F. Tung, and C. T. Striebel (1965).\nMaximum likelihood estimates of linear dynam-\nical systems. AIAA Journal 3, 1445–1450.\nRicotti, L. P., S. Ragazzini, and G. Martinelli (1988).\nLearning of word stress in a sub-optimal second\norder backpropagation neural network. In Pro-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 743,
      "page_label": "724"
    }
  },
  {
    "page_content": "Ricotti, L. P., S. Ragazzini, and G. Martinelli (1988).\nLearning of word stress in a sub-optimal second\norder backpropagation neural network. In Pro-\nceedings of the IEEE International Conference\non Neural Networks, V olume 1, pp. 355–361.\nIEEE.\nRipley, B. D. (1996). Pattern Recognition and Neu-\nral Networks. Cambridge University Press.\nRobbins, H. and S. Monro (1951). A stochastic\napproximation method. Annals of Mathematical\nStatistics 22, 400–407.\nRobert, C. P. and G. Casella (1999). Monte Carlo\nStatistical Methods. Springer.\nRockafellar, R. (1972). Convex Analysis. Princeton\nUniversity Press.\nRosenblatt, F. (1962). Principles of Neurodynam-\nics: Perceptrons and the Theory of Brain Mech-\nanisms. Spartan.\nRoth, V . and V . Steinhage (2000). Nonlinear discrim-\ninant analysis using kernel functions. In S. A.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 743,
      "page_label": "724"
    }
  },
  {
    "page_content": "REFERENCES 725\nSolla, T. K. Leen, and K. R. M ¨uller (Eds.), Ad-\nvances in Neural Information Processing Sys-\ntems, V olume 12. MIT Press.\nRoweis, S. (1998). EM algorithms for PCA and\nSPCA. In M. I. Jordan, M. J. Kearns, and S. A.\nSolla (Eds.), Advances in Neural Information\nProcessing Systems, V olume 10, pp. 626–632.\nMIT Press.\nRoweis, S. and Z. Ghahramani (1999). A unifying\nreview of linear Gaussian models. Neural Com-\nputation 11(2), 305–345.\nRoweis, S. and L. Saul (2000, December). Nonlinear\ndimensionality reduction by locally linear em-\nbedding. Science 290, 2323–2326.\nRubin, D. B. (1983). Iteratively reweighted least\nsquares. In Encyclopedia of Statistical Sciences,\nV olume 4, pp. 272–275. Wiley.\nRubin, D. B. and D. T. Thayer (1982). EM al-\ngorithms for ML factor analysis. Psychome-\ntrika 47(1), 69–76.\nRumelhart, D. E., G. E. Hinton, and R. J. Williams\n(1986). Learning internal representations by er-\nror propagation. In D. E. Rumelhart, J. L. Mc-\nClelland, and the PDP Research Group (Eds.),\nParallel Distributed Processing: Explorations\nin the Microstructure of Cognition, V olume 1:\nFoundations, pp. 318–362. MIT Press. Reprinted\nin Anderson and Rosenfeld (1988).\nRumelhart, D. E., J. L. McClelland, and the PDP Re-\nsearch Group (Eds.) (1986). Parallel Distributed\nProcessing: Explorations in the Microstruc-\nture of Cognition, V olume 1: Foundations. MIT\nPress.\nSagan, H. (1969). Introduction to the Calculus of\nV ariations. Dover.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 744,
      "page_label": "725"
    }
  },
  {
    "page_content": "ture of Cognition, V olume 1: Foundations. MIT\nPress.\nSagan, H. (1969). Introduction to the Calculus of\nV ariations. Dover.\nSavage, L. J. (1961). The subjective basis of sta-\ntistical practice. Technical report, Department of\nStatistics, University of Michigan, Ann Arbor.\nSch¨olkopf, B., J. Platt, J. Shawe-Taylor, A. Smola,\nand R. C. Williamson (2001). Estimating the sup-\nport of a high-dimensional distribution.Neural\nComputation 13(7), 1433–1471.\nSch¨olkopf, B., A. Smola, and K.-R. M ¨uller (1998).\nNonlinear component analysis as a kernel\neigenvalue problem.Neural Computation 10(5),\n1299–1319.\nSch¨olkopf, B., A. Smola, R. C. Williamson, and P. L.\nBartlett (2000). New support vector algorithms.\nNeural Computation12(5), 1207–1245.\nSch¨olkopf, B. and A. J. Smola (2002).Learning with\nKernels. MIT Press.\nSchwarz, G. (1978). Estimating the dimension of a\nmodel. Annals of Statistics 6, 461–464.\nSchwarz, H. R. (1988).Finite element methods. Aca-\ndemic Press.\nSeeger, M. (2003).Bayesian Gaussian Process Mod-\nels: PAC-Bayesian Generalization Error Bounds\nand Sparse Approximations. Ph. D. thesis, Uni-\nversity of Edinburg.\nSeeger, M., C. K. I. Williams, and N. Lawrence\n(2003). Fast forward selection to speed up sparse\nGaussian processes. In C. M. Bishop and B. Frey\n(Eds.), Proceedings Ninth International W ork-\nshop on Artiﬁcial Intelligence and Statistics, Key\nW est, Florida.\nShachter, R. D. and M. Peot (1990). Simulation ap-\nproaches to general probabilistic inference on be-",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 744,
      "page_label": "725"
    }
  },
  {
    "page_content": "W est, Florida.\nShachter, R. D. and M. Peot (1990). Simulation ap-\nproaches to general probabilistic inference on be-\nlief networks. In P. P. Bonissone, M. Henrion,\nL. N. Kanal, and J. F. Lemmer (Eds.), Uncer-\ntainty in Artiﬁcial Intelligence, V olume 5. Else-\nvier.\nShannon, C. E. (1948). A mathematical theory of\ncommunication. The Bell System T echnical Jour-\nnal 27(3), 379–423 and 623–656.\nShawe-Taylor, J. and N. Cristianini (2004). Kernel\nMethods for Pattern Analysis. Cambridge Uni-\nversity Press.\nSietsma, J. and R. J. F. Dow (1991). Creating artiﬁ-\ncial neural networks that generalize. Neural Net-\nworks 4(1), 67–79.\nSimard, P., Y . Le Cun, and J. Denker (1993). Efﬁ-\ncient pattern recognition using a new transforma-\ntion distance. In S. J. Hanson, J. D. Cowan, and",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 744,
      "page_label": "725"
    }
  },
  {
    "page_content": "726 REFERENCES\nC. L. Giles (Eds.), Advances in Neural Informa-\ntion Processing Systems, V olume 5, pp. 50–58.\nMorgan Kaufmann.\nSimard, P., B. Victorri, Y . Le Cun, and J. Denker\n(1992). Tangent prop – a formalism for specify-\ning selected invariances in an adaptive network.\nIn J. E. Moody, S. J. Hanson, and R. P. Lippmann\n(Eds.),Advances in Neural Information Process-\ning Systems, V olume 4, pp. 895–903. Morgan\nKaufmann.\nSimard, P. Y ., D. Steinkraus, and J. Platt (2003).\nBest practice for convolutional neural networks\napplied to visual document analysis. In Pro-\nceedings International Conference on Document\nAnalysis and Recognition (ICDAR) , pp. 958–\n962. IEEE Computer Society.\nSirovich, L. (1987). Turbulence and the dynamics\nof coherent structures. Quarterly Applied Math-\nematics 45(3), 561–590.\nSmola, A. J. and P. Bartlett (2001). Sparse greedy\nGaussian process regression. In T. K. Leen, T. G.\nDietterich, and V . Tresp (Eds.),Advances in Neu-\nral Information Processing Systems, V olume 13,\npp. 619–625. MIT Press.\nSpiegelhalter, D. and S. Lauritzen (1990). Sequential\nupdating of conditional probabilities on directed\ngraphical structures. Networks 20, 579–605.\nStinchecombe, M. and H. White (1989). Universal\napproximation using feed-forward networks with\nnon-sigmoid hidden layer activation functions. In\nInternational Joint Conference on Neural Net-\nworks, V olume 1, pp. 613–618. IEEE.\nStone, J. V . (2004).Independent Component Analy-\nsis: A Tutorial Introduction. MIT Press.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 745,
      "page_label": "726"
    }
  },
  {
    "page_content": "works, V olume 1, pp. 613–618. IEEE.\nStone, J. V . (2004).Independent Component Analy-\nsis: A Tutorial Introduction. MIT Press.\nSung, K. K. and T. Poggio (1994). Example-based\nlearning for view-based human face detection.\nA.I. Memo 1521, MIT.\nSutton, R. S. and A. G. Barto (1998). Reinforcement\nLearning: An Introduction. MIT Press.\nSvens´en, M. and C. M. Bishop (2004). Ro-\nbust Bayesian mixture modelling. Neurocomput-\ning 64, 235–252.\nTarassenko, L. (1995). Novelty detection for the\nidentiﬁcation of masses in mamograms. In Pro-\nceedings F ourth IEE International Conference\non Artiﬁcial Neural Networks , V olume 4, pp.\n442–447. IEE.\nTax, D. and R. Duin (1999). Data domain descrip-\ntion by support vectors. In M. Verleysen (Ed.),\nProceedings European Symposium on Artiﬁcial\nNeural Networks, ESANN, pp. 251–256. D. Facto\nPress.\nTeh, Y . W., M. I. Jordan, M. J. Beal, and D. M. Blei\n(2006). Hierarchical Dirichlet processes. Journal\nof the Americal Statistical Association. to appear.\nTenenbaum, J. B., V . de Silva, and J. C. Langford\n(2000, December). A global framework for non-\nlinear dimensionality reduction.Science 290,\n2319–2323.\nTesauro, G. (1994). TD-Gammon, a self-teaching\nbackgammon program, achieves master-level\nplay. Neural Computation 6(2), 215–219.\nThiesson, B., D. M. Chickering, D. Heckerman, and\nC. Meek (2004). ARMA time-series modelling\nwith graphical models. In M. Chickering and\nJ. Halpern (Eds.), Proceedings of the Twentieth",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 745,
      "page_label": "726"
    }
  },
  {
    "page_content": "C. Meek (2004). ARMA time-series modelling\nwith graphical models. In M. Chickering and\nJ. Halpern (Eds.), Proceedings of the Twentieth\nConference on Uncertainty in Artiﬁcial Intelli-\ngence, Banff, Canada, pp. 552–560. AUAI Press.\nTibshirani, R. (1996). Regression shrinkage and se-\nlection via the lasso. Journal of the Royal Statis-\ntical Society, B 58, 267–288.\nTierney, L. (1994). Markov chains for exploring pos-\nterior distributions. Annals of Statistics 22(4),\n1701–1762.\nTikhonov, A. N. and V . Y . Arsenin (1977).Solutions\nof Ill-Posed Problems. V . H. Winston.\nTino, P. and I. T. Nabney (2002). Hierarchical\nGTM: constructing localized non-linear projec-\ntion manifolds in a principled way. IEEE Trans-\nactions on Pattern Analysis and Machine Intelli-\ngence24(5), 639–656.\nTino, P., I. T. Nabney, and Y . Sun (2001). Us-\ning directional curvatures to visualize folding\npatterns of the GTM projection manifolds. In",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 745,
      "page_label": "726"
    }
  },
  {
    "page_content": "REFERENCES 727\nG. Dorffner, H. Bischof, and K. Hornik (Eds.),\nArtiﬁcial Neural Networks – ICANN 2001, pp.\n421–428. Springer.\nTipping, M. E. (1999). Probabilistic visualisation of\nhigh-dimensional binary data. In M. S. Kearns,\nS. A. Solla, and D. A. Cohn (Eds.),Advances\nin Neural Information Processing Systems, V ol-\nume 11, pp. 592–598. MIT Press.\nTipping, M. E. (2001). Sparse Bayesian learning and\nthe relevance vector machine. Journal of Ma-\nchine Learning Research 1, 211–244.\nTipping, M. E. and C. M. Bishop (1997). Probabilis-\ntic principal component analysis. Technical Re-\nport NCRG/97/010, Neural Computing Research\nGroup, Aston University.\nTipping, M. E. and C. M. Bishop (1999a). Mixtures\nof probabilistic principal component analyzers.\nNeural Computation 11(2), 443–482.\nTipping, M. E. and C. M. Bishop (1999b). Prob-\nabilistic principal component analysis. Journal\nof the Royal Statistical Society, Series B 21(3),\n611–622.\nTipping, M. E. and A. Faul (2003). Fast marginal\nlikelihood maximization for sparse Bayesian\nmodels. In C. M. Bishop and B. Frey (Eds.),\nProceedings Ninth International W orkshop on\nArtiﬁcial Intelligence and Statistics, Key W est,\nFlorida.\nTong, S. and D. Koller (2000). Restricted Bayes op-\ntimal classiﬁers. In Proceedings 17th National\nConference on Artiﬁcial Intelligence, pp. 658–\n664. AAAI.\nTresp, V . (2001). Scaling kernel-based systems to\nlarge data sets.Data Mining and Knowledge Dis-\ncovery 5(3), 197–211.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 746,
      "page_label": "727"
    }
  },
  {
    "page_content": "664. AAAI.\nTresp, V . (2001). Scaling kernel-based systems to\nlarge data sets.Data Mining and Knowledge Dis-\ncovery 5(3), 197–211.\nUhlenbeck, G. E. and L. S. Ornstein (1930). On the\ntheory of Brownian motion. Phys. Rev. 36, 823–\n841.\nValiant, L. G. (1984). A theory of the learnable.\nCommunications of the Association for Comput-\ning Machinery 27, 1134–1142.\nVapnik, V . N. (1982). Estimation of dependences\nbased on empirical data. Springer.\nVapnik, V . N. (1995).The nature of statistical learn-\ning theory. Springer.\nVapnik, V . N. (1998).Statistical learning theory.W i -\nley.\nVeropoulos, K., C. Campbell, and N. Cristianini\n(1999). Controlling the sensitivity of support\nvector machines. InProceedings of the Interna-\ntional Joint Conference on Artiﬁcial Intelligence\n(IJCAI99), W orkshop ML3, pp. 55–60.\nVidakovic, B. (1999). Statistical Modelling by\nW avelets. Wiley.\nViola, P. and M. Jones (2004). Robust real-time face\ndetection. International Journal of Computer Vi-\nsion 57(2), 137–154.\nViterbi, A. J. (1967). Error bounds for convolu-\ntional codes and an asymptotically optimum de-\ncoding algorithm. IEEE Transactions on Infor-\nmation Theory IT-13, 260–267.\nViterbi, A. J. and J. K. Omura (1979). Principles of\nDigital Communication and Coding . McGraw-\nHill.\nWahba, G. (1975). A comparison of GCV and GML\nfor choosing the smoothing parameter in the gen-\neralized spline smoothing problem. Numerical\nMathematics 24, 383–393.\nWainwright, M. J., T. S. Jaakkola, and A. S. Willsky",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 746,
      "page_label": "727"
    }
  },
  {
    "page_content": "eralized spline smoothing problem. Numerical\nMathematics 24, 383–393.\nWainwright, M. J., T. S. Jaakkola, and A. S. Willsky\n(2005). A new class of upper bounds on the log\npartition function. IEEE Transactions on Infor-\nmation Theory 51, 2313–2335.\nWalker, A. M. (1969). On the asymptotic behaviour\nof posterior distributions. Journal of the Royal\nStatistical Society, B31(1), 80–88.\nWalker, S. G., P. Damien, P. W. Laud, and A. F. M.\nSmith (1999). Bayesian nonparametric inference\nfor random distributions and related functions\n(with discussion).Journal of the Royal Statisti-\ncal Society, B61(3), 485–527.\nWatson, G. S. (1964). Smooth regression analysis.\nSankhy¯a: The Indian Journal of Statistics. Series\nA 26, 359–372.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 746,
      "page_label": "727"
    }
  },
  {
    "page_content": "728 REFERENCES\nWebb, A. R. (1994). Functional approximation by\nfeed-forward networks: a least-squares approach\nto generalisation.IEEE Transactions on Neural\nNetworks 5(3), 363–371.\nWeisstein, E. W. (1999).CRC Concise Encyclopedia\nof Mathematics. Chapman and Hall, and CRC.\nWeston, J. and C. Watkins (1999). Multi-class sup-\nport vector machines. In M. Verlysen (Ed.), Pro-\nceedings ESANN’99, Brussels. D-Facto Publica-\ntions.\nWhittaker, J. (1990). Graphical Models in Applied\nMultivariate Statistics. Wiley.\nWidrow, B. and M. E. Hoff (1960). Adaptive\nswitching circuits. In IRE WESCON Convention\nRecord, V olume 4, pp. 96–104. Reprinted in An-\nderson and Rosenfeld (1988).\nWidrow, B. and M. A. Lehr (1990). 30 years of adap-\ntive neural networks: perceptron, madeline, and\nbackpropagation. Proceedings of the IEEE78(9),\n1415–1442.\nWiegerinck, W. and T. Heskes (2003). Fractional\nbelief propagation. In S. Becker, S. Thrun, and\nK. Obermayer (Eds.),Advances in Neural Infor-\nmation Processing Systems, V olume 15, pp. 455–\n462. MIT Press.\nWilliams, C. K. I. (1998). Computation with inﬁ-\nnite neural networks. Neural Computation10(5),\n1203–1216.\nWilliams, C. K. I. (1999). Prediction with Gaussian\nprocesses: from linear regression to linear pre-\ndiction and beyond. In M. I. Jordan (Ed.),Learn-\ning in Graphical Models , pp. 599–621. MIT\nPress.\nWilliams, C. K. I. and D. Barber (1998). Bayesian\nclassiﬁcation with Gaussian processes. IEEE\nTransactions on Pattern Analysis and Machine",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 747,
      "page_label": "728"
    }
  },
  {
    "page_content": "Press.\nWilliams, C. K. I. and D. Barber (1998). Bayesian\nclassiﬁcation with Gaussian processes. IEEE\nTransactions on Pattern Analysis and Machine\nIntelligence 20, 1342–1351.\nWilliams, C. K. I. and M. Seeger (2001). Using the\nNystrom method to speed up kernel machines. In\nT. K. Leen, T. G. Dietterich, and V . Tresp (Eds.),\nAdvances in Neural Information Processing Sys-\ntems, V olume 13, pp. 682–688. MIT Press.\nWilliams, O., A. Blake, and R. Cipolla (2005).\nSparse Bayesian learning for efﬁcient visual\ntracking.IEEE Transactions on Pattern Analysis\nand Machine Intelligence27(8), 1292–1304.\nWilliams, P. M. (1996). Using neural networks to\nmodel conditional multivariate densities. Neural\nComputation 8(4), 843–854.\nWinn, J. and C. M. Bishop (2005). Variational mes-\nsage passing. Journal of Machine Learning Re-\nsearch 6, 661–694.\nZarchan, P. and H. Musoff (2005). Fundamentals of\nKalman Filtering: A Practical Approach (Sec-\nond ed.). AIAA.",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 747,
      "page_label": "728"
    }
  },
  {
    "page_content": "INDEX 729\nIndex\nPage numbers in bold indicate the primary source of information for the corresponding topic.\n1-of-K coding scheme, 424\nacceptance criterion, 538, 541, 544\nactivation function, 180, 213, 227\nactive constraint, 328, 709\nAdaBoost, 657, 658\nadaline, 196\nadaptive rejection sampling, 530\nADF,see assumed density ﬁltering\nAIC, see Akaike information criterion\nAkaike information criterion, 33, 217\nα family of divergences, 469\nα recursion, 620\nancestral sampling, 365, 525, 613\nannular ﬂow, 679\nAR model,see autoregressive model\narc, 360\nARD,see automatic relevance determination\nARMA, see autoregressive moving average\nassumed density ﬁltering, 510\nautoassociative networks, 592\nautomatic relevance determination, 259, 312,349,\n485, 582\nautoregressive hidden Markov model, 632\nautoregressive model, 609\nautoregressive moving average, 304\nback-tracking, 415, 630\nbackgammon, 3\nbackpropagation, 241\nbagging, 656\nbasis function,138, 172, 204, 227\nbatch training, 240\nBaum-Welch algorithm, 618\nBayes’ theorem, 15\nBayes, Thomas, 21\nBayesian analysis, vii, 9,21\nhierarchical, 372\nmodel averaging, 654\nBayesian information criterion, 33, 216\nBayesian model comparison, 161, 473, 483\nBayesian network, 360\nBayesian probability, 21\nbelief propagation, 403\nBernoulli distribution,69, 113, 685\nmixture model, 444\nBernoulli, Jacob, 69\nbeta distribution, 71, 686\nbeta recursion, 621\nbetween-class covariance, 189\nbias,27, 149\nbias parameter, 138, 181, 227, 346\nbias-variance trade-off, 147",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 748,
      "page_label": "729"
    }
  },
  {
    "page_content": "beta recursion, 621\nbetween-class covariance, 189\nbias,27, 149\nbias parameter, 138, 181, 227, 346\nbias-variance trade-off, 147\nBIC,see Bayesian information criterion\nbinary entropy, 495\nbinomial distribution,70, 686",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 748,
      "page_label": "729"
    }
  },
  {
    "page_content": "730 INDEX\nbiological sequence, 610\nbipartite graph, 401\nbits, 49\nblind source separation, 591\nblocked path, 374, 378, 384\nBoltzmann distribution, 387\nBoltzmann, Ludwig Eduard, 53\nBoolean logic, 21\nboosting, 657\nbootstrap,23, 656\nbootstrap ﬁlter, 646\nbox constraints,333, 342\nBox-Muller method, 527\nC4.5, 663\ncalculus of variations, 462\ncanonical correlation analysis, 565\ncanonical link function, 212\nCART,see classiﬁcation and regression trees\nCauchy distribution, 527, 529, 692\ncausality, 366\nCCA,see canonical correlation analysis\ncentral differences, 246\ncentral limit theorem, 78\nchain graph, 393\nchaining, 555\nChapman-Kolmogorov equations, 397\nchild node, 361\nCholesky decomposition, 528\nchunking, 335\ncircular normal,see von Mises distribution\nclassical probability, 21\nclassiﬁcation, 3\nclassiﬁcation and regression trees, 663\nclique, 385\nclustering, 3\nclutter problem, 511\nco-parents,383, 492\ncode-book vectors, 429\ncombining models, 45,653\ncommittee, 655\ncomplete data set, 440\ncompleting the square, 86\ncomputational learning theory, 326, 344\nconcave function, 56\nconcentration parameter, 108, 693\ncondensation algorithm, 646\nconditional entropy, 55\nconditional expectation, 20\nconditional independence, 46, 372, 383\nconditional mixture model, see mixture model\nconditional probability, 14\nconjugate prior, 68, 98, 117, 490\nconvex duality, 494\nconvex function, 55, 493\nconvolutional neural network, 267\ncorrelation matrix, 567\ncost function, 41\ncovariance, 20\nbetween-class, 189",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 749,
      "page_label": "730"
    }
  },
  {
    "page_content": "convex function, 55, 493\nconvolutional neural network, 267\ncorrelation matrix, 567\ncost function, 41\ncovariance, 20\nbetween-class, 189\nwithin-class, 189\ncovariance matrix\ndiagonal, 84\nisotropic, 84\npartitioned, 85, 307\npositive deﬁnite, 308\nCox’s axioms, 21\ncredit assignment, 3\ncross-entropy error function, 206, 209, 235, 631,\n666\ncross-validation, 32, 161\ncumulative distribution function, 18\ncurse of dimensionality, 33,3 6\ncurve ﬁtting, 4\nD map, see dependency map\nd-separation, 373, 378, 443\nDAG, see directed acyclic graph\nDAGSVM, 339\ndata augmentation, 537\ndata compression, 429\ndecision boundary, 39, 179\ndecision region, 39, 179\ndecision surface, see decision boundary\ndecision theory, 38\ndecision tree, 654, 663, 673\ndecomposition methods, 335\ndegrees of freedom, 559\ndegrees-of-freedom parameter, 102, 693\ndensity estimation, 3, 67",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 749,
      "page_label": "730"
    }
  },
  {
    "page_content": "INDEX 731\ndensity network, 597\ndependency map, 392\ndescendant node, 376\ndesign matrix, 142, 347\ndifferential entropy, 53\ndigamma function, 687\ndirected acyclic graph, 362\ndirected cycle, 362\ndirected factorization, 381\nDirichlet distribution,76, 687\nDirichlet, Lejeune, 77\ndiscriminant function, 43, 180,181\ndiscriminative model, 43, 203\ndistortion measure, 424\ndistributive law of multiplication, 396\nDNA, 610\ndocument retrieval, 299\ndual representation,293, 329\ndual-energy gamma densitometry, 678\ndynamic programming, 411\ndynamical system, 548\nE step, see expectation step\nearly stopping, 259\nECM,see expectation conditional maximization\nedge, 360\neffective number of observations,72, 101\neffective number of parameters, 9, 170, 281\nelliptical K-means, 444\nEM, see expectation maximization\nemission probability, 611\nempirical Bayes,see evidence approximation\nenergy function, 387\nentropy, 49\nconditional, 55\ndifferential, 53\nrelative, 55\nEP, see expectation propagation\nϵ-tube, 341\nϵ-insensitive error function, 340\nequality constraint, 709\nequivalent kernel,159, 301\nerf function, 211\nerror backpropagation,see backpropagation\nerror function, 5,2 3\nerror-correcting output codes, 339\nEuler, Leonhard, 465\nEuler-Lagrange equations, 705\nevidence approximation, 165, 347, 581\nevidence function, 161\nexpectation, 19\nexpectation conditional maximization, 454\nexpectation maximization, 113,423, 440\nGaussian mixture, 435\ngeneralized, 454\nsampling methods, 536\nexpectation propagation, 315, 468, 505",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 750,
      "page_label": "731"
    }
  },
  {
    "page_content": "expectation maximization, 113,423, 440\nGaussian mixture, 435\ngeneralized, 454\nsampling methods, 536\nexpectation propagation, 315, 468, 505\nexpectation step, 437\nexplaining away, 378\nexploitation, 3\nexploration, 3\nexponential distribution,526, 688\nexponential family, 68, 113, 202, 490\nextensive variables, 490\nface detection, 2\nface tracking, 355\nfactor analysis, 583\nmixture model, 595\nfactor graph, 360, 399, 625\nfactor loading, 584\nfactorial hidden Markov model, 633\nfactorized distribution,464, 476\nfeature extraction, 2\nfeature map, 268\nfeature space,292, 586\nFisher information matrix, 298\nFisher kernel, 298\nFisher’s linear discriminant, 186\nﬂooding schedule, 417\nforward kinematics, 272\nforward problem, 272\nforward propagation,228, 243\nforward-backward algorithm, 618\nfractional belief propagation, 517\nfrequentist probability, 21\nfuel system, 376\nfunction interpolation, 299\nfunctional, 462,703\nderivative, 463",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 750,
      "page_label": "731"
    }
  },
  {
    "page_content": "732 INDEX\ngamma densitometry, 678\ngamma distribution, 529, 688\ngamma function, 71\ngating function, 672\nGauss, Carl Friedrich, 79\nGaussian, 24, 78, 688\nconditional, 85,9 3\nmarginal, 88,9 3\nmaximum likelihood, 93\nmixture, 110, 270, 273, 430\nsequential estimation, 94\nsufﬁcient statistics, 93\nwrapped, 110\nGaussian kernel, 296\nGaussian process, 160, 303\nGaussian random ﬁeld, 305\nGaussian-gamma distribution,101, 690\nGaussian-Wishart distribution, 102, 475, 478, 690\nGEM, see expectation maximization, generalized\ngeneralization, 2\ngeneralized linear model,180, 213\ngeneralized maximum likelihood, see evidence ap-\nproximation\ngenerative model, 43, 196, 297, 365, 572, 631\ngenerative topographic mapping, 597\ndirectional curvature, 599\nmagniﬁcation factor, 599\ngeodesic distance, 596\nGibbs sampling, 542\nblocking, 546\nGibbs, Josiah Willard, 543\nGini index, 666\nglobal minimum, 237\ngradient descent, 240\nGram matrix, 293\ngraph-cut algorithm, 390\ngraphical model, 359\nbipartite, 401\ndirected, 360\nfactorization, 362, 384\nfully connected, 361\ninference, 393\ntree, 398\ntreewidth, 417\ntriangulated, 416\nundirected, 360\nGreen’s function, 299\nGTM,see generative topographic mapping\nHamilton, William Rowan, 549\nHamiltonian dynamics, 548\nHamiltonian function, 549\nHammersley-Clifford theorem, 387\nhandwriting recognition, 1, 610, 614\nhandwritten digit, 565, 614,677\nhead-to-head path, 376\nhead-to-tail path, 375\nHeaviside step function, 206\nHellinger distance, 470\nHessian matrix, 167, 215, 217, 238,249",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 751,
      "page_label": "732"
    }
  },
  {
    "page_content": "head-to-head path, 376\nhead-to-tail path, 375\nHeaviside step function, 206\nHellinger distance, 470\nHessian matrix, 167, 215, 217, 238,249\ndiagonal approximation, 250\nexact evaluation, 253\nfast multiplication, 254\nﬁnite differences, 252\ninverse, 252\nouter product approximation, 251\nheteroscedastic, 273, 311\nhidden Markov model, 297, 610\nautoregressive, 632\nfactorial, 633\nforward-backward algorithm, 618\ninput-output, 633\nleft-to-right, 613\nmaximum likelihood, 615\nscaling factor, 627\nsum-product algorithm, 625\nswitching, 644\nvariational inference, 625\nhidden unit, 227\nhidden variable, 84,364, 430, 559\nhierarchical Bayesian model, 372\nhierarchical mixture of experts, 673\nhinge error function, 337\nHinton diagram, 584\nhistogram density estimation, 120\nHME,see hierarchical mixture of experts\nhold-out set, 11\nhomogeneous ﬂow, 679\nhomogeneous kernel, 292\nhomogeneous Markov chain,540, 608",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 751,
      "page_label": "732"
    }
  },
  {
    "page_content": "INDEX 733\nHooke’s law, 580\nhybrid Monte Carlo, 548\nhyperparameter,71, 280, 311, 346, 372, 502\nhyperprior, 372\nI map, see independence map\ni.i.d., see independent identically distributed\nICA, see independent component analysis\nICM, see iterated conditional modes\nID3, 663\nidentiﬁability, 435\nimage de-noising, 387\nimportance sampling, 525,532\nimportance weights, 533\nimproper prior,118, 259, 472\nimputation step, 537\nimputation-posterior algorithm, 537\ninactive constraint, 328, 709\nincomplete data set, 440\nindependence map, 392\nindependent component analysis, 591\nindependent factor analysis, 592\nindependent identically distributed,26, 379\nindependent variables, 17\nindependent, identically distributed, 605\ninduced factorization, 485\ninequality constraint, 709\ninference, 38,42\ninformation criterion, 33\ninformation geometry, 298\ninformation theory, 48\ninput-output hidden Markov model, 633\nintensive variables, 490\nintrinsic dimensionality, 559\ninvariance, 261\ninverse gamma distribution, 101\ninverse kinematics, 272\ninverse problem, 272\ninverse Wishart distribution, 102\nIP algorithm,see imputation-posterior algorithm\nIRLS, see iterative reweighted least squares\nIsing model, 389\nisomap, 596\nisometric feature map, 596\niterated conditional modes,389, 415\niterative reweighted least squares, 207, 210, 316,\n354, 672\nJacobian matrix, 247, 264\nJensen’s inequality, 56\njoin tree, 416\njunction tree algorithm, 392, 416\nK nearest neighbours, 125\nK-means clustering algorithm, 424, 443",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 752,
      "page_label": "733"
    }
  },
  {
    "page_content": "Jensen’s inequality, 56\njoin tree, 416\njunction tree algorithm, 392, 416\nK nearest neighbours, 125\nK-means clustering algorithm, 424, 443\nK-medoids algorithm, 428\nKalman ﬁlter, 304, 637\nextended, 644\nKalman gain matrix, 639\nKalman smoother, 637\nKarhunen-Lo`eve transform, 561\nKarush-Kuhn-Tucker conditions, 330, 333, 342,\n710\nkernel density estimator, 122, 326\nkernel function, 123, 292, 294\nFisher, 298\nGaussian, 296\nhomogeneous, 292\nnonvectorial inputs, 297\nstationary, 292\nkernel PCA, 586\nkernel regression, 300, 302\nkernel substitution, 292\nkernel trick, 292\nkinetic energy, 549\nKKT,see Karush-Kuhn-Tucker conditions\nKL divergence, see Kullback-Leibler divergence\nkriging, see Gaussian process\nKullback-Leibler divergence, 55, 451, 468, 505\nLagrange multiplier, 707\nLagrange, Joseph-Louis, 329\nLagrangian, 328, 332, 341, 708\nlaminar ﬂow, 678\nLaplace approximation, 213, 217, 278, 315, 354\nLaplace, Pierre-Simon, 24\nlarge margin, see margin\nlasso, 145\nlatent class analysis, 444\nlatent trait model, 597\nlatent variable, 84, 364, 430, 559",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 752,
      "page_label": "733"
    }
  },
  {
    "page_content": "734 INDEX\nlattice diagram, 414, 611, 621, 629\nLDS, see linear dynamical system\nleapfrog discretization, 551\nlearning, 2\nlearning rate parameter, 240\nleast-mean-squares algorithm, 144\nleave-one-out, 33\nlikelihood function, 22\nlikelihood weighted sampling, 534\nlinear discriminant, 181\nFisher, 186\nlinear dynamical system, 84, 635\ninference, 638\nlinear independence, 696\nlinear regression, 138\nEM, 448\nmixture model, 667\nvariational, 486\nlinear smoother, 159\nlinear-Gaussian model, 87,370\nlinearly separable, 179\nlink, 360\nlink function, 180, 213\nLiouville’s Theorem, 550\nLLE, see locally linear embedding\nLMS algorithm, see least-mean-squares algorithm\nlocal minimum, 237\nlocal receptive ﬁeld, 268\nlocally linear embedding, 596\nlocation parameter, 118\nlog odds, 197\nlogic sampling, 525\nlogistic regression, 205, 336\nBayesian, 217, 498\nmixture model, 670\nmulticlass, 209\nlogistic sigmoid, 114, 139, 197, 205, 220, 227, 495\nlogit function, 197\nloopy belief propagation, 417\nloss function, 41\nloss matrix, 41\nlossless data compression, 429\nlossy data compression, 429\nlower bound, 484\nM step, see maximization step\nmachine learning, vii\nmacrostate, 51\nMahalanobis distance, 80\nmanifold,38, 590, 595, 681\nMAP, see maximum posterior\nmargin, 326, 327, 502\nerror, 334\nsoft, 332\nmarginal likelihood, 162, 165\nmarginal probability, 14\nMarkov blanket, 382, 384, 545\nMarkov boundary, see Markov blanket\nMarkov chain, 397, 539\nﬁrst order, 607\nhomogeneous, 540, 608\nsecond order, 608\nMarkov chain Monte Carlo, 537",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 753,
      "page_label": "734"
    }
  },
  {
    "page_content": "Markov boundary, see Markov blanket\nMarkov chain, 397, 539\nﬁrst order, 607\nhomogeneous, 540, 608\nsecond order, 608\nMarkov chain Monte Carlo, 537\nMarkov model, 607\nhomogeneous, 612\nMarkov network, see Markov random ﬁeld\nMarkov random ﬁeld, 84, 360, 383\nmax-sum algorithm, 411, 629\nmaximal clique, 385\nmaximal spanning tree, 416\nmaximization step, 437\nmaximum likelihood, 9,23, 26, 116\nGaussian mixture, 432\nsingularities, 480\ntype 2,see evidence approximation\nmaximum margin, see margin\nmaximum posterior, 30, 441\nMCMC, see Markov chain Monte Carlo\nMDN, see mixture density network\nMDS, see multidimensional scaling\nmean, 24\nmean ﬁeld theory, 465\nmean value theorem, 52\nmeasure theory, 19\nmemory-based methods, 292\nmessage passing, 396\npending message, 417\nschedule, 417\nvariational, 491\nMetropolis algorithm, 538\nMetropolis-Hastings algorithm, 541",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 753,
      "page_label": "734"
    }
  },
  {
    "page_content": "INDEX 735\nmicrostate, 51\nminimum risk, 44\nMinkowski loss, 48\nmissing at random, 441, 579\nmissing data, 579\nmixing coefﬁcient, 111\nmixture component, 111\nmixture density network,272, 673\nmixture distribution, see mixture model\nmixture model, 162, 423\nconditional, 273, 666\nlinear regression, 667\nlogistic regression, 670\nsymmetries, 483\nmixture of experts, 672\nmixture of Gaussians, 110, 270, 273,430\nMLP, see multilayer perceptron\nMNIST data, 677\nmodel comparison, 6, 32, 161, 473, 483\nmodel evidence, 161\nmodel selection, 162\nmoment matching,506, 510\nmomentum variable, 548\nMonte Carlo EM algorithm, 536\nMonte Carlo sampling, 24,523\nMoore-Penrose pseudo-inverse, see pseudo-inverse\nmoralization, 391, 401\nMRF, see Markov random ﬁeld\nmultidimensional scaling, 596\nmultilayer perceptron, 226,229\nmultimodality, 272\nmultinomial distribution, 76, 114,690\nmultiplicity, 51\nmutual information, 55, 57\nNadaraya-Watson, see kernel regression\nnaive Bayes model, 46, 380\nnats, 50\nnatural language modelling, 610\nnatural parameters, 113\nnearest-neighbour methods, 124\nneural network, 225\nconvolutional, 267\nregularization, 256\nrelation to Gaussian process, 319\nNewton-Raphson, 207, 317\nnode, 360\nnoiseless coding theorem, 50\nnonidentiﬁability, 585\nnoninformative prior, 23, 117\nnonparametric methods, 68, 120\nnormal distribution, see Gaussian\nnormal equations, 142\nnormal-gamma distribution,101, 691\nnormal-Wishart distribution, 102, 475, 478, 691\nnormalized exponential, see softmax function",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 754,
      "page_label": "735"
    }
  },
  {
    "page_content": "normal equations, 142\nnormal-gamma distribution,101, 691\nnormal-Wishart distribution, 102, 475, 478, 691\nnormalized exponential, see softmax function\nnovelty detection, 44\nν-SVM, 334\nobject recognition, 366\nobserved variable, 364\nOccam factor, 217\noil ﬂow data, 34, 560, 568, 678\nOld Faithful data, 110, 479, 484, 681\non-line learning, see sequential learning\none-versus-one classiﬁer, 183, 339\none-versus-the-rest classiﬁer, 182, 338\nordered over-relaxation, 545\nOrnstein-Uhlenbeck process, 305\northogonal least squares, 301\noutlier, 44, 185, 212\noutliers, 103\nover-ﬁtting, 6, 147, 434, 464\nover-relaxation, 544\nPAC learning,see probably approximately correct\nPAC-Bayesian framework, 345\nparameter shrinkage, 144\nparent node, 361\nparticle ﬁlter, 645\npartition function, 386, 554\nParzen estimator, see kernel density estimator\nParzen window, 123\npattern recognition, vii\nPCA, see principal component analysis\npending message, 417\nperceptron, 192\nconvergence theorem, 194\nhardware, 196\nperceptron criterion, 193\nperfect map, 392",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 754,
      "page_label": "735"
    }
  },
  {
    "page_content": "736 INDEX\nperiodic variable, 105\nphase space, 549\nphoton noise, 680\nplate, 363\npolynomial curve ﬁtting, 4, 362\npolytree, 399\nposition variable, 548\npositive deﬁnite covariance, 81\npositive deﬁnite matrix, 701\npositive semideﬁnite covariance, 81\npositive semideﬁnite matrix, 701\nposterior probability, 17\nposterior step, 537\npotential energy, 549\npotential function, 386\npower EP, 517\npower method, 563\nprecision matrix, 85\nprecision parameter, 24\npredictive distribution,30, 156\npreprocessing, 2\nprincipal component analysis,561, 572, 593\nBayesian, 580\nEM algorithm, 577\nGibbs sampling, 583\nmixture distribution, 595\nphysical analogy, 580\nprincipal curve, 595\nprincipal subspace, 561\nprincipal surface, 596\nprior, 17\nconjugate, 68, 98, 117, 490\nconsistent, 257\nimproper,118, 259, 472\nnoninformative, 23, 117\nprobabilistic graphical model, see graphical model\nprobabilistic PCA, 570\nprobability, 12\nBayesian, 21\nclassical, 21\ndensity, 17\nfrequentist, 21\nmass function, 19\nprior, 45\nproduct rule, 13, 14, 359\nsum rule, 13, 14, 359\ntheory, 12\nprobably approximately correct, 344\nprobit function, 211, 219\nprobit regression, 210\nproduct rule of probability, 13,14, 359\nproposal distribution, 528, 532, 538\nprotected conjugate gradients, 335\nprotein sequence, 610\npseudo-inverse, 142, 185\npseudo-random numbers, 526\nquadratic discriminant, 199\nquality parameter, 351\nradial basis function, 292, 299\nRauch-Tung-Striebel equations, 637\nregression, 3\nregression function,47,9 5\nregularization, 10",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 755,
      "page_label": "736"
    }
  },
  {
    "page_content": "quality parameter, 351\nradial basis function, 292, 299\nRauch-Tung-Striebel equations, 637\nregression, 3\nregression function,47,9 5\nregularization, 10\nTikhonov, 267\nregularized least squares, 144\nreinforcement learning, 3\nreject option, 42,4 5\nrejection sampling, 528\nrelative entropy, 55\nrelevance vector, 348\nrelevance vector machine, 161, 345\nresponsibility, 112, 432, 477\nridge regression, 10\nRMS error, see root-mean-square error\nRobbins-Monro algorithm, 95\nrobot arm, 272\nrobustness, 103, 185\nroot node, 399\nroot-mean-square error, 6\nRosenblatt, Frank, 193\nrotation invariance, 573, 585\nRTS equations, see Rauch-Tung-Striebel equations\nrunning intersection property, 416\nRVM,see relevance vector machine\nsample mean, 27\nsample variance, 27\nsampling-importance-resampling, 534\nscale invariance, 119, 261",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 755,
      "page_label": "736"
    }
  },
  {
    "page_content": "INDEX 737\nscale parameter, 119\nscaling factor, 627\nSchwarz criterion,see Bayesian information crite-\nrion\nself-organizing map, 598\nsequential data, 605\nsequential estimation, 94\nsequential gradient descent, 144, 240\nsequential learning,73, 143\nsequential minimal optimization, 335\nserial message passing schedule, 417\nShannon, Claude, 55\nshared parameters, 368\nshrinkage, 10\nShur complement, 87\nsigmoid,see logistic sigmoid\nsimplex, 76\nsingle-class support vector machine, 339\nsingular value decomposition, 143\nsinusoidal data, 682\nSIR,see sampling-importance-resampling\nskip-layer connection, 229\nslack variable, 331\nslice sampling, 546\nSMO,see sequential minimal optimization\nsmoother matrix, 159\nsmoothing parameter, 122\nsoft margin, 332\nsoft weight sharing, 269\nsoftmax function, 115, 198, 236, 274, 356, 497\nSOM, see self-organizing map\nsparsity, 145, 347, 349, 582\nsparsity parameter, 351\nspectrogram, 606\nspeech recognition,605, 610\nsphereing, 568\nspline functions, 139\nstandard deviation, 24\nstandardizing, 425,567\nstate space model, 609\nswitching, 644\nstationary kernel, 292\nstatistical bias,see bias\nstatistical independence, see independent variables\nstatistical learning theory, see computational learn-\ning theory, 326, 344\nsteepest descent, 240\nStirling’s approximation, 51\nstochastic, 5\nstochastic EM, 536\nstochastic gradient descent, 144, 240\nstochastic process, 305\nstratiﬁed ﬂow, 678\nStudent’s t-distribution, 102, 483, 691\nsubsampling, 268\nsufﬁcient statistics, 69, 75, 116",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 756,
      "page_label": "737"
    }
  },
  {
    "page_content": "stochastic process, 305\nstratiﬁed ﬂow, 678\nStudent’s t-distribution, 102, 483, 691\nsubsampling, 268\nsufﬁcient statistics, 69, 75, 116\nsum rule of probability, 13, 14, 359\nsum-of-squares error, 5, 29, 184, 232, 662\nsum-product algorithm, 399, 402\nfor hidden Markov model, 625\nsupervised learning, 3\nsupport vector, 330\nsupport vector machine, 225\nfor regression, 339\nmulticlass, 338\nsurvival of the ﬁttest, 646\nSVD, see singular value decomposition\nSVM, see support vector machine\nswitching hidden Markov model, 644\nswitching state space model, 644\nsynthetic data sets, 682\ntail-to-tail path, 374\ntangent distance, 265\ntangent propagation, 262, 263\ntapped delay line, 609\ntarget vector, 2\ntest set, 2, 32\nthreshold parameter, 181\ntied parameters, 368\nTikhonov regularization, 267\ntime warping, 615\ntomography, 679\ntraining, 2\ntraining set, 2\ntransition probability, 540, 610\ntranslation invariance, 118, 261\ntree-reweighted message passing, 517\ntreewidth, 417",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 756,
      "page_label": "737"
    }
  },
  {
    "page_content": "738 INDEX\ntrellis diagram, see lattice diagram\ntriangulated graph, 416\ntype 2 maximum likelihood,see evidence approxi-\nmation\nundetermined multiplier, see Lagrange multiplier\nundirected graph, see Markov random ﬁeld\nuniform distribution, 692\nuniform sampling, 534\nuniquenesses, 584\nunobserved variable, see latent variable\nunsupervised learning, 3\nutility function, 41\nvalidation set, 11, 32\nVapnik-Chervonenkis dimension, 344\nvariance, 20, 24, 149\nvariational inference, 315, 462, 635\nfor Gaussian mixture, 474\nfor hidden Markov model, 625\nlocal, 493\nVC dimension, see Vapnik-Chervonenkis dimen-\nsion\nvector quantization, 429\nvertex,see node\nvisualization, 3\nViterbi algorithm, 415, 629\nvon Mises distribution, 108, 693\nwavelets, 139\nweak learner, 657\nweight decay, 10,144, 257\nweight parameter, 227\nweight sharing, 268\nsoft, 269\nweight vector, 181\nweight-space symmetry, 231, 281\nweighted least squares, 668\nwell-determined parameters, 170\nwhitening, 299,568\nWishart distribution, 102, 693\nwithin-class covariance, 189\nWoodbury identity, 696\nwrapped distribution, 110\nYellowstone National Park, 110,681",
    "metadata": {
      "producer": "Acrobat Distiller 6.0 (Windows)",
      "creator": "Adobe Acrobat 6.0",
      "creationdate": "2006-10-18T12:52:36+08:00",
      "author": "Christopher M. Bishop",
      "moddate": "2008-02-08T16:41:33+01:00",
      "title": "Pattern Recognition and Machine Learning",
      "source": "_backend/data/pdfs\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf",
      "total_pages": 758,
      "page": 757,
      "page_label": "738"
    }
  }
]